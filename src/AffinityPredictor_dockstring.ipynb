{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "from tqdm.auto import tqdm\n",
    "\n",
    "from build_corpus import build_corpus\n",
    "from build_vocab import WordVocab\n",
    "from utils import split\n",
    "\n",
    "import torch\n",
    "from torch import nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "\n",
    "import rdkit\n",
    "import rdkit.Chem as Chem\n",
    "from rdkit.Chem import AllChem\n",
    "from rdkit.Chem import Draw\n",
    "\n",
    "import wandb\n",
    "from rdkit.Chem import Draw\n",
    "from rdkit.Chem import MolFromSmiles\n",
    "\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('../checkpoints/LCK_DOCKSTRING_FAST_affinity_logps_qeds_sas_tpsas/sampled_mols.txt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.display import display"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total valid SMILES processed: 111\n"
     ]
    }
   ],
   "source": [
    "count=0\n",
    "for smile in df['SMILES']:\n",
    "    try:\n",
    "        mol = Chem.MolFromSmiles(smile)\n",
    "        if mol is not None:\n",
    "            # img = Draw.MolToImage(mol, size=(300, 300))\n",
    "            count+=1\n",
    "            # display(img)\n",
    "    except Exception as e:\n",
    "        print(f\"Error processing SMILES {smile}: {e}\")\n",
    "print(f\"Total valid SMILES processed: {count}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('../data/lck_dockstring_data.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sklearn\n",
    "# minmaxscaler\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "# scaler = MinMaxScaler()\n",
    "# df['qeds'] = scaler.fit_transform(df['qeds'].values.reshape(-1,1))\n",
    "# df['tpsas'] = scaler.fit_transform(df['tpsas'].values.reshape(-1,1))\n",
    "# df['logps'] = scaler.fit_transform(df['logps'].values.reshape(-1,1))\n",
    "# df['affinity'] = scaler.fit_transform(df['affinity'].values.reshape(-1,1))\n",
    "\n",
    "affinity_scaler = MinMaxScaler()\n",
    "qed_scaler = MinMaxScaler()\n",
    "logp_scaler = MinMaxScaler()\n",
    "tpsas_scaler = MinMaxScaler()\n",
    "\n",
    "affinity_scaler.fit(df['affinity'].values.reshape(-1,1))\n",
    "qed_scaler.fit(df['qeds'].values.reshape(-1,1))\n",
    "logp_scaler.fit(df['logps'].values.reshape(-1,1))\n",
    "tpsas_scaler.fit(df['tpsas'].values.reshape(-1,1))\n",
    "\n",
    "df['qeds'] = qed_scaler.transform(df['qeds'].values.reshape(-1,1))\n",
    "df['logps'] = logp_scaler.transform(df['logps'].values.reshape(-1,1))\n",
    "df['tpsas'] = tpsas_scaler.transform(df['tpsas'].values.reshape(-1,1))\n",
    "df['affinity'] = affinity_scaler.transform(df['affinity'].values.reshape(-1,1))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SMI_MAX_SIZE  254\n"
     ]
    }
   ],
   "source": [
    "SMI_MAX_SIZE= -1\n",
    "with open('../data/smiles_corpus.txt', 'w') as f:\n",
    "    train = []\n",
    "    test = []        \n",
    "    for i, row in df.iterrows():\n",
    "        if row['split'] == \"test\":\n",
    "            test.append(list(row.values))\n",
    "        else:\n",
    "            train.append(list(row.values))\n",
    "        f.write(split(row['smiles'] +'\\n'))\n",
    "        \n",
    "        if SMI_MAX_SIZE < len(row['smiles']):\n",
    "            SMI_MAX_SIZE = len(row['smiles'])\n",
    "f.close()\n",
    "print(\"SMI_MAX_SIZE \", SMI_MAX_SIZE)\n",
    "train_df = pd.DataFrame(train, columns=df.columns)\n",
    "test_df = pd.DataFrame(test, columns=df.columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Building Vocab\n"
     ]
    }
   ],
   "source": [
    "SMI_MAX_SIZE = 300\n",
    "SMI_MIN_FREQ=1\n",
    "with open(\"../data/smiles_corpus.txt\", \"r\") as f:\n",
    "    smiles_vocab = WordVocab(f, max_size=SMI_MAX_SIZE, min_freq=SMI_MIN_FREQ)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "class CustomTargetDataset(Dataset):\n",
    "    def __init__(self, dataframe, SmilesVocab, properties_list):\n",
    "        self.dataframe = dataframe\n",
    "        self.smiles_vocab = SmilesVocab\n",
    "        self.property_list = properties_list\n",
    "        self.build()\n",
    "        \n",
    "    def build(self):\n",
    "        smiles, properties, affinities= [],[],[]\n",
    "        smiles_encoding = []\n",
    "        for i, row in self.dataframe.iterrows():\n",
    "            smi = row['smiles']\n",
    "            # newsmi = Chem.MolToSmiles(Chem.MolFromSmiles(smi))\n",
    "            newsmi = smi\n",
    "            smiles.append(newsmi)\n",
    "            smiles_encoding.append(self.smiles_vocab.to_seq(split(newsmi), seq_len=SMI_MAX_SIZE))\n",
    "            props = []\n",
    "            for p in self.property_list:\n",
    "                props.append(row[p])\n",
    "            properties.append(props)\n",
    "\n",
    "        self.smiles_encodings = torch.tensor(smiles_encoding)\n",
    "        self.properties = torch.tensor(properties)\n",
    "        # self.affinities = torch.tensor(affinities)\n",
    "        print(\"dataset built\")\n",
    "        \n",
    "    def __len__(self):\n",
    "        return len(self.properties)\n",
    "    \n",
    "    def __getitem__(self, index):\n",
    "        return {\n",
    "            \"smiles_rep\": self.smiles_encodings[index],\n",
    "            \"properties\": self.properties[index],\n",
    "        }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dataset built\n"
     ]
    }
   ],
   "source": [
    "dataset = CustomTargetDataset(train_df, smiles_vocab, ['affinity'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataloader = DataLoader(dataset, batch_size=32, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = next(iter(dataloader))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "class PositionalEncodings(nn.Module):\n",
    "    \"\"\"Attention is All You Need positional encoding layer\"\"\"\n",
    "\n",
    "    def __init__(self, seq_len, d_model, p_dropout,n=10000):\n",
    "        \"\"\"Initializes the layer.\"\"\"\n",
    "        super(PositionalEncodings, self).__init__()\n",
    "        token_positions = torch.arange(start=0, end=seq_len).view(-1, 1)\n",
    "        dim_positions = torch.arange(start=0, end=d_model).view(1, -1)\n",
    "        angles = token_positions / (n ** ((2 * dim_positions) / d_model))\n",
    "\n",
    "        encodings = torch.zeros(1, seq_len, d_model)\n",
    "        encodings[0, :, ::2] = torch.cos(angles[:, ::2])\n",
    "        encodings[0, :, 1::2] = torch.sin(angles[:, 1::2])\n",
    "        encodings.requires_grad = False\n",
    "        self.register_buffer(\"positional_encodings\", encodings)\n",
    "\n",
    "        self.dropout = nn.Dropout(p_dropout)\n",
    "\n",
    "    def forward(self, x):\n",
    "        \"\"\"Performs forward pass of the module.\"\"\"\n",
    "        x = x + self.positional_encodings[:,:x.shape[1],:]\n",
    "        x = self.dropout(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "class PropertyEncoder(nn.Module):\n",
    "    def __init__(self, d_model, n_properties):\n",
    "        super(PropertyEncoder, self).__init__()\n",
    "        self.layer = nn.Linear(n_properties, d_model)\n",
    "        self.layer_final = nn.Linear(d_model, d_model)\n",
    "    def forward(self, x):\n",
    "        return self.layer_final(self.layer(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def set_up_causal_mask(seq_len):\n",
    "    mask = (torch.triu(torch.ones(seq_len, seq_len)) == 1).transpose(0, 1)\n",
    "    mask = mask.float().masked_fill(mask == 0, float('-inf')).masked_fill(mask == 1, float(0.0))\n",
    "    mask.requires_grad = False\n",
    "    return mask"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SmileDecoder(nn.Module):\n",
    "    def __init__(self, d_model, n_heads, n_layers, vocab, n_properties, hidden_units=1024, dropout=0.1):\n",
    "        super(SmileDecoder, self).__init__()\n",
    "        self.d_model = d_model\n",
    "        self.n_heads = n_heads\n",
    "        self.n_layers = n_layers\n",
    "        self.vocab = vocab\n",
    "        self.dropout = dropout\n",
    "        \n",
    "        self.embed = nn.Embedding(len(vocab), d_model)\n",
    "        self.smile_pe = PositionalEncodings(SMI_MAX_SIZE, d_model, dropout)\n",
    "        \n",
    "        self.trfmLayer = nn.TransformerEncoderLayer(d_model=d_model,\n",
    "                                                    nhead=n_heads,\n",
    "                                                    dim_feedforward=hidden_units,\n",
    "                                                    dropout=dropout,\n",
    "                                                    batch_first=True,\n",
    "                                                    norm_first=True,\n",
    "                                                    activation=\"gelu\")\n",
    "        self.trfm = nn.TransformerEncoder(encoder_layer=self.trfmLayer,\n",
    "                                          num_layers=n_layers,\n",
    "                                          norm=nn.LayerNorm(d_model))\n",
    "        self.ln_f = nn.LayerNorm(d_model)\n",
    "        self.final = nn.Linear(d_model, 1)\n",
    "        self.property_encoder = PropertyEncoder(d_model,n_properties=n_properties)\n",
    "        \n",
    "    def forward(self, x, property):\n",
    "        # property = self.property_encoder(property).unsqueeze(1)\n",
    "        \n",
    "        x = self.embed(x)\n",
    "        x = self.smile_pe(x)\n",
    "        \n",
    "        # x = torch.cat([property, x], dim=1).to(x.device)\n",
    "        \n",
    "        # mask = set_up_causal_mask(x.shape[1]).to(device)\n",
    "        x = self.trfm(src=x,\n",
    "                      mask=None,\n",
    "                      )\n",
    "        x = self.ln_f(x)\n",
    "        x = self.final(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home2/bhuvan.kapur/.local/lib/python3.10/site-packages/torch/nn/modules/transformer.py:382: UserWarning: enable_nested_tensor is True, but self.use_nested_tensor is False because encoder_layer.norm_first was True\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "net = SmileDecoder(d_model=512, n_heads=8, n_layers=6, vocab=smiles_vocab, n_properties=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "net.to(device)\n",
    "out = net(data['smiles_rep'].to(device), data['properties'].to(device))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([32, 300, 1])"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mThe Kernel crashed while executing code in the current cell or a previous cell. \n",
      "\u001b[1;31mPlease review the code in the cell(s) to identify a possible cause of the failure. \n",
      "\u001b[1;31mClick <a href='https://aka.ms/vscodeJupyterKernelCrash'>here</a> for more info. \n",
      "\u001b[1;31mView Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": [
    "out.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
