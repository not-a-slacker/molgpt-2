Starting ...
cuda
Model properties:  ['affinity']
Preference properties:  ['affinity']
Building Vocab
{'batch_size': 128, 'd_model': 512, 'n_heads': 8, 'n_layers': 6, 'hidden_units': 512, 'lr': 1e-05, 'epochs': 265, 'properties': ['affinity', 'logps', 'qeds', 'sas', 'tpsas'], 'model_properties': ['affinity'], 'run_name': 'LCK_DOCKSTRING_FAST_ACTUAL_affinity'}
length of preference data : 41795
Sample preference_data[0]: ['O1C(C(=O)N2CCN(CC2)C3=CC=C(N(=O)=O)C=C3)=C(C=4C1=CC=CC4)C', array([0.47959185, 0.53255087, 0.54797947, 0.15855041, 0.16055913]), [np.str_('O1C(=NC2=C(C1=O)C=CC=C2)C3=C(C(=O)NCCC4=CC=CC=C4)C=CC(OC)=C3'), np.float64(0.9873459430403937), array([0.52040816, 0.54211667, 0.55725834, 0.15815913, 0.16377715])], [np.str_('O1C(=NC2=C(C1=O)C=CC=C2)C3=CC(OC(=O)C=4C=CC=CC4)=C(OC)C=C3'), np.float64(0.9574398041990725), array([0.43877551, 0.55263869, 0.41284505, 0.14420153, 0.15814562])]]
LCK_DOCKSTRING_FAST_ACTUAL_affinity
len(target_smiles): 41800
len(data): 41800
LCK_DOCKSTRING_FAST_ACTUAL_affinity
Loading affinity-only preference data from: ../checkpoints/LCK_DOCKSTRING_FAST_ACTUAL_affinity_logps_qeds_sas_tpsas/PreferenceData_affinity.pkl
dataset built
cuda
True
2.7.1+cu118
No of GPUs available 4
No of GPUs available 4
{'batch_size': 128, 'd_model': 512, 'n_heads': 8, 'n_layers': 6, 'hidden_units': 512, 'lr': 1e-05, 'epochs': 10, 'model_properties': ['affinity'], 'preference_properties': ['affinity'], 'ipo': False, 'run_name': 'DPO_Finetuning_BETA_0.11_epochs_10_LCK_DOCKSTRING_FAST_ACTUAL_affinity_DPO_pref_affinity', 'beta': 0.11}
Temperature:  0.5
Sampling: iteration: 0/51{'Validity': 0.9970703125, 'Novelty': 1.0, 'Uniqueness': 0.9853085210577864}
Training Epoch: 0 | iteration: 0/262 | Loss: 0.7237588763237Training Epoch: 0 | iteration: 1/262 | Loss: 0.7543302774429321Training Epoch: 0 | iteration: 2/262 | Loss: 0.7415329813957214Training Epoch: 0 | iteration: 3/262 | Loss: 0.7484872341156006Training Epoch: 0 | iteration: 4/262 | Loss: 0.7799607515335083Training Epoch: 0 | iteration: 5/262 | Loss: 0.7683947086334229Training Epoch: 0 | iteration: 6/262 | Loss: 0.7879124879837036Training Epoch: 0 | iteration: 7/262 | Loss: 0.741384744644165Training Epoch: 0 | iteration: 8/262 | Loss: 0.7633590698242188Training Epoch: 0 | iteration: 9/262 | Loss: 0.7439501285552979Training Epoch: 0 | iteration: 10/262 | Loss: 0.784347653388977Training Epoch: 0 | iteration: 11/262 | Loss: 0.7916355133056641Training Epoch: 0 | iteration: 12/262 | Loss: 0.7108865976333618Training Epoch: 0 | iteration: 13/262 | Loss: 0.7500659227371216Training Epoch: 0 | iteration: 14/262 | Loss: 0.7561415433883667Training Epoch: 0 | iteration: 15/262 | Loss: 0.761176586151123Training Epoch: 0 | iteration: 16/262 | Loss: 0.7342932224273682Training Epoch: 0 | iteration: 17/262 | Loss: 0.7382925748825073Training Epoch: 0 | iteration: 18/262 | Loss: 0.7713278532028198Training Epoch: 0 | iteration: 19/262 | Loss: 0.7480833530426025Training Epoch: 0 | iteration: 20/262 | Loss: 0.7146312594413757Training Epoch: 0 | iteration: 21/262 | Loss: 0.755408525466919Training Epoch: 0 | iteration: 22/262 | Loss: 0.7808201909065247Training Epoch: 0 | iteration: 23/262 | Loss: 0.7046513557434082Training Epoch: 0 | iteration: 24/262 | Loss: 0.758276104927063Training Epoch: 0 | iteration: 25/262 | Loss: 0.7484028339385986Training Epoch: 0 | iteration: 26/262 | Loss: 0.7709900140762329Training Epoch: 0 | iteration: 27/262 | Loss: 0.7408039569854736Training Epoch: 0 | iteration: 28/262 | Loss: 0.7386685013771057Training Epoch: 0 | iteration: 29/262 | Loss: 0.7112827301025391Training Epoch: 0 | iteration: 30/262 | Loss: 0.687758207321167Training Epoch: 0 | iteration: 31/262 | Loss: 0.7601923942565918Training Epoch: 0 | iteration: 32/262 | Loss: 0.6880286931991577Training Epoch: 0 | iteration: 33/262 | Loss: 0.7312227487564087Training Epoch: 0 | iteration: 34/262 | Loss: 0.7657614350318909Training Epoch: 0 | iteration: 35/262 | Loss: 0.7483308911323547Training Epoch: 0 | iteration: 36/262 | Loss: 0.7296431064605713Training Epoch: 0 | iteration: 37/262 | Loss: 0.784381628036499Training Epoch: 0 | iteration: 38/262 | Loss: 0.7420135736465454Training Epoch: 0 | iteration: 39/262 | Loss: 0.7493391036987305Training Epoch: 0 | iteration: 40/262 | Loss: 0.6995395421981812Training Epoch: 0 | iteration: 41/262 | Loss: 0.7348036170005798Training Epoch: 0 | iteration: 42/262 | Loss: 0.7268486618995667Training Epoch: 0 | iteration: 43/262 | Loss: 0.6972960233688354Training Epoch: 0 | iteration: 44/262 | Loss: 0.7236728668212891Training Epoch: 0 | iteration: 45/262 | Loss: 0.7719031572341919Training Epoch: 0 | iteration: 46/262 | Loss: 0.7230923175811768Training Epoch: 0 | iteration: 47/262 | Loss: 0.7070515155792236Training Epoch: 0 | iteration: 48/262 | Loss: 0.7323215007781982Training Epoch: 0 | iteration: 49/262 | Loss: 0.7378966808319092Training Epoch: 0 | iteration: 50/262 | Loss: 0.7423593401908875Training Epoch: 0 | iteration: 51/262 | Loss: 0.7194174528121948Training Epoch: 0 | iteration: 52/262 | Loss: 0.7469346523284912Training Epoch: 0 | iteration: 53/262 | Loss: 0.7210096120834351Training Epoch: 0 | iteration: 54/262 | Loss: 0.6993290185928345Training Epoch: 0 | iteration: 55/262 | Loss: 0.7256821393966675Training Epoch: 0 | iteration: 56/262 | Loss: 0.771041750907898Training Epoch: 0 | iteration: 57/262 | Loss: 0.7545560598373413Training Epoch: 0 | iteration: 58/262 | Loss: 0.7099225521087646Training Epoch: 0 | iteration: 59/262 | Loss: 0.6962890625Training Epoch: 0 | iteration: 60/262 | Loss: 0.7289152145385742Training Epoch: 0 | iteration: 61/262 | Loss: 0.7170838713645935Training Epoch: 0 | iteration: 62/262 | Loss: 0.7589342594146729Training Epoch: 0 | iteration: 63/262 | Loss: 0.7051699161529541Training Epoch: 0 | iteration: 64/262 | Loss: 0.6968367099761963Training Epoch: 0 | iteration: 65/262 | Loss: 0.7394287586212158Training Epoch: 0 | iteration: 66/262 | Loss: 0.7363764047622681Training Epoch: 0 | iteration: 67/262 | Loss: 0.7324386239051819Training Epoch: 0 | iteration: 68/262 | Loss: 0.6887214183807373Training Epoch: 0 | iteration: 69/262 | Loss: 0.6852887868881226Training Epoch: 0 | iteration: 70/262 | Loss: 0.7019800543785095Training Epoch: 0 | iteration: 71/262 | Loss: 0.7366869449615479Training Epoch: 0 | iteration: 72/262 | Loss: 0.7024654150009155Training Epoch: 0 | iteration: 73/262 | Loss: 0.6909757256507874Training Epoch: 0 | iteration: 74/262 | Loss: 0.7536507844924927Training Epoch: 0 | iteration: 75/262 | Loss: 0.7163534164428711Training Epoch: 0 | iteration: 76/262 | Loss: 0.7271071076393127Training Epoch: 0 | iteration: 77/262 | Loss: 0.7182638645172119Training Epoch: 0 | iteration: 78/262 | Loss: 0.7238596677780151Training Epoch: 0 | iteration: 79/262 | Loss: 0.7427494525909424Training Epoch: 0 | iteration: 80/262 | Loss: 0.7306325435638428Training Epoch: 0 | iteration: 81/262 | Loss: 0.7583591938018799Training Epoch: 0 | iteration: 82/262 | Loss: 0.7409260869026184Training Epoch: 0 | iteration: 83/262 | Loss: 0.7296961545944214Training Epoch: 0 | iteration: 84/262 | Loss: 0.6824065446853638Training Epoch: 0 | iteration: 85/262 | Loss: 0.6935290098190308Training Epoch: 0 | iteration: 86/262 | Loss: 0.7327805161476135Training Epoch: 0 | iteration: 87/262 | Loss: 0.772075355052948Training Epoch: 0 | iteration: 88/262 | Loss: 0.6860598921775818Training Epoch: 0 | iteration: 89/262 | Loss: 0.7278075218200684Training Epoch: 0 | iteration: 90/262 | Loss: 0.729729175567627Training Epoch: 0 | iteration: 91/262 | Loss: 0.7043052315711975Training Epoch: 0 | iteration: 92/262 | Loss: 0.7263129949569702Training Epoch: 0 | iteration: 93/262 | Loss: 0.7486242055892944Training Epoch: 0 | iteration: 94/262 | Loss: 0.684390664100647Training Epoch: 0 | iteration: 95/262 | Loss: 0.7886718511581421Training Epoch: 0 | iteration: 96/262 | Loss: 0.7078042030334473Training Epoch: 0 | iteration: 97/262 | Loss: 0.7497522830963135Training Epoch: 0 | iteration: 98/262 | Loss: 0.7440430521965027Training Epoch: 0 | iteration: 99/262 | Loss: 0.7443720698356628Training Epoch: 0 | iteration: 100/262 | Loss: 0.7191101908683777Training Epoch: 0 | iteration: 101/262 | Loss: 0.715847909450531Training Epoch: 0 | iteration: 102/262 | Loss: 0.7398018836975098Training Epoch: 0 | iteration: 103/262 | Loss: 0.7253519892692566Training Epoch: 0 | iteration: 104/262 | Loss: 0.7251923084259033Training Epoch: 0 | iteration: 105/262 | Loss: 0.6896105408668518Training Epoch: 0 | iteration: 106/262 | Loss: 0.7184774875640869Training Epoch: 0 | iteration: 107/262 | Loss: 0.7205591201782227Training Epoch: 0 | iteration: 108/262 | Loss: 0.7720233201980591Training Epoch: 0 | iteration: 109/262 | Loss: 0.6720431447029114Training Epoch: 0 | iteration: 110/262 | Loss: 0.7296179533004761Training Epoch: 0 | iteration: 111/262 | Loss: 0.7404162287712097Training Epoch: 0 | iteration: 112/262 | Loss: 0.7372820377349854Training Epoch: 0 | iteration: 113/262 | Loss: 0.6892108917236328Training Epoch: 0 | iteration: 114/262 | Loss: 0.6902152299880981Training Epoch: 0 | iteration: 115/262 | Loss: 0.7230145931243896Training Epoch: 0 | iteration: 116/262 | Loss: 0.7161588668823242Training Epoch: 0 | iteration: 117/262 | Loss: 0.7115990519523621Training Epoch: 0 | iteration: 118/262 | Loss: 0.7003021240234375Training Epoch: 0 | iteration: 119/262 | Loss: 0.7539877891540527Training Epoch: 0 | iteration: 120/262 | Loss: 0.6772404909133911Training Epoch: 0 | iteration: 121/262 | Loss: 0.7322396636009216Training Epoch: 0 | iteration: 122/262 | Loss: 0.6870899796485901Training Epoch: 0 | iteration: 123/262 | Loss: 0.7096401453018188Training Epoch: 0 | iteration: 124/262 | Loss: 0.723121702671051Training Epoch: 0 | iteration: 125/262 | Loss: 0.7107410430908203Training Epoch: 0 | iteration: 126/262 | Loss: 0.7199838757514954Training Epoch: 0 | iteration: 127/262 | Loss: 0.7260118126869202Training Epoch: 0 | iteration: 128/262 | Loss: 0.7343891263008118Training Epoch: 0 | iteration: 129/262 | Loss: 0.6980904936790466Training Epoch: 0 | iteration: 130/262 | Loss: 0.6916235685348511Training Epoch: 0 | iteration: 131/262 | Loss: 0.7758914828300476Training Epoch: 0 | iteration: 132/262 | Loss: 0.696905255317688Training Epoch: 0 | iteration: 133/262 | Loss: 0.7408055067062378Training Epoch: 0 | iteration: 134/262 | Loss: 0.703708291053772Training Epoch: 0 | iteration: 135/262 | Loss: 0.6961822509765625Training Epoch: 0 | iteration: 136/262 | Loss: 0.6853174567222595Training Epoch: 0 | iteration: 137/262 | Loss: 0.7405529022216797Training Epoch: 0 | iteration: 138/262 | Loss: 0.6823908090591431Training Epoch: 0 | iteration: 139/262 | Loss: 0.6969577670097351Training Epoch: 0 | iteration: 140/262 | Loss: 0.6635399460792542Training Epoch: 0 | iteration: 141/262 | Loss: 0.7108809947967529Training Epoch: 0 | iteration: 142/262 | Loss: 0.7034130096435547Training Epoch: 0 | iteration: 143/262 | Loss: 0.6802242994308472Training Epoch: 0 | iteration: 144/262 | Loss: 0.6908186674118042Training Epoch: 0 | iteration: 145/262 | Loss: 0.683924674987793Training Epoch: 0 | iteration: 146/262 | Loss: 0.7085144519805908Training Epoch: 0 | iteration: 147/262 | Loss: 0.6992639303207397Training Epoch: 0 | iteration: 148/262 | Loss: 0.7430417537689209Training Epoch: 0 | iteration: 149/262 | Loss: 0.7477390766143799Training Epoch: 0 | iteration: 150/262 | Loss: 0.7146214842796326Training Epoch: 0 | iteration: 151/262 | Loss: 0.7475067973136902Training Epoch: 0 | iteration: 152/262 | Loss: 0.7272939682006836Training Epoch: 0 | iteration: 153/262 | Loss: 0.6995002031326294Training Epoch: 0 | iteration: 154/262 | Loss: 0.6863781809806824Training Epoch: 0 | iteration: 155/262 | Loss: 0.723146915435791Training Epoch: 0 | iteration: 156/262 | Loss: 0.67751145362854Training Epoch: 0 | iteration: 157/262 | Loss: 0.7230457663536072Training Epoch: 0 | iteration: 158/262 | Loss: 0.6880773305892944Training Epoch: 0 | iteration: 159/262 | Loss: 0.686805009841919Training Epoch: 0 | iteration: 160/262 | Loss: 0.6686176061630249Training Epoch: 0 | iteration: 161/262 | Loss: 0.6940643191337585Training Epoch: 0 | iteration: 162/262 | Loss: 0.7231531143188477Training Epoch: 0 | iteration: 163/262 | Loss: 0.7148436903953552Training Epoch: 0 | iteration: 164/262 | Loss: 0.7254761457443237Training Epoch: 0 | iteration: 165/262 | Loss: 0.7467894554138184Training Epoch: 0 | iteration: 166/262 | Loss: 0.6863540410995483Training Epoch: 0 | iteration: 167/262 | Loss: 0.7556489109992981Training Epoch: 0 | iteration: 168/262 | Loss: 0.7786380648612976Training Epoch: 0 | iteration: 169/262 | Loss: 0.6898579597473145Training Epoch: 0 | iteration: 170/262 | Loss: 0.7782694101333618Training Epoch: 0 | iteration: 171/262 | Loss: 0.7042982578277588Training Epoch: 0 | iteration: 172/262 | Loss: 0.6802082061767578Training Epoch: 0 | iteration: 173/262 | Loss: 0.7150442004203796Training Epoch: 0 | iteration: 174/262 | Loss: 0.6833853721618652Training Epoch: 0 | iteration: 175/262 | Loss: 0.7584697008132935Training Epoch: 0 | iteration: 176/262 | Loss: 0.7449617981910706Training Epoch: 0 | iteration: 177/262 | Loss: 0.6983997225761414Training Epoch: 0 | iteration: 178/262 | Loss: 0.6820002794265747Training Epoch: 0 | iteration: 179/262 | Loss: 0.7141545414924622Training Epoch: 0 | iteration: 180/262 | Loss: 0.7216446399688721Training Epoch: 0 | iteration: 181/262 | Loss: 0.7265056371688843Training Epoch: 0 | iteration: 182/262 | Loss: 0.6932034492492676Training Epoch: 0 | iteration: 183/262 | Loss: 0.7493210434913635Training Epoch: 0 | iteration: 184/262 | Loss: 0.739266037940979Training Epoch: 0 | iteration: 185/262 | Loss: 0.7354038953781128Training Epoch: 0 | iteration: 186/262 | Loss: 0.699068009853363Training Epoch: 0 | iteration: 187/262 | Loss: 0.7331655025482178Training Epoch: 0 | iteration: 188/262 | Loss: 0.7461821436882019Training Epoch: 0 | iteration: 189/262 | Loss: 0.7529224157333374Training Epoch: 0 | iteration: 190/262 | Loss: 0.6735534071922302Training Epoch: 0 | iteration: 191/262 | Loss: 0.7034308910369873Training Epoch: 0 | iteration: 192/262 | Loss: 0.729902982711792Training Epoch: 0 | iteration: 193/262 | Loss: 0.7260117530822754Training Epoch: 0 | iteration: 194/262 | Loss: 0.7021075487136841Training Epoch: 0 | iteration: 195/262 | Loss: 0.7059179544448853Training Epoch: 0 | iteration: 196/262 | Loss: 0.7386383414268494Training Epoch: 0 | iteration: 197/262 | Loss: 0.6361619234085083Training Epoch: 0 | iteration: 198/262 | Loss: 0.7319457530975342Training Epoch: 0 | iteration: 199/262 | Loss: 0.7575638294219971Training Epoch: 0 | iteration: 200/262 | Loss: 0.7029738426208496Training Epoch: 0 | iteration: 201/262 | Loss: 0.6467635631561279Training Epoch: 0 | iteration: 202/262 | Loss: 0.6366679072380066Training Epoch: 0 | iteration: 203/262 | Loss: 0.671290397644043Training Epoch: 0 | iteration: 204/262 | Loss: 0.746622622013092Training Epoch: 0 | iteration: 205/262 | Loss: 0.7245993614196777Training Epoch: 0 | iteration: 206/262 | Loss: 0.6902257204055786Training Epoch: 0 | iteration: 207/262 | Loss: 0.754826545715332Training Epoch: 0 | iteration: 208/262 | Loss: 0.699650764465332Training Epoch: 0 | iteration: 209/262 | Loss: 0.6933283805847168Training Epoch: 0 | iteration: 210/262 | Loss: 0.6746973991394043Training Epoch: 0 | iteration: 211/262 | Loss: 0.6893348693847656Training Epoch: 0 | iteration: 212/262 | Loss: 0.6722411513328552Training Epoch: 0 | iteration: 213/262 | Loss: 0.7277597188949585Training Epoch: 0 | iteration: 214/262 | Loss: 0.7028579711914062Training Epoch: 0 | iteration: 215/262 | Loss: 0.732097864151001Training Epoch: 0 | iteration: 216/262 | Loss: 0.7078072428703308Training Epoch: 0 | iteration: 217/262 | Loss: 0.7027499079704285Training Epoch: 0 | iteration: 218/262 | Loss: 0.7368970513343811Training Epoch: 0 | iteration: 219/262 | Loss: 0.7183607816696167Training Epoch: 0 | iteration: 220/262 | Loss: 0.6825757026672363Training Epoch: 0 | iteration: 221/262 | Loss: 0.67765212059021Training Epoch: 0 | iteration: 222/262 | Loss: 0.6977653503417969Training Epoch: 0 | iteration: 223/262 | Loss: 0.7035138607025146Training Epoch: 0 | iteration: 224/262 | Loss: 0.7265782356262207Training Epoch: 0 | iteration: 225/262 | Loss: 0.7253919839859009Training Epoch: 0 | iteration: 226/262 | Loss: 0.7339388132095337Training Epoch: 0 | iteration: 227/262 | Loss: 0.7153566479682922Training Epoch: 0 | iteration: 228/262 | Loss: 0.6876567602157593Training Epoch: 0 | iteration: 229/262 | Loss: 0.7215198874473572Training Epoch: 0 | iteration: 230/262 | Loss: 0.695702314376831Training Epoch: 0 | iteration: 231/262 | Loss: 0.676504373550415Training Epoch: 0 | iteration: 232/262 | Loss: 0.7043431401252747Training Epoch: 0 | iteration: 233/262 | Loss: 0.662580668926239Training Epoch: 0 | iteration: 234/262 | Loss: 0.7144030928611755Training Epoch: 0 | iteration: 235/262 | Loss: 0.7695555686950684Training Epoch: 0 | iteration: 236/262 | Loss: 0.734392523765564Training Epoch: 0 | iteration: 237/262 | Loss: 0.6987228393554688Training Epoch: 0 | iteration: 238/262 | Loss: 0.6758308410644531Training Epoch: 0 | iteration: 239/262 | Loss: 0.7145246267318726Training Epoch: 0 | iteration: 240/262 | Loss: 0.7287676334381104Training Epoch: 0 | iteration: 241/262 | Loss: 0.6973894238471985Training Epoch: 0 | iteration: 242/262 | Loss: 0.661603569984436Training Epoch: 0 | iteration: 243/262 | Loss: 0.6690797209739685Training Epoch: 0 | iteration: 244/262 | Loss: 0.7021799087524414Training Epoch: 0 | iteration: 245/262 | Loss: 0.7264533042907715Training Epoch: 0 | iteration: 246/262 | Loss: 0.6970067620277405Training Epoch: 0 | iteration: 247/262 | Loss: 0.6966434717178345Training Epoch: 0 | iteration: 248/262 | Loss: 0.6600055694580078Training Epoch: 0 | iteration: 249/262 | Loss: 0.7067117691040039Training Epoch: 0 | iteration: 250/262 | Loss: 0.7836504578590393Training Epoch: 0 | iteration: 251/262 | Loss: 0.6636581420898438Training Epoch: 0 | iteration: 252/262 | Loss: 0.6609833240509033Training Epoch: 0 | iteration: 253/262 | Loss: 0.685749888420105Training Epoch: 0 | iteration: 254/262 | Loss: 0.6973538994789124Training Epoch: 0 | iteration: 255/262 | Loss: 0.7850238084793091Training Epoch: 0 | iteration: 256/262 | Loss: 0.6999480724334717Training Epoch: 0 | iteration: 257/262 | Loss: 0.6515754461288452Training Epoch: 0 | iteration: 258/262 | Loss: 0.6578468084335327Training Epoch: 0 | iteration: 259/262 | Loss: 0.7094893455505371Training Epoch: 0 | iteration: 260/262 | Loss: 0.7306774854660034Training Epoch: 0 | iteration: 261/262 | Loss: 0.7666128873825073Validating Epoch: 0 | iteration: 0/66 | Loss: 0.6068787574768066Validating Epoch: 0 | iteration: 1/66 | Loss: 0.6169365048408508Validating Epoch: 0 | iteration: 2/66 | Loss: 0.6024665832519531Validating Epoch: 0 | iteration: 3/66 | Loss: 0.6425221562385559Validating Epoch: 0 | iteration: 4/66 | Loss: 0.616572380065918Validating Epoch: 0 | iteration: 5/66 | Loss: 0.6342002153396606Validating Epoch: 0 | iteration: 6/66 | Loss: 0.6228803396224976Validating Epoch: 0 | iteration: 7/66 | Loss: 0.6216667294502258Validating Epoch: 0 | iteration: 8/66 | Loss: 0.6364265084266663Validating Epoch: 0 | iteration: 9/66 | Loss: 0.630604088306427Validating Epoch: 0 | iteration: 10/66 | Loss: 0.6296812295913696Validating Epoch: 0 | iteration: 11/66 | Loss: 0.6179696321487427Validating Epoch: 0 | iteration: 12/66 | Loss: 0.6398659348487854Validating Epoch: 0 | iteration: 13/66 | Loss: 0.6308980584144592Validating Epoch: 0 | iteration: 14/66 | Loss: 0.6289557218551636Validating Epoch: 0 | iteration: 15/66 | Loss: 0.6502505540847778Validating Epoch: 0 | iteration: 16/66 | Loss: 0.6381836533546448Validating Epoch: 0 | iteration: 17/66 | Loss: 0.6565592288970947Validating Epoch: 0 | iteration: 18/66 | Loss: 0.6105799674987793Validating Epoch: 0 | iteration: 19/66 | Loss: 0.6107653379440308Validating Epoch: 0 | iteration: 20/66 | Loss: 0.6080684661865234Validating Epoch: 0 | iteration: 21/66 | Loss: 0.6101772785186768Validating Epoch: 0 | iteration: 22/66 | Loss: 0.6317230463027954Validating Epoch: 0 | iteration: 23/66 | Loss: 0.6182987689971924Validating Epoch: 0 | iteration: 24/66 | Loss: 0.6218674778938293Validating Epoch: 0 | iteration: 25/66 | Loss: 0.604521632194519Validating Epoch: 0 | iteration: 26/66 | Loss: 0.6231514811515808Validating Epoch: 0 | iteration: 27/66 | Loss: 0.6383863687515259Validating Epoch: 0 | iteration: 28/66 | Loss: 0.6316711902618408Validating Epoch: 0 | iteration: 29/66 | Loss: 0.6202373504638672Validating Epoch: 0 | iteration: 30/66 | Loss: 0.6243556141853333Validating Epoch: 0 | iteration: 31/66 | Loss: 0.5995626449584961Validating Epoch: 0 | iteration: 32/66 | Loss: 0.5996518135070801Validating Epoch: 0 | iteration: 33/66 | Loss: 0.6483056545257568Validating Epoch: 0 | iteration: 34/66 | Loss: 0.6388974785804749Validating Epoch: 0 | iteration: 35/66 | Loss: 0.6219244599342346Validating Epoch: 0 | iteration: 36/66 | Loss: 0.6177467107772827Validating Epoch: 0 | iteration: 37/66 | Loss: 0.6278555393218994Validating Epoch: 0 | iteration: 38/66 | Loss: 0.6286138296127319Validating Epoch: 0 | iteration: 39/66 | Loss: 0.6002203822135925Validating Epoch: 0 | iteration: 40/66 | Loss: 0.6336526870727539Validating Epoch: 0 | iteration: 41/66 | Loss: 0.6248814463615417Validating Epoch: 0 | iteration: 42/66 | Loss: 0.639987587928772Validating Epoch: 0 | iteration: 43/66 | Loss: 0.6130359768867493Validating Epoch: 0 | iteration: 44/66 | Loss: 0.5940922498703003Validating Epoch: 0 | iteration: 45/66 | Loss: 0.6194872856140137Validating Epoch: 0 | iteration: 46/66 | Loss: 0.6070762872695923Validating Epoch: 0 | iteration: 47/66 | Loss: 0.6216145157814026Validating Epoch: 0 | iteration: 48/66 | Loss: 0.6555522680282593Validating Epoch: 0 | iteration: 49/66 | Loss: 0.6301528215408325Validating Epoch: 0 | iteration: 50/66 | Loss: 0.6186503171920776Validating Epoch: 0 | iteration: 51/66 | Loss: 0.6415138244628906Validating Epoch: 0 | iteration: 52/66 | Loss: 0.6309794187545776Validating Epoch: 0 | iteration: 53/66 | Loss: 0.6077871322631836Validating Epoch: 0 | iteration: 54/66 | Loss: 0.5854681730270386Validating Epoch: 0 | iteration: 55/66 | Loss: 0.6391783952713013Validating Epoch: 0 | iteration: 56/66 | Loss: 0.6142290830612183Validating Epoch: 0 | iteration: 57/66 | Loss: 0.6254798173904419Validating Epoch: 0 | iteration: 58/66 | Loss: 0.6310150623321533Validating Epoch: 0 | iteration: 59/66 | Loss: 0.6224857568740845Validating Epoch: 0 | iteration: 60/66 | Loss: 0.6395808458328247Validating Epoch: 0 | iteration: 61/66 | Loss: 0.6075832843780518Validating Epoch: 0 | iteration: 62/66 | Loss: 0.6720765829086304Validating Epoch: 0 | iteration: 63/66 | Loss: 0.6325024366378784Validating Epoch: 0 | iteration: 64/66 | Loss: 0.6410384774208069Validating Epoch: 0 | iteration: 65/66 | Loss: 0.5884706377983093Temperature:  0.5
Sampling: iteration: 0/51{'Validity': 0.99609375, 'Novelty': 1.0, 'Uniqueness': 0.9696078431372549}
Training Epoch: 1 | iteration: 0/262 | Loss: 0.7015649676322937Training Epoch: 1 | iteration: 1/262 | Loss: 0.6826968193054199Training Epoch: 1 | iteration: 2/262 | Loss: 0.7123631238937378Training Epoch: 1 | iteration: 3/262 | Loss: 0.6916927099227905Training Epoch: 1 | iteration: 4/262 | Loss: 0.6554863452911377Training Epoch: 1 | iteration: 5/262 | Loss: 0.6706506013870239Training Epoch: 1 | iteration: 6/262 | Loss: 0.6910186409950256Training Epoch: 1 | iteration: 7/262 | Loss: 0.6215808987617493Training Epoch: 1 | iteration: 8/262 | Loss: 0.7183031439781189Training Epoch: 1 | iteration: 9/262 | Loss: 0.6753464937210083Training Epoch: 1 | iteration: 10/262 | Loss: 0.680599570274353Training Epoch: 1 | iteration: 11/262 | Loss: 0.7222990989685059Training Epoch: 1 | iteration: 12/262 | Loss: 0.6613860130310059Training Epoch: 1 | iteration: 13/262 | Loss: 0.6862848401069641Training Epoch: 1 | iteration: 14/262 | Loss: 0.6781253814697266Training Epoch: 1 | iteration: 15/262 | Loss: 0.650928258895874Training Epoch: 1 | iteration: 16/262 | Loss: 0.7155297994613647Training Epoch: 1 | iteration: 17/262 | Loss: 0.6861413717269897Training Epoch: 1 | iteration: 18/262 | Loss: 0.6628713607788086Training Epoch: 1 | iteration: 19/262 | Loss: 0.6955664157867432Training Epoch: 1 | iteration: 20/262 | Loss: 0.687112033367157Training Epoch: 1 | iteration: 21/262 | Loss: 0.7067628502845764Training Epoch: 1 | iteration: 22/262 | Loss: 0.7224669456481934Training Epoch: 1 | iteration: 23/262 | Loss: 0.6538201570510864Training Epoch: 1 | iteration: 24/262 | Loss: 0.6653172373771667Training Epoch: 1 | iteration: 25/262 | Loss: 0.663962185382843Training Epoch: 1 | iteration: 26/262 | Loss: 0.6917561292648315Training Epoch: 1 | iteration: 27/262 | Loss: 0.6661152839660645Training Epoch: 1 | iteration: 28/262 | Loss: 0.6802383661270142Training Epoch: 1 | iteration: 29/262 | Loss: 0.6593523025512695Training Epoch: 1 | iteration: 30/262 | Loss: 0.6841283440589905Training Epoch: 1 | iteration: 31/262 | Loss: 0.6799485087394714Training Epoch: 1 | iteration: 32/262 | Loss: 0.6577755212783813Training Epoch: 1 | iteration: 33/262 | Loss: 0.6398025751113892Training Epoch: 1 | iteration: 34/262 | Loss: 0.6758809089660645Training Epoch: 1 | iteration: 35/262 | Loss: 0.7020917534828186Training Epoch: 1 | iteration: 36/262 | Loss: 0.7055118680000305Training Epoch: 1 | iteration: 37/262 | Loss: 0.6954352855682373Training Epoch: 1 | iteration: 38/262 | Loss: 0.7167103290557861Training Epoch: 1 | iteration: 39/262 | Loss: 0.6712532043457031Training Epoch: 1 | iteration: 40/262 | Loss: 0.6425110101699829Training Epoch: 1 | iteration: 41/262 | Loss: 0.6338730454444885Training Epoch: 1 | iteration: 42/262 | Loss: 0.6852314472198486Training Epoch: 1 | iteration: 43/262 | Loss: 0.6660529375076294Training Epoch: 1 | iteration: 44/262 | Loss: 0.6696003079414368Training Epoch: 1 | iteration: 45/262 | Loss: 0.6491621136665344Training Epoch: 1 | iteration: 46/262 | Loss: 0.682173490524292Training Epoch: 1 | iteration: 47/262 | Loss: 0.7020614147186279Training Epoch: 1 | iteration: 48/262 | Loss: 0.6838718056678772Training Epoch: 1 | iteration: 49/262 | Loss: 0.7288066148757935Training Epoch: 1 | iteration: 50/262 | Loss: 0.7202011346817017Training Epoch: 1 | iteration: 51/262 | Loss: 0.649067223072052Training Epoch: 1 | iteration: 52/262 | Loss: 0.6619237065315247Training Epoch: 1 | iteration: 53/262 | Loss: 0.7104953527450562Training Epoch: 1 | iteration: 54/262 | Loss: 0.6909315586090088Training Epoch: 1 | iteration: 55/262 | Loss: 0.6223865747451782Training Epoch: 1 | iteration: 56/262 | Loss: 0.6783888936042786Training Epoch: 1 | iteration: 57/262 | Loss: 0.6918450593948364Training Epoch: 1 | iteration: 58/262 | Loss: 0.6923767924308777Training Epoch: 1 | iteration: 59/262 | Loss: 0.6657322645187378Training Epoch: 1 | iteration: 60/262 | Loss: 0.687279224395752Training Epoch: 1 | iteration: 61/262 | Loss: 0.6333514451980591Training Epoch: 1 | iteration: 62/262 | Loss: 0.678679347038269Training Epoch: 1 | iteration: 63/262 | Loss: 0.6629664301872253Training Epoch: 1 | iteration: 64/262 | Loss: 0.6623916029930115Training Epoch: 1 | iteration: 65/262 | Loss: 0.6721479892730713Training Epoch: 1 | iteration: 66/262 | Loss: 0.6835245490074158Training Epoch: 1 | iteration: 67/262 | Loss: 0.6623243093490601Training Epoch: 1 | iteration: 68/262 | Loss: 0.7022737860679626Training Epoch: 1 | iteration: 69/262 | Loss: 0.6480973958969116Training Epoch: 1 | iteration: 70/262 | Loss: 0.6254968643188477Training Epoch: 1 | iteration: 71/262 | Loss: 0.6580875515937805Training Epoch: 1 | iteration: 72/262 | Loss: 0.6027371883392334Training Epoch: 1 | iteration: 73/262 | Loss: 0.6624918580055237Training Epoch: 1 | iteration: 74/262 | Loss: 0.6426968574523926Training Epoch: 1 | iteration: 75/262 | Loss: 0.7076050639152527Training Epoch: 1 | iteration: 76/262 | Loss: 0.7021114826202393Training Epoch: 1 | iteration: 77/262 | Loss: 0.6857624053955078Training Epoch: 1 | iteration: 78/262 | Loss: 0.6244350671768188Training Epoch: 1 | iteration: 79/262 | Loss: 0.6325682401657104Training Epoch: 1 | iteration: 80/262 | Loss: 0.6707518100738525Training Epoch: 1 | iteration: 81/262 | Loss: 0.6452600955963135Training Epoch: 1 | iteration: 82/262 | Loss: 0.6837111711502075Training Epoch: 1 | iteration: 83/262 | Loss: 0.7142747640609741Training Epoch: 1 | iteration: 84/262 | Loss: 0.6758918762207031Training Epoch: 1 | iteration: 85/262 | Loss: 0.6391651630401611Training Epoch: 1 | iteration: 86/262 | Loss: 0.6407634615898132Training Epoch: 1 | iteration: 87/262 | Loss: 0.6430763006210327Training Epoch: 1 | iteration: 88/262 | Loss: 0.7211524248123169Training Epoch: 1 | iteration: 89/262 | Loss: 0.6376093626022339Training Epoch: 1 | iteration: 90/262 | Loss: 0.6450510025024414Training Epoch: 1 | iteration: 91/262 | Loss: 0.6478773951530457Training Epoch: 1 | iteration: 92/262 | Loss: 0.6850390434265137Training Epoch: 1 | iteration: 93/262 | Loss: 0.7013846635818481Training Epoch: 1 | iteration: 94/262 | Loss: 0.6736297607421875Training Epoch: 1 | iteration: 95/262 | Loss: 0.668758749961853Training Epoch: 1 | iteration: 96/262 | Loss: 0.6405597925186157Training Epoch: 1 | iteration: 97/262 | Loss: 0.6882717609405518Training Epoch: 1 | iteration: 98/262 | Loss: 0.6210222244262695Training Epoch: 1 | iteration: 99/262 | Loss: 0.6659213304519653Training Epoch: 1 | iteration: 100/262 | Loss: 0.6775190234184265Training Epoch: 1 | iteration: 101/262 | Loss: 0.7023696899414062Training Epoch: 1 | iteration: 102/262 | Loss: 0.6843348741531372Training Epoch: 1 | iteration: 103/262 | Loss: 0.668861985206604Training Epoch: 1 | iteration: 104/262 | Loss: 0.629966676235199Training Epoch: 1 | iteration: 105/262 | Loss: 0.6985564231872559Training Epoch: 1 | iteration: 106/262 | Loss: 0.701991081237793Training Epoch: 1 | iteration: 107/262 | Loss: 0.6186126470565796Training Epoch: 1 | iteration: 108/262 | Loss: 0.6476916074752808Training Epoch: 1 | iteration: 109/262 | Loss: 0.6407115459442139Training Epoch: 1 | iteration: 110/262 | Loss: 0.7006745338439941Training Epoch: 1 | iteration: 111/262 | Loss: 0.655599057674408Training Epoch: 1 | iteration: 112/262 | Loss: 0.7872817516326904Training Epoch: 1 | iteration: 113/262 | Loss: 0.723081111907959Training Epoch: 1 | iteration: 114/262 | Loss: 0.6657052040100098Training Epoch: 1 | iteration: 115/262 | Loss: 0.6390224695205688Training Epoch: 1 | iteration: 116/262 | Loss: 0.6584991812705994Training Epoch: 1 | iteration: 117/262 | Loss: 0.6876215934753418Training Epoch: 1 | iteration: 118/262 | Loss: 0.6455724239349365Training Epoch: 1 | iteration: 119/262 | Loss: 0.6303207874298096Training Epoch: 1 | iteration: 120/262 | Loss: 0.627069354057312Training Epoch: 1 | iteration: 121/262 | Loss: 0.643444836139679Training Epoch: 1 | iteration: 122/262 | Loss: 0.6726022958755493Training Epoch: 1 | iteration: 123/262 | Loss: 0.6577510833740234Training Epoch: 1 | iteration: 124/262 | Loss: 0.6859130859375Training Epoch: 1 | iteration: 125/262 | Loss: 0.714784562587738Training Epoch: 1 | iteration: 126/262 | Loss: 0.6667730808258057Training Epoch: 1 | iteration: 127/262 | Loss: 0.6963709592819214Training Epoch: 1 | iteration: 128/262 | Loss: 0.6379029750823975Training Epoch: 1 | iteration: 129/262 | Loss: 0.669756293296814Training Epoch: 1 | iteration: 130/262 | Loss: 0.6704512238502502Training Epoch: 1 | iteration: 131/262 | Loss: 0.6578457951545715Training Epoch: 1 | iteration: 132/262 | Loss: 0.6610487699508667Training Epoch: 1 | iteration: 133/262 | Loss: 0.7200874090194702Training Epoch: 1 | iteration: 134/262 | Loss: 0.6849520802497864Training Epoch: 1 | iteration: 135/262 | Loss: 0.7075931429862976Training Epoch: 1 | iteration: 136/262 | Loss: 0.670992910861969Training Epoch: 1 | iteration: 137/262 | Loss: 0.6832557916641235Training Epoch: 1 | iteration: 138/262 | Loss: 0.6651201844215393Training Epoch: 1 | iteration: 139/262 | Loss: 0.7090443968772888Training Epoch: 1 | iteration: 140/262 | Loss: 0.7031385898590088Training Epoch: 1 | iteration: 141/262 | Loss: 0.6403504610061646Training Epoch: 1 | iteration: 142/262 | Loss: 0.6620710492134094Training Epoch: 1 | iteration: 143/262 | Loss: 0.6897889375686646Training Epoch: 1 | iteration: 144/262 | Loss: 0.6852336525917053Training Epoch: 1 | iteration: 145/262 | Loss: 0.6851135492324829Training Epoch: 1 | iteration: 146/262 | Loss: 0.6446381211280823Training Epoch: 1 | iteration: 147/262 | Loss: 0.6823012232780457Training Epoch: 1 | iteration: 148/262 | Loss: 0.610621452331543Training Epoch: 1 | iteration: 149/262 | Loss: 0.6865764856338501Training Epoch: 1 | iteration: 150/262 | Loss: 0.6927832365036011Training Epoch: 1 | iteration: 151/262 | Loss: 0.6679421067237854Training Epoch: 1 | iteration: 152/262 | Loss: 0.6601916551589966Training Epoch: 1 | iteration: 153/262 | Loss: 0.6531669497489929Training Epoch: 1 | iteration: 154/262 | Loss: 0.690253734588623Training Epoch: 1 | iteration: 155/262 | Loss: 0.7104489207267761Training Epoch: 1 | iteration: 156/262 | Loss: 0.6834722757339478Training Epoch: 1 | iteration: 157/262 | Loss: 0.6644881963729858Training Epoch: 1 | iteration: 158/262 | Loss: 0.686864972114563Training Epoch: 1 | iteration: 159/262 | Loss: 0.662096381187439Training Epoch: 1 | iteration: 160/262 | Loss: 0.6825063228607178Training Epoch: 1 | iteration: 161/262 | Loss: 0.6745463609695435Training Epoch: 1 | iteration: 162/262 | Loss: 0.6466742753982544Training Epoch: 1 | iteration: 163/262 | Loss: 0.6577228307723999Training Epoch: 1 | iteration: 164/262 | Loss: 0.6316603422164917Training Epoch: 1 | iteration: 165/262 | Loss: 0.6200495958328247Training Epoch: 1 | iteration: 166/262 | Loss: 0.7075488567352295Training Epoch: 1 | iteration: 167/262 | Loss: 0.7050883173942566Training Epoch: 1 | iteration: 168/262 | Loss: 0.656124472618103Training Epoch: 1 | iteration: 169/262 | Loss: 0.7042956352233887Training Epoch: 1 | iteration: 170/262 | Loss: 0.7017203569412231Training Epoch: 1 | iteration: 171/262 | Loss: 0.6454110741615295Training Epoch: 1 | iteration: 172/262 | Loss: 0.6945018172264099Training Epoch: 1 | iteration: 173/262 | Loss: 0.6613727807998657Training Epoch: 1 | iteration: 174/262 | Loss: 0.6907473802566528Training Epoch: 1 | iteration: 175/262 | Loss: 0.649503231048584Training Epoch: 1 | iteration: 176/262 | Loss: 0.6566417217254639Training Epoch: 1 | iteration: 177/262 | Loss: 0.6007176041603088Training Epoch: 1 | iteration: 178/262 | Loss: 0.6611198782920837Training Epoch: 1 | iteration: 179/262 | Loss: 0.6692875623703003Training Epoch: 1 | iteration: 180/262 | Loss: 0.6848337650299072Training Epoch: 1 | iteration: 181/262 | Loss: 0.6911267638206482Training Epoch: 1 | iteration: 182/262 | Loss: 0.6503748893737793Training Epoch: 1 | iteration: 183/262 | Loss: 0.6809146404266357Training Epoch: 1 | iteration: 184/262 | Loss: 0.7004377841949463Training Epoch: 1 | iteration: 185/262 | Loss: 0.6518781185150146Training Epoch: 1 | iteration: 186/262 | Loss: 0.6728018522262573Training Epoch: 1 | iteration: 187/262 | Loss: 0.6858576536178589Training Epoch: 1 | iteration: 188/262 | Loss: 0.7359390258789062Training Epoch: 1 | iteration: 189/262 | Loss: 0.6704428195953369Training Epoch: 1 | iteration: 190/262 | Loss: 0.6973366737365723Training Epoch: 1 | iteration: 191/262 | Loss: 0.7279303073883057Training Epoch: 1 | iteration: 192/262 | Loss: 0.6845773458480835Training Epoch: 1 | iteration: 193/262 | Loss: 0.6684421896934509Training Epoch: 1 | iteration: 194/262 | Loss: 0.6408562064170837Training Epoch: 1 | iteration: 195/262 | Loss: 0.6699981093406677Training Epoch: 1 | iteration: 196/262 | Loss: 0.6944930553436279Training Epoch: 1 | iteration: 197/262 | Loss: 0.7432287335395813Training Epoch: 1 | iteration: 198/262 | Loss: 0.6982113122940063Training Epoch: 1 | iteration: 199/262 | Loss: 0.6702434420585632Training Epoch: 1 | iteration: 200/262 | Loss: 0.6781702637672424Training Epoch: 1 | iteration: 201/262 | Loss: 0.7216805219650269Training Epoch: 1 | iteration: 202/262 | Loss: 0.6413375735282898Training Epoch: 1 | iteration: 203/262 | Loss: 0.6733239889144897Training Epoch: 1 | iteration: 204/262 | Loss: 0.7123481631278992Training Epoch: 1 | iteration: 205/262 | Loss: 0.6743069887161255Training Epoch: 1 | iteration: 206/262 | Loss: 0.6672367453575134Training Epoch: 1 | iteration: 207/262 | Loss: 0.6841524243354797Training Epoch: 1 | iteration: 208/262 | Loss: 0.6403945088386536Training Epoch: 1 | iteration: 209/262 | Loss: 0.6529778242111206Training Epoch: 1 | iteration: 210/262 | Loss: 0.66954505443573Training Epoch: 1 | iteration: 211/262 | Loss: 0.6454683542251587Training Epoch: 1 | iteration: 212/262 | Loss: 0.6297022104263306Training Epoch: 1 | iteration: 213/262 | Loss: 0.6771999597549438Training Epoch: 1 | iteration: 214/262 | Loss: 0.7064449787139893Training Epoch: 1 | iteration: 215/262 | Loss: 0.6500377655029297Training Epoch: 1 | iteration: 216/262 | Loss: 0.6537866592407227Training Epoch: 1 | iteration: 217/262 | Loss: 0.662926435470581Training Epoch: 1 | iteration: 218/262 | Loss: 0.6700911521911621Training Epoch: 1 | iteration: 219/262 | Loss: 0.6840428113937378Training Epoch: 1 | iteration: 220/262 | Loss: 0.5988768339157104Training Epoch: 1 | iteration: 221/262 | Loss: 0.698621392250061Training Epoch: 1 | iteration: 222/262 | Loss: 0.6880757808685303Training Epoch: 1 | iteration: 223/262 | Loss: 0.6422640085220337Training Epoch: 1 | iteration: 224/262 | Loss: 0.6680158376693726Training Epoch: 1 | iteration: 225/262 | Loss: 0.6685376167297363Training Epoch: 1 | iteration: 226/262 | Loss: 0.7093191146850586Training Epoch: 1 | iteration: 227/262 | Loss: 0.6115032434463501Training Epoch: 1 | iteration: 228/262 | Loss: 0.6162855625152588Training Epoch: 1 | iteration: 229/262 | Loss: 0.6116334199905396Training Epoch: 1 | iteration: 230/262 | Loss: 0.6700990200042725Training Epoch: 1 | iteration: 231/262 | Loss: 0.7019572854042053Training Epoch: 1 | iteration: 232/262 | Loss: 0.6332194805145264Training Epoch: 1 | iteration: 233/262 | Loss: 0.6661258935928345Training Epoch: 1 | iteration: 234/262 | Loss: 0.691213071346283Training Epoch: 1 | iteration: 235/262 | Loss: 0.6409182548522949Training Epoch: 1 | iteration: 236/262 | Loss: 0.6456731557846069Training Epoch: 1 | iteration: 237/262 | Loss: 0.6167998909950256Training Epoch: 1 | iteration: 238/262 | Loss: 0.6602134108543396Training Epoch: 1 | iteration: 239/262 | Loss: 0.6794268488883972Training Epoch: 1 | iteration: 240/262 | Loss: 0.7479703426361084Training Epoch: 1 | iteration: 241/262 | Loss: 0.6616200804710388Training Epoch: 1 | iteration: 242/262 | Loss: 0.6517441272735596Training Epoch: 1 | iteration: 243/262 | Loss: 0.7046186923980713Training Epoch: 1 | iteration: 244/262 | Loss: 0.6840468645095825Training Epoch: 1 | iteration: 245/262 | Loss: 0.6456602215766907Training Epoch: 1 | iteration: 246/262 | Loss: 0.687498927116394Training Epoch: 1 | iteration: 247/262 | Loss: 0.6515394449234009Training Epoch: 1 | iteration: 248/262 | Loss: 0.668066143989563Training Epoch: 1 | iteration: 249/262 | Loss: 0.6285654306411743Training Epoch: 1 | iteration: 250/262 | Loss: 0.6834280490875244Training Epoch: 1 | iteration: 251/262 | Loss: 0.6969950199127197Training Epoch: 1 | iteration: 252/262 | Loss: 0.5997897982597351Training Epoch: 1 | iteration: 253/262 | Loss: 0.6845858097076416Training Epoch: 1 | iteration: 254/262 | Loss: 0.6163625121116638Training Epoch: 1 | iteration: 255/262 | Loss: 0.6630059480667114Training Epoch: 1 | iteration: 256/262 | Loss: 0.6729874610900879Training Epoch: 1 | iteration: 257/262 | Loss: 0.6907504796981812Training Epoch: 1 | iteration: 258/262 | Loss: 0.720267653465271Training Epoch: 1 | iteration: 259/262 | Loss: 0.6517997980117798Training Epoch: 1 | iteration: 260/262 | Loss: 0.6940844058990479Training Epoch: 1 | iteration: 261/262 | Loss: 0.7269482016563416Validating Epoch: 1 | iteration: 0/66 | Loss: 0.6160915493965149Validating Epoch: 1 | iteration: 1/66 | Loss: 0.6381130814552307Validating Epoch: 1 | iteration: 2/66 | Loss: 0.61763596534729Validating Epoch: 1 | iteration: 3/66 | Loss: 0.5785571336746216Validating Epoch: 1 | iteration: 4/66 | Loss: 0.590886116027832Validating Epoch: 1 | iteration: 5/66 | Loss: 0.6335204839706421Validating Epoch: 1 | iteration: 6/66 | Loss: 0.592380702495575Validating Epoch: 1 | iteration: 7/66 | Loss: 0.6033142805099487Validating Epoch: 1 | iteration: 8/66 | Loss: 0.5910571217536926Validating Epoch: 1 | iteration: 9/66 | Loss: 0.6031955480575562Validating Epoch: 1 | iteration: 10/66 | Loss: 0.624209463596344Validating Epoch: 1 | iteration: 11/66 | Loss: 0.617037296295166Validating Epoch: 1 | iteration: 12/66 | Loss: 0.6362398862838745Validating Epoch: 1 | iteration: 13/66 | Loss: 0.6022934913635254Validating Epoch: 1 | iteration: 14/66 | Loss: 0.6371510028839111Validating Epoch: 1 | iteration: 15/66 | Loss: 0.6383780241012573Validating Epoch: 1 | iteration: 16/66 | Loss: 0.5999356508255005Validating Epoch: 1 | iteration: 17/66 | Loss: 0.6311999559402466Validating Epoch: 1 | iteration: 18/66 | Loss: 0.6025382280349731Validating Epoch: 1 | iteration: 19/66 | Loss: 0.6091176271438599Validating Epoch: 1 | iteration: 20/66 | Loss: 0.6083081960678101Validating Epoch: 1 | iteration: 21/66 | Loss: 0.6223894357681274Validating Epoch: 1 | iteration: 22/66 | Loss: 0.6086078882217407Validating Epoch: 1 | iteration: 23/66 | Loss: 0.6261167526245117Validating Epoch: 1 | iteration: 24/66 | Loss: 0.6065666675567627Validating Epoch: 1 | iteration: 25/66 | Loss: 0.6364172101020813Validating Epoch: 1 | iteration: 26/66 | Loss: 0.6233299970626831Validating Epoch: 1 | iteration: 27/66 | Loss: 0.6122162342071533Validating Epoch: 1 | iteration: 28/66 | Loss: 0.6466552019119263Validating Epoch: 1 | iteration: 29/66 | Loss: 0.6234289407730103Validating Epoch: 1 | iteration: 30/66 | Loss: 0.6302328109741211Validating Epoch: 1 | iteration: 31/66 | Loss: 0.6326625347137451Validating Epoch: 1 | iteration: 32/66 | Loss: 0.5867421627044678Validating Epoch: 1 | iteration: 33/66 | Loss: 0.6430878043174744Validating Epoch: 1 | iteration: 34/66 | Loss: 0.5662528872489929Validating Epoch: 1 | iteration: 35/66 | Loss: 0.6285946369171143Validating Epoch: 1 | iteration: 36/66 | Loss: 0.5738744735717773Validating Epoch: 1 | iteration: 37/66 | Loss: 0.6165401339530945Validating Epoch: 1 | iteration: 38/66 | Loss: 0.6293038725852966Validating Epoch: 1 | iteration: 39/66 | Loss: 0.6229186058044434Validating Epoch: 1 | iteration: 40/66 | Loss: 0.5967491269111633Validating Epoch: 1 | iteration: 41/66 | Loss: 0.6286380887031555Validating Epoch: 1 | iteration: 42/66 | Loss: 0.6249582767486572Validating Epoch: 1 | iteration: 43/66 | Loss: 0.6444931030273438Validating Epoch: 1 | iteration: 44/66 | Loss: 0.5641008615493774Validating Epoch: 1 | iteration: 45/66 | Loss: 0.6280858516693115Validating Epoch: 1 | iteration: 46/66 | Loss: 0.6277912855148315Validating Epoch: 1 | iteration: 47/66 | Loss: 0.5934987664222717Validating Epoch: 1 | iteration: 48/66 | Loss: 0.5968300700187683Validating Epoch: 1 | iteration: 49/66 | Loss: 0.6291901469230652Validating Epoch: 1 | iteration: 50/66 | Loss: 0.6201741099357605Validating Epoch: 1 | iteration: 51/66 | Loss: 0.6324025988578796Validating Epoch: 1 | iteration: 52/66 | Loss: 0.5761721134185791Validating Epoch: 1 | iteration: 53/66 | Loss: 0.5914221405982971Validating Epoch: 1 | iteration: 54/66 | Loss: 0.5886967778205872Validating Epoch: 1 | iteration: 55/66 | Loss: 0.6291999220848083Validating Epoch: 1 | iteration: 56/66 | Loss: 0.6147925853729248Validating Epoch: 1 | iteration: 57/66 | Loss: 0.6115877628326416Validating Epoch: 1 | iteration: 58/66 | Loss: 0.6229109764099121Validating Epoch: 1 | iteration: 59/66 | Loss: 0.6206416487693787Validating Epoch: 1 | iteration: 60/66 | Loss: 0.6415941119194031Validating Epoch: 1 | iteration: 61/66 | Loss: 0.6066373586654663Validating Epoch: 1 | iteration: 62/66 | Loss: 0.621089518070221Validating Epoch: 1 | iteration: 63/66 | Loss: 0.6195496320724487Validating Epoch: 1 | iteration: 64/66 | Loss: 0.6023848056793213Validating Epoch: 1 | iteration: 65/66 | Loss: 0.5822308659553528Temperature:  0.5
Sampling: iteration: 0/51{'Validity': 0.9912109375, 'Novelty': 1.0, 'Uniqueness': 0.9625615763546798}
Training Epoch: 2 | iteration: 0/262 | Loss: 0.6065734624862671Training Epoch: 2 | iteration: 1/262 | Loss: 0.7041972875595093Training Epoch: 2 | iteration: 2/262 | Loss: 0.6371608972549438Training Epoch: 2 | iteration: 3/262 | Loss: 0.6208523511886597Training Epoch: 2 | iteration: 4/262 | Loss: 0.62410569190979Training Epoch: 2 | iteration: 5/262 | Loss: 0.6484898328781128Training Epoch: 2 | iteration: 6/262 | Loss: 0.6585904359817505Training Epoch: 2 | iteration: 7/262 | Loss: 0.6910781860351562Training Epoch: 2 | iteration: 8/262 | Loss: 0.6150048971176147Training Epoch: 2 | iteration: 9/262 | Loss: 0.6278932094573975Training Epoch: 2 | iteration: 10/262 | Loss: 0.660805344581604Training Epoch: 2 | iteration: 11/262 | Loss: 0.6270683407783508Training Epoch: 2 | iteration: 12/262 | Loss: 0.6625847220420837Training Epoch: 2 | iteration: 13/262 | Loss: 0.6480932235717773Training Epoch: 2 | iteration: 14/262 | Loss: 0.6411144137382507Training Epoch: 2 | iteration: 15/262 | Loss: 0.643532395362854Training Epoch: 2 | iteration: 16/262 | Loss: 0.6191778182983398Training Epoch: 2 | iteration: 17/262 | Loss: 0.6700201034545898Training Epoch: 2 | iteration: 18/262 | Loss: 0.7131965160369873Training Epoch: 2 | iteration: 19/262 | Loss: 0.6827122569084167Training Epoch: 2 | iteration: 20/262 | Loss: 0.6594674587249756Training Epoch: 2 | iteration: 21/262 | Loss: 0.6330612897872925Training Epoch: 2 | iteration: 22/262 | Loss: 0.6118489503860474Training Epoch: 2 | iteration: 23/262 | Loss: 0.6613668203353882Training Epoch: 2 | iteration: 24/262 | Loss: 0.6251857280731201Training Epoch: 2 | iteration: 25/262 | Loss: 0.6584542989730835Training Epoch: 2 | iteration: 26/262 | Loss: 0.6091791391372681Training Epoch: 2 | iteration: 27/262 | Loss: 0.6440473794937134Training Epoch: 2 | iteration: 28/262 | Loss: 0.695823609828949Training Epoch: 2 | iteration: 29/262 | Loss: 0.6481233835220337Training Epoch: 2 | iteration: 30/262 | Loss: 0.6837185025215149Training Epoch: 2 | iteration: 31/262 | Loss: 0.6748152375221252Training Epoch: 2 | iteration: 32/262 | Loss: 0.6396226286888123Training Epoch: 2 | iteration: 33/262 | Loss: 0.6962486505508423Training Epoch: 2 | iteration: 34/262 | Loss: 0.6960992813110352Training Epoch: 2 | iteration: 35/262 | Loss: 0.6850912570953369Training Epoch: 2 | iteration: 36/262 | Loss: 0.6128328442573547Training Epoch: 2 | iteration: 37/262 | Loss: 0.6400070190429688Training Epoch: 2 | iteration: 38/262 | Loss: 0.6257723569869995Training Epoch: 2 | iteration: 39/262 | Loss: 0.639716625213623Training Epoch: 2 | iteration: 40/262 | Loss: 0.6240208148956299Training Epoch: 2 | iteration: 41/262 | Loss: 0.6670403480529785Training Epoch: 2 | iteration: 42/262 | Loss: 0.6728628277778625Training Epoch: 2 | iteration: 43/262 | Loss: 0.6478755474090576Training Epoch: 2 | iteration: 44/262 | Loss: 0.6312440633773804Training Epoch: 2 | iteration: 45/262 | Loss: 0.653846800327301Training Epoch: 2 | iteration: 46/262 | Loss: 0.678149402141571Training Epoch: 2 | iteration: 47/262 | Loss: 0.672565221786499Training Epoch: 2 | iteration: 48/262 | Loss: 0.6719334125518799Training Epoch: 2 | iteration: 49/262 | Loss: 0.6719863414764404Training Epoch: 2 | iteration: 50/262 | Loss: 0.6268450021743774Training Epoch: 2 | iteration: 51/262 | Loss: 0.6279299855232239Training Epoch: 2 | iteration: 52/262 | Loss: 0.6727840900421143Training Epoch: 2 | iteration: 53/262 | Loss: 0.6168001890182495Training Epoch: 2 | iteration: 54/262 | Loss: 0.605140745639801Training Epoch: 2 | iteration: 55/262 | Loss: 0.631668210029602Training Epoch: 2 | iteration: 56/262 | Loss: 0.6726521253585815Training Epoch: 2 | iteration: 57/262 | Loss: 0.6100932955741882Training Epoch: 2 | iteration: 58/262 | Loss: 0.656877875328064Training Epoch: 2 | iteration: 59/262 | Loss: 0.6509791612625122Training Epoch: 2 | iteration: 60/262 | Loss: 0.6163209676742554Training Epoch: 2 | iteration: 61/262 | Loss: 0.6355358958244324Training Epoch: 2 | iteration: 62/262 | Loss: 0.6186718344688416Training Epoch: 2 | iteration: 63/262 | Loss: 0.635653018951416Training Epoch: 2 | iteration: 64/262 | Loss: 0.5940664410591125Training Epoch: 2 | iteration: 65/262 | Loss: 0.6246767044067383Training Epoch: 2 | iteration: 66/262 | Loss: 0.6357054114341736Training Epoch: 2 | iteration: 67/262 | Loss: 0.6235073208808899Training Epoch: 2 | iteration: 68/262 | Loss: 0.6506105661392212Training Epoch: 2 | iteration: 69/262 | Loss: 0.6814757585525513Training Epoch: 2 | iteration: 70/262 | Loss: 0.6850519180297852Training Epoch: 2 | iteration: 71/262 | Loss: 0.6530924439430237Training Epoch: 2 | iteration: 72/262 | Loss: 0.6790446043014526Training Epoch: 2 | iteration: 73/262 | Loss: 0.6611156463623047Training Epoch: 2 | iteration: 74/262 | Loss: 0.6097425818443298Training Epoch: 2 | iteration: 75/262 | Loss: 0.6612684726715088Training Epoch: 2 | iteration: 76/262 | Loss: 0.6166837215423584Training Epoch: 2 | iteration: 77/262 | Loss: 0.6771838068962097Training Epoch: 2 | iteration: 78/262 | Loss: 0.6510633230209351Training Epoch: 2 | iteration: 79/262 | Loss: 0.6619563102722168Training Epoch: 2 | iteration: 80/262 | Loss: 0.6663114428520203Training Epoch: 2 | iteration: 81/262 | Loss: 0.661224365234375Training Epoch: 2 | iteration: 82/262 | Loss: 0.6358263492584229Training Epoch: 2 | iteration: 83/262 | Loss: 0.6248885989189148Training Epoch: 2 | iteration: 84/262 | Loss: 0.671316385269165Training Epoch: 2 | iteration: 85/262 | Loss: 0.6233782172203064Training Epoch: 2 | iteration: 86/262 | Loss: 0.609054684638977Training Epoch: 2 | iteration: 87/262 | Loss: 0.6353328227996826Training Epoch: 2 | iteration: 88/262 | Loss: 0.6341829895973206Training Epoch: 2 | iteration: 89/262 | Loss: 0.6217025518417358Training Epoch: 2 | iteration: 90/262 | Loss: 0.6460933685302734Training Epoch: 2 | iteration: 91/262 | Loss: 0.6374984979629517Training Epoch: 2 | iteration: 92/262 | Loss: 0.6349568367004395Training Epoch: 2 | iteration: 93/262 | Loss: 0.6720397472381592Training Epoch: 2 | iteration: 94/262 | Loss: 0.6848677396774292Training Epoch: 2 | iteration: 95/262 | Loss: 0.6232852339744568Training Epoch: 2 | iteration: 96/262 | Loss: 0.6539239287376404Training Epoch: 2 | iteration: 97/262 | Loss: 0.6133716702461243Training Epoch: 2 | iteration: 98/262 | Loss: 0.6432216167449951Training Epoch: 2 | iteration: 99/262 | Loss: 0.6363102197647095Training Epoch: 2 | iteration: 100/262 | Loss: 0.6350919008255005Training Epoch: 2 | iteration: 101/262 | Loss: 0.6580561399459839Training Epoch: 2 | iteration: 102/262 | Loss: 0.6411606073379517Training Epoch: 2 | iteration: 103/262 | Loss: 0.6539409160614014Training Epoch: 2 | iteration: 104/262 | Loss: 0.6647348403930664Training Epoch: 2 | iteration: 105/262 | Loss: 0.639363706111908Training Epoch: 2 | iteration: 106/262 | Loss: 0.6255216598510742Training Epoch: 2 | iteration: 107/262 | Loss: 0.656364917755127Training Epoch: 2 | iteration: 108/262 | Loss: 0.6704960465431213Training Epoch: 2 | iteration: 109/262 | Loss: 0.6504092216491699Training Epoch: 2 | iteration: 110/262 | Loss: 0.6822097301483154Training Epoch: 2 | iteration: 111/262 | Loss: 0.6342817544937134Training Epoch: 2 | iteration: 112/262 | Loss: 0.6591254472732544Training Epoch: 2 | iteration: 113/262 | Loss: 0.6801875829696655Training Epoch: 2 | iteration: 114/262 | Loss: 0.6247776746749878Training Epoch: 2 | iteration: 115/262 | Loss: 0.6583555936813354Training Epoch: 2 | iteration: 116/262 | Loss: 0.6156112551689148Training Epoch: 2 | iteration: 117/262 | Loss: 0.6392720937728882Training Epoch: 2 | iteration: 118/262 | Loss: 0.6467053294181824Training Epoch: 2 | iteration: 119/262 | Loss: 0.6546593904495239Training Epoch: 2 | iteration: 120/262 | Loss: 0.6403995752334595Training Epoch: 2 | iteration: 121/262 | Loss: 0.676142692565918Training Epoch: 2 | iteration: 122/262 | Loss: 0.6183603405952454Training Epoch: 2 | iteration: 123/262 | Loss: 0.67087721824646Training Epoch: 2 | iteration: 124/262 | Loss: 0.6201673150062561Training Epoch: 2 | iteration: 125/262 | Loss: 0.6360145807266235Training Epoch: 2 | iteration: 126/262 | Loss: 0.6564792990684509Training Epoch: 2 | iteration: 127/262 | Loss: 0.5996572971343994Training Epoch: 2 | iteration: 128/262 | Loss: 0.6446554660797119Training Epoch: 2 | iteration: 129/262 | Loss: 0.6352803707122803Training Epoch: 2 | iteration: 130/262 | Loss: 0.695935070514679Training Epoch: 2 | iteration: 131/262 | Loss: 0.6126835346221924Training Epoch: 2 | iteration: 132/262 | Loss: 0.6357471942901611Training Epoch: 2 | iteration: 133/262 | Loss: 0.6552218198776245Training Epoch: 2 | iteration: 134/262 | Loss: 0.635593831539154Training Epoch: 2 | iteration: 135/262 | Loss: 0.5972536206245422Training Epoch: 2 | iteration: 136/262 | Loss: 0.6488953828811646Training Epoch: 2 | iteration: 137/262 | Loss: 0.7013134956359863Training Epoch: 2 | iteration: 138/262 | Loss: 0.6491140127182007Training Epoch: 2 | iteration: 139/262 | Loss: 0.6676568984985352Training Epoch: 2 | iteration: 140/262 | Loss: 0.5911529064178467Training Epoch: 2 | iteration: 141/262 | Loss: 0.6165206432342529Training Epoch: 2 | iteration: 142/262 | Loss: 0.6941431760787964Training Epoch: 2 | iteration: 143/262 | Loss: 0.6716513633728027Training Epoch: 2 | iteration: 144/262 | Loss: 0.6117669939994812Training Epoch: 2 | iteration: 145/262 | Loss: 0.6350494623184204Training Epoch: 2 | iteration: 146/262 | Loss: 0.6031616926193237Training Epoch: 2 | iteration: 147/262 | Loss: 0.6357588768005371Training Epoch: 2 | iteration: 148/262 | Loss: 0.6464016437530518Training Epoch: 2 | iteration: 149/262 | Loss: 0.6114243268966675Training Epoch: 2 | iteration: 150/262 | Loss: 0.6252182126045227Training Epoch: 2 | iteration: 151/262 | Loss: 0.6625467538833618Training Epoch: 2 | iteration: 152/262 | Loss: 0.6128347516059875Training Epoch: 2 | iteration: 153/262 | Loss: 0.6449339389801025Training Epoch: 2 | iteration: 154/262 | Loss: 0.608373761177063Training Epoch: 2 | iteration: 155/262 | Loss: 0.6410714387893677Training Epoch: 2 | iteration: 156/262 | Loss: 0.6560733914375305Training Epoch: 2 | iteration: 157/262 | Loss: 0.6568663120269775Training Epoch: 2 | iteration: 158/262 | Loss: 0.6099706292152405Training Epoch: 2 | iteration: 159/262 | Loss: 0.6733553409576416Training Epoch: 2 | iteration: 160/262 | Loss: 0.6676138639450073Training Epoch: 2 | iteration: 161/262 | Loss: 0.6147189140319824Training Epoch: 2 | iteration: 162/262 | Loss: 0.7034460306167603Training Epoch: 2 | iteration: 163/262 | Loss: 0.6391149759292603Training Epoch: 2 | iteration: 164/262 | Loss: 0.63242506980896Training Epoch: 2 | iteration: 165/262 | Loss: 0.65315842628479Training Epoch: 2 | iteration: 166/262 | Loss: 0.6975364685058594Training Epoch: 2 | iteration: 167/262 | Loss: 0.6634639501571655Training Epoch: 2 | iteration: 168/262 | Loss: 0.6606191396713257Training Epoch: 2 | iteration: 169/262 | Loss: 0.6747698187828064Training Epoch: 2 | iteration: 170/262 | Loss: 0.6198770999908447Training Epoch: 2 | iteration: 171/262 | Loss: 0.6793719530105591Training Epoch: 2 | iteration: 172/262 | Loss: 0.6576681733131409Training Epoch: 2 | iteration: 173/262 | Loss: 0.6332194209098816Training Epoch: 2 | iteration: 174/262 | Loss: 0.683050274848938Training Epoch: 2 | iteration: 175/262 | Loss: 0.5840314626693726Training Epoch: 2 | iteration: 176/262 | Loss: 0.5968821048736572Training Epoch: 2 | iteration: 177/262 | Loss: 0.647377610206604Training Epoch: 2 | iteration: 178/262 | Loss: 0.6206415295600891Training Epoch: 2 | iteration: 179/262 | Loss: 0.7023305296897888Training Epoch: 2 | iteration: 180/262 | Loss: 0.6841049194335938Training Epoch: 2 | iteration: 181/262 | Loss: 0.6421977281570435Training Epoch: 2 | iteration: 182/262 | Loss: 0.6264407634735107Training Epoch: 2 | iteration: 183/262 | Loss: 0.7124879360198975Training Epoch: 2 | iteration: 184/262 | Loss: 0.6723946332931519Training Epoch: 2 | iteration: 185/262 | Loss: 0.6357380151748657Training Epoch: 2 | iteration: 186/262 | Loss: 0.6323022246360779Training Epoch: 2 | iteration: 187/262 | Loss: 0.6712014675140381Training Epoch: 2 | iteration: 188/262 | Loss: 0.6308752298355103Training Epoch: 2 | iteration: 189/262 | Loss: 0.6491022706031799Training Epoch: 2 | iteration: 190/262 | Loss: 0.6393962502479553Training Epoch: 2 | iteration: 191/262 | Loss: 0.6076905727386475Training Epoch: 2 | iteration: 192/262 | Loss: 0.7061625719070435Training Epoch: 2 | iteration: 193/262 | Loss: 0.6231580972671509Training Epoch: 2 | iteration: 194/262 | Loss: 0.6215829253196716Training Epoch: 2 | iteration: 195/262 | Loss: 0.6363047957420349Training Epoch: 2 | iteration: 196/262 | Loss: 0.613353431224823Training Epoch: 2 | iteration: 197/262 | Loss: 0.5924503207206726Training Epoch: 2 | iteration: 198/262 | Loss: 0.6833811402320862Training Epoch: 2 | iteration: 199/262 | Loss: 0.6454505920410156Training Epoch: 2 | iteration: 200/262 | Loss: 0.6630173325538635Training Epoch: 2 | iteration: 201/262 | Loss: 0.6262130737304688Training Epoch: 2 | iteration: 202/262 | Loss: 0.666222095489502Training Epoch: 2 | iteration: 203/262 | Loss: 0.659806489944458Training Epoch: 2 | iteration: 204/262 | Loss: 0.5975986123085022Training Epoch: 2 | iteration: 205/262 | Loss: 0.7049078941345215Training Epoch: 2 | iteration: 206/262 | Loss: 0.6162879467010498Training Epoch: 2 | iteration: 207/262 | Loss: 0.6288984417915344Training Epoch: 2 | iteration: 208/262 | Loss: 0.6349084377288818Training Epoch: 2 | iteration: 209/262 | Loss: 0.6440702676773071Training Epoch: 2 | iteration: 210/262 | Loss: 0.5842515230178833Training Epoch: 2 | iteration: 211/262 | Loss: 0.7010773420333862Training Epoch: 2 | iteration: 212/262 | Loss: 0.6587048768997192Training Epoch: 2 | iteration: 213/262 | Loss: 0.6231212615966797Training Epoch: 2 | iteration: 214/262 | Loss: 0.6608240604400635Training Epoch: 2 | iteration: 215/262 | Loss: 0.6481772661209106Training Epoch: 2 | iteration: 216/262 | Loss: 0.6000642776489258Training Epoch: 2 | iteration: 217/262 | Loss: 0.708911120891571Training Epoch: 2 | iteration: 218/262 | Loss: 0.6168352365493774Training Epoch: 2 | iteration: 219/262 | Loss: 0.6740741729736328Training Epoch: 2 | iteration: 220/262 | Loss: 0.639360785484314Training Epoch: 2 | iteration: 221/262 | Loss: 0.6614454388618469Training Epoch: 2 | iteration: 222/262 | Loss: 0.6538511514663696Training Epoch: 2 | iteration: 223/262 | Loss: 0.6614488363265991Training Epoch: 2 | iteration: 224/262 | Loss: 0.6459699869155884Training Epoch: 2 | iteration: 225/262 | Loss: 0.6529026031494141Training Epoch: 2 | iteration: 226/262 | Loss: 0.6270136833190918Training Epoch: 2 | iteration: 227/262 | Loss: 0.6625206470489502Training Epoch: 2 | iteration: 228/262 | Loss: 0.6892952919006348Training Epoch: 2 | iteration: 229/262 | Loss: 0.6726434826850891Training Epoch: 2 | iteration: 230/262 | Loss: 0.6588329076766968Training Epoch: 2 | iteration: 231/262 | Loss: 0.6193902492523193Training Epoch: 2 | iteration: 232/262 | Loss: 0.7320209741592407Training Epoch: 2 | iteration: 233/262 | Loss: 0.6577721834182739Training Epoch: 2 | iteration: 234/262 | Loss: 0.6409205198287964Training Epoch: 2 | iteration: 235/262 | Loss: 0.614424467086792Training Epoch: 2 | iteration: 236/262 | Loss: 0.6314401030540466Training Epoch: 2 | iteration: 237/262 | Loss: 0.6704481840133667Training Epoch: 2 | iteration: 238/262 | Loss: 0.6876718401908875Training Epoch: 2 | iteration: 239/262 | Loss: 0.6268497705459595Training Epoch: 2 | iteration: 240/262 | Loss: 0.6257201433181763Training Epoch: 2 | iteration: 241/262 | Loss: 0.6212310194969177Training Epoch: 2 | iteration: 242/262 | Loss: 0.6816312074661255Training Epoch: 2 | iteration: 243/262 | Loss: 0.6808801889419556Training Epoch: 2 | iteration: 244/262 | Loss: 0.6397400498390198Training Epoch: 2 | iteration: 245/262 | Loss: 0.672437310218811Training Epoch: 2 | iteration: 246/262 | Loss: 0.6706041097640991Training Epoch: 2 | iteration: 247/262 | Loss: 0.6365579962730408Training Epoch: 2 | iteration: 248/262 | Loss: 0.639098584651947Training Epoch: 2 | iteration: 249/262 | Loss: 0.6805051565170288Training Epoch: 2 | iteration: 250/262 | Loss: 0.600813627243042Training Epoch: 2 | iteration: 251/262 | Loss: 0.7193694114685059Training Epoch: 2 | iteration: 252/262 | Loss: 0.6577566862106323Training Epoch: 2 | iteration: 253/262 | Loss: 0.6154653429985046Training Epoch: 2 | iteration: 254/262 | Loss: 0.6413679122924805Training Epoch: 2 | iteration: 255/262 | Loss: 0.6235935091972351Training Epoch: 2 | iteration: 256/262 | Loss: 0.6448889970779419Training Epoch: 2 | iteration: 257/262 | Loss: 0.6248341798782349Training Epoch: 2 | iteration: 258/262 | Loss: 0.6107765436172485Training Epoch: 2 | iteration: 259/262 | Loss: 0.656913697719574Training Epoch: 2 | iteration: 260/262 | Loss: 0.6955780982971191Training Epoch: 2 | iteration: 261/262 | Loss: 0.6694013476371765Validating Epoch: 2 | iteration: 0/66 | Loss: 0.6007486581802368Validating Epoch: 2 | iteration: 1/66 | Loss: 0.6096721887588501Validating Epoch: 2 | iteration: 2/66 | Loss: 0.6576366424560547Validating Epoch: 2 | iteration: 3/66 | Loss: 0.6384122371673584Validating Epoch: 2 | iteration: 4/66 | Loss: 0.6104966998100281Validating Epoch: 2 | iteration: 5/66 | Loss: 0.5669274926185608Validating Epoch: 2 | iteration: 6/66 | Loss: 0.5965802073478699Validating Epoch: 2 | iteration: 7/66 | Loss: 0.6258671283721924Validating Epoch: 2 | iteration: 8/66 | Loss: 0.5727599859237671Validating Epoch: 2 | iteration: 9/66 | Loss: 0.6039329767227173Validating Epoch: 2 | iteration: 10/66 | Loss: 0.6090977191925049Validating Epoch: 2 | iteration: 11/66 | Loss: 0.5507741570472717Validating Epoch: 2 | iteration: 12/66 | Loss: 0.5994763970375061Validating Epoch: 2 | iteration: 13/66 | Loss: 0.5776143074035645Validating Epoch: 2 | iteration: 14/66 | Loss: 0.6011968851089478Validating Epoch: 2 | iteration: 15/66 | Loss: 0.5888634920120239Validating Epoch: 2 | iteration: 16/66 | Loss: 0.5925899744033813Validating Epoch: 2 | iteration: 17/66 | Loss: 0.5979076623916626Validating Epoch: 2 | iteration: 18/66 | Loss: 0.5982635617256165Validating Epoch: 2 | iteration: 19/66 | Loss: 0.6341860294342041Validating Epoch: 2 | iteration: 20/66 | Loss: 0.6393415331840515Validating Epoch: 2 | iteration: 21/66 | Loss: 0.644883930683136Validating Epoch: 2 | iteration: 22/66 | Loss: 0.6046991348266602Validating Epoch: 2 | iteration: 23/66 | Loss: 0.615276038646698Validating Epoch: 2 | iteration: 24/66 | Loss: 0.5964109897613525Validating Epoch: 2 | iteration: 25/66 | Loss: 0.5970454812049866Validating Epoch: 2 | iteration: 26/66 | Loss: 0.5889752507209778Validating Epoch: 2 | iteration: 27/66 | Loss: 0.6049889922142029Validating Epoch: 2 | iteration: 28/66 | Loss: 0.5963892936706543Validating Epoch: 2 | iteration: 29/66 | Loss: 0.6368058323860168Validating Epoch: 2 | iteration: 30/66 | Loss: 0.6153925657272339Validating Epoch: 2 | iteration: 31/66 | Loss: 0.6163928508758545Validating Epoch: 2 | iteration: 32/66 | Loss: 0.67188560962677Validating Epoch: 2 | iteration: 33/66 | Loss: 0.5810495018959045Validating Epoch: 2 | iteration: 34/66 | Loss: 0.6054195761680603Validating Epoch: 2 | iteration: 35/66 | Loss: 0.6072708368301392Validating Epoch: 2 | iteration: 36/66 | Loss: 0.5973114967346191Validating Epoch: 2 | iteration: 37/66 | Loss: 0.6285194158554077Validating Epoch: 2 | iteration: 38/66 | Loss: 0.6240280866622925Validating Epoch: 2 | iteration: 39/66 | Loss: 0.6254838705062866Validating Epoch: 2 | iteration: 40/66 | Loss: 0.6120511293411255Validating Epoch: 2 | iteration: 41/66 | Loss: 0.6162489652633667Validating Epoch: 2 | iteration: 42/66 | Loss: 0.6234472990036011Validating Epoch: 2 | iteration: 43/66 | Loss: 0.6219582557678223Validating Epoch: 2 | iteration: 44/66 | Loss: 0.5943058729171753Validating Epoch: 2 | iteration: 45/66 | Loss: 0.6491349935531616Validating Epoch: 2 | iteration: 46/66 | Loss: 0.5778564214706421Validating Epoch: 2 | iteration: 47/66 | Loss: 0.5919974446296692Validating Epoch: 2 | iteration: 48/66 | Loss: 0.6304395794868469Validating Epoch: 2 | iteration: 49/66 | Loss: 0.6414434909820557Validating Epoch: 2 | iteration: 50/66 | Loss: 0.6176754236221313Validating Epoch: 2 | iteration: 51/66 | Loss: 0.6147600412368774Validating Epoch: 2 | iteration: 52/66 | Loss: 0.6190235018730164Validating Epoch: 2 | iteration: 53/66 | Loss: 0.5767968893051147Validating Epoch: 2 | iteration: 54/66 | Loss: 0.6295271515846252Validating Epoch: 2 | iteration: 55/66 | Loss: 0.5786275267601013Validating Epoch: 2 | iteration: 56/66 | Loss: 0.5942021012306213Validating Epoch: 2 | iteration: 57/66 | Loss: 0.6205273866653442Validating Epoch: 2 | iteration: 58/66 | Loss: 0.5996803641319275Validating Epoch: 2 | iteration: 59/66 | Loss: 0.6371779441833496Validating Epoch: 2 | iteration: 60/66 | Loss: 0.6220372915267944Validating Epoch: 2 | iteration: 61/66 | Loss: 0.5993790030479431Validating Epoch: 2 | iteration: 62/66 | Loss: 0.6274877786636353Validating Epoch: 2 | iteration: 63/66 | Loss: 0.598149836063385Validating Epoch: 2 | iteration: 64/66 | Loss: 0.5763925909996033Validating Epoch: 2 | iteration: 65/66 | Loss: 0.5900259017944336Temperature:  0.5
Sampling: iteration: 0/51{'Validity': 0.9833984375, 'Novelty': 1.0, 'Uniqueness': 0.9523336643495531}
Training Epoch: 3 | iteration: 0/262 | Loss: 0.606187105178833Training Epoch: 3 | iteration: 1/262 | Loss: 0.6152325868606567Training Epoch: 3 | iteration: 2/262 | Loss: 0.6339763402938843Training Epoch: 3 | iteration: 3/262 | Loss: 0.6717087030410767Training Epoch: 3 | iteration: 4/262 | Loss: 0.5991742610931396Training Epoch: 3 | iteration: 5/262 | Loss: 0.6172950267791748Training Epoch: 3 | iteration: 6/262 | Loss: 0.643149733543396Training Epoch: 3 | iteration: 7/262 | Loss: 0.5893489122390747Training Epoch: 3 | iteration: 8/262 | Loss: 0.6206014156341553Training Epoch: 3 | iteration: 9/262 | Loss: 0.6373443603515625Training Epoch: 3 | iteration: 10/262 | Loss: 0.7005623579025269Training Epoch: 3 | iteration: 11/262 | Loss: 0.6455363035202026Training Epoch: 3 | iteration: 12/262 | Loss: 0.6168040037155151Training Epoch: 3 | iteration: 13/262 | Loss: 0.6548733115196228Training Epoch: 3 | iteration: 14/262 | Loss: 0.6361311674118042Training Epoch: 3 | iteration: 15/262 | Loss: 0.6405089497566223Training Epoch: 3 | iteration: 16/262 | Loss: 0.6250840425491333Training Epoch: 3 | iteration: 17/262 | Loss: 0.6430633068084717Training Epoch: 3 | iteration: 18/262 | Loss: 0.6467242240905762Training Epoch: 3 | iteration: 19/262 | Loss: 0.6167538166046143Training Epoch: 3 | iteration: 20/262 | Loss: 0.6352096796035767Training Epoch: 3 | iteration: 21/262 | Loss: 0.6430608034133911Training Epoch: 3 | iteration: 22/262 | Loss: 0.6040937304496765Training Epoch: 3 | iteration: 23/262 | Loss: 0.5829765200614929Training Epoch: 3 | iteration: 24/262 | Loss: 0.6176958084106445Training Epoch: 3 | iteration: 25/262 | Loss: 0.6637686491012573Training Epoch: 3 | iteration: 26/262 | Loss: 0.6412550806999207Training Epoch: 3 | iteration: 27/262 | Loss: 0.6340672373771667Training Epoch: 3 | iteration: 28/262 | Loss: 0.6235237121582031Training Epoch: 3 | iteration: 29/262 | Loss: 0.5786749720573425Training Epoch: 3 | iteration: 30/262 | Loss: 0.6772593259811401Training Epoch: 3 | iteration: 31/262 | Loss: 0.6425226926803589Training Epoch: 3 | iteration: 32/262 | Loss: 0.5814492106437683Training Epoch: 3 | iteration: 33/262 | Loss: 0.6141860485076904Training Epoch: 3 | iteration: 34/262 | Loss: 0.6189395189285278Training Epoch: 3 | iteration: 35/262 | Loss: 0.6297885179519653Training Epoch: 3 | iteration: 36/262 | Loss: 0.6475480198860168Training Epoch: 3 | iteration: 37/262 | Loss: 0.6051819324493408Training Epoch: 3 | iteration: 38/262 | Loss: 0.6465474367141724Training Epoch: 3 | iteration: 39/262 | Loss: 0.6508954167366028Training Epoch: 3 | iteration: 40/262 | Loss: 0.5915395021438599Training Epoch: 3 | iteration: 41/262 | Loss: 0.6396547555923462Training Epoch: 3 | iteration: 42/262 | Loss: 0.6217858791351318Training Epoch: 3 | iteration: 43/262 | Loss: 0.5833959579467773Training Epoch: 3 | iteration: 44/262 | Loss: 0.5834026336669922Training Epoch: 3 | iteration: 45/262 | Loss: 0.6426487565040588Training Epoch: 3 | iteration: 46/262 | Loss: 0.6260266304016113Training Epoch: 3 | iteration: 47/262 | Loss: 0.5969806909561157Training Epoch: 3 | iteration: 48/262 | Loss: 0.6365270018577576Training Epoch: 3 | iteration: 49/262 | Loss: 0.6767386198043823Training Epoch: 3 | iteration: 50/262 | Loss: 0.6671050190925598Training Epoch: 3 | iteration: 51/262 | Loss: 0.6293233633041382Training Epoch: 3 | iteration: 52/262 | Loss: 0.5923916697502136Training Epoch: 3 | iteration: 53/262 | Loss: 0.6502677202224731Training Epoch: 3 | iteration: 54/262 | Loss: 0.6061082482337952Training Epoch: 3 | iteration: 55/262 | Loss: 0.5853890180587769Training Epoch: 3 | iteration: 56/262 | Loss: 0.6532745957374573Training Epoch: 3 | iteration: 57/262 | Loss: 0.6315269470214844Training Epoch: 3 | iteration: 58/262 | Loss: 0.5898300409317017Training Epoch: 3 | iteration: 59/262 | Loss: 0.5954512357711792Training Epoch: 3 | iteration: 60/262 | Loss: 0.6372904777526855Training Epoch: 3 | iteration: 61/262 | Loss: 0.6970279216766357Training Epoch: 3 | iteration: 62/262 | Loss: 0.6042606830596924Training Epoch: 3 | iteration: 63/262 | Loss: 0.6054872274398804Training Epoch: 3 | iteration: 64/262 | Loss: 0.5968238115310669Training Epoch: 3 | iteration: 65/262 | Loss: 0.6582127213478088Training Epoch: 3 | iteration: 66/262 | Loss: 0.6096728444099426Training Epoch: 3 | iteration: 67/262 | Loss: 0.6105348467826843Training Epoch: 3 | iteration: 68/262 | Loss: 0.6066479682922363Training Epoch: 3 | iteration: 69/262 | Loss: 0.5996642708778381Training Epoch: 3 | iteration: 70/262 | Loss: 0.6405360698699951Training Epoch: 3 | iteration: 71/262 | Loss: 0.6092444658279419Training Epoch: 3 | iteration: 72/262 | Loss: 0.6594656109809875Training Epoch: 3 | iteration: 73/262 | Loss: 0.6465458273887634Training Epoch: 3 | iteration: 74/262 | Loss: 0.6354459524154663Training Epoch: 3 | iteration: 75/262 | Loss: 0.6244449615478516Training Epoch: 3 | iteration: 76/262 | Loss: 0.6361868381500244Training Epoch: 3 | iteration: 77/262 | Loss: 0.6285990476608276Training Epoch: 3 | iteration: 78/262 | Loss: 0.6784149408340454Training Epoch: 3 | iteration: 79/262 | Loss: 0.6361412405967712Training Epoch: 3 | iteration: 80/262 | Loss: 0.6233545541763306Training Epoch: 3 | iteration: 81/262 | Loss: 0.6493171453475952Training Epoch: 3 | iteration: 82/262 | Loss: 0.6260736584663391Training Epoch: 3 | iteration: 83/262 | Loss: 0.5783213973045349Training Epoch: 3 | iteration: 84/262 | Loss: 0.6702710390090942Training Epoch: 3 | iteration: 85/262 | Loss: 0.6608138084411621Training Epoch: 3 | iteration: 86/262 | Loss: 0.6326353549957275Training Epoch: 3 | iteration: 87/262 | Loss: 0.6216595768928528Training Epoch: 3 | iteration: 88/262 | Loss: 0.7005207538604736Training Epoch: 3 | iteration: 89/262 | Loss: 0.6059709191322327Training Epoch: 3 | iteration: 90/262 | Loss: 0.6341792941093445Training Epoch: 3 | iteration: 91/262 | Loss: 0.6856489181518555Training Epoch: 3 | iteration: 92/262 | Loss: 0.6155811548233032Training Epoch: 3 | iteration: 93/262 | Loss: 0.6106406450271606Training Epoch: 3 | iteration: 94/262 | Loss: 0.597913384437561Training Epoch: 3 | iteration: 95/262 | Loss: 0.5904430150985718Training Epoch: 3 | iteration: 96/262 | Loss: 0.6084092855453491Training Epoch: 3 | iteration: 97/262 | Loss: 0.6894199848175049Training Epoch: 3 | iteration: 98/262 | Loss: 0.6305283308029175Training Epoch: 3 | iteration: 99/262 | Loss: 0.5923047065734863Training Epoch: 3 | iteration: 100/262 | Loss: 0.6038652062416077Training Epoch: 3 | iteration: 101/262 | Loss: 0.6618525385856628Training Epoch: 3 | iteration: 102/262 | Loss: 0.664823055267334Training Epoch: 3 | iteration: 103/262 | Loss: 0.6159912347793579Training Epoch: 3 | iteration: 104/262 | Loss: 0.6652822494506836Training Epoch: 3 | iteration: 105/262 | Loss: 0.6207016706466675Training Epoch: 3 | iteration: 106/262 | Loss: 0.6405607461929321Training Epoch: 3 | iteration: 107/262 | Loss: 0.6312030553817749Training Epoch: 3 | iteration: 108/262 | Loss: 0.582451343536377Training Epoch: 3 | iteration: 109/262 | Loss: 0.623524010181427Training Epoch: 3 | iteration: 110/262 | Loss: 0.6163235902786255Training Epoch: 3 | iteration: 111/262 | Loss: 0.6744004487991333Training Epoch: 3 | iteration: 112/262 | Loss: 0.6211204528808594Training Epoch: 3 | iteration: 113/262 | Loss: 0.5982515215873718Training Epoch: 3 | iteration: 114/262 | Loss: 0.6142376065254211Training Epoch: 3 | iteration: 115/262 | Loss: 0.6582799553871155Training Epoch: 3 | iteration: 116/262 | Loss: 0.6193892955780029Training Epoch: 3 | iteration: 117/262 | Loss: 0.607500433921814Training Epoch: 3 | iteration: 118/262 | Loss: 0.5631547570228577Training Epoch: 3 | iteration: 119/262 | Loss: 0.6447752118110657Training Epoch: 3 | iteration: 120/262 | Loss: 0.6429451704025269Training Epoch: 3 | iteration: 121/262 | Loss: 0.6326579451560974Training Epoch: 3 | iteration: 122/262 | Loss: 0.628463864326477Training Epoch: 3 | iteration: 123/262 | Loss: 0.683388352394104Training Epoch: 3 | iteration: 124/262 | Loss: 0.6300176382064819Training Epoch: 3 | iteration: 125/262 | Loss: 0.6189285516738892Training Epoch: 3 | iteration: 126/262 | Loss: 0.5731263160705566Training Epoch: 3 | iteration: 127/262 | Loss: 0.6793970465660095Training Epoch: 3 | iteration: 128/262 | Loss: 0.6309358477592468Training Epoch: 3 | iteration: 129/262 | Loss: 0.6008647680282593Training Epoch: 3 | iteration: 130/262 | Loss: 0.5660868883132935Training Epoch: 3 | iteration: 131/262 | Loss: 0.6260369420051575Training Epoch: 3 | iteration: 132/262 | Loss: 0.6093432903289795Training Epoch: 3 | iteration: 133/262 | Loss: 0.6838808059692383Training Epoch: 3 | iteration: 134/262 | Loss: 0.6438668966293335Training Epoch: 3 | iteration: 135/262 | Loss: 0.5981357097625732Training Epoch: 3 | iteration: 136/262 | Loss: 0.60175621509552Training Epoch: 3 | iteration: 137/262 | Loss: 0.5972080230712891Training Epoch: 3 | iteration: 138/262 | Loss: 0.6250341534614563Training Epoch: 3 | iteration: 139/262 | Loss: 0.6540908813476562Training Epoch: 3 | iteration: 140/262 | Loss: 0.5936636924743652Training Epoch: 3 | iteration: 141/262 | Loss: 0.6115372776985168Training Epoch: 3 | iteration: 142/262 | Loss: 0.672694206237793Training Epoch: 3 | iteration: 143/262 | Loss: 0.5803333520889282Training Epoch: 3 | iteration: 144/262 | Loss: 0.6374714374542236Training Epoch: 3 | iteration: 145/262 | Loss: 0.6275936365127563Training Epoch: 3 | iteration: 146/262 | Loss: 0.6458024978637695Training Epoch: 3 | iteration: 147/262 | Loss: 0.6645894050598145Training Epoch: 3 | iteration: 148/262 | Loss: 0.6680703163146973Training Epoch: 3 | iteration: 149/262 | Loss: 0.6418694257736206Training Epoch: 3 | iteration: 150/262 | Loss: 0.5844412446022034Training Epoch: 3 | iteration: 151/262 | Loss: 0.6429444551467896Training Epoch: 3 | iteration: 152/262 | Loss: 0.6547278165817261Training Epoch: 3 | iteration: 153/262 | Loss: 0.6833474636077881Training Epoch: 3 | iteration: 154/262 | Loss: 0.6487319469451904Training Epoch: 3 | iteration: 155/262 | Loss: 0.6130362749099731Training Epoch: 3 | iteration: 156/262 | Loss: 0.5993184447288513Training Epoch: 3 | iteration: 157/262 | Loss: 0.6835613250732422Training Epoch: 3 | iteration: 158/262 | Loss: 0.6465493440628052Training Epoch: 3 | iteration: 159/262 | Loss: 0.6129775047302246Training Epoch: 3 | iteration: 160/262 | Loss: 0.6238960027694702Training Epoch: 3 | iteration: 161/262 | Loss: 0.6286773681640625Training Epoch: 3 | iteration: 162/262 | Loss: 0.6606366634368896Training Epoch: 3 | iteration: 163/262 | Loss: 0.6266318559646606Training Epoch: 3 | iteration: 164/262 | Loss: 0.6398220062255859Training Epoch: 3 | iteration: 165/262 | Loss: 0.6691209077835083Training Epoch: 3 | iteration: 166/262 | Loss: 0.6129341125488281Training Epoch: 3 | iteration: 167/262 | Loss: 0.666648805141449Training Epoch: 3 | iteration: 168/262 | Loss: 0.5469608306884766Training Epoch: 3 | iteration: 169/262 | Loss: 0.6095327138900757Training Epoch: 3 | iteration: 170/262 | Loss: 0.5949552059173584Training Epoch: 3 | iteration: 171/262 | Loss: 0.6279096603393555Training Epoch: 3 | iteration: 172/262 | Loss: 0.6435908675193787Training Epoch: 3 | iteration: 173/262 | Loss: 0.6445529460906982Training Epoch: 3 | iteration: 174/262 | Loss: 0.6269406080245972Training Epoch: 3 | iteration: 175/262 | Loss: 0.6268760561943054Training Epoch: 3 | iteration: 176/262 | Loss: 0.6631613373756409Training Epoch: 3 | iteration: 177/262 | Loss: 0.6504112482070923Training Epoch: 3 | iteration: 178/262 | Loss: 0.5907790064811707Training Epoch: 3 | iteration: 179/262 | Loss: 0.6458169221878052Training Epoch: 3 | iteration: 180/262 | Loss: 0.6077844500541687Training Epoch: 3 | iteration: 181/262 | Loss: 0.6199619770050049Training Epoch: 3 | iteration: 182/262 | Loss: 0.6839686036109924Training Epoch: 3 | iteration: 183/262 | Loss: 0.637332022190094Training Epoch: 3 | iteration: 184/262 | Loss: 0.5939725041389465Training Epoch: 3 | iteration: 185/262 | Loss: 0.6387559175491333Training Epoch: 3 | iteration: 186/262 | Loss: 0.616513729095459Training Epoch: 3 | iteration: 187/262 | Loss: 0.6235793232917786Training Epoch: 3 | iteration: 188/262 | Loss: 0.667835533618927Training Epoch: 3 | iteration: 189/262 | Loss: 0.5940094590187073Training Epoch: 3 | iteration: 190/262 | Loss: 0.5915560722351074Training Epoch: 3 | iteration: 191/262 | Loss: 0.6457691788673401Training Epoch: 3 | iteration: 192/262 | Loss: 0.5940123796463013Training Epoch: 3 | iteration: 193/262 | Loss: 0.6308749914169312Training Epoch: 3 | iteration: 194/262 | Loss: 0.6076639890670776Training Epoch: 3 | iteration: 195/262 | Loss: 0.5904682874679565Training Epoch: 3 | iteration: 196/262 | Loss: 0.6571673154830933Training Epoch: 3 | iteration: 197/262 | Loss: 0.5904839038848877Training Epoch: 3 | iteration: 198/262 | Loss: 0.6739886999130249Training Epoch: 3 | iteration: 199/262 | Loss: 0.6434422135353088Training Epoch: 3 | iteration: 200/262 | Loss: 0.5983023047447205Training Epoch: 3 | iteration: 201/262 | Loss: 0.6455036401748657Training Epoch: 3 | iteration: 202/262 | Loss: 0.624992847442627Training Epoch: 3 | iteration: 203/262 | Loss: 0.6682058572769165Training Epoch: 3 | iteration: 204/262 | Loss: 0.6403917670249939Training Epoch: 3 | iteration: 205/262 | Loss: 0.6587510704994202Training Epoch: 3 | iteration: 206/262 | Loss: 0.6367776393890381Training Epoch: 3 | iteration: 207/262 | Loss: 0.6266855001449585Training Epoch: 3 | iteration: 208/262 | Loss: 0.5805804133415222Training Epoch: 3 | iteration: 209/262 | Loss: 0.6087028980255127Training Epoch: 3 | iteration: 210/262 | Loss: 0.5858765244483948Training Epoch: 3 | iteration: 211/262 | Loss: 0.6420388221740723Training Epoch: 3 | iteration: 212/262 | Loss: 0.6587165594100952Training Epoch: 3 | iteration: 213/262 | Loss: 0.6622723340988159Training Epoch: 3 | iteration: 214/262 | Loss: 0.5753566026687622Training Epoch: 3 | iteration: 215/262 | Loss: 0.7134473919868469Training Epoch: 3 | iteration: 216/262 | Loss: 0.642188549041748Training Epoch: 3 | iteration: 217/262 | Loss: 0.6500238180160522Training Epoch: 3 | iteration: 218/262 | Loss: 0.6369765400886536Training Epoch: 3 | iteration: 219/262 | Loss: 0.6359347105026245Training Epoch: 3 | iteration: 220/262 | Loss: 0.6114896535873413Training Epoch: 3 | iteration: 221/262 | Loss: 0.6240749359130859Training Epoch: 3 | iteration: 222/262 | Loss: 0.6281766891479492Training Epoch: 3 | iteration: 223/262 | Loss: 0.5985580682754517Training Epoch: 3 | iteration: 224/262 | Loss: 0.6182702779769897Training Epoch: 3 | iteration: 225/262 | Loss: 0.6323845386505127Training Epoch: 3 | iteration: 226/262 | Loss: 0.6212805509567261Training Epoch: 3 | iteration: 227/262 | Loss: 0.6390325427055359Training Epoch: 3 | iteration: 228/262 | Loss: 0.6255377531051636Training Epoch: 3 | iteration: 229/262 | Loss: 0.6066938638687134Training Epoch: 3 | iteration: 230/262 | Loss: 0.6430366039276123Training Epoch: 3 | iteration: 231/262 | Loss: 0.6754228472709656Training Epoch: 3 | iteration: 232/262 | Loss: 0.6202586889266968Training Epoch: 3 | iteration: 233/262 | Loss: 0.6676568984985352Training Epoch: 3 | iteration: 234/262 | Loss: 0.6291882395744324Training Epoch: 3 | iteration: 235/262 | Loss: 0.6713690161705017Training Epoch: 3 | iteration: 236/262 | Loss: 0.6283618807792664Training Epoch: 3 | iteration: 237/262 | Loss: 0.6847090125083923Training Epoch: 3 | iteration: 238/262 | Loss: 0.6325138807296753Training Epoch: 3 | iteration: 239/262 | Loss: 0.6663798689842224Training Epoch: 3 | iteration: 240/262 | Loss: 0.6483138203620911Training Epoch: 3 | iteration: 241/262 | Loss: 0.6239575147628784Training Epoch: 3 | iteration: 242/262 | Loss: 0.6032700538635254Training Epoch: 3 | iteration: 243/262 | Loss: 0.6132681965827942Training Epoch: 3 | iteration: 244/262 | Loss: 0.6536380052566528Training Epoch: 3 | iteration: 245/262 | Loss: 0.6383310556411743Training Epoch: 3 | iteration: 246/262 | Loss: 0.6919103860855103Training Epoch: 3 | iteration: 247/262 | Loss: 0.5887497067451477Training Epoch: 3 | iteration: 248/262 | Loss: 0.6661618947982788Training Epoch: 3 | iteration: 249/262 | Loss: 0.6253499984741211Training Epoch: 3 | iteration: 250/262 | Loss: 0.6029955148696899Training Epoch: 3 | iteration: 251/262 | Loss: 0.6242779493331909Training Epoch: 3 | iteration: 252/262 | Loss: 0.6411641836166382Training Epoch: 3 | iteration: 253/262 | Loss: 0.629378616809845Training Epoch: 3 | iteration: 254/262 | Loss: 0.6494088768959045Training Epoch: 3 | iteration: 255/262 | Loss: 0.6652017831802368Training Epoch: 3 | iteration: 256/262 | Loss: 0.6507651805877686Training Epoch: 3 | iteration: 257/262 | Loss: 0.6825175285339355Training Epoch: 3 | iteration: 258/262 | Loss: 0.624749481678009Training Epoch: 3 | iteration: 259/262 | Loss: 0.6611648797988892Training Epoch: 3 | iteration: 260/262 | Loss: 0.6454803347587585Training Epoch: 3 | iteration: 261/262 | Loss: 0.4988672137260437Validating Epoch: 3 | iteration: 0/66 | Loss: 0.5606159567832947Validating Epoch: 3 | iteration: 1/66 | Loss: 0.6003934144973755Validating Epoch: 3 | iteration: 2/66 | Loss: 0.6209307312965393Validating Epoch: 3 | iteration: 3/66 | Loss: 0.5814114809036255Validating Epoch: 3 | iteration: 4/66 | Loss: 0.6293729543685913Validating Epoch: 3 | iteration: 5/66 | Loss: 0.6382725238800049Validating Epoch: 3 | iteration: 6/66 | Loss: 0.6241714954376221Validating Epoch: 3 | iteration: 7/66 | Loss: 0.5943028926849365Validating Epoch: 3 | iteration: 8/66 | Loss: 0.5933001041412354Validating Epoch: 3 | iteration: 9/66 | Loss: 0.5680122375488281Validating Epoch: 3 | iteration: 10/66 | Loss: 0.5843656063079834Validating Epoch: 3 | iteration: 11/66 | Loss: 0.6401956081390381Validating Epoch: 3 | iteration: 12/66 | Loss: 0.6311882734298706Validating Epoch: 3 | iteration: 13/66 | Loss: 0.6401264071464539Validating Epoch: 3 | iteration: 14/66 | Loss: 0.5735045671463013Validating Epoch: 3 | iteration: 15/66 | Loss: 0.570054829120636Validating Epoch: 3 | iteration: 16/66 | Loss: 0.5910772085189819Validating Epoch: 3 | iteration: 17/66 | Loss: 0.6139004826545715Validating Epoch: 3 | iteration: 18/66 | Loss: 0.6140624284744263Validating Epoch: 3 | iteration: 19/66 | Loss: 0.5569545030593872Validating Epoch: 3 | iteration: 20/66 | Loss: 0.5949789881706238Validating Epoch: 3 | iteration: 21/66 | Loss: 0.620452880859375Validating Epoch: 3 | iteration: 22/66 | Loss: 0.5860797166824341Validating Epoch: 3 | iteration: 23/66 | Loss: 0.61573326587677Validating Epoch: 3 | iteration: 24/66 | Loss: 0.6425285339355469Validating Epoch: 3 | iteration: 25/66 | Loss: 0.6111102104187012Validating Epoch: 3 | iteration: 26/66 | Loss: 0.6199926137924194Validating Epoch: 3 | iteration: 27/66 | Loss: 0.5859540700912476Validating Epoch: 3 | iteration: 28/66 | Loss: 0.6139689087867737Validating Epoch: 3 | iteration: 29/66 | Loss: 0.6093473434448242Validating Epoch: 3 | iteration: 30/66 | Loss: 0.6296350955963135Validating Epoch: 3 | iteration: 31/66 | Loss: 0.6085553169250488Validating Epoch: 3 | iteration: 32/66 | Loss: 0.5814303159713745Validating Epoch: 3 | iteration: 33/66 | Loss: 0.5618783831596375Validating Epoch: 3 | iteration: 34/66 | Loss: 0.6183577179908752Validating Epoch: 3 | iteration: 35/66 | Loss: 0.5809005498886108Validating Epoch: 3 | iteration: 36/66 | Loss: 0.6108081340789795Validating Epoch: 3 | iteration: 37/66 | Loss: 0.6287487745285034Validating Epoch: 3 | iteration: 38/66 | Loss: 0.5988274812698364Validating Epoch: 3 | iteration: 39/66 | Loss: 0.5817315578460693Validating Epoch: 3 | iteration: 40/66 | Loss: 0.5846325755119324Validating Epoch: 3 | iteration: 41/66 | Loss: 0.5977665185928345Validating Epoch: 3 | iteration: 42/66 | Loss: 0.5961548686027527Validating Epoch: 3 | iteration: 43/66 | Loss: 0.6138901710510254Validating Epoch: 3 | iteration: 44/66 | Loss: 0.6104494333267212Validating Epoch: 3 | iteration: 45/66 | Loss: 0.5947667360305786Validating Epoch: 3 | iteration: 46/66 | Loss: 0.5460363626480103Validating Epoch: 3 | iteration: 47/66 | Loss: 0.6074820756912231Validating Epoch: 3 | iteration: 48/66 | Loss: 0.5998460054397583Validating Epoch: 3 | iteration: 49/66 | Loss: 0.5682706832885742Validating Epoch: 3 | iteration: 50/66 | Loss: 0.6175345182418823Validating Epoch: 3 | iteration: 51/66 | Loss: 0.5809465646743774Validating Epoch: 3 | iteration: 52/66 | Loss: 0.5820950865745544Validating Epoch: 3 | iteration: 53/66 | Loss: 0.6048634648323059Validating Epoch: 3 | iteration: 54/66 | Loss: 0.6128207445144653Validating Epoch: 3 | iteration: 55/66 | Loss: 0.5744791030883789Validating Epoch: 3 | iteration: 56/66 | Loss: 0.5957372188568115Validating Epoch: 3 | iteration: 57/66 | Loss: 0.593906581401825Validating Epoch: 3 | iteration: 58/66 | Loss: 0.5849529504776001Validating Epoch: 3 | iteration: 59/66 | Loss: 0.6139932870864868Validating Epoch: 3 | iteration: 60/66 | Loss: 0.6031373143196106Validating Epoch: 3 | iteration: 61/66 | Loss: 0.5668278336524963Validating Epoch: 3 | iteration: 62/66 | Loss: 0.59772789478302Validating Epoch: 3 | iteration: 63/66 | Loss: 0.5587918758392334Validating Epoch: 3 | iteration: 64/66 | Loss: 0.5694800615310669Validating Epoch: 3 | iteration: 65/66 | Loss: 0.6423100233078003Temperature:  0.5
Sampling: iteration: 0/51{'Validity': 0.982421875, 'Novelty': 1.0, 'Uniqueness': 0.941351888667992}
Training Epoch: 4 | iteration: 0/262 | Loss: 0.5943127870559692Training Epoch: 4 | iteration: 1/262 | Loss: 0.6521579027175903Training Epoch: 4 | iteration: 2/262 | Loss: 0.5918034911155701Training Epoch: 4 | iteration: 3/262 | Loss: 0.6384482383728027Training Epoch: 4 | iteration: 4/262 | Loss: 0.592883288860321Training Epoch: 4 | iteration: 5/262 | Loss: 0.6600233316421509Training Epoch: 4 | iteration: 6/262 | Loss: 0.6244609951972961Training Epoch: 4 | iteration: 7/262 | Loss: 0.6295919418334961Training Epoch: 4 | iteration: 8/262 | Loss: 0.5718991756439209Training Epoch: 4 | iteration: 9/262 | Loss: 0.5929606556892395Training Epoch: 4 | iteration: 10/262 | Loss: 0.5874561667442322Training Epoch: 4 | iteration: 11/262 | Loss: 0.6059454083442688Training Epoch: 4 | iteration: 12/262 | Loss: 0.5808355212211609Training Epoch: 4 | iteration: 13/262 | Loss: 0.6701946258544922Training Epoch: 4 | iteration: 14/262 | Loss: 0.581194281578064Training Epoch: 4 | iteration: 15/262 | Loss: 0.6469623446464539Training Epoch: 4 | iteration: 16/262 | Loss: 0.6020107269287109Training Epoch: 4 | iteration: 17/262 | Loss: 0.588760495185852Training Epoch: 4 | iteration: 18/262 | Loss: 0.6413268446922302Training Epoch: 4 | iteration: 19/262 | Loss: 0.6209698915481567Training Epoch: 4 | iteration: 20/262 | Loss: 0.5774385929107666Training Epoch: 4 | iteration: 21/262 | Loss: 0.6079641580581665Training Epoch: 4 | iteration: 22/262 | Loss: 0.6335024833679199Training Epoch: 4 | iteration: 23/262 | Loss: 0.6212130784988403Training Epoch: 4 | iteration: 24/262 | Loss: 0.5797775387763977Training Epoch: 4 | iteration: 25/262 | Loss: 0.6314208507537842Training Epoch: 4 | iteration: 26/262 | Loss: 0.6148371696472168Training Epoch: 4 | iteration: 27/262 | Loss: 0.6255481243133545Training Epoch: 4 | iteration: 28/262 | Loss: 0.6008238792419434Training Epoch: 4 | iteration: 29/262 | Loss: 0.6860085725784302Training Epoch: 4 | iteration: 30/262 | Loss: 0.5895062685012817Training Epoch: 4 | iteration: 31/262 | Loss: 0.5756012201309204Training Epoch: 4 | iteration: 32/262 | Loss: 0.6108167171478271Training Epoch: 4 | iteration: 33/262 | Loss: 0.6172767877578735Training Epoch: 4 | iteration: 34/262 | Loss: 0.6125597953796387Training Epoch: 4 | iteration: 35/262 | Loss: 0.554413378238678Training Epoch: 4 | iteration: 36/262 | Loss: 0.6867205500602722Training Epoch: 4 | iteration: 37/262 | Loss: 0.582058310508728Training Epoch: 4 | iteration: 38/262 | Loss: 0.6525536775588989Training Epoch: 4 | iteration: 39/262 | Loss: 0.6060621738433838Training Epoch: 4 | iteration: 40/262 | Loss: 0.5810270309448242Training Epoch: 4 | iteration: 41/262 | Loss: 0.6241089701652527Training Epoch: 4 | iteration: 42/262 | Loss: 0.594017505645752Training Epoch: 4 | iteration: 43/262 | Loss: 0.6133707761764526Training Epoch: 4 | iteration: 44/262 | Loss: 0.6431031823158264Training Epoch: 4 | iteration: 45/262 | Loss: 0.6406881213188171Training Epoch: 4 | iteration: 46/262 | Loss: 0.5872635841369629Training Epoch: 4 | iteration: 47/262 | Loss: 0.6284199953079224Training Epoch: 4 | iteration: 48/262 | Loss: 0.5789149403572083Training Epoch: 4 | iteration: 49/262 | Loss: 0.664436936378479Training Epoch: 4 | iteration: 50/262 | Loss: 0.6571015119552612Training Epoch: 4 | iteration: 51/262 | Loss: 0.5889191627502441Training Epoch: 4 | iteration: 52/262 | Loss: 0.6226440668106079Training Epoch: 4 | iteration: 53/262 | Loss: 0.6242746114730835Training Epoch: 4 | iteration: 54/262 | Loss: 0.5949382185935974Training Epoch: 4 | iteration: 55/262 | Loss: 0.6151241064071655Training Epoch: 4 | iteration: 56/262 | Loss: 0.5995416641235352Training Epoch: 4 | iteration: 57/262 | Loss: 0.5800862908363342Training Epoch: 4 | iteration: 58/262 | Loss: 0.6080615520477295Training Epoch: 4 | iteration: 59/262 | Loss: 0.6522035002708435Training Epoch: 4 | iteration: 60/262 | Loss: 0.6210001707077026Training Epoch: 4 | iteration: 61/262 | Loss: 0.593950629234314Training Epoch: 4 | iteration: 62/262 | Loss: 0.6160331964492798Training Epoch: 4 | iteration: 63/262 | Loss: 0.663092851638794Training Epoch: 4 | iteration: 64/262 | Loss: 0.6526061296463013Training Epoch: 4 | iteration: 65/262 | Loss: 0.5844822525978088Training Epoch: 4 | iteration: 66/262 | Loss: 0.6701737642288208Training Epoch: 4 | iteration: 67/262 | Loss: 0.6176053285598755Training Epoch: 4 | iteration: 68/262 | Loss: 0.6152133941650391Training Epoch: 4 | iteration: 69/262 | Loss: 0.6411936283111572Training Epoch: 4 | iteration: 70/262 | Loss: 0.5977786779403687Training Epoch: 4 | iteration: 71/262 | Loss: 0.5674490332603455Training Epoch: 4 | iteration: 72/262 | Loss: 0.6194930076599121Training Epoch: 4 | iteration: 73/262 | Loss: 0.6289973855018616Training Epoch: 4 | iteration: 74/262 | Loss: 0.6225497722625732Training Epoch: 4 | iteration: 75/262 | Loss: 0.6001590490341187Training Epoch: 4 | iteration: 76/262 | Loss: 0.6058570146560669Training Epoch: 4 | iteration: 77/262 | Loss: 0.6178309917449951Training Epoch: 4 | iteration: 78/262 | Loss: 0.5766745209693909Training Epoch: 4 | iteration: 79/262 | Loss: 0.5821583271026611Training Epoch: 4 | iteration: 80/262 | Loss: 0.591022253036499Training Epoch: 4 | iteration: 81/262 | Loss: 0.619138240814209Training Epoch: 4 | iteration: 82/262 | Loss: 0.5778565406799316Training Epoch: 4 | iteration: 83/262 | Loss: 0.6601603031158447Training Epoch: 4 | iteration: 84/262 | Loss: 0.6609870195388794Training Epoch: 4 | iteration: 85/262 | Loss: 0.6430258750915527Training Epoch: 4 | iteration: 86/262 | Loss: 0.6294466257095337Training Epoch: 4 | iteration: 87/262 | Loss: 0.6086575388908386Training Epoch: 4 | iteration: 88/262 | Loss: 0.555528998374939Training Epoch: 4 | iteration: 89/262 | Loss: 0.6171345710754395Training Epoch: 4 | iteration: 90/262 | Loss: 0.5963879227638245Training Epoch: 4 | iteration: 91/262 | Loss: 0.6532282829284668Training Epoch: 4 | iteration: 92/262 | Loss: 0.5937867760658264Training Epoch: 4 | iteration: 93/262 | Loss: 0.6115239858627319Training Epoch: 4 | iteration: 94/262 | Loss: 0.6026166081428528Training Epoch: 4 | iteration: 95/262 | Loss: 0.6208142638206482Training Epoch: 4 | iteration: 96/262 | Loss: 0.6346059441566467Training Epoch: 4 | iteration: 97/262 | Loss: 0.6088874340057373Training Epoch: 4 | iteration: 98/262 | Loss: 0.5981143712997437Training Epoch: 4 | iteration: 99/262 | Loss: 0.5929507613182068Training Epoch: 4 | iteration: 100/262 | Loss: 0.6330094933509827Training Epoch: 4 | iteration: 101/262 | Loss: 0.603501558303833Training Epoch: 4 | iteration: 102/262 | Loss: 0.5877286195755005Training Epoch: 4 | iteration: 103/262 | Loss: 0.5840073823928833Training Epoch: 4 | iteration: 104/262 | Loss: 0.650848388671875Training Epoch: 4 | iteration: 105/262 | Loss: 0.6737716197967529Training Epoch: 4 | iteration: 106/262 | Loss: 0.6128595471382141Training Epoch: 4 | iteration: 107/262 | Loss: 0.5923810005187988Training Epoch: 4 | iteration: 108/262 | Loss: 0.5960499048233032Training Epoch: 4 | iteration: 109/262 | Loss: 0.5981156826019287Training Epoch: 4 | iteration: 110/262 | Loss: 0.5742694735527039Training Epoch: 4 | iteration: 111/262 | Loss: 0.5891995429992676Training Epoch: 4 | iteration: 112/262 | Loss: 0.6013376712799072Training Epoch: 4 | iteration: 113/262 | Loss: 0.6303929686546326Training Epoch: 4 | iteration: 114/262 | Loss: 0.6481631994247437Training Epoch: 4 | iteration: 115/262 | Loss: 0.583202600479126Training Epoch: 4 | iteration: 116/262 | Loss: 0.6158720254898071Training Epoch: 4 | iteration: 117/262 | Loss: 0.6160647869110107Training Epoch: 4 | iteration: 118/262 | Loss: 0.5897002816200256Training Epoch: 4 | iteration: 119/262 | Loss: 0.5838655233383179Training Epoch: 4 | iteration: 120/262 | Loss: 0.656085729598999Training Epoch: 4 | iteration: 121/262 | Loss: 0.617831289768219Training Epoch: 4 | iteration: 122/262 | Loss: 0.6361987590789795Training Epoch: 4 | iteration: 123/262 | Loss: 0.591854453086853Training Epoch: 4 | iteration: 124/262 | Loss: 0.6090983152389526Training Epoch: 4 | iteration: 125/262 | Loss: 0.6242172718048096Training Epoch: 4 | iteration: 126/262 | Loss: 0.5846477746963501Training Epoch: 4 | iteration: 127/262 | Loss: 0.6034961938858032Training Epoch: 4 | iteration: 128/262 | Loss: 0.6407049298286438Training Epoch: 4 | iteration: 129/262 | Loss: 0.5887881517410278Training Epoch: 4 | iteration: 130/262 | Loss: 0.5820436477661133Training Epoch: 4 | iteration: 131/262 | Loss: 0.5541068315505981Training Epoch: 4 | iteration: 132/262 | Loss: 0.6171751618385315Training Epoch: 4 | iteration: 133/262 | Loss: 0.6194425225257874Training Epoch: 4 | iteration: 134/262 | Loss: 0.6428041458129883Training Epoch: 4 | iteration: 135/262 | Loss: 0.621545672416687Training Epoch: 4 | iteration: 136/262 | Loss: 0.5876134634017944Training Epoch: 4 | iteration: 137/262 | Loss: 0.6014498472213745Training Epoch: 4 | iteration: 138/262 | Loss: 0.5894359350204468Training Epoch: 4 | iteration: 139/262 | Loss: 0.6155821084976196Training Epoch: 4 | iteration: 140/262 | Loss: 0.6649937033653259Training Epoch: 4 | iteration: 141/262 | Loss: 0.636473536491394Training Epoch: 4 | iteration: 142/262 | Loss: 0.5845551490783691Training Epoch: 4 | iteration: 143/262 | Loss: 0.6115294098854065Training Epoch: 4 | iteration: 144/262 | Loss: 0.6232044696807861Training Epoch: 4 | iteration: 145/262 | Loss: 0.5797543525695801Training Epoch: 4 | iteration: 146/262 | Loss: 0.5899972319602966Training Epoch: 4 | iteration: 147/262 | Loss: 0.6253644227981567Training Epoch: 4 | iteration: 148/262 | Loss: 0.6288636922836304Training Epoch: 4 | iteration: 149/262 | Loss: 0.60432368516922Training Epoch: 4 | iteration: 150/262 | Loss: 0.6163345575332642Training Epoch: 4 | iteration: 151/262 | Loss: 0.6431418657302856Training Epoch: 4 | iteration: 152/262 | Loss: 0.6207351684570312Training Epoch: 4 | iteration: 153/262 | Loss: 0.5855969190597534Training Epoch: 4 | iteration: 154/262 | Loss: 0.6603579521179199Training Epoch: 4 | iteration: 155/262 | Loss: 0.6106482744216919Training Epoch: 4 | iteration: 156/262 | Loss: 0.6292036175727844Training Epoch: 4 | iteration: 157/262 | Loss: 0.6475348472595215Training Epoch: 4 | iteration: 158/262 | Loss: 0.6388151049613953Training Epoch: 4 | iteration: 159/262 | Loss: 0.6051430106163025Training Epoch: 4 | iteration: 160/262 | Loss: 0.6254208087921143Training Epoch: 4 | iteration: 161/262 | Loss: 0.6910108327865601Training Epoch: 4 | iteration: 162/262 | Loss: 0.6033244132995605Training Epoch: 4 | iteration: 163/262 | Loss: 0.580028772354126Training Epoch: 4 | iteration: 164/262 | Loss: 0.629378080368042Training Epoch: 4 | iteration: 165/262 | Loss: 0.6475657224655151Training Epoch: 4 | iteration: 166/262 | Loss: 0.6441821455955505Training Epoch: 4 | iteration: 167/262 | Loss: 0.584518551826477Training Epoch: 4 | iteration: 168/262 | Loss: 0.6107760667800903Training Epoch: 4 | iteration: 169/262 | Loss: 0.6226425766944885Training Epoch: 4 | iteration: 170/262 | Loss: 0.6009184122085571Training Epoch: 4 | iteration: 171/262 | Loss: 0.6006871461868286Training Epoch: 4 | iteration: 172/262 | Loss: 0.5597183108329773Training Epoch: 4 | iteration: 173/262 | Loss: 0.6083370447158813Training Epoch: 4 | iteration: 174/262 | Loss: 0.565475583076477Training Epoch: 4 | iteration: 175/262 | Loss: 0.6028586626052856Training Epoch: 4 | iteration: 176/262 | Loss: 0.6503791213035583Training Epoch: 4 | iteration: 177/262 | Loss: 0.5910957455635071Training Epoch: 4 | iteration: 178/262 | Loss: 0.6360502243041992Training Epoch: 4 | iteration: 179/262 | Loss: 0.5753122568130493Training Epoch: 4 | iteration: 180/262 | Loss: 0.6123263835906982Training Epoch: 4 | iteration: 181/262 | Loss: 0.6212051510810852Training Epoch: 4 | iteration: 182/262 | Loss: 0.5466899275779724Training Epoch: 4 | iteration: 183/262 | Loss: 0.6356006264686584Training Epoch: 4 | iteration: 184/262 | Loss: 0.6449282169342041Training Epoch: 4 | iteration: 185/262 | Loss: 0.616363525390625Training Epoch: 4 | iteration: 186/262 | Loss: 0.629108726978302Training Epoch: 4 | iteration: 187/262 | Loss: 0.5794664025306702Training Epoch: 4 | iteration: 188/262 | Loss: 0.6210228800773621Training Epoch: 4 | iteration: 189/262 | Loss: 0.6711006164550781Training Epoch: 4 | iteration: 190/262 | Loss: 0.6087856292724609Training Epoch: 4 | iteration: 191/262 | Loss: 0.6315624713897705Training Epoch: 4 | iteration: 192/262 | Loss: 0.6193424463272095Training Epoch: 4 | iteration: 193/262 | Loss: 0.6809623837471008Training Epoch: 4 | iteration: 194/262 | Loss: 0.6256567239761353Training Epoch: 4 | iteration: 195/262 | Loss: 0.6787235736846924Training Epoch: 4 | iteration: 196/262 | Loss: 0.6417928338050842Training Epoch: 4 | iteration: 197/262 | Loss: 0.6211453676223755Training Epoch: 4 | iteration: 198/262 | Loss: 0.5922043323516846Training Epoch: 4 | iteration: 199/262 | Loss: 0.6023141145706177Training Epoch: 4 | iteration: 200/262 | Loss: 0.5952340960502625Training Epoch: 4 | iteration: 201/262 | Loss: 0.6011037230491638Training Epoch: 4 | iteration: 202/262 | Loss: 0.6095698475837708Training Epoch: 4 | iteration: 203/262 | Loss: 0.606041669845581Training Epoch: 4 | iteration: 204/262 | Loss: 0.5953198671340942Training Epoch: 4 | iteration: 205/262 | Loss: 0.6018916368484497Training Epoch: 4 | iteration: 206/262 | Loss: 0.659727931022644Training Epoch: 4 | iteration: 207/262 | Loss: 0.5729275941848755Training Epoch: 4 | iteration: 208/262 | Loss: 0.5873230695724487Training Epoch: 4 | iteration: 209/262 | Loss: 0.6113747358322144Training Epoch: 4 | iteration: 210/262 | Loss: 0.6469544768333435Training Epoch: 4 | iteration: 211/262 | Loss: 0.6236552000045776Training Epoch: 4 | iteration: 212/262 | Loss: 0.6081811189651489Training Epoch: 4 | iteration: 213/262 | Loss: 0.5924695730209351Training Epoch: 4 | iteration: 214/262 | Loss: 0.6734219789505005Training Epoch: 4 | iteration: 215/262 | Loss: 0.5910780429840088Training Epoch: 4 | iteration: 216/262 | Loss: 0.6160948276519775Training Epoch: 4 | iteration: 217/262 | Loss: 0.5856808423995972Training Epoch: 4 | iteration: 218/262 | Loss: 0.6225617527961731Training Epoch: 4 | iteration: 219/262 | Loss: 0.5819437503814697Training Epoch: 4 | iteration: 220/262 | Loss: 0.6339733600616455Training Epoch: 4 | iteration: 221/262 | Loss: 0.6138765811920166Training Epoch: 4 | iteration: 222/262 | Loss: 0.6221244931221008Training Epoch: 4 | iteration: 223/262 | Loss: 0.6069476008415222Training Epoch: 4 | iteration: 224/262 | Loss: 0.6018843650817871Training Epoch: 4 | iteration: 225/262 | Loss: 0.6243683099746704Training Epoch: 4 | iteration: 226/262 | Loss: 0.5847532749176025Training Epoch: 4 | iteration: 227/262 | Loss: 0.5992859601974487Training Epoch: 4 | iteration: 228/262 | Loss: 0.6160648465156555Training Epoch: 4 | iteration: 229/262 | Loss: 0.6258915066719055Training Epoch: 4 | iteration: 230/262 | Loss: 0.6177157163619995Training Epoch: 4 | iteration: 231/262 | Loss: 0.6114597916603088Training Epoch: 4 | iteration: 232/262 | Loss: 0.6318082809448242Training Epoch: 4 | iteration: 233/262 | Loss: 0.5965498685836792Training Epoch: 4 | iteration: 234/262 | Loss: 0.6176590323448181Training Epoch: 4 | iteration: 235/262 | Loss: 0.6017408967018127Training Epoch: 4 | iteration: 236/262 | Loss: 0.6502069234848022Training Epoch: 4 | iteration: 237/262 | Loss: 0.6197659969329834Training Epoch: 4 | iteration: 238/262 | Loss: 0.597590446472168Training Epoch: 4 | iteration: 239/262 | Loss: 0.6738553047180176Training Epoch: 4 | iteration: 240/262 | Loss: 0.5971279144287109Training Epoch: 4 | iteration: 241/262 | Loss: 0.612992525100708Training Epoch: 4 | iteration: 242/262 | Loss: 0.579322099685669Training Epoch: 4 | iteration: 243/262 | Loss: 0.6351937055587769Training Epoch: 4 | iteration: 244/262 | Loss: 0.5962930917739868Training Epoch: 4 | iteration: 245/262 | Loss: 0.6234393119812012Training Epoch: 4 | iteration: 246/262 | Loss: 0.626937985420227Training Epoch: 4 | iteration: 247/262 | Loss: 0.5828803777694702Training Epoch: 4 | iteration: 248/262 | Loss: 0.5855979919433594Training Epoch: 4 | iteration: 249/262 | Loss: 0.6369286775588989Training Epoch: 4 | iteration: 250/262 | Loss: 0.6179707646369934Training Epoch: 4 | iteration: 251/262 | Loss: 0.6008493900299072Training Epoch: 4 | iteration: 252/262 | Loss: 0.6150434613227844Training Epoch: 4 | iteration: 253/262 | Loss: 0.6200103759765625Training Epoch: 4 | iteration: 254/262 | Loss: 0.6314029097557068Training Epoch: 4 | iteration: 255/262 | Loss: 0.6436712741851807Training Epoch: 4 | iteration: 256/262 | Loss: 0.6315568685531616Training Epoch: 4 | iteration: 257/262 | Loss: 0.6264656782150269Training Epoch: 4 | iteration: 258/262 | Loss: 0.5978311896324158Training Epoch: 4 | iteration: 259/262 | Loss: 0.6096402406692505Training Epoch: 4 | iteration: 260/262 | Loss: 0.5806235074996948Training Epoch: 4 | iteration: 261/262 | Loss: 0.6463117599487305Validating Epoch: 4 | iteration: 0/66 | Loss: 0.5983671545982361Validating Epoch: 4 | iteration: 1/66 | Loss: 0.5792670845985413Validating Epoch: 4 | iteration: 2/66 | Loss: 0.5818463563919067Validating Epoch: 4 | iteration: 3/66 | Loss: 0.6165770888328552Validating Epoch: 4 | iteration: 4/66 | Loss: 0.6265341639518738Validating Epoch: 4 | iteration: 5/66 | Loss: 0.5713969469070435Validating Epoch: 4 | iteration: 6/66 | Loss: 0.5691900253295898Validating Epoch: 4 | iteration: 7/66 | Loss: 0.6020618081092834Validating Epoch: 4 | iteration: 8/66 | Loss: 0.6172928810119629Validating Epoch: 4 | iteration: 9/66 | Loss: 0.6531001925468445Validating Epoch: 4 | iteration: 10/66 | Loss: 0.5853879451751709Validating Epoch: 4 | iteration: 11/66 | Loss: 0.627773642539978Validating Epoch: 4 | iteration: 12/66 | Loss: 0.5990033149719238Validating Epoch: 4 | iteration: 13/66 | Loss: 0.5686017870903015Validating Epoch: 4 | iteration: 14/66 | Loss: 0.5811854600906372Validating Epoch: 4 | iteration: 15/66 | Loss: 0.6331256628036499Validating Epoch: 4 | iteration: 16/66 | Loss: 0.5596386194229126Validating Epoch: 4 | iteration: 17/66 | Loss: 0.5723586678504944Validating Epoch: 4 | iteration: 18/66 | Loss: 0.5851601362228394Validating Epoch: 4 | iteration: 19/66 | Loss: 0.6505175232887268Validating Epoch: 4 | iteration: 20/66 | Loss: 0.5591738224029541Validating Epoch: 4 | iteration: 21/66 | Loss: 0.6148488521575928Validating Epoch: 4 | iteration: 22/66 | Loss: 0.566288411617279Validating Epoch: 4 | iteration: 23/66 | Loss: 0.6152505874633789Validating Epoch: 4 | iteration: 24/66 | Loss: 0.5857625007629395Validating Epoch: 4 | iteration: 25/66 | Loss: 0.6022988557815552Validating Epoch: 4 | iteration: 26/66 | Loss: 0.5908970832824707Validating Epoch: 4 | iteration: 27/66 | Loss: 0.576024055480957Validating Epoch: 4 | iteration: 28/66 | Loss: 0.6196904182434082Validating Epoch: 4 | iteration: 29/66 | Loss: 0.5874788761138916Validating Epoch: 4 | iteration: 30/66 | Loss: 0.6302728652954102Validating Epoch: 4 | iteration: 31/66 | Loss: 0.580554723739624Validating Epoch: 4 | iteration: 32/66 | Loss: 0.6433959603309631Validating Epoch: 4 | iteration: 33/66 | Loss: 0.6055128574371338Validating Epoch: 4 | iteration: 34/66 | Loss: 0.6262556314468384Validating Epoch: 4 | iteration: 35/66 | Loss: 0.5895946025848389Validating Epoch: 4 | iteration: 36/66 | Loss: 0.6279217004776001Validating Epoch: 4 | iteration: 37/66 | Loss: 0.593314528465271Validating Epoch: 4 | iteration: 38/66 | Loss: 0.5971022248268127Validating Epoch: 4 | iteration: 39/66 | Loss: 0.5762062072753906Validating Epoch: 4 | iteration: 40/66 | Loss: 0.5959447622299194Validating Epoch: 4 | iteration: 41/66 | Loss: 0.6008701324462891Validating Epoch: 4 | iteration: 42/66 | Loss: 0.6286802291870117Validating Epoch: 4 | iteration: 43/66 | Loss: 0.6102061867713928Validating Epoch: 4 | iteration: 44/66 | Loss: 0.6075471639633179Validating Epoch: 4 | iteration: 45/66 | Loss: 0.6073534488677979Validating Epoch: 4 | iteration: 46/66 | Loss: 0.6135793328285217Validating Epoch: 4 | iteration: 47/66 | Loss: 0.6065731048583984Validating Epoch: 4 | iteration: 48/66 | Loss: 0.5929698944091797Validating Epoch: 4 | iteration: 49/66 | Loss: 0.5822954177856445Validating Epoch: 4 | iteration: 50/66 | Loss: 0.579919695854187Validating Epoch: 4 | iteration: 51/66 | Loss: 0.6009453535079956Validating Epoch: 4 | iteration: 52/66 | Loss: 0.6066475510597229Validating Epoch: 4 | iteration: 53/66 | Loss: 0.5408303737640381Validating Epoch: 4 | iteration: 54/66 | Loss: 0.5906287431716919Validating Epoch: 4 | iteration: 55/66 | Loss: 0.5795371532440186Validating Epoch: 4 | iteration: 56/66 | Loss: 0.6034516096115112Validating Epoch: 4 | iteration: 57/66 | Loss: 0.5841531753540039Validating Epoch: 4 | iteration: 58/66 | Loss: 0.5887244939804077Validating Epoch: 4 | iteration: 59/66 | Loss: 0.5795942544937134Validating Epoch: 4 | iteration: 60/66 | Loss: 0.6096809506416321Validating Epoch: 4 | iteration: 61/66 | Loss: 0.5799846649169922Validating Epoch: 4 | iteration: 62/66 | Loss: 0.6034364700317383Validating Epoch: 4 | iteration: 63/66 | Loss: 0.5952020883560181Validating Epoch: 4 | iteration: 64/66 | Loss: 0.599337100982666Validating Epoch: 4 | iteration: 65/66 | Loss: 0.5641946792602539Temperature:  0.5
Sampling: iteration: 0/51{'Validity': 0.9384765625, 'Novelty': 1.0, 'Uniqueness': 0.9614984391259105}
Training Epoch: 5 | iteration: 0/262 | Loss: 0.5703262090682983Training Epoch: 5 | iteration: 1/262 | Loss: 0.6060558557510376Training Epoch: 5 | iteration: 2/262 | Loss: 0.595228374004364Training Epoch: 5 | iteration: 3/262 | Loss: 0.6084115505218506Training Epoch: 5 | iteration: 4/262 | Loss: 0.6608461141586304Training Epoch: 5 | iteration: 5/262 | Loss: 0.5730842351913452Training Epoch: 5 | iteration: 6/262 | Loss: 0.5754150152206421Training Epoch: 5 | iteration: 7/262 | Loss: 0.6230029463768005Training Epoch: 5 | iteration: 8/262 | Loss: 0.6263504028320312Training Epoch: 5 | iteration: 9/262 | Loss: 0.5934578776359558Training Epoch: 5 | iteration: 10/262 | Loss: 0.5939343571662903Training Epoch: 5 | iteration: 11/262 | Loss: 0.5861269235610962Training Epoch: 5 | iteration: 12/262 | Loss: 0.5632337331771851Training Epoch: 5 | iteration: 13/262 | Loss: 0.6034709811210632Training Epoch: 5 | iteration: 14/262 | Loss: 0.6036405563354492Training Epoch: 5 | iteration: 15/262 | Loss: 0.6173264384269714Training Epoch: 5 | iteration: 16/262 | Loss: 0.6219287514686584Training Epoch: 5 | iteration: 17/262 | Loss: 0.5936593413352966Training Epoch: 5 | iteration: 18/262 | Loss: 0.6007727980613708Training Epoch: 5 | iteration: 19/262 | Loss: 0.5999187231063843Training Epoch: 5 | iteration: 20/262 | Loss: 0.6034505367279053Training Epoch: 5 | iteration: 21/262 | Loss: 0.562578022480011Training Epoch: 5 | iteration: 22/262 | Loss: 0.5505519509315491Training Epoch: 5 | iteration: 23/262 | Loss: 0.6254864931106567Training Epoch: 5 | iteration: 24/262 | Loss: 0.570139467716217Training Epoch: 5 | iteration: 25/262 | Loss: 0.5968906879425049Training Epoch: 5 | iteration: 26/262 | Loss: 0.6617641448974609Training Epoch: 5 | iteration: 27/262 | Loss: 0.6371741890907288Training Epoch: 5 | iteration: 28/262 | Loss: 0.6198007464408875Training Epoch: 5 | iteration: 29/262 | Loss: 0.5940043926239014Training Epoch: 5 | iteration: 30/262 | Loss: 0.601240873336792Training Epoch: 5 | iteration: 31/262 | Loss: 0.5647726655006409Training Epoch: 5 | iteration: 32/262 | Loss: 0.5928831100463867Training Epoch: 5 | iteration: 33/262 | Loss: 0.56666499376297Training Epoch: 5 | iteration: 34/262 | Loss: 0.6438155174255371Training Epoch: 5 | iteration: 35/262 | Loss: 0.6124621629714966Training Epoch: 5 | iteration: 36/262 | Loss: 0.6305689811706543Training Epoch: 5 | iteration: 37/262 | Loss: 0.5839458703994751Training Epoch: 5 | iteration: 38/262 | Loss: 0.5682365894317627Training Epoch: 5 | iteration: 39/262 | Loss: 0.5957769155502319Training Epoch: 5 | iteration: 40/262 | Loss: 0.597434937953949Training Epoch: 5 | iteration: 41/262 | Loss: 0.5662761926651001Training Epoch: 5 | iteration: 42/262 | Loss: 0.5221163630485535Training Epoch: 5 | iteration: 43/262 | Loss: 0.6178940534591675Training Epoch: 5 | iteration: 44/262 | Loss: 0.5876374840736389Training Epoch: 5 | iteration: 45/262 | Loss: 0.6199226379394531Training Epoch: 5 | iteration: 46/262 | Loss: 0.5671476125717163Training Epoch: 5 | iteration: 47/262 | Loss: 0.6664108633995056Training Epoch: 5 | iteration: 48/262 | Loss: 0.5927389860153198Training Epoch: 5 | iteration: 49/262 | Loss: 0.6027047038078308Training Epoch: 5 | iteration: 50/262 | Loss: 0.5936322212219238Training Epoch: 5 | iteration: 51/262 | Loss: 0.577970027923584Training Epoch: 5 | iteration: 52/262 | Loss: 0.5816874504089355Training Epoch: 5 | iteration: 53/262 | Loss: 0.5816560387611389Training Epoch: 5 | iteration: 54/262 | Loss: 0.5901235342025757Training Epoch: 5 | iteration: 55/262 | Loss: 0.617483377456665Training Epoch: 5 | iteration: 56/262 | Loss: 0.581089198589325Training Epoch: 5 | iteration: 57/262 | Loss: 0.6445906162261963Training Epoch: 5 | iteration: 58/262 | Loss: 0.6043878793716431Training Epoch: 5 | iteration: 59/262 | Loss: 0.5816478729248047Training Epoch: 5 | iteration: 60/262 | Loss: 0.6140072345733643Training Epoch: 5 | iteration: 61/262 | Loss: 0.5702247619628906Training Epoch: 5 | iteration: 62/262 | Loss: 0.6038072109222412Training Epoch: 5 | iteration: 63/262 | Loss: 0.566734254360199Training Epoch: 5 | iteration: 64/262 | Loss: 0.5994287729263306Training Epoch: 5 | iteration: 65/262 | Loss: 0.6029888987541199Training Epoch: 5 | iteration: 66/262 | Loss: 0.5594720244407654Training Epoch: 5 | iteration: 67/262 | Loss: 0.6010514497756958Training Epoch: 5 | iteration: 68/262 | Loss: 0.6310742497444153Training Epoch: 5 | iteration: 69/262 | Loss: 0.5725768208503723Training Epoch: 5 | iteration: 70/262 | Loss: 0.6232966184616089Training Epoch: 5 | iteration: 71/262 | Loss: 0.5780631899833679Training Epoch: 5 | iteration: 72/262 | Loss: 0.5722882747650146Training Epoch: 5 | iteration: 73/262 | Loss: 0.5970333814620972Training Epoch: 5 | iteration: 74/262 | Loss: 0.5864129066467285Training Epoch: 5 | iteration: 75/262 | Loss: 0.6307594180107117Training Epoch: 5 | iteration: 76/262 | Loss: 0.5894466042518616Training Epoch: 5 | iteration: 77/262 | Loss: 0.6251665949821472Training Epoch: 5 | iteration: 78/262 | Loss: 0.5698516368865967Training Epoch: 5 | iteration: 79/262 | Loss: 0.5771031379699707Training Epoch: 5 | iteration: 80/262 | Loss: 0.5945874452590942Training Epoch: 5 | iteration: 81/262 | Loss: 0.6069101095199585Training Epoch: 5 | iteration: 82/262 | Loss: 0.5763686299324036Training Epoch: 5 | iteration: 83/262 | Loss: 0.6145898103713989Training Epoch: 5 | iteration: 84/262 | Loss: 0.5778555274009705Training Epoch: 5 | iteration: 85/262 | Loss: 0.6036813259124756Training Epoch: 5 | iteration: 86/262 | Loss: 0.609125554561615Training Epoch: 5 | iteration: 87/262 | Loss: 0.6047165393829346Training Epoch: 5 | iteration: 88/262 | Loss: 0.578010618686676Training Epoch: 5 | iteration: 89/262 | Loss: 0.5881242752075195Training Epoch: 5 | iteration: 90/262 | Loss: 0.6056452393531799Training Epoch: 5 | iteration: 91/262 | Loss: 0.5670073628425598Training Epoch: 5 | iteration: 92/262 | Loss: 0.5947075486183167Training Epoch: 5 | iteration: 93/262 | Loss: 0.600783109664917Training Epoch: 5 | iteration: 94/262 | Loss: 0.6193015575408936Training Epoch: 5 | iteration: 95/262 | Loss: 0.6201912760734558Training Epoch: 5 | iteration: 96/262 | Loss: 0.5901908278465271Training Epoch: 5 | iteration: 97/262 | Loss: 0.6619428396224976Training Epoch: 5 | iteration: 98/262 | Loss: 0.6106617450714111Training Epoch: 5 | iteration: 99/262 | Loss: 0.6237566471099854Training Epoch: 5 | iteration: 100/262 | Loss: 0.5988966226577759Training Epoch: 5 | iteration: 101/262 | Loss: 0.5787781476974487Training Epoch: 5 | iteration: 102/262 | Loss: 0.5467002391815186Training Epoch: 5 | iteration: 103/262 | Loss: 0.6223704218864441Training Epoch: 5 | iteration: 104/262 | Loss: 0.5966887474060059Training Epoch: 5 | iteration: 105/262 | Loss: 0.605538010597229Training Epoch: 5 | iteration: 106/262 | Loss: 0.6055539846420288Training Epoch: 5 | iteration: 107/262 | Loss: 0.6367744207382202Training Epoch: 5 | iteration: 108/262 | Loss: 0.6006714701652527Training Epoch: 5 | iteration: 109/262 | Loss: 0.5764415264129639Training Epoch: 5 | iteration: 110/262 | Loss: 0.5737847089767456Training Epoch: 5 | iteration: 111/262 | Loss: 0.547088086605072Training Epoch: 5 | iteration: 112/262 | Loss: 0.6033731698989868Training Epoch: 5 | iteration: 113/262 | Loss: 0.6167550683021545Training Epoch: 5 | iteration: 114/262 | Loss: 0.5766782760620117Training Epoch: 5 | iteration: 115/262 | Loss: 0.6424999237060547Training Epoch: 5 | iteration: 116/262 | Loss: 0.6149170398712158Training Epoch: 5 | iteration: 117/262 | Loss: 0.6203457117080688Training Epoch: 5 | iteration: 118/262 | Loss: 0.6035921573638916Training Epoch: 5 | iteration: 119/262 | Loss: 0.5712805986404419Training Epoch: 5 | iteration: 120/262 | Loss: 0.6077044606208801Training Epoch: 5 | iteration: 121/262 | Loss: 0.5489013195037842Training Epoch: 5 | iteration: 122/262 | Loss: 0.6022448539733887Training Epoch: 5 | iteration: 123/262 | Loss: 0.7112250328063965Training Epoch: 5 | iteration: 124/262 | Loss: 0.5663065910339355Training Epoch: 5 | iteration: 125/262 | Loss: 0.587550699710846Training Epoch: 5 | iteration: 126/262 | Loss: 0.5642915964126587Training Epoch: 5 | iteration: 127/262 | Loss: 0.6246113181114197Training Epoch: 5 | iteration: 128/262 | Loss: 0.586311936378479Training Epoch: 5 | iteration: 129/262 | Loss: 0.6443330645561218Training Epoch: 5 | iteration: 130/262 | Loss: 0.5506078600883484Training Epoch: 5 | iteration: 131/262 | Loss: 0.5596950054168701Training Epoch: 5 | iteration: 132/262 | Loss: 0.6123418807983398Training Epoch: 5 | iteration: 133/262 | Loss: 0.6783608794212341Training Epoch: 5 | iteration: 134/262 | Loss: 0.6119970083236694Training Epoch: 5 | iteration: 135/262 | Loss: 0.585124135017395Training Epoch: 5 | iteration: 136/262 | Loss: 0.6094648838043213Training Epoch: 5 | iteration: 137/262 | Loss: 0.5925052165985107Training Epoch: 5 | iteration: 138/262 | Loss: 0.58601975440979Training Epoch: 5 | iteration: 139/262 | Loss: 0.6207948923110962Training Epoch: 5 | iteration: 140/262 | Loss: 0.571922779083252Training Epoch: 5 | iteration: 141/262 | Loss: 0.5588890314102173Training Epoch: 5 | iteration: 142/262 | Loss: 0.5551122426986694Training Epoch: 5 | iteration: 143/262 | Loss: 0.5457215309143066Training Epoch: 5 | iteration: 144/262 | Loss: 0.5994045734405518Training Epoch: 5 | iteration: 145/262 | Loss: 0.5720775723457336Training Epoch: 5 | iteration: 146/262 | Loss: 0.5806107521057129Training Epoch: 5 | iteration: 147/262 | Loss: 0.6226199865341187Training Epoch: 5 | iteration: 148/262 | Loss: 0.5734161734580994Training Epoch: 5 | iteration: 149/262 | Loss: 0.6345325112342834Training Epoch: 5 | iteration: 150/262 | Loss: 0.5887457132339478Training Epoch: 5 | iteration: 151/262 | Loss: 0.5639752149581909Training Epoch: 5 | iteration: 152/262 | Loss: 0.5936089754104614Training Epoch: 5 | iteration: 153/262 | Loss: 0.5915277004241943Training Epoch: 5 | iteration: 154/262 | Loss: 0.5957802534103394Training Epoch: 5 | iteration: 155/262 | Loss: 0.603256106376648Training Epoch: 5 | iteration: 156/262 | Loss: 0.6069315671920776Training Epoch: 5 | iteration: 157/262 | Loss: 0.6542924642562866Training Epoch: 5 | iteration: 158/262 | Loss: 0.609822690486908Training Epoch: 5 | iteration: 159/262 | Loss: 0.6524664759635925Training Epoch: 5 | iteration: 160/262 | Loss: 0.5765740275382996Training Epoch: 5 | iteration: 161/262 | Loss: 0.6255825757980347Training Epoch: 5 | iteration: 162/262 | Loss: 0.5879075527191162Training Epoch: 5 | iteration: 163/262 | Loss: 0.6382228136062622Training Epoch: 5 | iteration: 164/262 | Loss: 0.5634027719497681Training Epoch: 5 | iteration: 165/262 | Loss: 0.5976625680923462Training Epoch: 5 | iteration: 166/262 | Loss: 0.5875163674354553Training Epoch: 5 | iteration: 167/262 | Loss: 0.5840107202529907Training Epoch: 5 | iteration: 168/262 | Loss: 0.6373917460441589Training Epoch: 5 | iteration: 169/262 | Loss: 0.5863309502601624Training Epoch: 5 | iteration: 170/262 | Loss: 0.5885636210441589Training Epoch: 5 | iteration: 171/262 | Loss: 0.5682776570320129Training Epoch: 5 | iteration: 172/262 | Loss: 0.6208435297012329Training Epoch: 5 | iteration: 173/262 | Loss: 0.5889724493026733Training Epoch: 5 | iteration: 174/262 | Loss: 0.610596776008606Training Epoch: 5 | iteration: 175/262 | Loss: 0.6281463503837585Training Epoch: 5 | iteration: 176/262 | Loss: 0.5973904132843018Training Epoch: 5 | iteration: 177/262 | Loss: 0.5918834209442139Training Epoch: 5 | iteration: 178/262 | Loss: 0.581563413143158Training Epoch: 5 | iteration: 179/262 | Loss: 0.6199474334716797Training Epoch: 5 | iteration: 180/262 | Loss: 0.613774299621582Training Epoch: 5 | iteration: 181/262 | Loss: 0.615876317024231Training Epoch: 5 | iteration: 182/262 | Loss: 0.5767115950584412Training Epoch: 5 | iteration: 183/262 | Loss: 0.6129209995269775Training Epoch: 5 | iteration: 184/262 | Loss: 0.5657245516777039Training Epoch: 5 | iteration: 185/262 | Loss: 0.5901687741279602Training Epoch: 5 | iteration: 186/262 | Loss: 0.575692355632782Training Epoch: 5 | iteration: 187/262 | Loss: 0.603522777557373Training Epoch: 5 | iteration: 188/262 | Loss: 0.6035990715026855Training Epoch: 5 | iteration: 189/262 | Loss: 0.661740243434906Training Epoch: 5 | iteration: 190/262 | Loss: 0.5833902359008789Training Epoch: 5 | iteration: 191/262 | Loss: 0.5966520309448242Training Epoch: 5 | iteration: 192/262 | Loss: 0.5873773097991943Training Epoch: 5 | iteration: 193/262 | Loss: 0.5814945697784424Training Epoch: 5 | iteration: 194/262 | Loss: 0.6284974813461304Training Epoch: 5 | iteration: 195/262 | Loss: 0.5950150489807129Training Epoch: 5 | iteration: 196/262 | Loss: 0.6336190700531006Training Epoch: 5 | iteration: 197/262 | Loss: 0.5722271203994751Training Epoch: 5 | iteration: 198/262 | Loss: 0.6421173810958862Training Epoch: 5 | iteration: 199/262 | Loss: 0.6069934368133545Training Epoch: 5 | iteration: 200/262 | Loss: 0.5855857729911804Training Epoch: 5 | iteration: 201/262 | Loss: 0.6344606876373291Training Epoch: 5 | iteration: 202/262 | Loss: 0.5854707956314087Training Epoch: 5 | iteration: 203/262 | Loss: 0.5959329605102539Training Epoch: 5 | iteration: 204/262 | Loss: 0.6026049852371216Training Epoch: 5 | iteration: 205/262 | Loss: 0.6081962585449219Training Epoch: 5 | iteration: 206/262 | Loss: 0.5718851089477539Training Epoch: 5 | iteration: 207/262 | Loss: 0.6495015621185303Training Epoch: 5 | iteration: 208/262 | Loss: 0.6345016956329346Training Epoch: 5 | iteration: 209/262 | Loss: 0.5683658123016357Training Epoch: 5 | iteration: 210/262 | Loss: 0.6283600330352783Training Epoch: 5 | iteration: 211/262 | Loss: 0.6221058964729309Training Epoch: 5 | iteration: 212/262 | Loss: 0.5814872980117798Training Epoch: 5 | iteration: 213/262 | Loss: 0.6389653086662292Training Epoch: 5 | iteration: 214/262 | Loss: 0.7129641771316528Training Epoch: 5 | iteration: 215/262 | Loss: 0.6023343205451965Training Epoch: 5 | iteration: 216/262 | Loss: 0.56919264793396Training Epoch: 5 | iteration: 217/262 | Loss: 0.6687647104263306Training Epoch: 5 | iteration: 218/262 | Loss: 0.5874524116516113Training Epoch: 5 | iteration: 219/262 | Loss: 0.6585701107978821Training Epoch: 5 | iteration: 220/262 | Loss: 0.6099141836166382Training Epoch: 5 | iteration: 221/262 | Loss: 0.5922724008560181Training Epoch: 5 | iteration: 222/262 | Loss: 0.6093498468399048Training Epoch: 5 | iteration: 223/262 | Loss: 0.5898960828781128Training Epoch: 5 | iteration: 224/262 | Loss: 0.5781723260879517Training Epoch: 5 | iteration: 225/262 | Loss: 0.5822099447250366Training Epoch: 5 | iteration: 226/262 | Loss: 0.647913932800293Training Epoch: 5 | iteration: 227/262 | Loss: 0.6009906530380249Training Epoch: 5 | iteration: 228/262 | Loss: 0.5986560583114624Training Epoch: 5 | iteration: 229/262 | Loss: 0.5713670253753662Training Epoch: 5 | iteration: 230/262 | Loss: 0.5828664898872375Training Epoch: 5 | iteration: 231/262 | Loss: 0.6368604898452759Training Epoch: 5 | iteration: 232/262 | Loss: 0.5980374813079834Training Epoch: 5 | iteration: 233/262 | Loss: 0.6088470816612244Training Epoch: 5 | iteration: 234/262 | Loss: 0.585530698299408Training Epoch: 5 | iteration: 235/262 | Loss: 0.5943801403045654Training Epoch: 5 | iteration: 236/262 | Loss: 0.679135799407959Training Epoch: 5 | iteration: 237/262 | Loss: 0.6389961838722229Training Epoch: 5 | iteration: 238/262 | Loss: 0.5947576761245728Training Epoch: 5 | iteration: 239/262 | Loss: 0.6248019933700562Training Epoch: 5 | iteration: 240/262 | Loss: 0.6250187158584595Training Epoch: 5 | iteration: 241/262 | Loss: 0.6138233542442322Training Epoch: 5 | iteration: 242/262 | Loss: 0.6116539239883423Training Epoch: 5 | iteration: 243/262 | Loss: 0.5802911520004272Training Epoch: 5 | iteration: 244/262 | Loss: 0.550718367099762Training Epoch: 5 | iteration: 245/262 | Loss: 0.5689322352409363Training Epoch: 5 | iteration: 246/262 | Loss: 0.6493992209434509Training Epoch: 5 | iteration: 247/262 | Loss: 0.5831884145736694Training Epoch: 5 | iteration: 248/262 | Loss: 0.6187725067138672Training Epoch: 5 | iteration: 249/262 | Loss: 0.6498125791549683Training Epoch: 5 | iteration: 250/262 | Loss: 0.5620917081832886Training Epoch: 5 | iteration: 251/262 | Loss: 0.5842376947402954Training Epoch: 5 | iteration: 252/262 | Loss: 0.5958248972892761Training Epoch: 5 | iteration: 253/262 | Loss: 0.607478141784668Training Epoch: 5 | iteration: 254/262 | Loss: 0.5559979677200317Training Epoch: 5 | iteration: 255/262 | Loss: 0.6026396751403809Training Epoch: 5 | iteration: 256/262 | Loss: 0.6137845516204834Training Epoch: 5 | iteration: 257/262 | Loss: 0.6346036195755005Training Epoch: 5 | iteration: 258/262 | Loss: 0.5838910341262817Training Epoch: 5 | iteration: 259/262 | Loss: 0.619649350643158Training Epoch: 5 | iteration: 260/262 | Loss: 0.5547278523445129Training Epoch: 5 | iteration: 261/262 | Loss: 0.5916849374771118Validating Epoch: 5 | iteration: 0/66 | Loss: 0.6175497770309448Validating Epoch: 5 | iteration: 1/66 | Loss: 0.6144827604293823Validating Epoch: 5 | iteration: 2/66 | Loss: 0.6220943927764893Validating Epoch: 5 | iteration: 3/66 | Loss: 0.6061617732048035Validating Epoch: 5 | iteration: 4/66 | Loss: 0.584478497505188Validating Epoch: 5 | iteration: 5/66 | Loss: 0.5984269380569458Validating Epoch: 5 | iteration: 6/66 | Loss: 0.5754774808883667Validating Epoch: 5 | iteration: 7/66 | Loss: 0.5757879018783569Validating Epoch: 5 | iteration: 8/66 | Loss: 0.6331520080566406Validating Epoch: 5 | iteration: 9/66 | Loss: 0.6051154136657715Validating Epoch: 5 | iteration: 10/66 | Loss: 0.6131176948547363Validating Epoch: 5 | iteration: 11/66 | Loss: 0.5502834916114807Validating Epoch: 5 | iteration: 12/66 | Loss: 0.6038946509361267Validating Epoch: 5 | iteration: 13/66 | Loss: 0.6471990346908569Validating Epoch: 5 | iteration: 14/66 | Loss: 0.612838864326477Validating Epoch: 5 | iteration: 15/66 | Loss: 0.6102093458175659Validating Epoch: 5 | iteration: 16/66 | Loss: 0.6232501864433289Validating Epoch: 5 | iteration: 17/66 | Loss: 0.5646170377731323Validating Epoch: 5 | iteration: 18/66 | Loss: 0.5944839119911194Validating Epoch: 5 | iteration: 19/66 | Loss: 0.5883045196533203Validating Epoch: 5 | iteration: 20/66 | Loss: 0.6013264656066895Validating Epoch: 5 | iteration: 21/66 | Loss: 0.5734469890594482Validating Epoch: 5 | iteration: 22/66 | Loss: 0.5715008974075317Validating Epoch: 5 | iteration: 23/66 | Loss: 0.5948792695999146Validating Epoch: 5 | iteration: 24/66 | Loss: 0.6154396533966064Validating Epoch: 5 | iteration: 25/66 | Loss: 0.5813523530960083Validating Epoch: 5 | iteration: 26/66 | Loss: 0.5941845178604126Validating Epoch: 5 | iteration: 27/66 | Loss: 0.6157848238945007Validating Epoch: 5 | iteration: 28/66 | Loss: 0.6252212524414062Validating Epoch: 5 | iteration: 29/66 | Loss: 0.5875664353370667Validating Epoch: 5 | iteration: 30/66 | Loss: 0.5968190431594849Validating Epoch: 5 | iteration: 31/66 | Loss: 0.5992984771728516Validating Epoch: 5 | iteration: 32/66 | Loss: 0.5775290131568909Validating Epoch: 5 | iteration: 33/66 | Loss: 0.5897291898727417Validating Epoch: 5 | iteration: 34/66 | Loss: 0.5518651008605957Validating Epoch: 5 | iteration: 35/66 | Loss: 0.628639817237854Validating Epoch: 5 | iteration: 36/66 | Loss: 0.6120308637619019Validating Epoch: 5 | iteration: 37/66 | Loss: 0.5953811407089233Validating Epoch: 5 | iteration: 38/66 | Loss: 0.6241353750228882Validating Epoch: 5 | iteration: 39/66 | Loss: 0.6143568754196167Validating Epoch: 5 | iteration: 40/66 | Loss: 0.6201643943786621Validating Epoch: 5 | iteration: 41/66 | Loss: 0.5468738079071045Validating Epoch: 5 | iteration: 42/66 | Loss: 0.5835402011871338Validating Epoch: 5 | iteration: 43/66 | Loss: 0.5920500755310059Validating Epoch: 5 | iteration: 44/66 | Loss: 0.5659220218658447Validating Epoch: 5 | iteration: 45/66 | Loss: 0.5850861072540283Validating Epoch: 5 | iteration: 46/66 | Loss: 0.5997623205184937Validating Epoch: 5 | iteration: 47/66 | Loss: 0.6014094352722168Validating Epoch: 5 | iteration: 48/66 | Loss: 0.6358257532119751Validating Epoch: 5 | iteration: 49/66 | Loss: 0.6951470971107483Validating Epoch: 5 | iteration: 50/66 | Loss: 0.5838690996170044Validating Epoch: 5 | iteration: 51/66 | Loss: 0.5514216423034668Validating Epoch: 5 | iteration: 52/66 | Loss: 0.5827367901802063Validating Epoch: 5 | iteration: 53/66 | Loss: 0.6069694757461548Validating Epoch: 5 | iteration: 54/66 | Loss: 0.6031914949417114Validating Epoch: 5 | iteration: 55/66 | Loss: 0.5848819017410278Validating Epoch: 5 | iteration: 56/66 | Loss: 0.5914531946182251Validating Epoch: 5 | iteration: 57/66 | Loss: 0.6278345584869385Validating Epoch: 5 | iteration: 58/66 | Loss: 0.5450563430786133Validating Epoch: 5 | iteration: 59/66 | Loss: 0.5674958229064941Validating Epoch: 5 | iteration: 60/66 | Loss: 0.5937014818191528Validating Epoch: 5 | iteration: 61/66 | Loss: 0.58002769947052Validating Epoch: 5 | iteration: 62/66 | Loss: 0.5674378871917725Validating Epoch: 5 | iteration: 63/66 | Loss: 0.5832034349441528Validating Epoch: 5 | iteration: 64/66 | Loss: 0.6003447771072388Validating Epoch: 5 | iteration: 65/66 | Loss: 0.579674482345581Temperature:  0.5
Sampling: iteration: 0/51{'Validity': 0.9716796875, 'Novelty': 1.0, 'Uniqueness': 0.9618090452261306}
Training Epoch: 6 | iteration: 0/262 | Loss: 0.560512125492096Training Epoch: 6 | iteration: 1/262 | Loss: 0.5908956527709961Training Epoch: 6 | iteration: 2/262 | Loss: 0.5835896730422974Training Epoch: 6 | iteration: 3/262 | Loss: 0.5787371397018433Training Epoch: 6 | iteration: 4/262 | Loss: 0.629365086555481Training Epoch: 6 | iteration: 5/262 | Loss: 0.5929372310638428Training Epoch: 6 | iteration: 6/262 | Loss: 0.602252721786499Training Epoch: 6 | iteration: 7/262 | Loss: 0.6122310757637024Training Epoch: 6 | iteration: 8/262 | Loss: 0.5598334074020386Training Epoch: 6 | iteration: 9/262 | Loss: 0.6594906449317932Training Epoch: 6 | iteration: 10/262 | Loss: 0.5613792538642883Training Epoch: 6 | iteration: 11/262 | Loss: 0.6018404960632324Training Epoch: 6 | iteration: 12/262 | Loss: 0.5911561846733093Training Epoch: 6 | iteration: 13/262 | Loss: 0.6143827438354492Training Epoch: 6 | iteration: 14/262 | Loss: 0.5924999713897705Training Epoch: 6 | iteration: 15/262 | Loss: 0.5387319326400757Training Epoch: 6 | iteration: 16/262 | Loss: 0.5969618558883667Training Epoch: 6 | iteration: 17/262 | Loss: 0.5554934740066528Training Epoch: 6 | iteration: 18/262 | Loss: 0.6468072533607483Training Epoch: 6 | iteration: 19/262 | Loss: 0.5700746774673462Training Epoch: 6 | iteration: 20/262 | Loss: 0.5704887509346008Training Epoch: 6 | iteration: 21/262 | Loss: 0.5791040658950806Training Epoch: 6 | iteration: 22/262 | Loss: 0.5827562808990479Training Epoch: 6 | iteration: 23/262 | Loss: 0.5772303342819214Training Epoch: 6 | iteration: 24/262 | Loss: 0.6239250302314758Training Epoch: 6 | iteration: 25/262 | Loss: 0.6053144931793213Training Epoch: 6 | iteration: 26/262 | Loss: 0.5884681940078735Training Epoch: 6 | iteration: 27/262 | Loss: 0.5802247524261475Training Epoch: 6 | iteration: 28/262 | Loss: 0.5941805243492126Training Epoch: 6 | iteration: 29/262 | Loss: 0.5545200109481812Training Epoch: 6 | iteration: 30/262 | Loss: 0.6107956171035767Training Epoch: 6 | iteration: 31/262 | Loss: 0.6007304191589355Training Epoch: 6 | iteration: 32/262 | Loss: 0.5701984167098999Training Epoch: 6 | iteration: 33/262 | Loss: 0.6104521155357361Training Epoch: 6 | iteration: 34/262 | Loss: 0.5682350397109985Training Epoch: 6 | iteration: 35/262 | Loss: 0.5896592140197754Training Epoch: 6 | iteration: 36/262 | Loss: 0.5499398708343506Training Epoch: 6 | iteration: 37/262 | Loss: 0.5543708205223083Training Epoch: 6 | iteration: 38/262 | Loss: 0.601938009262085Training Epoch: 6 | iteration: 39/262 | Loss: 0.6202635765075684Training Epoch: 6 | iteration: 40/262 | Loss: 0.5756664872169495Training Epoch: 6 | iteration: 41/262 | Loss: 0.6058048009872437Training Epoch: 6 | iteration: 42/262 | Loss: 0.563819944858551Training Epoch: 6 | iteration: 43/262 | Loss: 0.5421206951141357Training Epoch: 6 | iteration: 44/262 | Loss: 0.5870901346206665Training Epoch: 6 | iteration: 45/262 | Loss: 0.572950005531311Training Epoch: 6 | iteration: 46/262 | Loss: 0.5963809490203857Training Epoch: 6 | iteration: 47/262 | Loss: 0.6333287358283997Training Epoch: 6 | iteration: 48/262 | Loss: 0.5685235261917114Training Epoch: 6 | iteration: 49/262 | Loss: 0.5787138938903809Training Epoch: 6 | iteration: 50/262 | Loss: 0.5768576860427856Training Epoch: 6 | iteration: 51/262 | Loss: 0.6206063628196716Training Epoch: 6 | iteration: 52/262 | Loss: 0.6161635518074036Training Epoch: 6 | iteration: 53/262 | Loss: 0.5788062214851379Training Epoch: 6 | iteration: 54/262 | Loss: 0.5964018106460571Training Epoch: 6 | iteration: 55/262 | Loss: 0.5362103581428528Training Epoch: 6 | iteration: 56/262 | Loss: 0.622789204120636Training Epoch: 6 | iteration: 57/262 | Loss: 0.6200087070465088Training Epoch: 6 | iteration: 58/262 | Loss: 0.6122227907180786Training Epoch: 6 | iteration: 59/262 | Loss: 0.5982452630996704Training Epoch: 6 | iteration: 60/262 | Loss: 0.5956529974937439Training Epoch: 6 | iteration: 61/262 | Loss: 0.607825517654419Training Epoch: 6 | iteration: 62/262 | Loss: 0.6163083910942078Training Epoch: 6 | iteration: 63/262 | Loss: 0.6337447762489319Training Epoch: 6 | iteration: 64/262 | Loss: 0.6008722186088562Training Epoch: 6 | iteration: 65/262 | Loss: 0.6648047566413879Training Epoch: 6 | iteration: 66/262 | Loss: 0.5672780275344849Training Epoch: 6 | iteration: 67/262 | Loss: 0.5262841582298279Training Epoch: 6 | iteration: 68/262 | Loss: 0.6159179210662842Training Epoch: 6 | iteration: 69/262 | Loss: 0.5989001989364624Training Epoch: 6 | iteration: 70/262 | Loss: 0.5867135524749756Training Epoch: 6 | iteration: 71/262 | Loss: 0.5944529175758362Training Epoch: 6 | iteration: 72/262 | Loss: 0.6000713109970093Training Epoch: 6 | iteration: 73/262 | Loss: 0.6129469275474548Training Epoch: 6 | iteration: 74/262 | Loss: 0.5566714406013489Training Epoch: 6 | iteration: 75/262 | Loss: 0.5893073081970215Training Epoch: 6 | iteration: 76/262 | Loss: 0.6615900993347168Training Epoch: 6 | iteration: 77/262 | Loss: 0.5482101440429688Training Epoch: 6 | iteration: 78/262 | Loss: 0.5693342685699463Training Epoch: 6 | iteration: 79/262 | Loss: 0.6160560250282288Training Epoch: 6 | iteration: 80/262 | Loss: 0.5856796503067017Training Epoch: 6 | iteration: 81/262 | Loss: 0.5944694876670837Training Epoch: 6 | iteration: 82/262 | Loss: 0.5867315530776978Training Epoch: 6 | iteration: 83/262 | Loss: 0.6007165908813477Training Epoch: 6 | iteration: 84/262 | Loss: 0.6361488699913025Training Epoch: 6 | iteration: 85/262 | Loss: 0.583062469959259Training Epoch: 6 | iteration: 86/262 | Loss: 0.6006913185119629Training Epoch: 6 | iteration: 87/262 | Loss: 0.6416065096855164Training Epoch: 6 | iteration: 88/262 | Loss: 0.5797224044799805Training Epoch: 6 | iteration: 89/262 | Loss: 0.5753177404403687Training Epoch: 6 | iteration: 90/262 | Loss: 0.6116717457771301Training Epoch: 6 | iteration: 91/262 | Loss: 0.5934925079345703Training Epoch: 6 | iteration: 92/262 | Loss: 0.5842960476875305Training Epoch: 6 | iteration: 93/262 | Loss: 0.5678368806838989Training Epoch: 6 | iteration: 94/262 | Loss: 0.609269917011261Training Epoch: 6 | iteration: 95/262 | Loss: 0.6381814479827881Training Epoch: 6 | iteration: 96/262 | Loss: 0.6131933927536011Training Epoch: 6 | iteration: 97/262 | Loss: 0.5619156360626221Training Epoch: 6 | iteration: 98/262 | Loss: 0.5659523606300354Training Epoch: 6 | iteration: 99/262 | Loss: 0.5809762477874756Training Epoch: 6 | iteration: 100/262 | Loss: 0.5798418521881104Training Epoch: 6 | iteration: 101/262 | Loss: 0.5763720273971558Training Epoch: 6 | iteration: 102/262 | Loss: 0.6058835983276367Training Epoch: 6 | iteration: 103/262 | Loss: 0.5804720520973206Training Epoch: 6 | iteration: 104/262 | Loss: 0.6030640006065369Training Epoch: 6 | iteration: 105/262 | Loss: 0.5623672604560852Training Epoch: 6 | iteration: 106/262 | Loss: 0.5836336612701416Training Epoch: 6 | iteration: 107/262 | Loss: 0.5793250799179077Training Epoch: 6 | iteration: 108/262 | Loss: 0.593352735042572Training Epoch: 6 | iteration: 109/262 | Loss: 0.5836958289146423Training Epoch: 6 | iteration: 110/262 | Loss: 0.6167008876800537Training Epoch: 6 | iteration: 111/262 | Loss: 0.5863320827484131Training Epoch: 6 | iteration: 112/262 | Loss: 0.5867208242416382Training Epoch: 6 | iteration: 113/262 | Loss: 0.5855458974838257Training Epoch: 6 | iteration: 114/262 | Loss: 0.5912706255912781Training Epoch: 6 | iteration: 115/262 | Loss: 0.6362179517745972Training Epoch: 6 | iteration: 116/262 | Loss: 0.6289743781089783Training Epoch: 6 | iteration: 117/262 | Loss: 0.5655422210693359Training Epoch: 6 | iteration: 118/262 | Loss: 0.5996588468551636Training Epoch: 6 | iteration: 119/262 | Loss: 0.5889719724655151Training Epoch: 6 | iteration: 120/262 | Loss: 0.5674145221710205Training Epoch: 6 | iteration: 121/262 | Loss: 0.5772779583930969Training Epoch: 6 | iteration: 122/262 | Loss: 0.5890334248542786Training Epoch: 6 | iteration: 123/262 | Loss: 0.5896623730659485Training Epoch: 6 | iteration: 124/262 | Loss: 0.6283313632011414Training Epoch: 6 | iteration: 125/262 | Loss: 0.562226414680481Training Epoch: 6 | iteration: 126/262 | Loss: 0.5481534004211426Training Epoch: 6 | iteration: 127/262 | Loss: 0.6248415112495422Training Epoch: 6 | iteration: 128/262 | Loss: 0.6426774859428406Training Epoch: 6 | iteration: 129/262 | Loss: 0.6169605255126953Training Epoch: 6 | iteration: 130/262 | Loss: 0.58017897605896Training Epoch: 6 | iteration: 131/262 | Loss: 0.5901026725769043Training Epoch: 6 | iteration: 132/262 | Loss: 0.573014497756958Training Epoch: 6 | iteration: 133/262 | Loss: 0.5712721943855286Training Epoch: 6 | iteration: 134/262 | Loss: 0.5695398449897766Training Epoch: 6 | iteration: 135/262 | Loss: 0.5750405788421631Training Epoch: 6 | iteration: 136/262 | Loss: 0.5751774311065674Training Epoch: 6 | iteration: 137/262 | Loss: 0.5364086627960205Training Epoch: 6 | iteration: 138/262 | Loss: 0.5818377733230591Training Epoch: 6 | iteration: 139/262 | Loss: 0.5501374006271362Training Epoch: 6 | iteration: 140/262 | Loss: 0.5726464986801147Training Epoch: 6 | iteration: 141/262 | Loss: 0.6068217158317566Training Epoch: 6 | iteration: 142/262 | Loss: 0.5585872530937195Training Epoch: 6 | iteration: 143/262 | Loss: 0.6201512813568115Training Epoch: 6 | iteration: 144/262 | Loss: 0.5724756121635437Training Epoch: 6 | iteration: 145/262 | Loss: 0.5551213026046753Training Epoch: 6 | iteration: 146/262 | Loss: 0.5455416440963745Training Epoch: 6 | iteration: 147/262 | Loss: 0.6099948883056641Training Epoch: 6 | iteration: 148/262 | Loss: 0.5839211344718933Training Epoch: 6 | iteration: 149/262 | Loss: 0.605040431022644Training Epoch: 6 | iteration: 150/262 | Loss: 0.5909730195999146Training Epoch: 6 | iteration: 151/262 | Loss: 0.6080145835876465Training Epoch: 6 | iteration: 152/262 | Loss: 0.5950533747673035Training Epoch: 6 | iteration: 153/262 | Loss: 0.5779895186424255Training Epoch: 6 | iteration: 154/262 | Loss: 0.6104105114936829Training Epoch: 6 | iteration: 155/262 | Loss: 0.5979735255241394Training Epoch: 6 | iteration: 156/262 | Loss: 0.6010036468505859Training Epoch: 6 | iteration: 157/262 | Loss: 0.5858603715896606Training Epoch: 6 | iteration: 158/262 | Loss: 0.5794488191604614Training Epoch: 6 | iteration: 159/262 | Loss: 0.5971532464027405Training Epoch: 6 | iteration: 160/262 | Loss: 0.5804345607757568Training Epoch: 6 | iteration: 161/262 | Loss: 0.6169679164886475Training Epoch: 6 | iteration: 162/262 | Loss: 0.6179789304733276Training Epoch: 6 | iteration: 163/262 | Loss: 0.605984091758728Training Epoch: 6 | iteration: 164/262 | Loss: 0.5827738642692566Training Epoch: 6 | iteration: 165/262 | Loss: 0.5987809300422668Training Epoch: 6 | iteration: 166/262 | Loss: 0.6114016771316528Training Epoch: 6 | iteration: 167/262 | Loss: 0.5796540379524231Training Epoch: 6 | iteration: 168/262 | Loss: 0.5836681127548218Training Epoch: 6 | iteration: 169/262 | Loss: 0.6275395750999451Training Epoch: 6 | iteration: 170/262 | Loss: 0.5734918117523193Training Epoch: 6 | iteration: 171/262 | Loss: 0.5582317113876343Training Epoch: 6 | iteration: 172/262 | Loss: 0.5729938745498657Training Epoch: 6 | iteration: 173/262 | Loss: 0.5756076574325562Training Epoch: 6 | iteration: 174/262 | Loss: 0.5948169231414795Training Epoch: 6 | iteration: 175/262 | Loss: 0.6090132594108582Training Epoch: 6 | iteration: 176/262 | Loss: 0.6400668621063232Training Epoch: 6 | iteration: 177/262 | Loss: 0.5583615899085999Training Epoch: 6 | iteration: 178/262 | Loss: 0.5744484663009644Training Epoch: 6 | iteration: 179/262 | Loss: 0.5795660018920898Training Epoch: 6 | iteration: 180/262 | Loss: 0.6072615385055542Training Epoch: 6 | iteration: 181/262 | Loss: 0.5902833938598633Training Epoch: 6 | iteration: 182/262 | Loss: 0.5554839968681335Training Epoch: 6 | iteration: 183/262 | Loss: 0.5922836065292358Training Epoch: 6 | iteration: 184/262 | Loss: 0.5509529709815979Training Epoch: 6 | iteration: 185/262 | Loss: 0.5837342739105225Training Epoch: 6 | iteration: 186/262 | Loss: 0.5398070216178894Training Epoch: 6 | iteration: 187/262 | Loss: 0.5739692449569702Training Epoch: 6 | iteration: 188/262 | Loss: 0.5783575773239136Training Epoch: 6 | iteration: 189/262 | Loss: 0.6275204420089722Training Epoch: 6 | iteration: 190/262 | Loss: 0.5613662600517273Training Epoch: 6 | iteration: 191/262 | Loss: 0.5872124433517456Training Epoch: 6 | iteration: 192/262 | Loss: 0.6254602670669556Training Epoch: 6 | iteration: 193/262 | Loss: 0.5778717994689941Training Epoch: 6 | iteration: 194/262 | Loss: 0.5660812854766846Training Epoch: 6 | iteration: 195/262 | Loss: 0.564588189125061Training Epoch: 6 | iteration: 196/262 | Loss: 0.6441577076911926Training Epoch: 6 | iteration: 197/262 | Loss: 0.6534391641616821Training Epoch: 6 | iteration: 198/262 | Loss: 0.640384316444397Training Epoch: 6 | iteration: 199/262 | Loss: 0.5905472636222839Training Epoch: 6 | iteration: 200/262 | Loss: 0.6262000799179077Training Epoch: 6 | iteration: 201/262 | Loss: 0.560862123966217Training Epoch: 6 | iteration: 202/262 | Loss: 0.6428331136703491Training Epoch: 6 | iteration: 203/262 | Loss: 0.5647804141044617Training Epoch: 6 | iteration: 204/262 | Loss: 0.5751713514328003Training Epoch: 6 | iteration: 205/262 | Loss: 0.5645130276679993Training Epoch: 6 | iteration: 206/262 | Loss: 0.5942082405090332Training Epoch: 6 | iteration: 207/262 | Loss: 0.5997982025146484Training Epoch: 6 | iteration: 208/262 | Loss: 0.6150352954864502Training Epoch: 6 | iteration: 209/262 | Loss: 0.5813460350036621Training Epoch: 6 | iteration: 210/262 | Loss: 0.5832327604293823Training Epoch: 6 | iteration: 211/262 | Loss: 0.6158095598220825Training Epoch: 6 | iteration: 212/262 | Loss: 0.5790435075759888Training Epoch: 6 | iteration: 213/262 | Loss: 0.5823724269866943Training Epoch: 6 | iteration: 214/262 | Loss: 0.607983410358429Training Epoch: 6 | iteration: 215/262 | Loss: 0.5562484264373779Training Epoch: 6 | iteration: 216/262 | Loss: 0.6000868082046509Training Epoch: 6 | iteration: 217/262 | Loss: 0.563646674156189Training Epoch: 6 | iteration: 218/262 | Loss: 0.5433773994445801Training Epoch: 6 | iteration: 219/262 | Loss: 0.5486626625061035Training Epoch: 6 | iteration: 220/262 | Loss: 0.6084854602813721Training Epoch: 6 | iteration: 221/262 | Loss: 0.5675534009933472Training Epoch: 6 | iteration: 222/262 | Loss: 0.5383354425430298Training Epoch: 6 | iteration: 223/262 | Loss: 0.5845152139663696Training Epoch: 6 | iteration: 224/262 | Loss: 0.6124469041824341Training Epoch: 6 | iteration: 225/262 | Loss: 0.5722002983093262Training Epoch: 6 | iteration: 226/262 | Loss: 0.6078835129737854Training Epoch: 6 | iteration: 227/262 | Loss: 0.6135491132736206Training Epoch: 6 | iteration: 228/262 | Loss: 0.6046112179756165Training Epoch: 6 | iteration: 229/262 | Loss: 0.589810848236084Training Epoch: 6 | iteration: 230/262 | Loss: 0.5865117311477661Training Epoch: 6 | iteration: 231/262 | Loss: 0.5939756631851196Training Epoch: 6 | iteration: 232/262 | Loss: 0.5993565320968628Training Epoch: 6 | iteration: 233/262 | Loss: 0.6370704770088196Training Epoch: 6 | iteration: 234/262 | Loss: 0.5448678731918335Training Epoch: 6 | iteration: 235/262 | Loss: 0.5859564542770386Training Epoch: 6 | iteration: 236/262 | Loss: 0.6232348680496216Training Epoch: 6 | iteration: 237/262 | Loss: 0.6210380792617798Training Epoch: 6 | iteration: 238/262 | Loss: 0.6467981338500977Training Epoch: 6 | iteration: 239/262 | Loss: 0.5807043313980103Training Epoch: 6 | iteration: 240/262 | Loss: 0.6084178686141968Training Epoch: 6 | iteration: 241/262 | Loss: 0.5840147733688354Training Epoch: 6 | iteration: 242/262 | Loss: 0.6117907762527466Training Epoch: 6 | iteration: 243/262 | Loss: 0.6036498546600342Training Epoch: 6 | iteration: 244/262 | Loss: 0.5442513823509216Training Epoch: 6 | iteration: 245/262 | Loss: 0.564613938331604Training Epoch: 6 | iteration: 246/262 | Loss: 0.5590200424194336Training Epoch: 6 | iteration: 247/262 | Loss: 0.5812766551971436Training Epoch: 6 | iteration: 248/262 | Loss: 0.5812522768974304Training Epoch: 6 | iteration: 249/262 | Loss: 0.5871384143829346Training Epoch: 6 | iteration: 250/262 | Loss: 0.5791700482368469Training Epoch: 6 | iteration: 251/262 | Loss: 0.6588042974472046Training Epoch: 6 | iteration: 252/262 | Loss: 0.5761843919754028Training Epoch: 6 | iteration: 253/262 | Loss: 0.5422303676605225Training Epoch: 6 | iteration: 254/262 | Loss: 0.5760425329208374Training Epoch: 6 | iteration: 255/262 | Loss: 0.572726845741272Training Epoch: 6 | iteration: 256/262 | Loss: 0.6073290109634399Training Epoch: 6 | iteration: 257/262 | Loss: 0.6132470369338989Training Epoch: 6 | iteration: 258/262 | Loss: 0.5968073606491089Training Epoch: 6 | iteration: 259/262 | Loss: 0.5797461271286011Training Epoch: 6 | iteration: 260/262 | Loss: 0.5703474283218384Training Epoch: 6 | iteration: 261/262 | Loss: 0.6184458136558533Validating Epoch: 6 | iteration: 0/66 | Loss: 0.5566328763961792Validating Epoch: 6 | iteration: 1/66 | Loss: 0.6157069206237793Validating Epoch: 6 | iteration: 2/66 | Loss: 0.6326584219932556Validating Epoch: 6 | iteration: 3/66 | Loss: 0.5362535715103149Validating Epoch: 6 | iteration: 4/66 | Loss: 0.6066228151321411Validating Epoch: 6 | iteration: 5/66 | Loss: 0.6170463562011719Validating Epoch: 6 | iteration: 6/66 | Loss: 0.6228687763214111Validating Epoch: 6 | iteration: 7/66 | Loss: 0.5578335523605347Validating Epoch: 6 | iteration: 8/66 | Loss: 0.5816236734390259Validating Epoch: 6 | iteration: 9/66 | Loss: 0.5720361471176147Validating Epoch: 6 | iteration: 10/66 | Loss: 0.5534740686416626Validating Epoch: 6 | iteration: 11/66 | Loss: 0.6190475225448608Validating Epoch: 6 | iteration: 12/66 | Loss: 0.5557582378387451Validating Epoch: 6 | iteration: 13/66 | Loss: 0.6069636344909668Validating Epoch: 6 | iteration: 14/66 | Loss: 0.5558997392654419Validating Epoch: 6 | iteration: 15/66 | Loss: 0.6298168897628784Validating Epoch: 6 | iteration: 16/66 | Loss: 0.6135324239730835Validating Epoch: 6 | iteration: 17/66 | Loss: 0.5831259489059448Validating Epoch: 6 | iteration: 18/66 | Loss: 0.5860695838928223Validating Epoch: 6 | iteration: 19/66 | Loss: 0.6236191987991333Validating Epoch: 6 | iteration: 20/66 | Loss: 0.6024066209793091Validating Epoch: 6 | iteration: 21/66 | Loss: 0.6155115962028503Validating Epoch: 6 | iteration: 22/66 | Loss: 0.6350642442703247Validating Epoch: 6 | iteration: 23/66 | Loss: 0.5969440937042236Validating Epoch: 6 | iteration: 24/66 | Loss: 0.5995771884918213Validating Epoch: 6 | iteration: 25/66 | Loss: 0.5605306625366211Validating Epoch: 6 | iteration: 26/66 | Loss: 0.5922940969467163Validating Epoch: 6 | iteration: 27/66 | Loss: 0.5662229061126709Validating Epoch: 6 | iteration: 28/66 | Loss: 0.5951909422874451Validating Epoch: 6 | iteration: 29/66 | Loss: 0.5943223237991333Validating Epoch: 6 | iteration: 30/66 | Loss: 0.5846949219703674Validating Epoch: 6 | iteration: 31/66 | Loss: 0.5831301212310791Validating Epoch: 6 | iteration: 32/66 | Loss: 0.5797977447509766Validating Epoch: 6 | iteration: 33/66 | Loss: 0.6135138273239136Validating Epoch: 6 | iteration: 34/66 | Loss: 0.5975059270858765Validating Epoch: 6 | iteration: 35/66 | Loss: 0.5867482423782349Validating Epoch: 6 | iteration: 36/66 | Loss: 0.6207361221313477Validating Epoch: 6 | iteration: 37/66 | Loss: 0.6092212200164795Validating Epoch: 6 | iteration: 38/66 | Loss: 0.5876140594482422Validating Epoch: 6 | iteration: 39/66 | Loss: 0.5711303353309631Validating Epoch: 6 | iteration: 40/66 | Loss: 0.5650913715362549Validating Epoch: 6 | iteration: 41/66 | Loss: 0.6184812784194946Validating Epoch: 6 | iteration: 42/66 | Loss: 0.5740966200828552Validating Epoch: 6 | iteration: 43/66 | Loss: 0.640887975692749Validating Epoch: 6 | iteration: 44/66 | Loss: 0.6335588693618774Validating Epoch: 6 | iteration: 45/66 | Loss: 0.5825273990631104Validating Epoch: 6 | iteration: 46/66 | Loss: 0.5459152460098267Validating Epoch: 6 | iteration: 47/66 | Loss: 0.6187918186187744Validating Epoch: 6 | iteration: 48/66 | Loss: 0.6025103330612183Validating Epoch: 6 | iteration: 49/66 | Loss: 0.6232874989509583Validating Epoch: 6 | iteration: 50/66 | Loss: 0.6173746585845947Validating Epoch: 6 | iteration: 51/66 | Loss: 0.5803687572479248Validating Epoch: 6 | iteration: 52/66 | Loss: 0.6337677240371704Validating Epoch: 6 | iteration: 53/66 | Loss: 0.6012271642684937Validating Epoch: 6 | iteration: 54/66 | Loss: 0.5848444104194641Validating Epoch: 6 | iteration: 55/66 | Loss: 0.5810413360595703Validating Epoch: 6 | iteration: 56/66 | Loss: 0.5897068977355957Validating Epoch: 6 | iteration: 57/66 | Loss: 0.6104137897491455Validating Epoch: 6 | iteration: 58/66 | Loss: 0.5849502086639404Validating Epoch: 6 | iteration: 59/66 | Loss: 0.5795255899429321Validating Epoch: 6 | iteration: 60/66 | Loss: 0.5996609926223755Validating Epoch: 6 | iteration: 61/66 | Loss: 0.5946560502052307Validating Epoch: 6 | iteration: 62/66 | Loss: 0.5812727212905884Validating Epoch: 6 | iteration: 63/66 | Loss: 0.6286745071411133Validating Epoch: 6 | iteration: 64/66 | Loss: 0.5945674180984497Validating Epoch: 6 | iteration: 65/66 | Loss: 0.5643211007118225Temperature:  0.5
Sampling: iteration: 0/51{'Validity': 0.923828125, 'Novelty': 1.0, 'Uniqueness': 0.9429175475687104}
Training Epoch: 7 | iteration: 0/262 | Loss: 0.624201774597168Training Epoch: 7 | iteration: 1/262 | Loss: 0.5719279050827026Training Epoch: 7 | iteration: 2/262 | Loss: 0.5888373255729675Training Epoch: 7 | iteration: 3/262 | Loss: 0.576740026473999Training Epoch: 7 | iteration: 4/262 | Loss: 0.5897760987281799Training Epoch: 7 | iteration: 5/262 | Loss: 0.6512675285339355Training Epoch: 7 | iteration: 6/262 | Loss: 0.5135139226913452Training Epoch: 7 | iteration: 7/262 | Loss: 0.5978455543518066Training Epoch: 7 | iteration: 8/262 | Loss: 0.5529904961585999Training Epoch: 7 | iteration: 9/262 | Loss: 0.5747208595275879Training Epoch: 7 | iteration: 10/262 | Loss: 0.5879948139190674Training Epoch: 7 | iteration: 11/262 | Loss: 0.5905420780181885Training Epoch: 7 | iteration: 12/262 | Loss: 0.552482008934021Training Epoch: 7 | iteration: 13/262 | Loss: 0.5824975967407227Training Epoch: 7 | iteration: 14/262 | Loss: 0.5818337202072144Training Epoch: 7 | iteration: 15/262 | Loss: 0.5491708517074585Training Epoch: 7 | iteration: 16/262 | Loss: 0.5678603053092957Training Epoch: 7 | iteration: 17/262 | Loss: 0.5670708417892456Training Epoch: 7 | iteration: 18/262 | Loss: 0.5900938510894775Training Epoch: 7 | iteration: 19/262 | Loss: 0.6329618096351624Training Epoch: 7 | iteration: 20/262 | Loss: 0.5828331112861633Training Epoch: 7 | iteration: 21/262 | Loss: 0.5635619163513184Training Epoch: 7 | iteration: 22/262 | Loss: 0.5644946098327637Training Epoch: 7 | iteration: 23/262 | Loss: 0.6530380845069885Training Epoch: 7 | iteration: 24/262 | Loss: 0.575083315372467Training Epoch: 7 | iteration: 25/262 | Loss: 0.5402355194091797Training Epoch: 7 | iteration: 26/262 | Loss: 0.5886590480804443Training Epoch: 7 | iteration: 27/262 | Loss: 0.5859294533729553Training Epoch: 7 | iteration: 28/262 | Loss: 0.5777875185012817Training Epoch: 7 | iteration: 29/262 | Loss: 0.5736780166625977Training Epoch: 7 | iteration: 30/262 | Loss: 0.550995945930481Training Epoch: 7 | iteration: 31/262 | Loss: 0.5861748456954956Training Epoch: 7 | iteration: 32/262 | Loss: 0.5575283169746399Training Epoch: 7 | iteration: 33/262 | Loss: 0.5524013042449951Training Epoch: 7 | iteration: 34/262 | Loss: 0.5999107360839844Training Epoch: 7 | iteration: 35/262 | Loss: 0.6147170066833496Training Epoch: 7 | iteration: 36/262 | Loss: 0.5914763808250427Training Epoch: 7 | iteration: 37/262 | Loss: 0.5721715092658997Training Epoch: 7 | iteration: 38/262 | Loss: 0.5458893775939941Training Epoch: 7 | iteration: 39/262 | Loss: 0.5407788753509521Training Epoch: 7 | iteration: 40/262 | Loss: 0.5740578174591064Training Epoch: 7 | iteration: 41/262 | Loss: 0.5788002014160156Training Epoch: 7 | iteration: 42/262 | Loss: 0.5935972929000854Training Epoch: 7 | iteration: 43/262 | Loss: 0.5704532861709595Training Epoch: 7 | iteration: 44/262 | Loss: 0.587601900100708Training Epoch: 7 | iteration: 45/262 | Loss: 0.5689319372177124Training Epoch: 7 | iteration: 46/262 | Loss: 0.5855240821838379Training Epoch: 7 | iteration: 47/262 | Loss: 0.5932571291923523Training Epoch: 7 | iteration: 48/262 | Loss: 0.5837135314941406Training Epoch: 7 | iteration: 49/262 | Loss: 0.5260800123214722Training Epoch: 7 | iteration: 50/262 | Loss: 0.5795043706893921Training Epoch: 7 | iteration: 51/262 | Loss: 0.6475002765655518Training Epoch: 7 | iteration: 52/262 | Loss: 0.6132216453552246Training Epoch: 7 | iteration: 53/262 | Loss: 0.5721004009246826Training Epoch: 7 | iteration: 54/262 | Loss: 0.5652672052383423Training Epoch: 7 | iteration: 55/262 | Loss: 0.6062085032463074Training Epoch: 7 | iteration: 56/262 | Loss: 0.5893369913101196Training Epoch: 7 | iteration: 57/262 | Loss: 0.5777196884155273Training Epoch: 7 | iteration: 58/262 | Loss: 0.5920592546463013Training Epoch: 7 | iteration: 59/262 | Loss: 0.5526105165481567Training Epoch: 7 | iteration: 60/262 | Loss: 0.5449178218841553Training Epoch: 7 | iteration: 61/262 | Loss: 0.5932942628860474Training Epoch: 7 | iteration: 62/262 | Loss: 0.564649224281311Training Epoch: 7 | iteration: 63/262 | Loss: 0.5958335995674133Training Epoch: 7 | iteration: 64/262 | Loss: 0.552574634552002Training Epoch: 7 | iteration: 65/262 | Loss: 0.5840097665786743Training Epoch: 7 | iteration: 66/262 | Loss: 0.5954558253288269Training Epoch: 7 | iteration: 67/262 | Loss: 0.6057662963867188Training Epoch: 7 | iteration: 68/262 | Loss: 0.5745741724967957Training Epoch: 7 | iteration: 69/262 | Loss: 0.5523221492767334Training Epoch: 7 | iteration: 70/262 | Loss: 0.6322909593582153Training Epoch: 7 | iteration: 71/262 | Loss: 0.5999008417129517Training Epoch: 7 | iteration: 72/262 | Loss: 0.5928430557250977Training Epoch: 7 | iteration: 73/262 | Loss: 0.5785691738128662Training Epoch: 7 | iteration: 74/262 | Loss: 0.5853986144065857Training Epoch: 7 | iteration: 75/262 | Loss: 0.5688163638114929Training Epoch: 7 | iteration: 76/262 | Loss: 0.6128637790679932Training Epoch: 7 | iteration: 77/262 | Loss: 0.5668429732322693Training Epoch: 7 | iteration: 78/262 | Loss: 0.5955398082733154Training Epoch: 7 | iteration: 79/262 | Loss: 0.5966202020645142Training Epoch: 7 | iteration: 80/262 | Loss: 0.5639978647232056Training Epoch: 7 | iteration: 81/262 | Loss: 0.5950218439102173Training Epoch: 7 | iteration: 82/262 | Loss: 0.5934063196182251Training Epoch: 7 | iteration: 83/262 | Loss: 0.610196590423584Training Epoch: 7 | iteration: 84/262 | Loss: 0.5644063949584961Training Epoch: 7 | iteration: 85/262 | Loss: 0.5901739001274109Training Epoch: 7 | iteration: 86/262 | Loss: 0.5497891306877136Training Epoch: 7 | iteration: 87/262 | Loss: 0.5698868036270142Training Epoch: 7 | iteration: 88/262 | Loss: 0.61158287525177Training Epoch: 7 | iteration: 89/262 | Loss: 0.6084649562835693Training Epoch: 7 | iteration: 90/262 | Loss: 0.5719389915466309Training Epoch: 7 | iteration: 91/262 | Loss: 0.5767443180084229Training Epoch: 7 | iteration: 92/262 | Loss: 0.5796933770179749Training Epoch: 7 | iteration: 93/262 | Loss: 0.5930415391921997Training Epoch: 7 | iteration: 94/262 | Loss: 0.567108154296875Training Epoch: 7 | iteration: 95/262 | Loss: 0.5802576541900635Training Epoch: 7 | iteration: 96/262 | Loss: 0.5803636312484741Training Epoch: 7 | iteration: 97/262 | Loss: 0.5410054922103882Training Epoch: 7 | iteration: 98/262 | Loss: 0.5777636766433716Training Epoch: 7 | iteration: 99/262 | Loss: 0.5950543880462646Training Epoch: 7 | iteration: 100/262 | Loss: 0.5452287197113037Training Epoch: 7 | iteration: 101/262 | Loss: 0.5800873637199402Training Epoch: 7 | iteration: 102/262 | Loss: 0.5828219652175903Training Epoch: 7 | iteration: 103/262 | Loss: 0.5493463277816772Training Epoch: 7 | iteration: 104/262 | Loss: 0.6330522894859314Training Epoch: 7 | iteration: 105/262 | Loss: 0.5362711548805237Training Epoch: 7 | iteration: 106/262 | Loss: 0.6213924884796143Training Epoch: 7 | iteration: 107/262 | Loss: 0.5872235298156738Training Epoch: 7 | iteration: 108/262 | Loss: 0.5664709806442261Training Epoch: 7 | iteration: 109/262 | Loss: 0.6011188626289368Training Epoch: 7 | iteration: 110/262 | Loss: 0.526185154914856Training Epoch: 7 | iteration: 111/262 | Loss: 0.5486711263656616Training Epoch: 7 | iteration: 112/262 | Loss: 0.6647825241088867Training Epoch: 7 | iteration: 113/262 | Loss: 0.5855692028999329Training Epoch: 7 | iteration: 114/262 | Loss: 0.5770183205604553Training Epoch: 7 | iteration: 115/262 | Loss: 0.5437326431274414Training Epoch: 7 | iteration: 116/262 | Loss: 0.5931923389434814Training Epoch: 7 | iteration: 117/262 | Loss: 0.5546557307243347Training Epoch: 7 | iteration: 118/262 | Loss: 0.5916197299957275Training Epoch: 7 | iteration: 119/262 | Loss: 0.5593357086181641Training Epoch: 7 | iteration: 120/262 | Loss: 0.5858728289604187Training Epoch: 7 | iteration: 121/262 | Loss: 0.5509087443351746Training Epoch: 7 | iteration: 122/262 | Loss: 0.58673095703125Training Epoch: 7 | iteration: 123/262 | Loss: 0.5674406290054321Training Epoch: 7 | iteration: 124/262 | Loss: 0.5660431385040283Training Epoch: 7 | iteration: 125/262 | Loss: 0.6114632487297058Training Epoch: 7 | iteration: 126/262 | Loss: 0.5924805402755737Training Epoch: 7 | iteration: 127/262 | Loss: 0.5603911876678467Training Epoch: 7 | iteration: 128/262 | Loss: 0.5810933113098145Training Epoch: 7 | iteration: 129/262 | Loss: 0.5867828726768494Training Epoch: 7 | iteration: 130/262 | Loss: 0.5561301112174988Training Epoch: 7 | iteration: 131/262 | Loss: 0.5982714891433716Training Epoch: 7 | iteration: 132/262 | Loss: 0.593750536441803Training Epoch: 7 | iteration: 133/262 | Loss: 0.5987955927848816Training Epoch: 7 | iteration: 134/262 | Loss: 0.5333946943283081Training Epoch: 7 | iteration: 135/262 | Loss: 0.6386634111404419Training Epoch: 7 | iteration: 136/262 | Loss: 0.5362755060195923Training Epoch: 7 | iteration: 137/262 | Loss: 0.5615723133087158Training Epoch: 7 | iteration: 138/262 | Loss: 0.5878294706344604Training Epoch: 7 | iteration: 139/262 | Loss: 0.5812861323356628Training Epoch: 7 | iteration: 140/262 | Loss: 0.5839704871177673Training Epoch: 7 | iteration: 141/262 | Loss: 0.5745511054992676Training Epoch: 7 | iteration: 142/262 | Loss: 0.5580968260765076Training Epoch: 7 | iteration: 143/262 | Loss: 0.5545703172683716Training Epoch: 7 | iteration: 144/262 | Loss: 0.5923246145248413Training Epoch: 7 | iteration: 145/262 | Loss: 0.6066176295280457Training Epoch: 7 | iteration: 146/262 | Loss: 0.6096270084381104Training Epoch: 7 | iteration: 147/262 | Loss: 0.584015965461731Training Epoch: 7 | iteration: 148/262 | Loss: 0.5865031480789185Training Epoch: 7 | iteration: 149/262 | Loss: 0.6036832332611084Training Epoch: 7 | iteration: 150/262 | Loss: 0.5728312134742737Training Epoch: 7 | iteration: 151/262 | Loss: 0.5633805990219116Training Epoch: 7 | iteration: 152/262 | Loss: 0.6089648008346558Training Epoch: 7 | iteration: 153/262 | Loss: 0.5521335601806641Training Epoch: 7 | iteration: 154/262 | Loss: 0.5771015882492065Training Epoch: 7 | iteration: 155/262 | Loss: 0.6003110408782959Training Epoch: 7 | iteration: 156/262 | Loss: 0.5854893922805786Training Epoch: 7 | iteration: 157/262 | Loss: 0.5561520457267761Training Epoch: 7 | iteration: 158/262 | Loss: 0.5731765031814575Training Epoch: 7 | iteration: 159/262 | Loss: 0.60075843334198Training Epoch: 7 | iteration: 160/262 | Loss: 0.6338416934013367Training Epoch: 7 | iteration: 161/262 | Loss: 0.6063292026519775Training Epoch: 7 | iteration: 162/262 | Loss: 0.5713140964508057Training Epoch: 7 | iteration: 163/262 | Loss: 0.5473548769950867Training Epoch: 7 | iteration: 164/262 | Loss: 0.5951328277587891Training Epoch: 7 | iteration: 165/262 | Loss: 0.5636583566665649Training Epoch: 7 | iteration: 166/262 | Loss: 0.6296278238296509Training Epoch: 7 | iteration: 167/262 | Loss: 0.5638782382011414Training Epoch: 7 | iteration: 168/262 | Loss: 0.5689668655395508Training Epoch: 7 | iteration: 169/262 | Loss: 0.5484124422073364Training Epoch: 7 | iteration: 170/262 | Loss: 0.6324738264083862Training Epoch: 7 | iteration: 171/262 | Loss: 0.588647723197937Training Epoch: 7 | iteration: 172/262 | Loss: 0.5663372278213501Training Epoch: 7 | iteration: 173/262 | Loss: 0.6127912998199463Training Epoch: 7 | iteration: 174/262 | Loss: 0.5822596549987793Training Epoch: 7 | iteration: 175/262 | Loss: 0.5637736320495605Training Epoch: 7 | iteration: 176/262 | Loss: 0.5893818736076355Training Epoch: 7 | iteration: 177/262 | Loss: 0.5799704790115356Training Epoch: 7 | iteration: 178/262 | Loss: 0.6263002157211304Training Epoch: 7 | iteration: 179/262 | Loss: 0.5931597948074341Training Epoch: 7 | iteration: 180/262 | Loss: 0.6042007207870483Training Epoch: 7 | iteration: 181/262 | Loss: 0.5611100792884827Training Epoch: 7 | iteration: 182/262 | Loss: 0.5705794095993042Training Epoch: 7 | iteration: 183/262 | Loss: 0.5906293392181396Training Epoch: 7 | iteration: 184/262 | Loss: 0.5493069291114807Training Epoch: 7 | iteration: 185/262 | Loss: 0.5832343697547913Training Epoch: 7 | iteration: 186/262 | Loss: 0.5810415744781494Training Epoch: 7 | iteration: 187/262 | Loss: 0.6400039196014404Training Epoch: 7 | iteration: 188/262 | Loss: 0.5694127082824707Training Epoch: 7 | iteration: 189/262 | Loss: 0.6097588539123535Training Epoch: 7 | iteration: 190/262 | Loss: 0.5587738752365112Training Epoch: 7 | iteration: 191/262 | Loss: 0.5683061480522156Training Epoch: 7 | iteration: 192/262 | Loss: 0.5449081659317017Training Epoch: 7 | iteration: 193/262 | Loss: 0.5662742853164673Training Epoch: 7 | iteration: 194/262 | Loss: 0.5623974800109863Training Epoch: 7 | iteration: 195/262 | Loss: 0.5576948523521423Training Epoch: 7 | iteration: 196/262 | Loss: 0.5849406719207764Training Epoch: 7 | iteration: 197/262 | Loss: 0.5514254570007324Training Epoch: 7 | iteration: 198/262 | Loss: 0.5557668805122375Training Epoch: 7 | iteration: 199/262 | Loss: 0.5545699596405029Training Epoch: 7 | iteration: 200/262 | Loss: 0.5666735172271729Training Epoch: 7 | iteration: 201/262 | Loss: 0.5973552465438843Training Epoch: 7 | iteration: 202/262 | Loss: 0.5982152819633484Training Epoch: 7 | iteration: 203/262 | Loss: 0.5617285966873169Training Epoch: 7 | iteration: 204/262 | Loss: 0.5716195106506348Training Epoch: 7 | iteration: 205/262 | Loss: 0.5961179733276367Training Epoch: 7 | iteration: 206/262 | Loss: 0.612116277217865Training Epoch: 7 | iteration: 207/262 | Loss: 0.5619759559631348Training Epoch: 7 | iteration: 208/262 | Loss: 0.5760523080825806Training Epoch: 7 | iteration: 209/262 | Loss: 0.5753790140151978Training Epoch: 7 | iteration: 210/262 | Loss: 0.577415406703949Training Epoch: 7 | iteration: 211/262 | Loss: 0.5993508100509644Training Epoch: 7 | iteration: 212/262 | Loss: 0.5653423070907593Training Epoch: 7 | iteration: 213/262 | Loss: 0.6336959600448608Training Epoch: 7 | iteration: 214/262 | Loss: 0.5801918506622314Training Epoch: 7 | iteration: 215/262 | Loss: 0.5457340478897095Training Epoch: 7 | iteration: 216/262 | Loss: 0.5641012191772461Training Epoch: 7 | iteration: 217/262 | Loss: 0.5671017169952393Training Epoch: 7 | iteration: 218/262 | Loss: 0.5994417667388916Training Epoch: 7 | iteration: 219/262 | Loss: 0.5885242223739624Training Epoch: 7 | iteration: 220/262 | Loss: 0.5529680252075195Training Epoch: 7 | iteration: 221/262 | Loss: 0.5810189247131348Training Epoch: 7 | iteration: 222/262 | Loss: 0.5440052151679993Training Epoch: 7 | iteration: 223/262 | Loss: 0.648301362991333Training Epoch: 7 | iteration: 224/262 | Loss: 0.6103103756904602Training Epoch: 7 | iteration: 225/262 | Loss: 0.5611634254455566Training Epoch: 7 | iteration: 226/262 | Loss: 0.5612439513206482Training Epoch: 7 | iteration: 227/262 | Loss: 0.5942134261131287Training Epoch: 7 | iteration: 228/262 | Loss: 0.6047022342681885Training Epoch: 7 | iteration: 229/262 | Loss: 0.6324665546417236Training Epoch: 7 | iteration: 230/262 | Loss: 0.5706157684326172Training Epoch: 7 | iteration: 231/262 | Loss: 0.593647837638855Training Epoch: 7 | iteration: 232/262 | Loss: 0.5625414848327637Training Epoch: 7 | iteration: 233/262 | Loss: 0.5761916637420654Training Epoch: 7 | iteration: 234/262 | Loss: 0.5706983208656311Training Epoch: 7 | iteration: 235/262 | Loss: 0.5971087217330933Training Epoch: 7 | iteration: 236/262 | Loss: 0.555791974067688Training Epoch: 7 | iteration: 237/262 | Loss: 0.5532956123352051Training Epoch: 7 | iteration: 238/262 | Loss: 0.57194983959198Training Epoch: 7 | iteration: 239/262 | Loss: 0.5903723239898682Training Epoch: 7 | iteration: 240/262 | Loss: 0.5653108358383179Training Epoch: 7 | iteration: 241/262 | Loss: 0.6180925369262695Training Epoch: 7 | iteration: 242/262 | Loss: 0.5700600147247314Training Epoch: 7 | iteration: 243/262 | Loss: 0.5230282545089722Training Epoch: 7 | iteration: 244/262 | Loss: 0.595673680305481Training Epoch: 7 | iteration: 245/262 | Loss: 0.6055428981781006Training Epoch: 7 | iteration: 246/262 | Loss: 0.6061841249465942Training Epoch: 7 | iteration: 247/262 | Loss: 0.6384279131889343Training Epoch: 7 | iteration: 248/262 | Loss: 0.5407391786575317Training Epoch: 7 | iteration: 249/262 | Loss: 0.5341384410858154Training Epoch: 7 | iteration: 250/262 | Loss: 0.5751781463623047Training Epoch: 7 | iteration: 251/262 | Loss: 0.591803789138794Training Epoch: 7 | iteration: 252/262 | Loss: 0.5459243059158325Training Epoch: 7 | iteration: 253/262 | Loss: 0.5850886106491089Training Epoch: 7 | iteration: 254/262 | Loss: 0.631049394607544Training Epoch: 7 | iteration: 255/262 | Loss: 0.5863832235336304Training Epoch: 7 | iteration: 256/262 | Loss: 0.5675248503684998Training Epoch: 7 | iteration: 257/262 | Loss: 0.5701593160629272Training Epoch: 7 | iteration: 258/262 | Loss: 0.589104413986206Training Epoch: 7 | iteration: 259/262 | Loss: 0.6174556016921997Training Epoch: 7 | iteration: 260/262 | Loss: 0.5943585634231567Training Epoch: 7 | iteration: 261/262 | Loss: 0.5254731774330139Validating Epoch: 7 | iteration: 0/66 | Loss: 0.6202175617218018Validating Epoch: 7 | iteration: 1/66 | Loss: 0.5985735654830933Validating Epoch: 7 | iteration: 2/66 | Loss: 0.5776978731155396Validating Epoch: 7 | iteration: 3/66 | Loss: 0.5616053342819214Validating Epoch: 7 | iteration: 4/66 | Loss: 0.6349204182624817Validating Epoch: 7 | iteration: 5/66 | Loss: 0.5514742732048035Validating Epoch: 7 | iteration: 6/66 | Loss: 0.5644571185112Validating Epoch: 7 | iteration: 7/66 | Loss: 0.5912883281707764Validating Epoch: 7 | iteration: 8/66 | Loss: 0.6010555624961853Validating Epoch: 7 | iteration: 9/66 | Loss: 0.5927824974060059Validating Epoch: 7 | iteration: 10/66 | Loss: 0.5859503746032715Validating Epoch: 7 | iteration: 11/66 | Loss: 0.6178210973739624Validating Epoch: 7 | iteration: 12/66 | Loss: 0.5785520076751709Validating Epoch: 7 | iteration: 13/66 | Loss: 0.591952919960022Validating Epoch: 7 | iteration: 14/66 | Loss: 0.5708211660385132Validating Epoch: 7 | iteration: 15/66 | Loss: 0.5995897054672241Validating Epoch: 7 | iteration: 16/66 | Loss: 0.5850125551223755Validating Epoch: 7 | iteration: 17/66 | Loss: 0.6049196124076843Validating Epoch: 7 | iteration: 18/66 | Loss: 0.6214773654937744Validating Epoch: 7 | iteration: 19/66 | Loss: 0.5860689878463745Validating Epoch: 7 | iteration: 20/66 | Loss: 0.5586395263671875Validating Epoch: 7 | iteration: 21/66 | Loss: 0.5933104753494263Validating Epoch: 7 | iteration: 22/66 | Loss: 0.579860508441925Validating Epoch: 7 | iteration: 23/66 | Loss: 0.6379805207252502Validating Epoch: 7 | iteration: 24/66 | Loss: 0.6366013288497925Validating Epoch: 7 | iteration: 25/66 | Loss: 0.5957996845245361Validating Epoch: 7 | iteration: 26/66 | Loss: 0.5872606635093689Validating Epoch: 7 | iteration: 27/66 | Loss: 0.5817115902900696Validating Epoch: 7 | iteration: 28/66 | Loss: 0.6044287085533142Validating Epoch: 7 | iteration: 29/66 | Loss: 0.5834125876426697Validating Epoch: 7 | iteration: 30/66 | Loss: 0.5669071674346924Validating Epoch: 7 | iteration: 31/66 | Loss: 0.609220027923584Validating Epoch: 7 | iteration: 32/66 | Loss: 0.5794227123260498Validating Epoch: 7 | iteration: 33/66 | Loss: 0.5740693807601929Validating Epoch: 7 | iteration: 34/66 | Loss: 0.5680402517318726Validating Epoch: 7 | iteration: 35/66 | Loss: 0.5628279447555542Validating Epoch: 7 | iteration: 36/66 | Loss: 0.6325789093971252Validating Epoch: 7 | iteration: 37/66 | Loss: 0.5933225750923157Validating Epoch: 7 | iteration: 38/66 | Loss: 0.624000072479248Validating Epoch: 7 | iteration: 39/66 | Loss: 0.5944770574569702Validating Epoch: 7 | iteration: 40/66 | Loss: 0.5347537398338318Validating Epoch: 7 | iteration: 41/66 | Loss: 0.5246971845626831Validating Epoch: 7 | iteration: 42/66 | Loss: 0.5891430377960205Validating Epoch: 7 | iteration: 43/66 | Loss: 0.6291636228561401Validating Epoch: 7 | iteration: 44/66 | Loss: 0.597176194190979Validating Epoch: 7 | iteration: 45/66 | Loss: 0.5685180425643921Validating Epoch: 7 | iteration: 46/66 | Loss: 0.5846874713897705Validating Epoch: 7 | iteration: 47/66 | Loss: 0.5740764141082764Validating Epoch: 7 | iteration: 48/66 | Loss: 0.571016788482666Validating Epoch: 7 | iteration: 49/66 | Loss: 0.6093493103981018Validating Epoch: 7 | iteration: 50/66 | Loss: 0.5674053430557251Validating Epoch: 7 | iteration: 51/66 | Loss: 0.5620481967926025Validating Epoch: 7 | iteration: 52/66 | Loss: 0.6212012767791748Validating Epoch: 7 | iteration: 53/66 | Loss: 0.5615054368972778Validating Epoch: 7 | iteration: 54/66 | Loss: 0.5759869813919067Validating Epoch: 7 | iteration: 55/66 | Loss: 0.5903531312942505Validating Epoch: 7 | iteration: 56/66 | Loss: 0.5636897683143616Validating Epoch: 7 | iteration: 57/66 | Loss: 0.611667275428772Validating Epoch: 7 | iteration: 58/66 | Loss: 0.5907955765724182Validating Epoch: 7 | iteration: 59/66 | Loss: 0.6366080641746521Validating Epoch: 7 | iteration: 60/66 | Loss: 0.5846771001815796Validating Epoch: 7 | iteration: 61/66 | Loss: 0.582696795463562Validating Epoch: 7 | iteration: 62/66 | Loss: 0.6410961151123047Validating Epoch: 7 | iteration: 63/66 | Loss: 0.5638226270675659Validating Epoch: 7 | iteration: 64/66 | Loss: 0.6031032800674438Validating Epoch: 7 | iteration: 65/66 | Loss: 0.5605823397636414Temperature:  0.5
Sampling: iteration: 0/51{'Validity': 0.9658203125, 'Novelty': 1.0, 'Uniqueness': 0.9474216380182002}
Training Epoch: 8 | iteration: 0/262 | Loss: 0.5931392908096313Training Epoch: 8 | iteration: 1/262 | Loss: 0.5904426574707031Training Epoch: 8 | iteration: 2/262 | Loss: 0.5823717713356018Training Epoch: 8 | iteration: 3/262 | Loss: 0.5281388759613037Training Epoch: 8 | iteration: 4/262 | Loss: 0.5494635105133057Training Epoch: 8 | iteration: 5/262 | Loss: 0.6070380210876465Training Epoch: 8 | iteration: 6/262 | Loss: 0.577420711517334Training Epoch: 8 | iteration: 7/262 | Loss: 0.5436182022094727Training Epoch: 8 | iteration: 8/262 | Loss: 0.5122573375701904Training Epoch: 8 | iteration: 9/262 | Loss: 0.5722318887710571Training Epoch: 8 | iteration: 10/262 | Loss: 0.5997740030288696Training Epoch: 8 | iteration: 11/262 | Loss: 0.5378542542457581Training Epoch: 8 | iteration: 12/262 | Loss: 0.5867754817008972Training Epoch: 8 | iteration: 13/262 | Loss: 0.5452263951301575Training Epoch: 8 | iteration: 14/262 | Loss: 0.6011559963226318Training Epoch: 8 | iteration: 15/262 | Loss: 0.5645419955253601Training Epoch: 8 | iteration: 16/262 | Loss: 0.579268753528595Training Epoch: 8 | iteration: 17/262 | Loss: 0.5954293608665466Training Epoch: 8 | iteration: 18/262 | Loss: 0.531937837600708Training Epoch: 8 | iteration: 19/262 | Loss: 0.5676575303077698Training Epoch: 8 | iteration: 20/262 | Loss: 0.5243539214134216Training Epoch: 8 | iteration: 21/262 | Loss: 0.6109305620193481Training Epoch: 8 | iteration: 22/262 | Loss: 0.5991690158843994Training Epoch: 8 | iteration: 23/262 | Loss: 0.5672621726989746Training Epoch: 8 | iteration: 24/262 | Loss: 0.5546958446502686Training Epoch: 8 | iteration: 25/262 | Loss: 0.5596232414245605Training Epoch: 8 | iteration: 26/262 | Loss: 0.5595705509185791Training Epoch: 8 | iteration: 27/262 | Loss: 0.581946611404419Training Epoch: 8 | iteration: 28/262 | Loss: 0.533671498298645Training Epoch: 8 | iteration: 29/262 | Loss: 0.5932216048240662Training Epoch: 8 | iteration: 30/262 | Loss: 0.5558345913887024Training Epoch: 8 | iteration: 31/262 | Loss: 0.5320040583610535Training Epoch: 8 | iteration: 32/262 | Loss: 0.5743151903152466Training Epoch: 8 | iteration: 33/262 | Loss: 0.5455199480056763Training Epoch: 8 | iteration: 34/262 | Loss: 0.5649800300598145Training Epoch: 8 | iteration: 35/262 | Loss: 0.5853995084762573Training Epoch: 8 | iteration: 36/262 | Loss: 0.5615003108978271Training Epoch: 8 | iteration: 37/262 | Loss: 0.5357807874679565Training Epoch: 8 | iteration: 38/262 | Loss: 0.575415849685669Training Epoch: 8 | iteration: 39/262 | Loss: 0.5622910261154175Training Epoch: 8 | iteration: 40/262 | Loss: 0.5742281675338745Training Epoch: 8 | iteration: 41/262 | Loss: 0.5854921936988831Training Epoch: 8 | iteration: 42/262 | Loss: 0.6009290218353271Training Epoch: 8 | iteration: 43/262 | Loss: 0.5741713047027588Training Epoch: 8 | iteration: 44/262 | Loss: 0.597183346748352Training Epoch: 8 | iteration: 45/262 | Loss: 0.5755643844604492Training Epoch: 8 | iteration: 46/262 | Loss: 0.5080026984214783Training Epoch: 8 | iteration: 47/262 | Loss: 0.5946319103240967Training Epoch: 8 | iteration: 48/262 | Loss: 0.5357086062431335Training Epoch: 8 | iteration: 49/262 | Loss: 0.5534746646881104Training Epoch: 8 | iteration: 50/262 | Loss: 0.5660237669944763Training Epoch: 8 | iteration: 51/262 | Loss: 0.5314851999282837Training Epoch: 8 | iteration: 52/262 | Loss: 0.6097339391708374Training Epoch: 8 | iteration: 53/262 | Loss: 0.5608407258987427Training Epoch: 8 | iteration: 54/262 | Loss: 0.5967462062835693Training Epoch: 8 | iteration: 55/262 | Loss: 0.568755030632019Training Epoch: 8 | iteration: 56/262 | Loss: 0.5379785299301147Training Epoch: 8 | iteration: 57/262 | Loss: 0.5292072296142578Training Epoch: 8 | iteration: 58/262 | Loss: 0.5560135841369629Training Epoch: 8 | iteration: 59/262 | Loss: 0.5752551555633545Training Epoch: 8 | iteration: 60/262 | Loss: 0.5563746690750122Training Epoch: 8 | iteration: 61/262 | Loss: 0.6209165453910828Training Epoch: 8 | iteration: 62/262 | Loss: 0.5900634527206421Training Epoch: 8 | iteration: 63/262 | Loss: 0.560888409614563Training Epoch: 8 | iteration: 64/262 | Loss: 0.5915976762771606Training Epoch: 8 | iteration: 65/262 | Loss: 0.5396720767021179Training Epoch: 8 | iteration: 66/262 | Loss: 0.5837842226028442Training Epoch: 8 | iteration: 67/262 | Loss: 0.541414737701416Training Epoch: 8 | iteration: 68/262 | Loss: 0.5247207880020142Training Epoch: 8 | iteration: 69/262 | Loss: 0.541008472442627Training Epoch: 8 | iteration: 70/262 | Loss: 0.5342597961425781Training Epoch: 8 | iteration: 71/262 | Loss: 0.5454225540161133Training Epoch: 8 | iteration: 72/262 | Loss: 0.5399072766304016Training Epoch: 8 | iteration: 73/262 | Loss: 0.5645986795425415Training Epoch: 8 | iteration: 74/262 | Loss: 0.5345039367675781Training Epoch: 8 | iteration: 75/262 | Loss: 0.546755313873291Training Epoch: 8 | iteration: 76/262 | Loss: 0.5874603986740112Training Epoch: 8 | iteration: 77/262 | Loss: 0.5800491571426392Training Epoch: 8 | iteration: 78/262 | Loss: 0.5531340837478638Training Epoch: 8 | iteration: 79/262 | Loss: 0.5823283195495605Training Epoch: 8 | iteration: 80/262 | Loss: 0.6184394955635071Training Epoch: 8 | iteration: 81/262 | Loss: 0.5906240344047546Training Epoch: 8 | iteration: 82/262 | Loss: 0.5780577063560486Training Epoch: 8 | iteration: 83/262 | Loss: 0.563291072845459Training Epoch: 8 | iteration: 84/262 | Loss: 0.5992544293403625Training Epoch: 8 | iteration: 85/262 | Loss: 0.5798821449279785Training Epoch: 8 | iteration: 86/262 | Loss: 0.5815452337265015Training Epoch: 8 | iteration: 87/262 | Loss: 0.5988321304321289Training Epoch: 8 | iteration: 88/262 | Loss: 0.5197960138320923Training Epoch: 8 | iteration: 89/262 | Loss: 0.5554839372634888Training Epoch: 8 | iteration: 90/262 | Loss: 0.527543306350708Training Epoch: 8 | iteration: 91/262 | Loss: 0.5377848744392395Training Epoch: 8 | iteration: 92/262 | Loss: 0.5709484815597534Training Epoch: 8 | iteration: 93/262 | Loss: 0.5192804932594299Training Epoch: 8 | iteration: 94/262 | Loss: 0.5603325366973877Training Epoch: 8 | iteration: 95/262 | Loss: 0.5725103616714478Training Epoch: 8 | iteration: 96/262 | Loss: 0.5294643640518188Training Epoch: 8 | iteration: 97/262 | Loss: 0.54903244972229Training Epoch: 8 | iteration: 98/262 | Loss: 0.5985573530197144Training Epoch: 8 | iteration: 99/262 | Loss: 0.5422292351722717Training Epoch: 8 | iteration: 100/262 | Loss: 0.5368786454200745Training Epoch: 8 | iteration: 101/262 | Loss: 0.6192932724952698Training Epoch: 8 | iteration: 102/262 | Loss: 0.6321401000022888Training Epoch: 8 | iteration: 103/262 | Loss: 0.6060722470283508Training Epoch: 8 | iteration: 104/262 | Loss: 0.6026005744934082Training Epoch: 8 | iteration: 105/262 | Loss: 0.6232712268829346Training Epoch: 8 | iteration: 106/262 | Loss: 0.5531230568885803Training Epoch: 8 | iteration: 107/262 | Loss: 0.5619372129440308Training Epoch: 8 | iteration: 108/262 | Loss: 0.5849046111106873Training Epoch: 8 | iteration: 109/262 | Loss: 0.5595391988754272Training Epoch: 8 | iteration: 110/262 | Loss: 0.5962833762168884Training Epoch: 8 | iteration: 111/262 | Loss: 0.5620739459991455Training Epoch: 8 | iteration: 112/262 | Loss: 0.5575408935546875Training Epoch: 8 | iteration: 113/262 | Loss: 0.5583611726760864Training Epoch: 8 | iteration: 114/262 | Loss: 0.5757867097854614Training Epoch: 8 | iteration: 115/262 | Loss: 0.6106597781181335Training Epoch: 8 | iteration: 116/262 | Loss: 0.5399547815322876Training Epoch: 8 | iteration: 117/262 | Loss: 0.6245344877243042Training Epoch: 8 | iteration: 118/262 | Loss: 0.5020060539245605Training Epoch: 8 | iteration: 119/262 | Loss: 0.5761837959289551Training Epoch: 8 | iteration: 120/262 | Loss: 0.5584352016448975Training Epoch: 8 | iteration: 121/262 | Loss: 0.6062096357345581Training Epoch: 8 | iteration: 122/262 | Loss: 0.6100811958312988Training Epoch: 8 | iteration: 123/262 | Loss: 0.5588893890380859Training Epoch: 8 | iteration: 124/262 | Loss: 0.5578534603118896Training Epoch: 8 | iteration: 125/262 | Loss: 0.6060358285903931Training Epoch: 8 | iteration: 126/262 | Loss: 0.6183937788009644Training Epoch: 8 | iteration: 127/262 | Loss: 0.5684425830841064Training Epoch: 8 | iteration: 128/262 | Loss: 0.5744596123695374Training Epoch: 8 | iteration: 129/262 | Loss: 0.5440744757652283Training Epoch: 8 | iteration: 130/262 | Loss: 0.5529729127883911Training Epoch: 8 | iteration: 131/262 | Loss: 0.5455422401428223Training Epoch: 8 | iteration: 132/262 | Loss: 0.6066088676452637Training Epoch: 8 | iteration: 133/262 | Loss: 0.5167397260665894Training Epoch: 8 | iteration: 134/262 | Loss: 0.5549626350402832Training Epoch: 8 | iteration: 135/262 | Loss: 0.5463294386863708Training Epoch: 8 | iteration: 136/262 | Loss: 0.5843697786331177Training Epoch: 8 | iteration: 137/262 | Loss: 0.5678871273994446Training Epoch: 8 | iteration: 138/262 | Loss: 0.556107223033905Training Epoch: 8 | iteration: 139/262 | Loss: 0.5601574182510376Training Epoch: 8 | iteration: 140/262 | Loss: 0.5627235174179077Training Epoch: 8 | iteration: 141/262 | Loss: 0.564050555229187Training Epoch: 8 | iteration: 142/262 | Loss: 0.5459052920341492Training Epoch: 8 | iteration: 143/262 | Loss: 0.5442785620689392Training Epoch: 8 | iteration: 144/262 | Loss: 0.5853910446166992Training Epoch: 8 | iteration: 145/262 | Loss: 0.5310406684875488Training Epoch: 8 | iteration: 146/262 | Loss: 0.6027230024337769Training Epoch: 8 | iteration: 147/262 | Loss: 0.5317716598510742Training Epoch: 8 | iteration: 148/262 | Loss: 0.5558156967163086Training Epoch: 8 | iteration: 149/262 | Loss: 0.5991461277008057Training Epoch: 8 | iteration: 150/262 | Loss: 0.5983127951622009Training Epoch: 8 | iteration: 151/262 | Loss: 0.5761944055557251Training Epoch: 8 | iteration: 152/262 | Loss: 0.5659166574478149Training Epoch: 8 | iteration: 153/262 | Loss: 0.5289075374603271Training Epoch: 8 | iteration: 154/262 | Loss: 0.5978928804397583Training Epoch: 8 | iteration: 155/262 | Loss: 0.6137582063674927Training Epoch: 8 | iteration: 156/262 | Loss: 0.6056983470916748Training Epoch: 8 | iteration: 157/262 | Loss: 0.554595947265625Training Epoch: 8 | iteration: 158/262 | Loss: 0.5335793495178223Training Epoch: 8 | iteration: 159/262 | Loss: 0.5750747919082642Training Epoch: 8 | iteration: 160/262 | Loss: 0.6189802885055542Training Epoch: 8 | iteration: 161/262 | Loss: 0.5672298669815063Training Epoch: 8 | iteration: 162/262 | Loss: 0.5711907744407654Training Epoch: 8 | iteration: 163/262 | Loss: 0.5480254888534546Training Epoch: 8 | iteration: 164/262 | Loss: 0.5760853290557861Training Epoch: 8 | iteration: 165/262 | Loss: 0.6109668016433716Training Epoch: 8 | iteration: 166/262 | Loss: 0.587260365486145Training Epoch: 8 | iteration: 167/262 | Loss: 0.5560547113418579Training Epoch: 8 | iteration: 168/262 | Loss: 0.5716596841812134Training Epoch: 8 | iteration: 169/262 | Loss: 0.5752639174461365Training Epoch: 8 | iteration: 170/262 | Loss: 0.5456491112709045Training Epoch: 8 | iteration: 171/262 | Loss: 0.5937577486038208Training Epoch: 8 | iteration: 172/262 | Loss: 0.5881903171539307Training Epoch: 8 | iteration: 173/262 | Loss: 0.5630937218666077Training Epoch: 8 | iteration: 174/262 | Loss: 0.5654342770576477Training Epoch: 8 | iteration: 175/262 | Loss: 0.5342303514480591Training Epoch: 8 | iteration: 176/262 | Loss: 0.5235313773155212Training Epoch: 8 | iteration: 177/262 | Loss: 0.5396302938461304Training Epoch: 8 | iteration: 178/262 | Loss: 0.5813877582550049Training Epoch: 8 | iteration: 179/262 | Loss: 0.5873783826828003Training Epoch: 8 | iteration: 180/262 | Loss: 0.552781343460083Training Epoch: 8 | iteration: 181/262 | Loss: 0.5392431616783142Training Epoch: 8 | iteration: 182/262 | Loss: 0.5368704795837402Training Epoch: 8 | iteration: 183/262 | Loss: 0.5433370471000671Training Epoch: 8 | iteration: 184/262 | Loss: 0.5363800525665283Training Epoch: 8 | iteration: 185/262 | Loss: 0.5324438810348511Training Epoch: 8 | iteration: 186/262 | Loss: 0.6173754930496216Training Epoch: 8 | iteration: 187/262 | Loss: 0.5583063364028931Training Epoch: 8 | iteration: 188/262 | Loss: 0.5677118897438049Training Epoch: 8 | iteration: 189/262 | Loss: 0.5118893980979919Training Epoch: 8 | iteration: 190/262 | Loss: 0.5502013564109802Training Epoch: 8 | iteration: 191/262 | Loss: 0.5593593120574951Training Epoch: 8 | iteration: 192/262 | Loss: 0.5814840793609619Training Epoch: 8 | iteration: 193/262 | Loss: 0.5932269096374512Training Epoch: 8 | iteration: 194/262 | Loss: 0.5592673420906067Training Epoch: 8 | iteration: 195/262 | Loss: 0.6233295202255249Training Epoch: 8 | iteration: 196/262 | Loss: 0.5553975701332092Training Epoch: 8 | iteration: 197/262 | Loss: 0.5567903518676758Training Epoch: 8 | iteration: 198/262 | Loss: 0.5813535451889038Training Epoch: 8 | iteration: 199/262 | Loss: 0.5606657266616821Training Epoch: 8 | iteration: 200/262 | Loss: 0.5412211418151855Training Epoch: 8 | iteration: 201/262 | Loss: 0.5679835081100464Training Epoch: 8 | iteration: 202/262 | Loss: 0.5903438925743103Training Epoch: 8 | iteration: 203/262 | Loss: 0.5858052968978882Training Epoch: 8 | iteration: 204/262 | Loss: 0.615606427192688Training Epoch: 8 | iteration: 205/262 | Loss: 0.5678067207336426Training Epoch: 8 | iteration: 206/262 | Loss: 0.5888437032699585Training Epoch: 8 | iteration: 207/262 | Loss: 0.5536878108978271Training Epoch: 8 | iteration: 208/262 | Loss: 0.5758289098739624Training Epoch: 8 | iteration: 209/262 | Loss: 0.5432784557342529Training Epoch: 8 | iteration: 210/262 | Loss: 0.5646085143089294Training Epoch: 8 | iteration: 211/262 | Loss: 0.5733262300491333Training Epoch: 8 | iteration: 212/262 | Loss: 0.5776488184928894Training Epoch: 8 | iteration: 213/262 | Loss: 0.5165462493896484Training Epoch: 8 | iteration: 214/262 | Loss: 0.554481565952301Training Epoch: 8 | iteration: 215/262 | Loss: 0.5624309778213501Training Epoch: 8 | iteration: 216/262 | Loss: 0.5380048155784607Training Epoch: 8 | iteration: 217/262 | Loss: 0.5781854391098022Training Epoch: 8 | iteration: 218/262 | Loss: 0.5828766822814941Training Epoch: 8 | iteration: 219/262 | Loss: 0.5188087224960327Training Epoch: 8 | iteration: 220/262 | Loss: 0.6073037385940552Training Epoch: 8 | iteration: 221/262 | Loss: 0.5791406631469727Training Epoch: 8 | iteration: 222/262 | Loss: 0.5464645624160767Training Epoch: 8 | iteration: 223/262 | Loss: 0.5901460647583008Training Epoch: 8 | iteration: 224/262 | Loss: 0.5275468826293945Training Epoch: 8 | iteration: 225/262 | Loss: 0.5911237001419067Training Epoch: 8 | iteration: 226/262 | Loss: 0.5937053561210632Training Epoch: 8 | iteration: 227/262 | Loss: 0.560828447341919Training Epoch: 8 | iteration: 228/262 | Loss: 0.6084594130516052Training Epoch: 8 | iteration: 229/262 | Loss: 0.5970063209533691Training Epoch: 8 | iteration: 230/262 | Loss: 0.6198971271514893Training Epoch: 8 | iteration: 231/262 | Loss: 0.5432497262954712Training Epoch: 8 | iteration: 232/262 | Loss: 0.5148019194602966Training Epoch: 8 | iteration: 233/262 | Loss: 0.5146036148071289Training Epoch: 8 | iteration: 234/262 | Loss: 0.5628982782363892Training Epoch: 8 | iteration: 235/262 | Loss: 0.5392105579376221Training Epoch: 8 | iteration: 236/262 | Loss: 0.6228370070457458Training Epoch: 8 | iteration: 237/262 | Loss: 0.6272730827331543Training Epoch: 8 | iteration: 238/262 | Loss: 0.5988733768463135Training Epoch: 8 | iteration: 239/262 | Loss: 0.5940012335777283Training Epoch: 8 | iteration: 240/262 | Loss: 0.5847019553184509Training Epoch: 8 | iteration: 241/262 | Loss: 0.5441350936889648Training Epoch: 8 | iteration: 242/262 | Loss: 0.5602802038192749Training Epoch: 8 | iteration: 243/262 | Loss: 0.5264176726341248Training Epoch: 8 | iteration: 244/262 | Loss: 0.5664247870445251Training Epoch: 8 | iteration: 245/262 | Loss: 0.536726713180542Training Epoch: 8 | iteration: 246/262 | Loss: 0.6136401891708374Training Epoch: 8 | iteration: 247/262 | Loss: 0.5829930305480957Training Epoch: 8 | iteration: 248/262 | Loss: 0.5674996972084045Training Epoch: 8 | iteration: 249/262 | Loss: 0.5652937293052673Training Epoch: 8 | iteration: 250/262 | Loss: 0.5844063758850098Training Epoch: 8 | iteration: 251/262 | Loss: 0.6023415327072144Training Epoch: 8 | iteration: 252/262 | Loss: 0.5253757834434509Training Epoch: 8 | iteration: 253/262 | Loss: 0.5675151348114014Training Epoch: 8 | iteration: 254/262 | Loss: 0.5527493953704834Training Epoch: 8 | iteration: 255/262 | Loss: 0.5589001178741455Training Epoch: 8 | iteration: 256/262 | Loss: 0.5809644460678101Training Epoch: 8 | iteration: 257/262 | Loss: 0.5766479969024658Training Epoch: 8 | iteration: 258/262 | Loss: 0.5446668863296509Training Epoch: 8 | iteration: 259/262 | Loss: 0.5356076955795288Training Epoch: 8 | iteration: 260/262 | Loss: 0.5822658538818359Training Epoch: 8 | iteration: 261/262 | Loss: 0.4995698928833008Validating Epoch: 8 | iteration: 0/66 | Loss: 0.5977218151092529Validating Epoch: 8 | iteration: 1/66 | Loss: 0.6358866691589355Validating Epoch: 8 | iteration: 2/66 | Loss: 0.5788238644599915Validating Epoch: 8 | iteration: 3/66 | Loss: 0.6089191436767578Validating Epoch: 8 | iteration: 4/66 | Loss: 0.5641390085220337Validating Epoch: 8 | iteration: 5/66 | Loss: 0.5620585680007935Validating Epoch: 8 | iteration: 6/66 | Loss: 0.6031112670898438Validating Epoch: 8 | iteration: 7/66 | Loss: 0.5434379577636719Validating Epoch: 8 | iteration: 8/66 | Loss: 0.5841811895370483Validating Epoch: 8 | iteration: 9/66 | Loss: 0.6108565926551819Validating Epoch: 8 | iteration: 10/66 | Loss: 0.6378346681594849Validating Epoch: 8 | iteration: 11/66 | Loss: 0.5611960887908936Validating Epoch: 8 | iteration: 12/66 | Loss: 0.6095831394195557Validating Epoch: 8 | iteration: 13/66 | Loss: 0.5689907073974609Validating Epoch: 8 | iteration: 14/66 | Loss: 0.5973303914070129Validating Epoch: 8 | iteration: 15/66 | Loss: 0.5874025821685791Validating Epoch: 8 | iteration: 16/66 | Loss: 0.6009534597396851Validating Epoch: 8 | iteration: 17/66 | Loss: 0.5858016014099121Validating Epoch: 8 | iteration: 18/66 | Loss: 0.5898556113243103Validating Epoch: 8 | iteration: 19/66 | Loss: 0.5748944878578186Validating Epoch: 8 | iteration: 20/66 | Loss: 0.5845367908477783Validating Epoch: 8 | iteration: 21/66 | Loss: 0.61311936378479Validating Epoch: 8 | iteration: 22/66 | Loss: 0.606013834476471Validating Epoch: 8 | iteration: 23/66 | Loss: 0.5943900346755981Validating Epoch: 8 | iteration: 24/66 | Loss: 0.5804502964019775Validating Epoch: 8 | iteration: 25/66 | Loss: 0.5947748422622681Validating Epoch: 8 | iteration: 26/66 | Loss: 0.6515575647354126Validating Epoch: 8 | iteration: 27/66 | Loss: 0.5895800590515137Validating Epoch: 8 | iteration: 28/66 | Loss: 0.587317168712616Validating Epoch: 8 | iteration: 29/66 | Loss: 0.620215892791748Validating Epoch: 8 | iteration: 30/66 | Loss: 0.5438122749328613Validating Epoch: 8 | iteration: 31/66 | Loss: 0.6186160445213318Validating Epoch: 8 | iteration: 32/66 | Loss: 0.5543723106384277Validating Epoch: 8 | iteration: 33/66 | Loss: 0.5679295063018799Validating Epoch: 8 | iteration: 34/66 | Loss: 0.5828078985214233Validating Epoch: 8 | iteration: 35/66 | Loss: 0.590461015701294Validating Epoch: 8 | iteration: 36/66 | Loss: 0.5913763642311096Validating Epoch: 8 | iteration: 37/66 | Loss: 0.5528668165206909Validating Epoch: 8 | iteration: 38/66 | Loss: 0.5695222616195679Validating Epoch: 8 | iteration: 39/66 | Loss: 0.573836088180542Validating Epoch: 8 | iteration: 40/66 | Loss: 0.5657401084899902Validating Epoch: 8 | iteration: 41/66 | Loss: 0.5646216869354248Validating Epoch: 8 | iteration: 42/66 | Loss: 0.613233745098114Validating Epoch: 8 | iteration: 43/66 | Loss: 0.6068110466003418Validating Epoch: 8 | iteration: 44/66 | Loss: 0.5797600746154785Validating Epoch: 8 | iteration: 45/66 | Loss: 0.6002368927001953Validating Epoch: 8 | iteration: 46/66 | Loss: 0.5848218202590942Validating Epoch: 8 | iteration: 47/66 | Loss: 0.5994523763656616Validating Epoch: 8 | iteration: 48/66 | Loss: 0.6157865524291992Validating Epoch: 8 | iteration: 49/66 | Loss: 0.5968422293663025Validating Epoch: 8 | iteration: 50/66 | Loss: 0.5660997629165649Validating Epoch: 8 | iteration: 51/66 | Loss: 0.5850337147712708Validating Epoch: 8 | iteration: 52/66 | Loss: 0.6024856567382812Validating Epoch: 8 | iteration: 53/66 | Loss: 0.5300931930541992Validating Epoch: 8 | iteration: 54/66 | Loss: 0.6120070219039917Validating Epoch: 8 | iteration: 55/66 | Loss: 0.5770472288131714Validating Epoch: 8 | iteration: 56/66 | Loss: 0.6220704317092896Validating Epoch: 8 | iteration: 57/66 | Loss: 0.5853042602539062Validating Epoch: 8 | iteration: 58/66 | Loss: 0.5962221026420593Validating Epoch: 8 | iteration: 59/66 | Loss: 0.6108924150466919Validating Epoch: 8 | iteration: 60/66 | Loss: 0.5669982433319092Validating Epoch: 8 | iteration: 61/66 | Loss: 0.6025101542472839Validating Epoch: 8 | iteration: 62/66 | Loss: 0.6062466502189636Validating Epoch: 8 | iteration: 63/66 | Loss: 0.5962070226669312Validating Epoch: 8 | iteration: 64/66 | Loss: 0.5607661008834839Validating Epoch: 8 | iteration: 65/66 | Loss: 0.614891767501831Temperature:  0.5
Sampling: iteration: 0/51{'Validity': 0.939453125, 'Novelty': 1.0, 'Uniqueness': 0.9511434511434511}
Training Epoch: 9 | iteration: 0/262 | Loss: 0.5527705550193787Training Epoch: 9 | iteration: 1/262 | Loss: 0.5371297001838684Training Epoch: 9 | iteration: 2/262 | Loss: 0.5891247987747192Training Epoch: 9 | iteration: 3/262 | Loss: 0.5626252889633179Training Epoch: 9 | iteration: 4/262 | Loss: 0.5277829170227051Training Epoch: 9 | iteration: 5/262 | Loss: 0.5431327819824219Training Epoch: 9 | iteration: 6/262 | Loss: 0.5727765560150146Training Epoch: 9 | iteration: 7/262 | Loss: 0.5267080068588257Training Epoch: 9 | iteration: 8/262 | Loss: 0.5612093210220337Training Epoch: 9 | iteration: 9/262 | Loss: 0.5725142955780029Training Epoch: 9 | iteration: 10/262 | Loss: 0.5510298013687134Training Epoch: 9 | iteration: 11/262 | Loss: 0.5255093574523926Training Epoch: 9 | iteration: 12/262 | Loss: 0.5875587463378906Training Epoch: 9 | iteration: 13/262 | Loss: 0.5822503566741943Training Epoch: 9 | iteration: 14/262 | Loss: 0.5583313703536987Training Epoch: 9 | iteration: 15/262 | Loss: 0.5460509061813354Training Epoch: 9 | iteration: 16/262 | Loss: 0.5222533941268921Training Epoch: 9 | iteration: 17/262 | Loss: 0.5359214544296265Training Epoch: 9 | iteration: 18/262 | Loss: 0.5551277995109558Training Epoch: 9 | iteration: 19/262 | Loss: 0.572597324848175Training Epoch: 9 | iteration: 20/262 | Loss: 0.5873796939849854Training Epoch: 9 | iteration: 21/262 | Loss: 0.5527305603027344Training Epoch: 9 | iteration: 22/262 | Loss: 0.5945155620574951Training Epoch: 9 | iteration: 23/262 | Loss: 0.5508010387420654Training Epoch: 9 | iteration: 24/262 | Loss: 0.5676349401473999Training Epoch: 9 | iteration: 25/262 | Loss: 0.6351850032806396Training Epoch: 9 | iteration: 26/262 | Loss: 0.5678637027740479Training Epoch: 9 | iteration: 27/262 | Loss: 0.5616505146026611Training Epoch: 9 | iteration: 28/262 | Loss: 0.5614327788352966Training Epoch: 9 | iteration: 29/262 | Loss: 0.5594034790992737Training Epoch: 9 | iteration: 30/262 | Loss: 0.5804928541183472Training Epoch: 9 | iteration: 31/262 | Loss: 0.5615885853767395Training Epoch: 9 | iteration: 32/262 | Loss: 0.5413074493408203Training Epoch: 9 | iteration: 33/262 | Loss: 0.5744114518165588Training Epoch: 9 | iteration: 34/262 | Loss: 0.5727033019065857Training Epoch: 9 | iteration: 35/262 | Loss: 0.5375641584396362Training Epoch: 9 | iteration: 36/262 | Loss: 0.5215884447097778Training Epoch: 9 | iteration: 37/262 | Loss: 0.5675011873245239Training Epoch: 9 | iteration: 38/262 | Loss: 0.584433913230896Training Epoch: 9 | iteration: 39/262 | Loss: 0.5683703422546387Training Epoch: 9 | iteration: 40/262 | Loss: 0.5981690883636475Training Epoch: 9 | iteration: 41/262 | Loss: 0.5452876091003418Training Epoch: 9 | iteration: 42/262 | Loss: 0.581540584564209Training Epoch: 9 | iteration: 43/262 | Loss: 0.5725995302200317Training Epoch: 9 | iteration: 44/262 | Loss: 0.5810560584068298Training Epoch: 9 | iteration: 45/262 | Loss: 0.552369236946106Training Epoch: 9 | iteration: 46/262 | Loss: 0.5314596891403198Training Epoch: 9 | iteration: 47/262 | Loss: 0.5813949108123779Training Epoch: 9 | iteration: 48/262 | Loss: 0.5768904089927673Training Epoch: 9 | iteration: 49/262 | Loss: 0.5663577914237976Training Epoch: 9 | iteration: 50/262 | Loss: 0.5391111969947815Training Epoch: 9 | iteration: 51/262 | Loss: 0.627711296081543Training Epoch: 9 | iteration: 52/262 | Loss: 0.5601812601089478Training Epoch: 9 | iteration: 53/262 | Loss: 0.48034149408340454Training Epoch: 9 | iteration: 54/262 | Loss: 0.5411555171012878Training Epoch: 9 | iteration: 55/262 | Loss: 0.5550490617752075Training Epoch: 9 | iteration: 56/262 | Loss: 0.5561258792877197Training Epoch: 9 | iteration: 57/262 | Loss: 0.5796148180961609Training Epoch: 9 | iteration: 58/262 | Loss: 0.571657657623291Training Epoch: 9 | iteration: 59/262 | Loss: 0.5547127723693848Training Epoch: 9 | iteration: 60/262 | Loss: 0.5738023519515991Training Epoch: 9 | iteration: 61/262 | Loss: 0.5520784854888916Training Epoch: 9 | iteration: 62/262 | Loss: 0.5842806100845337Training Epoch: 9 | iteration: 63/262 | Loss: 0.5532755851745605Training Epoch: 9 | iteration: 64/262 | Loss: 0.5831855535507202Training Epoch: 9 | iteration: 65/262 | Loss: 0.5587600469589233Training Epoch: 9 | iteration: 66/262 | Loss: 0.5453678965568542Training Epoch: 9 | iteration: 67/262 | Loss: 0.6018434762954712Training Epoch: 9 | iteration: 68/262 | Loss: 0.5585165023803711Training Epoch: 9 | iteration: 69/262 | Loss: 0.575183629989624Training Epoch: 9 | iteration: 70/262 | Loss: 0.5879648923873901Training Epoch: 9 | iteration: 71/262 | Loss: 0.511705756187439Training Epoch: 9 | iteration: 72/262 | Loss: 0.5667150020599365Training Epoch: 9 | iteration: 73/262 | Loss: 0.5270232558250427Training Epoch: 9 | iteration: 74/262 | Loss: 0.5299072265625Training Epoch: 9 | iteration: 75/262 | Loss: 0.5864112973213196Training Epoch: 9 | iteration: 76/262 | Loss: 0.5382662415504456Training Epoch: 9 | iteration: 77/262 | Loss: 0.5803087949752808Training Epoch: 9 | iteration: 78/262 | Loss: 0.5814062356948853Training Epoch: 9 | iteration: 79/262 | Loss: 0.5259507894515991Training Epoch: 9 | iteration: 80/262 | Loss: 0.6171085238456726Training Epoch: 9 | iteration: 81/262 | Loss: 0.5602447986602783Training Epoch: 9 | iteration: 82/262 | Loss: 0.5393884181976318Training Epoch: 9 | iteration: 83/262 | Loss: 0.4949650466442108Training Epoch: 9 | iteration: 84/262 | Loss: 0.6010331511497498Training Epoch: 9 | iteration: 85/262 | Loss: 0.5772515535354614Training Epoch: 9 | iteration: 86/262 | Loss: 0.594099760055542Training Epoch: 9 | iteration: 87/262 | Loss: 0.54255211353302Training Epoch: 9 | iteration: 88/262 | Loss: 0.5983057022094727Training Epoch: 9 | iteration: 89/262 | Loss: 0.596969485282898Training Epoch: 9 | iteration: 90/262 | Loss: 0.5297266244888306Training Epoch: 9 | iteration: 91/262 | Loss: 0.5854133367538452Training Epoch: 9 | iteration: 92/262 | Loss: 0.5471975803375244Training Epoch: 9 | iteration: 93/262 | Loss: 0.6049431562423706Training Epoch: 9 | iteration: 94/262 | Loss: 0.5482951402664185Training Epoch: 9 | iteration: 95/262 | Loss: 0.5806044936180115Training Epoch: 9 | iteration: 96/262 | Loss: 0.5573608875274658Training Epoch: 9 | iteration: 97/262 | Loss: 0.5456321835517883Training Epoch: 9 | iteration: 98/262 | Loss: 0.5471069812774658Training Epoch: 9 | iteration: 99/262 | Loss: 0.5419858694076538Training Epoch: 9 | iteration: 100/262 | Loss: 0.5766392946243286Training Epoch: 9 | iteration: 101/262 | Loss: 0.5438644886016846Training Epoch: 9 | iteration: 102/262 | Loss: 0.5643048286437988Training Epoch: 9 | iteration: 103/262 | Loss: 0.5655256509780884Training Epoch: 9 | iteration: 104/262 | Loss: 0.5371718406677246Training Epoch: 9 | iteration: 105/262 | Loss: 0.5662078857421875Training Epoch: 9 | iteration: 106/262 | Loss: 0.5060169100761414Training Epoch: 9 | iteration: 107/262 | Loss: 0.5833224058151245Training Epoch: 9 | iteration: 108/262 | Loss: 0.587496280670166Training Epoch: 9 | iteration: 109/262 | Loss: 0.5695268511772156Training Epoch: 9 | iteration: 110/262 | Loss: 0.542413592338562Training Epoch: 9 | iteration: 111/262 | Loss: 0.5721803307533264Training Epoch: 9 | iteration: 112/262 | Loss: 0.5605056285858154Training Epoch: 9 | iteration: 113/262 | Loss: 0.5963895916938782Training Epoch: 9 | iteration: 114/262 | Loss: 0.5386689901351929Training Epoch: 9 | iteration: 115/262 | Loss: 0.5803873538970947Training Epoch: 9 | iteration: 116/262 | Loss: 0.5376850366592407Training Epoch: 9 | iteration: 117/262 | Loss: 0.5370331406593323Training Epoch: 9 | iteration: 118/262 | Loss: 0.5298804044723511Training Epoch: 9 | iteration: 119/262 | Loss: 0.5708202719688416Training Epoch: 9 | iteration: 120/262 | Loss: 0.5653244256973267Training Epoch: 9 | iteration: 121/262 | Loss: 0.6018162965774536Training Epoch: 9 | iteration: 122/262 | Loss: 0.5705243349075317Training Epoch: 9 | iteration: 123/262 | Loss: 0.5393671989440918Training Epoch: 9 | iteration: 124/262 | Loss: 0.5816891193389893Training Epoch: 9 | iteration: 125/262 | Loss: 0.5786842703819275Training Epoch: 9 | iteration: 126/262 | Loss: 0.6332253813743591Training Epoch: 9 | iteration: 127/262 | Loss: 0.5881645679473877Training Epoch: 9 | iteration: 128/262 | Loss: 0.5548171997070312Training Epoch: 9 | iteration: 129/262 | Loss: 0.5523552298545837Training Epoch: 9 | iteration: 130/262 | Loss: 0.5621639490127563Training Epoch: 9 | iteration: 131/262 | Loss: 0.5795384049415588Training Epoch: 9 | iteration: 132/262 | Loss: 0.5670566558837891Training Epoch: 9 | iteration: 133/262 | Loss: 0.5525996685028076Training Epoch: 9 | iteration: 134/262 | Loss: 0.535474956035614Training Epoch: 9 | iteration: 135/262 | Loss: 0.571719765663147Training Epoch: 9 | iteration: 136/262 | Loss: 0.5422450304031372Training Epoch: 9 | iteration: 137/262 | Loss: 0.5225831270217896Training Epoch: 9 | iteration: 138/262 | Loss: 0.5649219751358032Training Epoch: 9 | iteration: 139/262 | Loss: 0.5735716819763184Training Epoch: 9 | iteration: 140/262 | Loss: 0.5548512935638428Training Epoch: 9 | iteration: 141/262 | Loss: 0.5432340502738953Training Epoch: 9 | iteration: 142/262 | Loss: 0.5286146402359009Training Epoch: 9 | iteration: 143/262 | Loss: 0.5742907524108887Training Epoch: 9 | iteration: 144/262 | Loss: 0.5664876699447632Training Epoch: 9 | iteration: 145/262 | Loss: 0.5925400257110596Training Epoch: 9 | iteration: 146/262 | Loss: 0.5568153858184814Training Epoch: 9 | iteration: 147/262 | Loss: 0.5587971210479736Training Epoch: 9 | iteration: 148/262 | Loss: 0.5373794436454773Training Epoch: 9 | iteration: 149/262 | Loss: 0.5496672987937927Training Epoch: 9 | iteration: 150/262 | Loss: 0.5705389976501465Training Epoch: 9 | iteration: 151/262 | Loss: 0.5422525405883789Training Epoch: 9 | iteration: 152/262 | Loss: 0.5297430753707886Training Epoch: 9 | iteration: 153/262 | Loss: 0.6037989854812622Training Epoch: 9 | iteration: 154/262 | Loss: 0.5841934680938721Training Epoch: 9 | iteration: 155/262 | Loss: 0.6103924512863159Training Epoch: 9 | iteration: 156/262 | Loss: 0.5859133005142212Training Epoch: 9 | iteration: 157/262 | Loss: 0.5141539573669434Training Epoch: 9 | iteration: 158/262 | Loss: 0.5654834508895874Training Epoch: 9 | iteration: 159/262 | Loss: 0.5489194393157959Training Epoch: 9 | iteration: 160/262 | Loss: 0.5591561794281006Training Epoch: 9 | iteration: 161/262 | Loss: 0.5405707359313965Training Epoch: 9 | iteration: 162/262 | Loss: 0.5424935817718506Training Epoch: 9 | iteration: 163/262 | Loss: 0.5487631559371948Training Epoch: 9 | iteration: 164/262 | Loss: 0.5812867879867554Training Epoch: 9 | iteration: 165/262 | Loss: 0.5578678846359253Training Epoch: 9 | iteration: 166/262 | Loss: 0.5771838426589966Training Epoch: 9 | iteration: 167/262 | Loss: 0.5740194916725159Training Epoch: 9 | iteration: 168/262 | Loss: 0.5641360878944397Training Epoch: 9 | iteration: 169/262 | Loss: 0.539763331413269Training Epoch: 9 | iteration: 170/262 | Loss: 0.5560561418533325Training Epoch: 9 | iteration: 171/262 | Loss: 0.5716419816017151Training Epoch: 9 | iteration: 172/262 | Loss: 0.5545563697814941Training Epoch: 9 | iteration: 173/262 | Loss: 0.5190607905387878Training Epoch: 9 | iteration: 174/262 | Loss: 0.5474289059638977Training Epoch: 9 | iteration: 175/262 | Loss: 0.6201162934303284Training Epoch: 9 | iteration: 176/262 | Loss: 0.5548076033592224Training Epoch: 9 | iteration: 177/262 | Loss: 0.5418474078178406Training Epoch: 9 | iteration: 178/262 | Loss: 0.5375498533248901Training Epoch: 9 | iteration: 179/262 | Loss: 0.5293976068496704Training Epoch: 9 | iteration: 180/262 | Loss: 0.5408271551132202Training Epoch: 9 | iteration: 181/262 | Loss: 0.60251784324646Training Epoch: 9 | iteration: 182/262 | Loss: 0.6082710027694702Training Epoch: 9 | iteration: 183/262 | Loss: 0.6225607395172119Training Epoch: 9 | iteration: 184/262 | Loss: 0.5704533457756042Training Epoch: 9 | iteration: 185/262 | Loss: 0.5883086323738098Training Epoch: 9 | iteration: 186/262 | Loss: 0.5152258276939392Training Epoch: 9 | iteration: 187/262 | Loss: 0.5872184038162231Training Epoch: 9 | iteration: 188/262 | Loss: 0.5472761392593384Training Epoch: 9 | iteration: 189/262 | Loss: 0.5806246995925903Training Epoch: 9 | iteration: 190/262 | Loss: 0.5618016719818115Training Epoch: 9 | iteration: 191/262 | Loss: 0.5973255634307861Training Epoch: 9 | iteration: 192/262 | Loss: 0.5209155082702637Training Epoch: 9 | iteration: 193/262 | Loss: 0.5586861371994019Training Epoch: 9 | iteration: 194/262 | Loss: 0.5874001383781433Training Epoch: 9 | iteration: 195/262 | Loss: 0.5155261158943176Training Epoch: 9 | iteration: 196/262 | Loss: 0.5608621835708618Training Epoch: 9 | iteration: 197/262 | Loss: 0.5644036531448364Training Epoch: 9 | iteration: 198/262 | Loss: 0.5916492342948914Training Epoch: 9 | iteration: 199/262 | Loss: 0.6258857250213623Training Epoch: 9 | iteration: 200/262 | Loss: 0.557308554649353Training Epoch: 9 | iteration: 201/262 | Loss: 0.5757112503051758Training Epoch: 9 | iteration: 202/262 | Loss: 0.5643355846405029Training Epoch: 9 | iteration: 203/262 | Loss: 0.5714744925498962Training Epoch: 9 | iteration: 204/262 | Loss: 0.5249574184417725Training Epoch: 9 | iteration: 205/262 | Loss: 0.5972815752029419Training Epoch: 9 | iteration: 206/262 | Loss: 0.5358486771583557Training Epoch: 9 | iteration: 207/262 | Loss: 0.581019401550293Training Epoch: 9 | iteration: 208/262 | Loss: 0.5997310876846313Training Epoch: 9 | iteration: 209/262 | Loss: 0.5561879873275757Training Epoch: 9 | iteration: 210/262 | Loss: 0.5905984044075012Training Epoch: 9 | iteration: 211/262 | Loss: 0.5952101945877075Training Epoch: 9 | iteration: 212/262 | Loss: 0.5741524696350098Training Epoch: 9 | iteration: 213/262 | Loss: 0.5476967692375183Training Epoch: 9 | iteration: 214/262 | Loss: 0.5648095011711121Training Epoch: 9 | iteration: 215/262 | Loss: 0.5581251382827759Training Epoch: 9 | iteration: 216/262 | Loss: 0.5451853275299072Training Epoch: 9 | iteration: 217/262 | Loss: 0.5669891834259033Training Epoch: 9 | iteration: 218/262 | Loss: 0.5303200483322144Training Epoch: 9 | iteration: 219/262 | Loss: 0.6007359027862549Training Epoch: 9 | iteration: 220/262 | Loss: 0.5532388091087341Training Epoch: 9 | iteration: 221/262 | Loss: 0.5315431952476501Training Epoch: 9 | iteration: 222/262 | Loss: 0.570528507232666Training Epoch: 9 | iteration: 223/262 | Loss: 0.5240492224693298Training Epoch: 9 | iteration: 224/262 | Loss: 0.5882409811019897Training Epoch: 9 | iteration: 225/262 | Loss: 0.5538680553436279Training Epoch: 9 | iteration: 226/262 | Loss: 0.5535974502563477Training Epoch: 9 | iteration: 227/262 | Loss: 0.5779651999473572Training Epoch: 9 | iteration: 228/262 | Loss: 0.6183862090110779Training Epoch: 9 | iteration: 229/262 | Loss: 0.56083083152771Training Epoch: 9 | iteration: 230/262 | Loss: 0.5833538174629211Training Epoch: 9 | iteration: 231/262 | Loss: 0.5533470511436462Training Epoch: 9 | iteration: 232/262 | Loss: 0.570199728012085Training Epoch: 9 | iteration: 233/262 | Loss: 0.5262212753295898Training Epoch: 9 | iteration: 234/262 | Loss: 0.5797685384750366Training Epoch: 9 | iteration: 235/262 | Loss: 0.5140743255615234Training Epoch: 9 | iteration: 236/262 | Loss: 0.5706173181533813Training Epoch: 9 | iteration: 237/262 | Loss: 0.5679271221160889Training Epoch: 9 | iteration: 238/262 | Loss: 0.5652515888214111Training Epoch: 9 | iteration: 239/262 | Loss: 0.5634709596633911Training Epoch: 9 | iteration: 240/262 | Loss: 0.5564615726470947Training Epoch: 9 | iteration: 241/262 | Loss: 0.5487054586410522Training Epoch: 9 | iteration: 242/262 | Loss: 0.5648229718208313Training Epoch: 9 | iteration: 243/262 | Loss: 0.562623918056488Training Epoch: 9 | iteration: 244/262 | Loss: 0.5630434155464172Training Epoch: 9 | iteration: 245/262 | Loss: 0.5505282282829285Training Epoch: 9 | iteration: 246/262 | Loss: 0.5725119709968567Training Epoch: 9 | iteration: 247/262 | Loss: 0.5773167014122009Training Epoch: 9 | iteration: 248/262 | Loss: 0.5292125940322876Training Epoch: 9 | iteration: 249/262 | Loss: 0.5694050192832947Training Epoch: 9 | iteration: 250/262 | Loss: 0.543816864490509Training Epoch: 9 | iteration: 251/262 | Loss: 0.5975559949874878Training Epoch: 9 | iteration: 252/262 | Loss: 0.5768362283706665Training Epoch: 9 | iteration: 253/262 | Loss: 0.5390109419822693Training Epoch: 9 | iteration: 254/262 | Loss: 0.5706239342689514Training Epoch: 9 | iteration: 255/262 | Loss: 0.5760114192962646Training Epoch: 9 | iteration: 256/262 | Loss: 0.6051124930381775Training Epoch: 9 | iteration: 257/262 | Loss: 0.5530439615249634Training Epoch: 9 | iteration: 258/262 | Loss: 0.577684760093689Training Epoch: 9 | iteration: 259/262 | Loss: 0.5628917813301086Training Epoch: 9 | iteration: 260/262 | Loss: 0.5415369272232056Training Epoch: 9 | iteration: 261/262 | Loss: 0.5356490015983582Validating Epoch: 9 | iteration: 0/66 | Loss: 0.5872225165367126Validating Epoch: 9 | iteration: 1/66 | Loss: 0.6055392026901245Validating Epoch: 9 | iteration: 2/66 | Loss: 0.5889135003089905Validating Epoch: 9 | iteration: 3/66 | Loss: 0.5902083516120911Validating Epoch: 9 | iteration: 4/66 | Loss: 0.6175397634506226Validating Epoch: 9 | iteration: 5/66 | Loss: 0.6169866323471069Validating Epoch: 9 | iteration: 6/66 | Loss: 0.5832550525665283Validating Epoch: 9 | iteration: 7/66 | Loss: 0.5823684930801392Validating Epoch: 9 | iteration: 8/66 | Loss: 0.6143974661827087Validating Epoch: 9 | iteration: 9/66 | Loss: 0.5723511576652527Validating Epoch: 9 | iteration: 10/66 | Loss: 0.6399407982826233Validating Epoch: 9 | iteration: 11/66 | Loss: 0.6388592720031738Validating Epoch: 9 | iteration: 12/66 | Loss: 0.5669865608215332Validating Epoch: 9 | iteration: 13/66 | Loss: 0.5615099668502808Validating Epoch: 9 | iteration: 14/66 | Loss: 0.6220377683639526Validating Epoch: 9 | iteration: 15/66 | Loss: 0.5969945192337036Validating Epoch: 9 | iteration: 16/66 | Loss: 0.5928598642349243Validating Epoch: 9 | iteration: 17/66 | Loss: 0.5745976567268372Validating Epoch: 9 | iteration: 18/66 | Loss: 0.6006672382354736Validating Epoch: 9 | iteration: 19/66 | Loss: 0.6096256971359253Validating Epoch: 9 | iteration: 20/66 | Loss: 0.6086515188217163Validating Epoch: 9 | iteration: 21/66 | Loss: 0.5480585694313049Validating Epoch: 9 | iteration: 22/66 | Loss: 0.6471797227859497Validating Epoch: 9 | iteration: 23/66 | Loss: 0.58961021900177Validating Epoch: 9 | iteration: 24/66 | Loss: 0.6191082000732422Validating Epoch: 9 | iteration: 25/66 | Loss: 0.6104583740234375Validating Epoch: 9 | iteration: 26/66 | Loss: 0.607207179069519Validating Epoch: 9 | iteration: 27/66 | Loss: 0.5891883373260498Validating Epoch: 9 | iteration: 28/66 | Loss: 0.5616838932037354Validating Epoch: 9 | iteration: 29/66 | Loss: 0.6149837374687195Validating Epoch: 9 | iteration: 30/66 | Loss: 0.5825871825218201Validating Epoch: 9 | iteration: 31/66 | Loss: 0.6329811811447144Validating Epoch: 9 | iteration: 32/66 | Loss: 0.5662137269973755Validating Epoch: 9 | iteration: 33/66 | Loss: 0.6215779781341553Validating Epoch: 9 | iteration: 34/66 | Loss: 0.6113463640213013Validating Epoch: 9 | iteration: 35/66 | Loss: 0.5933290719985962Validating Epoch: 9 | iteration: 36/66 | Loss: 0.5741496086120605Validating Epoch: 9 | iteration: 37/66 | Loss: 0.602992057800293Validating Epoch: 9 | iteration: 38/66 | Loss: 0.5982452630996704Validating Epoch: 9 | iteration: 39/66 | Loss: 0.5819971561431885Validating Epoch: 9 | iteration: 40/66 | Loss: 0.6145225763320923Validating Epoch: 9 | iteration: 41/66 | Loss: 0.5947458744049072Validating Epoch: 9 | iteration: 42/66 | Loss: 0.5563252568244934Validating Epoch: 9 | iteration: 43/66 | Loss: 0.5632973909378052Validating Epoch: 9 | iteration: 44/66 | Loss: 0.553343653678894Validating Epoch: 9 | iteration: 45/66 | Loss: 0.6055562496185303Validating Epoch: 9 | iteration: 46/66 | Loss: 0.6022595167160034Validating Epoch: 9 | iteration: 47/66 | Loss: 0.6295634508132935Validating Epoch: 9 | iteration: 48/66 | Loss: 0.5821757316589355Validating Epoch: 9 | iteration: 49/66 | Loss: 0.6034375429153442Validating Epoch: 9 | iteration: 50/66 | Loss: 0.5622602105140686Validating Epoch: 9 | iteration: 51/66 | Loss: 0.5387005805969238Validating Epoch: 9 | iteration: 52/66 | Loss: 0.5783113241195679Validating Epoch: 9 | iteration: 53/66 | Loss: 0.576653242111206Validating Epoch: 9 | iteration: 54/66 | Loss: 0.6621304750442505Validating Epoch: 9 | iteration: 55/66 | Loss: 0.5656538605690002Validating Epoch: 9 | iteration: 56/66 | Loss: 0.5818132162094116Validating Epoch: 9 | iteration: 57/66 | Loss: 0.6003623008728027Validating Epoch: 9 | iteration: 58/66 | Loss: 0.5564221143722534Validating Epoch: 9 | iteration: 59/66 | Loss: 0.6007153987884521Validating Epoch: 9 | iteration: 60/66 | Loss: 0.6160354614257812Validating Epoch: 9 | iteration: 61/66 | Loss: 0.5898860692977905Validating Epoch: 9 | iteration: 62/66 | Loss: 0.5903631448745728Validating Epoch: 9 | iteration: 63/66 | Loss: 0.5770671367645264Validating Epoch: 9 | iteration: 64/66 | Loss: 0.5435315370559692Validating Epoch: 9 | iteration: 65/66 | Loss: 0.5234076380729675No of GPUs available 4

==================================================
Generating molecules with target properties...
==================================================

Generating for -10...
Generating for -9...
Generating for -8...
Generating for -7...
Generating for -6...

==================================================
Generated molecules saved to: ../checkpoints/DPO_Finetuning_BETA_0.11_epochs_10_LCK_DOCKSTRING_FAST_ACTUAL_affinity_DPO_pref_affinity/generated_molecules.pkl
To analyze and plot results, run:
python analyze_generated_molecules.py --checkpoint_dir ../checkpoints/DPO_Finetuning_BETA_0.11_epochs_10_LCK_DOCKSTRING_FAST_ACTUAL_affinity_DPO_pref_affinity --properties affinity
==================================================

[1;34mwandb[0m: 
[1;34mwandb[0m:  View run [33mDPO_Finetuning_BETA_0.11_epochs_10_LCK_DOCKSTRING_FAST_ACTUAL_affinity_DPO_pref_affinity[0m at: [34mhttps://wandb.ai/bhuvan-kapur1-iiith/molgpt2.0%20FINAL/runs/4ify3ssh[0m
[1;34mwandb[0m: Find logs at: [1;35mwandb/run-20260131_235328-4ify3ssh/logs[0m
