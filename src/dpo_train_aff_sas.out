Starting ...
cuda
Model properties:  ['affinity', 'sas']
Preference properties:  ['affinity']
Building Vocab
{'batch_size': 128, 'd_model': 512, 'n_heads': 8, 'n_layers': 6, 'hidden_units': 512, 'lr': 1e-05, 'epochs': 265, 'properties': ['affinity', 'logps', 'qeds', 'sas', 'tpsas'], 'model_properties': ['affinity', 'sas'], 'run_name': 'LCK_DOCKSTRING_FAST_ACTUAL_affinity_sas'}
length of preference data : 41795
Sample preference_data[0]: ['O1C(C(=O)N2CCN(CC2)C3=CC=C(N(=O)=O)C=C3)=C(C=4C1=CC=CC4)C', array([0.47959185, 0.53255087, 0.54797947, 0.15855041, 0.16055913]), [np.str_('O1C(=NC2=C(C1=O)C=CC=C2)C3=C(C(=O)NCCC4=CC=CC=C4)C=CC(OC)=C3'), np.float64(0.9873459430403937), array([0.52040816, 0.54211667, 0.55725834, 0.15815913, 0.16377715])], [np.str_('O1C(=NC2=C(C1=O)C=CC=C2)C3=CC(OC(=O)C=4C=CC=CC4)=C(OC)C=C3'), np.float64(0.9574398041990725), array([0.43877551, 0.55263869, 0.41284505, 0.14420153, 0.15814562])]]
LCK_DOCKSTRING_FAST_ACTUAL_affinity_sas
len(target_smiles): 41800
len(data): 41800
LCK_DOCKSTRING_FAST_ACTUAL_affinity_sas
Loading affinity-only preference data from: ../checkpoints/LCK_DOCKSTRING_FAST_ACTUAL_affinity_logps_qeds_sas_tpsas/PreferenceData_affinity.pkl
dataset built
cuda
True
2.7.1+cu118
No of GPUs available 4
No of GPUs available 4
{'batch_size': 128, 'd_model': 512, 'n_heads': 8, 'n_layers': 6, 'hidden_units': 512, 'lr': 1e-05, 'epochs': 10, 'model_properties': ['affinity', 'sas'], 'preference_properties': ['affinity'], 'ipo': False, 'run_name': 'DPO_Finetuning_BETA_0.11_epochs_10_LCK_DOCKSTRING_FAST_ACTUAL_affinity_sas_DPO_pref_affinity', 'beta': 0.11}
Temperature:  0.5
Sampling: iteration: 0/51{'Validity': 0.9990234375, 'Novelty': 1.0, 'Uniqueness': 0.9960899315738025}
Training Epoch: 0 | iteration: 0/262 | Loss: 0.8982919454574585Training Epoch: 0 | iteration: 1/262 | Loss: 0.9379238486289978Training Epoch: 0 | iteration: 2/262 | Loss: 0.8731688261032104Training Epoch: 0 | iteration: 3/262 | Loss: 0.9629471302032471Training Epoch: 0 | iteration: 4/262 | Loss: 0.9344644546508789Training Epoch: 0 | iteration: 5/262 | Loss: 0.9176205396652222Training Epoch: 0 | iteration: 6/262 | Loss: 0.8672191500663757Training Epoch: 0 | iteration: 7/262 | Loss: 0.9282678365707397Training Epoch: 0 | iteration: 8/262 | Loss: 0.912203311920166Training Epoch: 0 | iteration: 9/262 | Loss: 0.9627866744995117Training Epoch: 0 | iteration: 10/262 | Loss: 0.8970970511436462Training Epoch: 0 | iteration: 11/262 | Loss: 0.8862078189849854Training Epoch: 0 | iteration: 12/262 | Loss: 0.8530857563018799Training Epoch: 0 | iteration: 13/262 | Loss: 0.8685468435287476Training Epoch: 0 | iteration: 14/262 | Loss: 0.9060308933258057Training Epoch: 0 | iteration: 15/262 | Loss: 0.863793134689331Training Epoch: 0 | iteration: 16/262 | Loss: 0.9197473526000977Training Epoch: 0 | iteration: 17/262 | Loss: 0.7908913493156433Training Epoch: 0 | iteration: 18/262 | Loss: 0.8114131093025208Training Epoch: 0 | iteration: 19/262 | Loss: 0.9221343994140625Training Epoch: 0 | iteration: 20/262 | Loss: 0.7868239879608154Training Epoch: 0 | iteration: 21/262 | Loss: 0.8452481627464294Training Epoch: 0 | iteration: 22/262 | Loss: 0.9509227275848389Training Epoch: 0 | iteration: 23/262 | Loss: 0.8751727342605591Training Epoch: 0 | iteration: 24/262 | Loss: 0.9439993500709534Training Epoch: 0 | iteration: 25/262 | Loss: 0.8959118127822876Training Epoch: 0 | iteration: 26/262 | Loss: 0.7920317053794861Training Epoch: 0 | iteration: 27/262 | Loss: 0.9567713141441345Training Epoch: 0 | iteration: 28/262 | Loss: 0.8287246823310852Training Epoch: 0 | iteration: 29/262 | Loss: 0.8969279527664185Training Epoch: 0 | iteration: 30/262 | Loss: 0.8526777029037476Training Epoch: 0 | iteration: 31/262 | Loss: 0.9028418064117432Training Epoch: 0 | iteration: 32/262 | Loss: 0.9068456888198853Training Epoch: 0 | iteration: 33/262 | Loss: 0.9067708253860474Training Epoch: 0 | iteration: 34/262 | Loss: 0.8801141381263733Training Epoch: 0 | iteration: 35/262 | Loss: 0.8853265643119812Training Epoch: 0 | iteration: 36/262 | Loss: 0.9259891510009766Training Epoch: 0 | iteration: 37/262 | Loss: 0.7724478840827942Training Epoch: 0 | iteration: 38/262 | Loss: 0.8178641200065613Training Epoch: 0 | iteration: 39/262 | Loss: 0.8513562679290771Training Epoch: 0 | iteration: 40/262 | Loss: 0.7966195940971375Training Epoch: 0 | iteration: 41/262 | Loss: 0.8717196583747864Training Epoch: 0 | iteration: 42/262 | Loss: 0.8433530330657959Training Epoch: 0 | iteration: 43/262 | Loss: 0.8914321660995483Training Epoch: 0 | iteration: 44/262 | Loss: 0.9009450078010559Training Epoch: 0 | iteration: 45/262 | Loss: 0.8926173448562622Training Epoch: 0 | iteration: 46/262 | Loss: 0.8079221248626709Training Epoch: 0 | iteration: 47/262 | Loss: 0.9239739775657654Training Epoch: 0 | iteration: 48/262 | Loss: 0.9152032732963562Training Epoch: 0 | iteration: 49/262 | Loss: 0.8534876108169556Training Epoch: 0 | iteration: 50/262 | Loss: 0.7810037136077881Training Epoch: 0 | iteration: 51/262 | Loss: 0.8306897282600403Training Epoch: 0 | iteration: 52/262 | Loss: 0.8195854425430298Training Epoch: 0 | iteration: 53/262 | Loss: 0.8792734742164612Training Epoch: 0 | iteration: 54/262 | Loss: 0.8274606466293335Training Epoch: 0 | iteration: 55/262 | Loss: 0.8801225423812866Training Epoch: 0 | iteration: 56/262 | Loss: 0.8948560953140259Training Epoch: 0 | iteration: 57/262 | Loss: 0.8447803258895874Training Epoch: 0 | iteration: 58/262 | Loss: 0.8275786638259888Training Epoch: 0 | iteration: 59/262 | Loss: 0.8413779735565186Training Epoch: 0 | iteration: 60/262 | Loss: 0.7682065963745117Training Epoch: 0 | iteration: 61/262 | Loss: 0.8021615743637085Training Epoch: 0 | iteration: 62/262 | Loss: 0.8310532569885254Training Epoch: 0 | iteration: 63/262 | Loss: 0.9342913627624512Training Epoch: 0 | iteration: 64/262 | Loss: 0.8040973544120789Training Epoch: 0 | iteration: 65/262 | Loss: 0.8740880489349365Training Epoch: 0 | iteration: 66/262 | Loss: 0.8295331001281738Training Epoch: 0 | iteration: 67/262 | Loss: 0.8446202874183655Training Epoch: 0 | iteration: 68/262 | Loss: 0.8539659976959229Training Epoch: 0 | iteration: 69/262 | Loss: 0.9331713914871216Training Epoch: 0 | iteration: 70/262 | Loss: 0.8895342350006104Training Epoch: 0 | iteration: 71/262 | Loss: 0.8492323756217957Training Epoch: 0 | iteration: 72/262 | Loss: 0.9350613951683044Training Epoch: 0 | iteration: 73/262 | Loss: 0.7713513374328613Training Epoch: 0 | iteration: 74/262 | Loss: 0.8081903457641602Training Epoch: 0 | iteration: 75/262 | Loss: 0.799654483795166Training Epoch: 0 | iteration: 76/262 | Loss: 0.9201684594154358Training Epoch: 0 | iteration: 77/262 | Loss: 0.8111079931259155Training Epoch: 0 | iteration: 78/262 | Loss: 0.8423354625701904Training Epoch: 0 | iteration: 79/262 | Loss: 0.889914870262146Training Epoch: 0 | iteration: 80/262 | Loss: 0.7694547176361084Training Epoch: 0 | iteration: 81/262 | Loss: 0.9045847654342651Training Epoch: 0 | iteration: 82/262 | Loss: 0.741325855255127Training Epoch: 0 | iteration: 83/262 | Loss: 0.7934424877166748Training Epoch: 0 | iteration: 84/262 | Loss: 0.9124242067337036Training Epoch: 0 | iteration: 85/262 | Loss: 0.8325010538101196Training Epoch: 0 | iteration: 86/262 | Loss: 0.8697413206100464Training Epoch: 0 | iteration: 87/262 | Loss: 0.8997832536697388Training Epoch: 0 | iteration: 88/262 | Loss: 0.8778244256973267Training Epoch: 0 | iteration: 89/262 | Loss: 0.7604893445968628Training Epoch: 0 | iteration: 90/262 | Loss: 0.8601535558700562Training Epoch: 0 | iteration: 91/262 | Loss: 0.798741340637207Training Epoch: 0 | iteration: 92/262 | Loss: 0.8102615475654602Training Epoch: 0 | iteration: 93/262 | Loss: 0.9253488779067993Training Epoch: 0 | iteration: 94/262 | Loss: 0.9125372171401978Training Epoch: 0 | iteration: 95/262 | Loss: 0.9172841906547546Training Epoch: 0 | iteration: 96/262 | Loss: 0.8999252319335938Training Epoch: 0 | iteration: 97/262 | Loss: 0.8391019105911255Training Epoch: 0 | iteration: 98/262 | Loss: 0.8487899303436279Training Epoch: 0 | iteration: 99/262 | Loss: 0.8251687288284302Training Epoch: 0 | iteration: 100/262 | Loss: 0.8393846750259399Training Epoch: 0 | iteration: 101/262 | Loss: 0.8668237924575806Training Epoch: 0 | iteration: 102/262 | Loss: 0.8222343921661377Training Epoch: 0 | iteration: 103/262 | Loss: 0.8372178077697754Training Epoch: 0 | iteration: 104/262 | Loss: 0.8208849430084229Training Epoch: 0 | iteration: 105/262 | Loss: 0.8536442518234253Training Epoch: 0 | iteration: 106/262 | Loss: 0.8745801448822021Training Epoch: 0 | iteration: 107/262 | Loss: 0.8360743522644043Training Epoch: 0 | iteration: 108/262 | Loss: 0.8187036514282227Training Epoch: 0 | iteration: 109/262 | Loss: 0.7799381017684937Training Epoch: 0 | iteration: 110/262 | Loss: 0.7959432601928711Training Epoch: 0 | iteration: 111/262 | Loss: 0.8326078653335571Training Epoch: 0 | iteration: 112/262 | Loss: 0.8496718406677246Training Epoch: 0 | iteration: 113/262 | Loss: 0.7459933161735535Training Epoch: 0 | iteration: 114/262 | Loss: 0.8120781183242798Training Epoch: 0 | iteration: 115/262 | Loss: 0.822331964969635Training Epoch: 0 | iteration: 116/262 | Loss: 0.8308401107788086Training Epoch: 0 | iteration: 117/262 | Loss: 0.7805884480476379Training Epoch: 0 | iteration: 118/262 | Loss: 0.8874866962432861Training Epoch: 0 | iteration: 119/262 | Loss: 0.7854677438735962Training Epoch: 0 | iteration: 120/262 | Loss: 0.7513703107833862Training Epoch: 0 | iteration: 121/262 | Loss: 0.8350623250007629Training Epoch: 0 | iteration: 122/262 | Loss: 0.887264609336853Training Epoch: 0 | iteration: 123/262 | Loss: 0.8182458877563477Training Epoch: 0 | iteration: 124/262 | Loss: 0.7471184134483337Training Epoch: 0 | iteration: 125/262 | Loss: 0.8281161785125732Training Epoch: 0 | iteration: 126/262 | Loss: 0.8127257823944092Training Epoch: 0 | iteration: 127/262 | Loss: 0.8705482482910156Training Epoch: 0 | iteration: 128/262 | Loss: 0.8519666194915771Training Epoch: 0 | iteration: 129/262 | Loss: 0.8489447236061096Training Epoch: 0 | iteration: 130/262 | Loss: 0.9102437496185303Training Epoch: 0 | iteration: 131/262 | Loss: 0.8388931751251221Training Epoch: 0 | iteration: 132/262 | Loss: 0.8289508819580078Training Epoch: 0 | iteration: 133/262 | Loss: 0.8112307786941528Training Epoch: 0 | iteration: 134/262 | Loss: 0.8517254590988159Training Epoch: 0 | iteration: 135/262 | Loss: 0.8523117303848267Training Epoch: 0 | iteration: 136/262 | Loss: 0.7885338068008423Training Epoch: 0 | iteration: 137/262 | Loss: 0.9184060096740723Training Epoch: 0 | iteration: 138/262 | Loss: 0.8516782522201538Training Epoch: 0 | iteration: 139/262 | Loss: 0.7946603298187256Training Epoch: 0 | iteration: 140/262 | Loss: 0.7509903907775879Training Epoch: 0 | iteration: 141/262 | Loss: 0.8079374432563782Training Epoch: 0 | iteration: 142/262 | Loss: 0.792704701423645Training Epoch: 0 | iteration: 143/262 | Loss: 0.8106967210769653Training Epoch: 0 | iteration: 144/262 | Loss: 0.816697359085083Training Epoch: 0 | iteration: 145/262 | Loss: 0.8429648876190186Training Epoch: 0 | iteration: 146/262 | Loss: 0.8127817511558533Training Epoch: 0 | iteration: 147/262 | Loss: 0.8828250765800476Training Epoch: 0 | iteration: 148/262 | Loss: 0.7757027745246887Training Epoch: 0 | iteration: 149/262 | Loss: 0.8982532024383545Training Epoch: 0 | iteration: 150/262 | Loss: 0.8101934790611267Training Epoch: 0 | iteration: 151/262 | Loss: 0.7852750420570374Training Epoch: 0 | iteration: 152/262 | Loss: 0.7834385633468628Training Epoch: 0 | iteration: 153/262 | Loss: 0.9268865585327148Training Epoch: 0 | iteration: 154/262 | Loss: 0.8970127105712891Training Epoch: 0 | iteration: 155/262 | Loss: 0.8455573320388794Training Epoch: 0 | iteration: 156/262 | Loss: 0.8605251312255859Training Epoch: 0 | iteration: 157/262 | Loss: 0.8104836344718933Training Epoch: 0 | iteration: 158/262 | Loss: 0.7602799534797668Training Epoch: 0 | iteration: 159/262 | Loss: 0.8970538377761841Training Epoch: 0 | iteration: 160/262 | Loss: 0.8230338096618652Training Epoch: 0 | iteration: 161/262 | Loss: 0.8068114519119263Training Epoch: 0 | iteration: 162/262 | Loss: 0.795769453048706Training Epoch: 0 | iteration: 163/262 | Loss: 0.8633074164390564Training Epoch: 0 | iteration: 164/262 | Loss: 0.8669792413711548Training Epoch: 0 | iteration: 165/262 | Loss: 0.8465284109115601Training Epoch: 0 | iteration: 166/262 | Loss: 0.7834247946739197Training Epoch: 0 | iteration: 167/262 | Loss: 0.9199641942977905Training Epoch: 0 | iteration: 168/262 | Loss: 0.8180554509162903Training Epoch: 0 | iteration: 169/262 | Loss: 0.8408008813858032Training Epoch: 0 | iteration: 170/262 | Loss: 0.8611372709274292Training Epoch: 0 | iteration: 171/262 | Loss: 0.8747861981391907Training Epoch: 0 | iteration: 172/262 | Loss: 0.7555981278419495Training Epoch: 0 | iteration: 173/262 | Loss: 0.8633235692977905Training Epoch: 0 | iteration: 174/262 | Loss: 0.8920727968215942Training Epoch: 0 | iteration: 175/262 | Loss: 0.8345413208007812Training Epoch: 0 | iteration: 176/262 | Loss: 0.7942525148391724Training Epoch: 0 | iteration: 177/262 | Loss: 0.8355994820594788Training Epoch: 0 | iteration: 178/262 | Loss: 0.7818087339401245Training Epoch: 0 | iteration: 179/262 | Loss: 0.7818917036056519Training Epoch: 0 | iteration: 180/262 | Loss: 0.8304725289344788Training Epoch: 0 | iteration: 181/262 | Loss: 0.7713803052902222Training Epoch: 0 | iteration: 182/262 | Loss: 0.8088754415512085Training Epoch: 0 | iteration: 183/262 | Loss: 0.8056816458702087Training Epoch: 0 | iteration: 184/262 | Loss: 0.765393078327179Training Epoch: 0 | iteration: 185/262 | Loss: 0.846230685710907Training Epoch: 0 | iteration: 186/262 | Loss: 0.8251084089279175Training Epoch: 0 | iteration: 187/262 | Loss: 0.7791798710823059Training Epoch: 0 | iteration: 188/262 | Loss: 0.7436895966529846Training Epoch: 0 | iteration: 189/262 | Loss: 0.797797679901123Training Epoch: 0 | iteration: 190/262 | Loss: 0.7719697952270508Training Epoch: 0 | iteration: 191/262 | Loss: 0.7902513742446899Training Epoch: 0 | iteration: 192/262 | Loss: 0.7986592054367065Training Epoch: 0 | iteration: 193/262 | Loss: 0.7600616812705994Training Epoch: 0 | iteration: 194/262 | Loss: 0.8239792585372925Training Epoch: 0 | iteration: 195/262 | Loss: 0.7841846346855164Training Epoch: 0 | iteration: 196/262 | Loss: 0.8081368207931519Training Epoch: 0 | iteration: 197/262 | Loss: 0.779305100440979Training Epoch: 0 | iteration: 198/262 | Loss: 0.7307661771774292Training Epoch: 0 | iteration: 199/262 | Loss: 0.7989115715026855Training Epoch: 0 | iteration: 200/262 | Loss: 0.7774602174758911Training Epoch: 0 | iteration: 201/262 | Loss: 0.8294605612754822Training Epoch: 0 | iteration: 202/262 | Loss: 0.7615225911140442Training Epoch: 0 | iteration: 203/262 | Loss: 0.9066673517227173Training Epoch: 0 | iteration: 204/262 | Loss: 0.8399624228477478Training Epoch: 0 | iteration: 205/262 | Loss: 0.8128305673599243Training Epoch: 0 | iteration: 206/262 | Loss: 0.7910250425338745Training Epoch: 0 | iteration: 207/262 | Loss: 0.7988458871841431Training Epoch: 0 | iteration: 208/262 | Loss: 0.7479043006896973Training Epoch: 0 | iteration: 209/262 | Loss: 0.8792271614074707Training Epoch: 0 | iteration: 210/262 | Loss: 0.796282172203064Training Epoch: 0 | iteration: 211/262 | Loss: 0.8138660192489624Training Epoch: 0 | iteration: 212/262 | Loss: 0.8980681896209717Training Epoch: 0 | iteration: 213/262 | Loss: 0.7706351280212402Training Epoch: 0 | iteration: 214/262 | Loss: 0.7837425470352173Training Epoch: 0 | iteration: 215/262 | Loss: 0.8491877317428589Training Epoch: 0 | iteration: 216/262 | Loss: 0.7760019302368164Training Epoch: 0 | iteration: 217/262 | Loss: 0.6758880615234375Training Epoch: 0 | iteration: 218/262 | Loss: 0.8299550414085388Training Epoch: 0 | iteration: 219/262 | Loss: 0.8702765107154846Training Epoch: 0 | iteration: 220/262 | Loss: 0.8529316782951355Training Epoch: 0 | iteration: 221/262 | Loss: 0.7406612038612366Training Epoch: 0 | iteration: 222/262 | Loss: 0.9531970024108887Training Epoch: 0 | iteration: 223/262 | Loss: 0.8851162195205688Training Epoch: 0 | iteration: 224/262 | Loss: 0.7901938557624817Training Epoch: 0 | iteration: 225/262 | Loss: 0.7232422828674316Training Epoch: 0 | iteration: 226/262 | Loss: 0.8010542392730713Training Epoch: 0 | iteration: 227/262 | Loss: 0.861211359500885Training Epoch: 0 | iteration: 228/262 | Loss: 0.7782285809516907Training Epoch: 0 | iteration: 229/262 | Loss: 0.825372576713562Training Epoch: 0 | iteration: 230/262 | Loss: 0.8574893474578857Training Epoch: 0 | iteration: 231/262 | Loss: 0.8017100095748901Training Epoch: 0 | iteration: 232/262 | Loss: 0.8044767379760742Training Epoch: 0 | iteration: 233/262 | Loss: 0.8403137922286987Training Epoch: 0 | iteration: 234/262 | Loss: 0.8517551422119141Training Epoch: 0 | iteration: 235/262 | Loss: 0.8296502828598022Training Epoch: 0 | iteration: 236/262 | Loss: 0.7977423667907715Training Epoch: 0 | iteration: 237/262 | Loss: 0.7706628441810608Training Epoch: 0 | iteration: 238/262 | Loss: 0.7687740325927734Training Epoch: 0 | iteration: 239/262 | Loss: 0.8478628993034363Training Epoch: 0 | iteration: 240/262 | Loss: 0.8249123096466064Training Epoch: 0 | iteration: 241/262 | Loss: 0.8128023147583008Training Epoch: 0 | iteration: 242/262 | Loss: 0.8308907747268677Training Epoch: 0 | iteration: 243/262 | Loss: 0.7799152731895447Training Epoch: 0 | iteration: 244/262 | Loss: 0.8945571184158325Training Epoch: 0 | iteration: 245/262 | Loss: 0.7608238458633423Training Epoch: 0 | iteration: 246/262 | Loss: 0.8051533699035645Training Epoch: 0 | iteration: 247/262 | Loss: 0.7790392637252808Training Epoch: 0 | iteration: 248/262 | Loss: 0.808059811592102Training Epoch: 0 | iteration: 249/262 | Loss: 0.7788291573524475Training Epoch: 0 | iteration: 250/262 | Loss: 0.80307936668396Training Epoch: 0 | iteration: 251/262 | Loss: 0.7915744185447693Training Epoch: 0 | iteration: 252/262 | Loss: 0.8126627206802368Training Epoch: 0 | iteration: 253/262 | Loss: 0.7833612561225891Training Epoch: 0 | iteration: 254/262 | Loss: 0.7883945107460022Training Epoch: 0 | iteration: 255/262 | Loss: 0.6994336247444153Training Epoch: 0 | iteration: 256/262 | Loss: 0.9336556792259216Training Epoch: 0 | iteration: 257/262 | Loss: 0.8532731533050537Training Epoch: 0 | iteration: 258/262 | Loss: 0.8094967603683472Training Epoch: 0 | iteration: 259/262 | Loss: 0.8227320909500122Training Epoch: 0 | iteration: 260/262 | Loss: 0.8403403759002686Training Epoch: 0 | iteration: 261/262 | Loss: 0.9205501079559326Validating Epoch: 0 | iteration: 0/66 | Loss: 0.7033841609954834Validating Epoch: 0 | iteration: 1/66 | Loss: 0.6662243604660034Validating Epoch: 0 | iteration: 2/66 | Loss: 0.6318910121917725Validating Epoch: 0 | iteration: 3/66 | Loss: 0.6872894763946533Validating Epoch: 0 | iteration: 4/66 | Loss: 0.7048274874687195Validating Epoch: 0 | iteration: 5/66 | Loss: 0.6762614846229553Validating Epoch: 0 | iteration: 6/66 | Loss: 0.7111879587173462Validating Epoch: 0 | iteration: 7/66 | Loss: 0.6523194313049316Validating Epoch: 0 | iteration: 8/66 | Loss: 0.6638692617416382Validating Epoch: 0 | iteration: 9/66 | Loss: 0.6869796514511108Validating Epoch: 0 | iteration: 10/66 | Loss: 0.6924564838409424Validating Epoch: 0 | iteration: 11/66 | Loss: 0.634739875793457Validating Epoch: 0 | iteration: 12/66 | Loss: 0.665742814540863Validating Epoch: 0 | iteration: 13/66 | Loss: 0.7137410640716553Validating Epoch: 0 | iteration: 14/66 | Loss: 0.6976781487464905Validating Epoch: 0 | iteration: 15/66 | Loss: 0.6600490808486938Validating Epoch: 0 | iteration: 16/66 | Loss: 0.7411813735961914Validating Epoch: 0 | iteration: 17/66 | Loss: 0.7104206085205078Validating Epoch: 0 | iteration: 18/66 | Loss: 0.6558332443237305Validating Epoch: 0 | iteration: 19/66 | Loss: 0.6547667384147644Validating Epoch: 0 | iteration: 20/66 | Loss: 0.6582092642784119Validating Epoch: 0 | iteration: 21/66 | Loss: 0.6673086881637573Validating Epoch: 0 | iteration: 22/66 | Loss: 0.693609356880188Validating Epoch: 0 | iteration: 23/66 | Loss: 0.6110858917236328Validating Epoch: 0 | iteration: 24/66 | Loss: 0.6764969825744629Validating Epoch: 0 | iteration: 25/66 | Loss: 0.6864738464355469Validating Epoch: 0 | iteration: 26/66 | Loss: 0.6852805614471436Validating Epoch: 0 | iteration: 27/66 | Loss: 0.645725429058075Validating Epoch: 0 | iteration: 28/66 | Loss: 0.7129220962524414Validating Epoch: 0 | iteration: 29/66 | Loss: 0.6546266078948975Validating Epoch: 0 | iteration: 30/66 | Loss: 0.6922211050987244Validating Epoch: 0 | iteration: 31/66 | Loss: 0.6481132507324219Validating Epoch: 0 | iteration: 32/66 | Loss: 0.6647164821624756Validating Epoch: 0 | iteration: 33/66 | Loss: 0.6557655334472656Validating Epoch: 0 | iteration: 34/66 | Loss: 0.6741192936897278Validating Epoch: 0 | iteration: 35/66 | Loss: 0.7155272960662842Validating Epoch: 0 | iteration: 36/66 | Loss: 0.656212568283081Validating Epoch: 0 | iteration: 37/66 | Loss: 0.665240466594696Validating Epoch: 0 | iteration: 38/66 | Loss: 0.7022397518157959Validating Epoch: 0 | iteration: 39/66 | Loss: 0.6560227274894714Validating Epoch: 0 | iteration: 40/66 | Loss: 0.6123273372650146Validating Epoch: 0 | iteration: 41/66 | Loss: 0.6969494819641113Validating Epoch: 0 | iteration: 42/66 | Loss: 0.692646324634552Validating Epoch: 0 | iteration: 43/66 | Loss: 0.7240707874298096Validating Epoch: 0 | iteration: 44/66 | Loss: 0.6300105452537537Validating Epoch: 0 | iteration: 45/66 | Loss: 0.6318143606185913Validating Epoch: 0 | iteration: 46/66 | Loss: 0.6898699998855591Validating Epoch: 0 | iteration: 47/66 | Loss: 0.706283450126648Validating Epoch: 0 | iteration: 48/66 | Loss: 0.6980127096176147Validating Epoch: 0 | iteration: 49/66 | Loss: 0.6821181178092957Validating Epoch: 0 | iteration: 50/66 | Loss: 0.6124573945999146Validating Epoch: 0 | iteration: 51/66 | Loss: 0.7005536556243896Validating Epoch: 0 | iteration: 52/66 | Loss: 0.6762449741363525Validating Epoch: 0 | iteration: 53/66 | Loss: 0.6700451374053955Validating Epoch: 0 | iteration: 54/66 | Loss: 0.6850231289863586Validating Epoch: 0 | iteration: 55/66 | Loss: 0.6634224653244019Validating Epoch: 0 | iteration: 56/66 | Loss: 0.6735399961471558Validating Epoch: 0 | iteration: 57/66 | Loss: 0.6664988398551941Validating Epoch: 0 | iteration: 58/66 | Loss: 0.6548115015029907Validating Epoch: 0 | iteration: 59/66 | Loss: 0.665044367313385Validating Epoch: 0 | iteration: 60/66 | Loss: 0.6833276748657227Validating Epoch: 0 | iteration: 61/66 | Loss: 0.6926859617233276Validating Epoch: 0 | iteration: 62/66 | Loss: 0.7753145098686218Validating Epoch: 0 | iteration: 63/66 | Loss: 0.6783131957054138Validating Epoch: 0 | iteration: 64/66 | Loss: 0.7308171391487122Validating Epoch: 0 | iteration: 65/66 | Loss: 0.7219891548156738Temperature:  0.5
Sampling: iteration: 0/51{'Validity': 0.998046875, 'Novelty': 1.0, 'Uniqueness': 0.9951076320939335}
Training Epoch: 1 | iteration: 0/262 | Loss: 0.8050642013549805Training Epoch: 1 | iteration: 1/262 | Loss: 0.737903356552124Training Epoch: 1 | iteration: 2/262 | Loss: 0.6967194676399231Training Epoch: 1 | iteration: 3/262 | Loss: 0.7177531719207764Training Epoch: 1 | iteration: 4/262 | Loss: 0.7262803316116333Training Epoch: 1 | iteration: 5/262 | Loss: 0.7750027179718018Training Epoch: 1 | iteration: 6/262 | Loss: 0.7473527193069458Training Epoch: 1 | iteration: 7/262 | Loss: 0.7077256441116333Training Epoch: 1 | iteration: 8/262 | Loss: 0.7674708962440491Training Epoch: 1 | iteration: 9/262 | Loss: 0.7355088591575623Training Epoch: 1 | iteration: 10/262 | Loss: 0.7529528141021729Training Epoch: 1 | iteration: 11/262 | Loss: 0.7891524434089661Training Epoch: 1 | iteration: 12/262 | Loss: 0.7329766750335693Training Epoch: 1 | iteration: 13/262 | Loss: 0.7786285281181335Training Epoch: 1 | iteration: 14/262 | Loss: 0.7794390916824341Training Epoch: 1 | iteration: 15/262 | Loss: 0.799325704574585Training Epoch: 1 | iteration: 16/262 | Loss: 0.712466835975647Training Epoch: 1 | iteration: 17/262 | Loss: 0.8026103973388672Training Epoch: 1 | iteration: 18/262 | Loss: 0.7343636751174927Training Epoch: 1 | iteration: 19/262 | Loss: 0.7468796372413635Training Epoch: 1 | iteration: 20/262 | Loss: 0.7902359962463379Training Epoch: 1 | iteration: 21/262 | Loss: 0.6736562252044678Training Epoch: 1 | iteration: 22/262 | Loss: 0.7833555340766907Training Epoch: 1 | iteration: 23/262 | Loss: 0.8157556056976318Training Epoch: 1 | iteration: 24/262 | Loss: 0.7889413833618164Training Epoch: 1 | iteration: 25/262 | Loss: 0.7223457098007202Training Epoch: 1 | iteration: 26/262 | Loss: 0.6993851661682129Training Epoch: 1 | iteration: 27/262 | Loss: 0.703127384185791Training Epoch: 1 | iteration: 28/262 | Loss: 0.7522678971290588Training Epoch: 1 | iteration: 29/262 | Loss: 0.7391626834869385Training Epoch: 1 | iteration: 30/262 | Loss: 0.7957335710525513Training Epoch: 1 | iteration: 31/262 | Loss: 0.7585157155990601Training Epoch: 1 | iteration: 32/262 | Loss: 0.7216259241104126Training Epoch: 1 | iteration: 33/262 | Loss: 0.736292839050293Training Epoch: 1 | iteration: 34/262 | Loss: 0.792260468006134Training Epoch: 1 | iteration: 35/262 | Loss: 0.8321574926376343Training Epoch: 1 | iteration: 36/262 | Loss: 0.7272701859474182Training Epoch: 1 | iteration: 37/262 | Loss: 0.765964150428772Training Epoch: 1 | iteration: 38/262 | Loss: 0.7356445789337158Training Epoch: 1 | iteration: 39/262 | Loss: 0.6991156935691833Training Epoch: 1 | iteration: 40/262 | Loss: 0.7516304850578308Training Epoch: 1 | iteration: 41/262 | Loss: 0.8125454783439636Training Epoch: 1 | iteration: 42/262 | Loss: 0.665209174156189Training Epoch: 1 | iteration: 43/262 | Loss: 0.776161789894104Training Epoch: 1 | iteration: 44/262 | Loss: 0.725623369216919Training Epoch: 1 | iteration: 45/262 | Loss: 0.7794517278671265Training Epoch: 1 | iteration: 46/262 | Loss: 0.7657239437103271Training Epoch: 1 | iteration: 47/262 | Loss: 0.7801550626754761Training Epoch: 1 | iteration: 48/262 | Loss: 0.8645836114883423Training Epoch: 1 | iteration: 49/262 | Loss: 0.7131453156471252Training Epoch: 1 | iteration: 50/262 | Loss: 0.784077525138855Training Epoch: 1 | iteration: 51/262 | Loss: 0.7563350200653076Training Epoch: 1 | iteration: 52/262 | Loss: 0.7452837228775024Training Epoch: 1 | iteration: 53/262 | Loss: 0.8309515714645386Training Epoch: 1 | iteration: 54/262 | Loss: 0.7045691013336182Training Epoch: 1 | iteration: 55/262 | Loss: 0.8032143712043762Training Epoch: 1 | iteration: 56/262 | Loss: 0.7037930488586426Training Epoch: 1 | iteration: 57/262 | Loss: 0.7820302248001099Training Epoch: 1 | iteration: 58/262 | Loss: 0.8202489614486694Training Epoch: 1 | iteration: 59/262 | Loss: 0.7353257536888123Training Epoch: 1 | iteration: 60/262 | Loss: 0.8443778157234192Training Epoch: 1 | iteration: 61/262 | Loss: 0.7028807401657104Training Epoch: 1 | iteration: 62/262 | Loss: 0.7068485021591187Training Epoch: 1 | iteration: 63/262 | Loss: 0.7635284066200256Training Epoch: 1 | iteration: 64/262 | Loss: 0.7783958911895752Training Epoch: 1 | iteration: 65/262 | Loss: 0.8340498805046082Training Epoch: 1 | iteration: 66/262 | Loss: 0.7610103487968445Training Epoch: 1 | iteration: 67/262 | Loss: 0.8286181688308716Training Epoch: 1 | iteration: 68/262 | Loss: 0.8133478760719299Training Epoch: 1 | iteration: 69/262 | Loss: 0.6845920085906982Training Epoch: 1 | iteration: 70/262 | Loss: 0.8211350440979004Training Epoch: 1 | iteration: 71/262 | Loss: 0.7905858159065247Training Epoch: 1 | iteration: 72/262 | Loss: 0.8276453614234924Training Epoch: 1 | iteration: 73/262 | Loss: 0.7113306522369385Training Epoch: 1 | iteration: 74/262 | Loss: 0.742935299873352Training Epoch: 1 | iteration: 75/262 | Loss: 0.7390748262405396Training Epoch: 1 | iteration: 76/262 | Loss: 0.720868706703186Training Epoch: 1 | iteration: 77/262 | Loss: 0.6951716542243958Training Epoch: 1 | iteration: 78/262 | Loss: 0.8098592758178711Training Epoch: 1 | iteration: 79/262 | Loss: 0.7234368920326233Training Epoch: 1 | iteration: 80/262 | Loss: 0.6883972883224487Training Epoch: 1 | iteration: 81/262 | Loss: 0.7727258205413818Training Epoch: 1 | iteration: 82/262 | Loss: 0.682802677154541Training Epoch: 1 | iteration: 83/262 | Loss: 0.7989122867584229Training Epoch: 1 | iteration: 84/262 | Loss: 0.7179499864578247Training Epoch: 1 | iteration: 85/262 | Loss: 0.7396247386932373Training Epoch: 1 | iteration: 86/262 | Loss: 0.686904788017273Training Epoch: 1 | iteration: 87/262 | Loss: 0.8586182594299316Training Epoch: 1 | iteration: 88/262 | Loss: 0.7211480736732483Training Epoch: 1 | iteration: 89/262 | Loss: 0.7421388030052185Training Epoch: 1 | iteration: 90/262 | Loss: 0.8498144149780273Training Epoch: 1 | iteration: 91/262 | Loss: 0.7235119342803955Training Epoch: 1 | iteration: 92/262 | Loss: 0.688385009765625Training Epoch: 1 | iteration: 93/262 | Loss: 0.7485685348510742Training Epoch: 1 | iteration: 94/262 | Loss: 0.7751695513725281Training Epoch: 1 | iteration: 95/262 | Loss: 0.7654838562011719Training Epoch: 1 | iteration: 96/262 | Loss: 0.7575325965881348Training Epoch: 1 | iteration: 97/262 | Loss: 0.7855973839759827Training Epoch: 1 | iteration: 98/262 | Loss: 0.7779805660247803Training Epoch: 1 | iteration: 99/262 | Loss: 0.7076379060745239Training Epoch: 1 | iteration: 100/262 | Loss: 0.7193161249160767Training Epoch: 1 | iteration: 101/262 | Loss: 0.736850380897522Training Epoch: 1 | iteration: 102/262 | Loss: 0.7663270235061646Training Epoch: 1 | iteration: 103/262 | Loss: 0.7350949048995972Training Epoch: 1 | iteration: 104/262 | Loss: 0.737320601940155Training Epoch: 1 | iteration: 105/262 | Loss: 0.7285544276237488Training Epoch: 1 | iteration: 106/262 | Loss: 0.7432413697242737Training Epoch: 1 | iteration: 107/262 | Loss: 0.7700358629226685Training Epoch: 1 | iteration: 108/262 | Loss: 0.7987366914749146Training Epoch: 1 | iteration: 109/262 | Loss: 0.7301921248435974Training Epoch: 1 | iteration: 110/262 | Loss: 0.7686892747879028Training Epoch: 1 | iteration: 111/262 | Loss: 0.7905285358428955Training Epoch: 1 | iteration: 112/262 | Loss: 0.7783306837081909Training Epoch: 1 | iteration: 113/262 | Loss: 0.7611680030822754Training Epoch: 1 | iteration: 114/262 | Loss: 0.786946713924408Training Epoch: 1 | iteration: 115/262 | Loss: 0.7745265960693359Training Epoch: 1 | iteration: 116/262 | Loss: 0.7814640998840332Training Epoch: 1 | iteration: 117/262 | Loss: 0.8502781987190247Training Epoch: 1 | iteration: 118/262 | Loss: 0.7311800718307495Training Epoch: 1 | iteration: 119/262 | Loss: 0.7493429780006409Training Epoch: 1 | iteration: 120/262 | Loss: 0.7635908126831055Training Epoch: 1 | iteration: 121/262 | Loss: 0.7002148032188416Training Epoch: 1 | iteration: 122/262 | Loss: 0.8327945470809937Training Epoch: 1 | iteration: 123/262 | Loss: 0.7467448115348816Training Epoch: 1 | iteration: 124/262 | Loss: 0.7398842573165894Training Epoch: 1 | iteration: 125/262 | Loss: 0.8588131666183472Training Epoch: 1 | iteration: 126/262 | Loss: 0.7200174331665039Training Epoch: 1 | iteration: 127/262 | Loss: 0.8207873106002808Training Epoch: 1 | iteration: 128/262 | Loss: 0.7659497261047363Training Epoch: 1 | iteration: 129/262 | Loss: 0.6693497896194458Training Epoch: 1 | iteration: 130/262 | Loss: 0.7670403718948364Training Epoch: 1 | iteration: 131/262 | Loss: 0.839684009552002Training Epoch: 1 | iteration: 132/262 | Loss: 0.7602006793022156Training Epoch: 1 | iteration: 133/262 | Loss: 0.753117561340332Training Epoch: 1 | iteration: 134/262 | Loss: 0.7513132691383362Training Epoch: 1 | iteration: 135/262 | Loss: 0.6707080602645874Training Epoch: 1 | iteration: 136/262 | Loss: 0.7877487540245056Training Epoch: 1 | iteration: 137/262 | Loss: 0.7231405973434448Training Epoch: 1 | iteration: 138/262 | Loss: 0.724181056022644Training Epoch: 1 | iteration: 139/262 | Loss: 0.7911354303359985Training Epoch: 1 | iteration: 140/262 | Loss: 0.7116841077804565Training Epoch: 1 | iteration: 141/262 | Loss: 0.8576151132583618Training Epoch: 1 | iteration: 142/262 | Loss: 0.7896859645843506Training Epoch: 1 | iteration: 143/262 | Loss: 0.7703105211257935Training Epoch: 1 | iteration: 144/262 | Loss: 0.7987378835678101Training Epoch: 1 | iteration: 145/262 | Loss: 0.7132565975189209Training Epoch: 1 | iteration: 146/262 | Loss: 0.7133969664573669Training Epoch: 1 | iteration: 147/262 | Loss: 0.7883164882659912Training Epoch: 1 | iteration: 148/262 | Loss: 0.675209641456604Training Epoch: 1 | iteration: 149/262 | Loss: 0.7324175834655762Training Epoch: 1 | iteration: 150/262 | Loss: 0.7389079332351685Training Epoch: 1 | iteration: 151/262 | Loss: 0.7371376752853394Training Epoch: 1 | iteration: 152/262 | Loss: 0.7265841960906982Training Epoch: 1 | iteration: 153/262 | Loss: 0.8058484792709351Training Epoch: 1 | iteration: 154/262 | Loss: 0.7202603220939636Training Epoch: 1 | iteration: 155/262 | Loss: 0.7823379635810852Training Epoch: 1 | iteration: 156/262 | Loss: 0.8142819404602051Training Epoch: 1 | iteration: 157/262 | Loss: 0.7745654582977295Training Epoch: 1 | iteration: 158/262 | Loss: 0.7851405143737793Training Epoch: 1 | iteration: 159/262 | Loss: 0.6858198642730713Training Epoch: 1 | iteration: 160/262 | Loss: 0.7500296831130981Training Epoch: 1 | iteration: 161/262 | Loss: 0.7255018353462219Training Epoch: 1 | iteration: 162/262 | Loss: 0.712509036064148Training Epoch: 1 | iteration: 163/262 | Loss: 0.7148869037628174Training Epoch: 1 | iteration: 164/262 | Loss: 0.7016986608505249Training Epoch: 1 | iteration: 165/262 | Loss: 0.7099291682243347Training Epoch: 1 | iteration: 166/262 | Loss: 0.7200872302055359Training Epoch: 1 | iteration: 167/262 | Loss: 0.8037585020065308Training Epoch: 1 | iteration: 168/262 | Loss: 0.7008094787597656Training Epoch: 1 | iteration: 169/262 | Loss: 0.7387571930885315Training Epoch: 1 | iteration: 170/262 | Loss: 0.6916188597679138Training Epoch: 1 | iteration: 171/262 | Loss: 0.6503778696060181Training Epoch: 1 | iteration: 172/262 | Loss: 0.7733360528945923Training Epoch: 1 | iteration: 173/262 | Loss: 0.7735347151756287Training Epoch: 1 | iteration: 174/262 | Loss: 0.7475440502166748Training Epoch: 1 | iteration: 175/262 | Loss: 0.7111495733261108Training Epoch: 1 | iteration: 176/262 | Loss: 0.8368399143218994Training Epoch: 1 | iteration: 177/262 | Loss: 0.7614281177520752Training Epoch: 1 | iteration: 178/262 | Loss: 0.7292783260345459Training Epoch: 1 | iteration: 179/262 | Loss: 0.8230400085449219Training Epoch: 1 | iteration: 180/262 | Loss: 0.8559825420379639Training Epoch: 1 | iteration: 181/262 | Loss: 0.785290002822876Training Epoch: 1 | iteration: 182/262 | Loss: 0.6626623272895813Training Epoch: 1 | iteration: 183/262 | Loss: 0.729937732219696Training Epoch: 1 | iteration: 184/262 | Loss: 0.7692853212356567Training Epoch: 1 | iteration: 185/262 | Loss: 0.7553863525390625Training Epoch: 1 | iteration: 186/262 | Loss: 0.7782903909683228Training Epoch: 1 | iteration: 187/262 | Loss: 0.7788536548614502Training Epoch: 1 | iteration: 188/262 | Loss: 0.718397855758667Training Epoch: 1 | iteration: 189/262 | Loss: 0.8406954407691956Training Epoch: 1 | iteration: 190/262 | Loss: 0.7468717098236084Training Epoch: 1 | iteration: 191/262 | Loss: 0.7442848086357117Training Epoch: 1 | iteration: 192/262 | Loss: 0.6892279982566833Training Epoch: 1 | iteration: 193/262 | Loss: 0.7848140001296997Training Epoch: 1 | iteration: 194/262 | Loss: 0.7896891832351685Training Epoch: 1 | iteration: 195/262 | Loss: 0.7355979084968567Training Epoch: 1 | iteration: 196/262 | Loss: 0.8499400019645691Training Epoch: 1 | iteration: 197/262 | Loss: 0.8187099695205688Training Epoch: 1 | iteration: 198/262 | Loss: 0.7535974979400635Training Epoch: 1 | iteration: 199/262 | Loss: 0.7378063201904297Training Epoch: 1 | iteration: 200/262 | Loss: 0.6927277445793152Training Epoch: 1 | iteration: 201/262 | Loss: 0.8049055337905884Training Epoch: 1 | iteration: 202/262 | Loss: 0.7147600650787354Training Epoch: 1 | iteration: 203/262 | Loss: 0.6657072305679321Training Epoch: 1 | iteration: 204/262 | Loss: 0.7685577869415283Training Epoch: 1 | iteration: 205/262 | Loss: 0.7609558701515198Training Epoch: 1 | iteration: 206/262 | Loss: 0.799263596534729Training Epoch: 1 | iteration: 207/262 | Loss: 0.7741917967796326Training Epoch: 1 | iteration: 208/262 | Loss: 0.7327059507369995Training Epoch: 1 | iteration: 209/262 | Loss: 0.7980101108551025Training Epoch: 1 | iteration: 210/262 | Loss: 0.7037385702133179Training Epoch: 1 | iteration: 211/262 | Loss: 0.757006824016571Training Epoch: 1 | iteration: 212/262 | Loss: 0.7179485559463501Training Epoch: 1 | iteration: 213/262 | Loss: 0.6987346410751343Training Epoch: 1 | iteration: 214/262 | Loss: 0.7483183145523071Training Epoch: 1 | iteration: 215/262 | Loss: 0.7736678719520569Training Epoch: 1 | iteration: 216/262 | Loss: 0.7592961192131042Training Epoch: 1 | iteration: 217/262 | Loss: 0.715313196182251Training Epoch: 1 | iteration: 218/262 | Loss: 0.811583399772644Training Epoch: 1 | iteration: 219/262 | Loss: 0.8007172346115112Training Epoch: 1 | iteration: 220/262 | Loss: 0.7869656085968018Training Epoch: 1 | iteration: 221/262 | Loss: 0.7394192218780518Training Epoch: 1 | iteration: 222/262 | Loss: 0.7386795282363892Training Epoch: 1 | iteration: 223/262 | Loss: 0.8263583183288574Training Epoch: 1 | iteration: 224/262 | Loss: 0.7852093577384949Training Epoch: 1 | iteration: 225/262 | Loss: 0.7494449019432068Training Epoch: 1 | iteration: 226/262 | Loss: 0.7367557287216187Training Epoch: 1 | iteration: 227/262 | Loss: 0.7891409397125244Training Epoch: 1 | iteration: 228/262 | Loss: 0.7629300951957703Training Epoch: 1 | iteration: 229/262 | Loss: 0.7552223205566406Training Epoch: 1 | iteration: 230/262 | Loss: 0.7566618323326111Training Epoch: 1 | iteration: 231/262 | Loss: 0.6829497814178467Training Epoch: 1 | iteration: 232/262 | Loss: 0.7300288677215576Training Epoch: 1 | iteration: 233/262 | Loss: 0.6996142864227295Training Epoch: 1 | iteration: 234/262 | Loss: 0.732698917388916Training Epoch: 1 | iteration: 235/262 | Loss: 0.7600977420806885Training Epoch: 1 | iteration: 236/262 | Loss: 0.7486330270767212Training Epoch: 1 | iteration: 237/262 | Loss: 0.7430849075317383Training Epoch: 1 | iteration: 238/262 | Loss: 0.8099535703659058Training Epoch: 1 | iteration: 239/262 | Loss: 0.8245395421981812Training Epoch: 1 | iteration: 240/262 | Loss: 0.7507938146591187Training Epoch: 1 | iteration: 241/262 | Loss: 0.8441118597984314Training Epoch: 1 | iteration: 242/262 | Loss: 0.7071769833564758Training Epoch: 1 | iteration: 243/262 | Loss: 0.7864384055137634Training Epoch: 1 | iteration: 244/262 | Loss: 0.8028669357299805Training Epoch: 1 | iteration: 245/262 | Loss: 0.7766479253768921Training Epoch: 1 | iteration: 246/262 | Loss: 0.7415623664855957Training Epoch: 1 | iteration: 247/262 | Loss: 0.7493284344673157Training Epoch: 1 | iteration: 248/262 | Loss: 0.7925549745559692Training Epoch: 1 | iteration: 249/262 | Loss: 0.7566680312156677Training Epoch: 1 | iteration: 250/262 | Loss: 0.7880990505218506Training Epoch: 1 | iteration: 251/262 | Loss: 0.758796215057373Training Epoch: 1 | iteration: 252/262 | Loss: 0.7775800824165344Training Epoch: 1 | iteration: 253/262 | Loss: 0.7755264639854431Training Epoch: 1 | iteration: 254/262 | Loss: 0.8602739572525024Training Epoch: 1 | iteration: 255/262 | Loss: 0.7728791832923889Training Epoch: 1 | iteration: 256/262 | Loss: 0.7108646631240845Training Epoch: 1 | iteration: 257/262 | Loss: 0.7034687399864197Training Epoch: 1 | iteration: 258/262 | Loss: 0.6948257684707642Training Epoch: 1 | iteration: 259/262 | Loss: 0.6743805408477783Training Epoch: 1 | iteration: 260/262 | Loss: 0.7333143353462219Training Epoch: 1 | iteration: 261/262 | Loss: 0.6944547891616821Validating Epoch: 1 | iteration: 0/66 | Loss: 0.7162816524505615Validating Epoch: 1 | iteration: 1/66 | Loss: 0.6663112640380859Validating Epoch: 1 | iteration: 2/66 | Loss: 0.645330548286438Validating Epoch: 1 | iteration: 3/66 | Loss: 0.6839590668678284Validating Epoch: 1 | iteration: 4/66 | Loss: 0.6569549441337585Validating Epoch: 1 | iteration: 5/66 | Loss: 0.6454491019248962Validating Epoch: 1 | iteration: 6/66 | Loss: 0.7073298692703247Validating Epoch: 1 | iteration: 7/66 | Loss: 0.7054356336593628Validating Epoch: 1 | iteration: 8/66 | Loss: 0.6218914985656738Validating Epoch: 1 | iteration: 9/66 | Loss: 0.6811190247535706Validating Epoch: 1 | iteration: 10/66 | Loss: 0.7030599117279053Validating Epoch: 1 | iteration: 11/66 | Loss: 0.6641479730606079Validating Epoch: 1 | iteration: 12/66 | Loss: 0.6085741519927979Validating Epoch: 1 | iteration: 13/66 | Loss: 0.6504592895507812Validating Epoch: 1 | iteration: 14/66 | Loss: 0.6874990463256836Validating Epoch: 1 | iteration: 15/66 | Loss: 0.5894393920898438Validating Epoch: 1 | iteration: 16/66 | Loss: 0.6878767013549805Validating Epoch: 1 | iteration: 17/66 | Loss: 0.6625159978866577Validating Epoch: 1 | iteration: 18/66 | Loss: 0.6266252398490906Validating Epoch: 1 | iteration: 19/66 | Loss: 0.6953862309455872Validating Epoch: 1 | iteration: 20/66 | Loss: 0.6378250122070312Validating Epoch: 1 | iteration: 21/66 | Loss: 0.688558042049408Validating Epoch: 1 | iteration: 22/66 | Loss: 0.7115008234977722Validating Epoch: 1 | iteration: 23/66 | Loss: 0.6079769730567932Validating Epoch: 1 | iteration: 24/66 | Loss: 0.6525121927261353Validating Epoch: 1 | iteration: 25/66 | Loss: 0.6631345748901367Validating Epoch: 1 | iteration: 26/66 | Loss: 0.6270677447319031Validating Epoch: 1 | iteration: 27/66 | Loss: 0.7052875757217407Validating Epoch: 1 | iteration: 28/66 | Loss: 0.6826364398002625Validating Epoch: 1 | iteration: 29/66 | Loss: 0.6298933029174805Validating Epoch: 1 | iteration: 30/66 | Loss: 0.6992047429084778Validating Epoch: 1 | iteration: 31/66 | Loss: 0.7077610492706299Validating Epoch: 1 | iteration: 32/66 | Loss: 0.7038484811782837Validating Epoch: 1 | iteration: 33/66 | Loss: 0.6293973326683044Validating Epoch: 1 | iteration: 34/66 | Loss: 0.5952678918838501Validating Epoch: 1 | iteration: 35/66 | Loss: 0.6848324537277222Validating Epoch: 1 | iteration: 36/66 | Loss: 0.7196962833404541Validating Epoch: 1 | iteration: 37/66 | Loss: 0.6886351108551025Validating Epoch: 1 | iteration: 38/66 | Loss: 0.6708217859268188Validating Epoch: 1 | iteration: 39/66 | Loss: 0.6246542930603027Validating Epoch: 1 | iteration: 40/66 | Loss: 0.6444191932678223Validating Epoch: 1 | iteration: 41/66 | Loss: 0.7473351359367371Validating Epoch: 1 | iteration: 42/66 | Loss: 0.6950588226318359Validating Epoch: 1 | iteration: 43/66 | Loss: 0.6591995358467102Validating Epoch: 1 | iteration: 44/66 | Loss: 0.6368615627288818Validating Epoch: 1 | iteration: 45/66 | Loss: 0.6940699815750122Validating Epoch: 1 | iteration: 46/66 | Loss: 0.6052013635635376Validating Epoch: 1 | iteration: 47/66 | Loss: 0.6988086700439453Validating Epoch: 1 | iteration: 48/66 | Loss: 0.7396833896636963Validating Epoch: 1 | iteration: 49/66 | Loss: 0.6987652778625488Validating Epoch: 1 | iteration: 50/66 | Loss: 0.5985299944877625Validating Epoch: 1 | iteration: 51/66 | Loss: 0.6887117028236389Validating Epoch: 1 | iteration: 52/66 | Loss: 0.6432431936264038Validating Epoch: 1 | iteration: 53/66 | Loss: 0.7301090955734253Validating Epoch: 1 | iteration: 54/66 | Loss: 0.6203321814537048Validating Epoch: 1 | iteration: 55/66 | Loss: 0.6872106790542603Validating Epoch: 1 | iteration: 56/66 | Loss: 0.7234032154083252Validating Epoch: 1 | iteration: 57/66 | Loss: 0.6616730093955994Validating Epoch: 1 | iteration: 58/66 | Loss: 0.7175099849700928Validating Epoch: 1 | iteration: 59/66 | Loss: 0.690178394317627Validating Epoch: 1 | iteration: 60/66 | Loss: 0.6809871196746826Validating Epoch: 1 | iteration: 61/66 | Loss: 0.6445571184158325Validating Epoch: 1 | iteration: 62/66 | Loss: 0.710297703742981Validating Epoch: 1 | iteration: 63/66 | Loss: 0.6612592935562134Validating Epoch: 1 | iteration: 64/66 | Loss: 0.6410968899726868Validating Epoch: 1 | iteration: 65/66 | Loss: 0.6458941102027893Temperature:  0.5
Sampling: iteration: 0/51{'Validity': 0.9921875, 'Novelty': 1.0, 'Uniqueness': 0.9970472440944882}
Training Epoch: 2 | iteration: 0/262 | Loss: 0.6685221195220947Training Epoch: 2 | iteration: 1/262 | Loss: 0.7556231617927551Training Epoch: 2 | iteration: 2/262 | Loss: 0.6794202923774719Training Epoch: 2 | iteration: 3/262 | Loss: 0.6994597315788269Training Epoch: 2 | iteration: 4/262 | Loss: 0.6629663705825806Training Epoch: 2 | iteration: 5/262 | Loss: 0.7261533737182617Training Epoch: 2 | iteration: 6/262 | Loss: 0.675198495388031Training Epoch: 2 | iteration: 7/262 | Loss: 0.6466470956802368Training Epoch: 2 | iteration: 8/262 | Loss: 0.667910099029541Training Epoch: 2 | iteration: 9/262 | Loss: 0.7236776351928711Training Epoch: 2 | iteration: 10/262 | Loss: 0.7024037837982178Training Epoch: 2 | iteration: 11/262 | Loss: 0.6488102674484253Training Epoch: 2 | iteration: 12/262 | Loss: 0.6683402061462402Training Epoch: 2 | iteration: 13/262 | Loss: 0.781374454498291Training Epoch: 2 | iteration: 14/262 | Loss: 0.7210816144943237Training Epoch: 2 | iteration: 15/262 | Loss: 0.7208008170127869Training Epoch: 2 | iteration: 16/262 | Loss: 0.6993503570556641Training Epoch: 2 | iteration: 17/262 | Loss: 0.6766454577445984Training Epoch: 2 | iteration: 18/262 | Loss: 0.7577658891677856Training Epoch: 2 | iteration: 19/262 | Loss: 0.6441774964332581Training Epoch: 2 | iteration: 20/262 | Loss: 0.7430498003959656Training Epoch: 2 | iteration: 21/262 | Loss: 0.6571537256240845Training Epoch: 2 | iteration: 22/262 | Loss: 0.689188539981842Training Epoch: 2 | iteration: 23/262 | Loss: 0.7646397352218628Training Epoch: 2 | iteration: 24/262 | Loss: 0.6886179447174072Training Epoch: 2 | iteration: 25/262 | Loss: 0.675043523311615Training Epoch: 2 | iteration: 26/262 | Loss: 0.713068425655365Training Epoch: 2 | iteration: 27/262 | Loss: 0.7389874458312988Training Epoch: 2 | iteration: 28/262 | Loss: 0.6733697652816772Training Epoch: 2 | iteration: 29/262 | Loss: 0.6403614282608032Training Epoch: 2 | iteration: 30/262 | Loss: 0.6337549090385437Training Epoch: 2 | iteration: 31/262 | Loss: 0.707112193107605Training Epoch: 2 | iteration: 32/262 | Loss: 0.684199333190918Training Epoch: 2 | iteration: 33/262 | Loss: 0.6535059213638306Training Epoch: 2 | iteration: 34/262 | Loss: 0.8121404051780701Training Epoch: 2 | iteration: 35/262 | Loss: 0.6755014657974243Training Epoch: 2 | iteration: 36/262 | Loss: 0.6630913019180298Training Epoch: 2 | iteration: 37/262 | Loss: 0.746476411819458Training Epoch: 2 | iteration: 38/262 | Loss: 0.6932410001754761Training Epoch: 2 | iteration: 39/262 | Loss: 0.6358643174171448Training Epoch: 2 | iteration: 40/262 | Loss: 0.7283848524093628Training Epoch: 2 | iteration: 41/262 | Loss: 0.7525167465209961Training Epoch: 2 | iteration: 42/262 | Loss: 0.6804022789001465Training Epoch: 2 | iteration: 43/262 | Loss: 0.7086536884307861Training Epoch: 2 | iteration: 44/262 | Loss: 0.6805574893951416Training Epoch: 2 | iteration: 45/262 | Loss: 0.7035343050956726Training Epoch: 2 | iteration: 46/262 | Loss: 0.7022547125816345Training Epoch: 2 | iteration: 47/262 | Loss: 0.7245914936065674Training Epoch: 2 | iteration: 48/262 | Loss: 0.7547332048416138Training Epoch: 2 | iteration: 49/262 | Loss: 0.7372666001319885Training Epoch: 2 | iteration: 50/262 | Loss: 0.6829789876937866Training Epoch: 2 | iteration: 51/262 | Loss: 0.760265588760376Training Epoch: 2 | iteration: 52/262 | Loss: 0.8108097314834595Training Epoch: 2 | iteration: 53/262 | Loss: 0.6679832935333252Training Epoch: 2 | iteration: 54/262 | Loss: 0.6971719264984131Training Epoch: 2 | iteration: 55/262 | Loss: 0.721563458442688Training Epoch: 2 | iteration: 56/262 | Loss: 0.7717779874801636Training Epoch: 2 | iteration: 57/262 | Loss: 0.7533465623855591Training Epoch: 2 | iteration: 58/262 | Loss: 0.6920315027236938Training Epoch: 2 | iteration: 59/262 | Loss: 0.7269659042358398Training Epoch: 2 | iteration: 60/262 | Loss: 0.6780216097831726Training Epoch: 2 | iteration: 61/262 | Loss: 0.7028783559799194Training Epoch: 2 | iteration: 62/262 | Loss: 0.6958822011947632Training Epoch: 2 | iteration: 63/262 | Loss: 0.7614784240722656Training Epoch: 2 | iteration: 64/262 | Loss: 0.654869556427002Training Epoch: 2 | iteration: 65/262 | Loss: 0.6900546550750732Training Epoch: 2 | iteration: 66/262 | Loss: 0.6716813445091248Training Epoch: 2 | iteration: 67/262 | Loss: 0.7021677494049072Training Epoch: 2 | iteration: 68/262 | Loss: 0.7183424830436707Training Epoch: 2 | iteration: 69/262 | Loss: 0.7704297304153442Training Epoch: 2 | iteration: 70/262 | Loss: 0.7331362962722778Training Epoch: 2 | iteration: 71/262 | Loss: 0.6525677442550659Training Epoch: 2 | iteration: 72/262 | Loss: 0.7261002063751221Training Epoch: 2 | iteration: 73/262 | Loss: 0.6949924826622009Training Epoch: 2 | iteration: 74/262 | Loss: 0.725608766078949Training Epoch: 2 | iteration: 75/262 | Loss: 0.7368913888931274Training Epoch: 2 | iteration: 76/262 | Loss: 0.6477788686752319Training Epoch: 2 | iteration: 77/262 | Loss: 0.6943609714508057Training Epoch: 2 | iteration: 78/262 | Loss: 0.7378479242324829Training Epoch: 2 | iteration: 79/262 | Loss: 0.7608120441436768Training Epoch: 2 | iteration: 80/262 | Loss: 0.6802939176559448Training Epoch: 2 | iteration: 81/262 | Loss: 0.7106966972351074Training Epoch: 2 | iteration: 82/262 | Loss: 0.7590491771697998Training Epoch: 2 | iteration: 83/262 | Loss: 0.7070705890655518Training Epoch: 2 | iteration: 84/262 | Loss: 0.8341264128684998Training Epoch: 2 | iteration: 85/262 | Loss: 0.6610077023506165Training Epoch: 2 | iteration: 86/262 | Loss: 0.8169771432876587Training Epoch: 2 | iteration: 87/262 | Loss: 0.7066185474395752Training Epoch: 2 | iteration: 88/262 | Loss: 0.6747546195983887Training Epoch: 2 | iteration: 89/262 | Loss: 0.753825843334198Training Epoch: 2 | iteration: 90/262 | Loss: 0.683793842792511Training Epoch: 2 | iteration: 91/262 | Loss: 0.7135019302368164Training Epoch: 2 | iteration: 92/262 | Loss: 0.7800408601760864Training Epoch: 2 | iteration: 93/262 | Loss: 0.6858333945274353Training Epoch: 2 | iteration: 94/262 | Loss: 0.6328142285346985Training Epoch: 2 | iteration: 95/262 | Loss: 0.7157171368598938Training Epoch: 2 | iteration: 96/262 | Loss: 0.7618598937988281Training Epoch: 2 | iteration: 97/262 | Loss: 0.7492260932922363Training Epoch: 2 | iteration: 98/262 | Loss: 0.7207965850830078Training Epoch: 2 | iteration: 99/262 | Loss: 0.6396158337593079Training Epoch: 2 | iteration: 100/262 | Loss: 0.7617672085762024Training Epoch: 2 | iteration: 101/262 | Loss: 0.7460174560546875Training Epoch: 2 | iteration: 102/262 | Loss: 0.7645934820175171Training Epoch: 2 | iteration: 103/262 | Loss: 0.7438405752182007Training Epoch: 2 | iteration: 104/262 | Loss: 0.7353809475898743Training Epoch: 2 | iteration: 105/262 | Loss: 0.7523568868637085Training Epoch: 2 | iteration: 106/262 | Loss: 0.7151079177856445Training Epoch: 2 | iteration: 107/262 | Loss: 0.642573356628418Training Epoch: 2 | iteration: 108/262 | Loss: 0.6527453660964966Training Epoch: 2 | iteration: 109/262 | Loss: 0.6044657826423645Training Epoch: 2 | iteration: 110/262 | Loss: 0.671924889087677Training Epoch: 2 | iteration: 111/262 | Loss: 0.7228087186813354Training Epoch: 2 | iteration: 112/262 | Loss: 0.7392392754554749Training Epoch: 2 | iteration: 113/262 | Loss: 0.6992461681365967Training Epoch: 2 | iteration: 114/262 | Loss: 0.6761308312416077Training Epoch: 2 | iteration: 115/262 | Loss: 0.7866935729980469Training Epoch: 2 | iteration: 116/262 | Loss: 0.6678503155708313Training Epoch: 2 | iteration: 117/262 | Loss: 0.6945323944091797Training Epoch: 2 | iteration: 118/262 | Loss: 0.6777324676513672Training Epoch: 2 | iteration: 119/262 | Loss: 0.7084254026412964Training Epoch: 2 | iteration: 120/262 | Loss: 0.6693143844604492Training Epoch: 2 | iteration: 121/262 | Loss: 0.6801924705505371Training Epoch: 2 | iteration: 122/262 | Loss: 0.7667899131774902Training Epoch: 2 | iteration: 123/262 | Loss: 0.7198621034622192Training Epoch: 2 | iteration: 124/262 | Loss: 0.7282371520996094Training Epoch: 2 | iteration: 125/262 | Loss: 0.6566914916038513Training Epoch: 2 | iteration: 126/262 | Loss: 0.7430534362792969Training Epoch: 2 | iteration: 127/262 | Loss: 0.70918869972229Training Epoch: 2 | iteration: 128/262 | Loss: 0.6788198351860046Training Epoch: 2 | iteration: 129/262 | Loss: 0.6619017720222473Training Epoch: 2 | iteration: 130/262 | Loss: 0.6940100193023682Training Epoch: 2 | iteration: 131/262 | Loss: 0.6611995697021484Training Epoch: 2 | iteration: 132/262 | Loss: 0.6269569993019104Training Epoch: 2 | iteration: 133/262 | Loss: 0.715366780757904Training Epoch: 2 | iteration: 134/262 | Loss: 0.7613587975502014Training Epoch: 2 | iteration: 135/262 | Loss: 0.6852976083755493Training Epoch: 2 | iteration: 136/262 | Loss: 0.7210674285888672Training Epoch: 2 | iteration: 137/262 | Loss: 0.7797281742095947Training Epoch: 2 | iteration: 138/262 | Loss: 0.6945781111717224Training Epoch: 2 | iteration: 139/262 | Loss: 0.6926376223564148Training Epoch: 2 | iteration: 140/262 | Loss: 0.6622545719146729Training Epoch: 2 | iteration: 141/262 | Loss: 0.6892567873001099Training Epoch: 2 | iteration: 142/262 | Loss: 0.8022176623344421Training Epoch: 2 | iteration: 143/262 | Loss: 0.7625005841255188Training Epoch: 2 | iteration: 144/262 | Loss: 0.627297580242157Training Epoch: 2 | iteration: 145/262 | Loss: 0.6604283452033997Training Epoch: 2 | iteration: 146/262 | Loss: 0.8212753534317017Training Epoch: 2 | iteration: 147/262 | Loss: 0.7897945642471313Training Epoch: 2 | iteration: 148/262 | Loss: 0.7011600732803345Training Epoch: 2 | iteration: 149/262 | Loss: 0.7701311707496643Training Epoch: 2 | iteration: 150/262 | Loss: 0.6422455906867981Training Epoch: 2 | iteration: 151/262 | Loss: 0.6647998690605164Training Epoch: 2 | iteration: 152/262 | Loss: 0.736042857170105Training Epoch: 2 | iteration: 153/262 | Loss: 0.6911666393280029Training Epoch: 2 | iteration: 154/262 | Loss: 0.7100826501846313Training Epoch: 2 | iteration: 155/262 | Loss: 0.75757896900177Training Epoch: 2 | iteration: 156/262 | Loss: 0.6694700121879578Training Epoch: 2 | iteration: 157/262 | Loss: 0.62703537940979Training Epoch: 2 | iteration: 158/262 | Loss: 0.6984028220176697Training Epoch: 2 | iteration: 159/262 | Loss: 0.716149091720581Training Epoch: 2 | iteration: 160/262 | Loss: 0.7472037672996521Training Epoch: 2 | iteration: 161/262 | Loss: 0.7597039937973022Training Epoch: 2 | iteration: 162/262 | Loss: 0.7710909843444824Training Epoch: 2 | iteration: 163/262 | Loss: 0.703041672706604Training Epoch: 2 | iteration: 164/262 | Loss: 0.7426366209983826Training Epoch: 2 | iteration: 165/262 | Loss: 0.7027158737182617Training Epoch: 2 | iteration: 166/262 | Loss: 0.7951995134353638Training Epoch: 2 | iteration: 167/262 | Loss: 0.6939386129379272Training Epoch: 2 | iteration: 168/262 | Loss: 0.6561648845672607Training Epoch: 2 | iteration: 169/262 | Loss: 0.6918966770172119Training Epoch: 2 | iteration: 170/262 | Loss: 0.7722813487052917Training Epoch: 2 | iteration: 171/262 | Loss: 0.6829380393028259Training Epoch: 2 | iteration: 172/262 | Loss: 0.7273577451705933Training Epoch: 2 | iteration: 173/262 | Loss: 0.6674604415893555Training Epoch: 2 | iteration: 174/262 | Loss: 0.702053427696228Training Epoch: 2 | iteration: 175/262 | Loss: 0.6486090421676636Training Epoch: 2 | iteration: 176/262 | Loss: 0.731148898601532Training Epoch: 2 | iteration: 177/262 | Loss: 0.7381463050842285Training Epoch: 2 | iteration: 178/262 | Loss: 0.8103389739990234Training Epoch: 2 | iteration: 179/262 | Loss: 0.692999005317688Training Epoch: 2 | iteration: 180/262 | Loss: 0.8043758869171143Training Epoch: 2 | iteration: 181/262 | Loss: 0.7379988431930542Training Epoch: 2 | iteration: 182/262 | Loss: 0.707764208316803Training Epoch: 2 | iteration: 183/262 | Loss: 0.7521202564239502Training Epoch: 2 | iteration: 184/262 | Loss: 0.7584750652313232Training Epoch: 2 | iteration: 185/262 | Loss: 0.7457751035690308Training Epoch: 2 | iteration: 186/262 | Loss: 0.7346150875091553Training Epoch: 2 | iteration: 187/262 | Loss: 0.70932936668396Training Epoch: 2 | iteration: 188/262 | Loss: 0.6929419636726379Training Epoch: 2 | iteration: 189/262 | Loss: 0.7415872812271118Training Epoch: 2 | iteration: 190/262 | Loss: 0.7764909267425537Training Epoch: 2 | iteration: 191/262 | Loss: 0.7788777947425842Training Epoch: 2 | iteration: 192/262 | Loss: 0.614289402961731Training Epoch: 2 | iteration: 193/262 | Loss: 0.7200426459312439Training Epoch: 2 | iteration: 194/262 | Loss: 0.7245851159095764Training Epoch: 2 | iteration: 195/262 | Loss: 0.7549875378608704Training Epoch: 2 | iteration: 196/262 | Loss: 0.7685965299606323Training Epoch: 2 | iteration: 197/262 | Loss: 0.7282262444496155Training Epoch: 2 | iteration: 198/262 | Loss: 0.6907863020896912Training Epoch: 2 | iteration: 199/262 | Loss: 0.8025808930397034Training Epoch: 2 | iteration: 200/262 | Loss: 0.7078965902328491Training Epoch: 2 | iteration: 201/262 | Loss: 0.7085170745849609Training Epoch: 2 | iteration: 202/262 | Loss: 0.8166361451148987Training Epoch: 2 | iteration: 203/262 | Loss: 0.6916835308074951Training Epoch: 2 | iteration: 204/262 | Loss: 0.701888382434845Training Epoch: 2 | iteration: 205/262 | Loss: 0.6634474992752075Training Epoch: 2 | iteration: 206/262 | Loss: 0.7338592410087585Training Epoch: 2 | iteration: 207/262 | Loss: 0.7320287823677063Training Epoch: 2 | iteration: 208/262 | Loss: 0.7433659434318542Training Epoch: 2 | iteration: 209/262 | Loss: 0.6651304960250854Training Epoch: 2 | iteration: 210/262 | Loss: 0.6390735507011414Training Epoch: 2 | iteration: 211/262 | Loss: 0.7416465282440186Training Epoch: 2 | iteration: 212/262 | Loss: 0.6439050436019897Training Epoch: 2 | iteration: 213/262 | Loss: 0.6896536350250244Training Epoch: 2 | iteration: 214/262 | Loss: 0.6709654927253723Training Epoch: 2 | iteration: 215/262 | Loss: 0.7909373044967651Training Epoch: 2 | iteration: 216/262 | Loss: 0.7277746200561523Training Epoch: 2 | iteration: 217/262 | Loss: 0.6926060318946838Training Epoch: 2 | iteration: 218/262 | Loss: 0.6927454471588135Training Epoch: 2 | iteration: 219/262 | Loss: 0.6674307584762573Training Epoch: 2 | iteration: 220/262 | Loss: 0.7448884844779968Training Epoch: 2 | iteration: 221/262 | Loss: 0.6841729283332825Training Epoch: 2 | iteration: 222/262 | Loss: 0.716677188873291Training Epoch: 2 | iteration: 223/262 | Loss: 0.6924962997436523Training Epoch: 2 | iteration: 224/262 | Loss: 0.6834073066711426Training Epoch: 2 | iteration: 225/262 | Loss: 0.7596126198768616Training Epoch: 2 | iteration: 226/262 | Loss: 0.693787157535553Training Epoch: 2 | iteration: 227/262 | Loss: 0.7311102151870728Training Epoch: 2 | iteration: 228/262 | Loss: 0.7615534067153931Training Epoch: 2 | iteration: 229/262 | Loss: 0.7039848566055298Training Epoch: 2 | iteration: 230/262 | Loss: 0.6830781698226929Training Epoch: 2 | iteration: 231/262 | Loss: 0.6755463480949402Training Epoch: 2 | iteration: 232/262 | Loss: 0.7093537449836731Training Epoch: 2 | iteration: 233/262 | Loss: 0.6439671516418457Training Epoch: 2 | iteration: 234/262 | Loss: 0.69691002368927Training Epoch: 2 | iteration: 235/262 | Loss: 0.7698683738708496Training Epoch: 2 | iteration: 236/262 | Loss: 0.6396477222442627Training Epoch: 2 | iteration: 237/262 | Loss: 0.6749359369277954Training Epoch: 2 | iteration: 238/262 | Loss: 0.6781917810440063Training Epoch: 2 | iteration: 239/262 | Loss: 0.731789231300354Training Epoch: 2 | iteration: 240/262 | Loss: 0.6587134599685669Training Epoch: 2 | iteration: 241/262 | Loss: 0.7039293050765991Training Epoch: 2 | iteration: 242/262 | Loss: 0.6703461408615112Training Epoch: 2 | iteration: 243/262 | Loss: 0.7061882019042969Training Epoch: 2 | iteration: 244/262 | Loss: 0.7295611500740051Training Epoch: 2 | iteration: 245/262 | Loss: 0.7748094797134399Training Epoch: 2 | iteration: 246/262 | Loss: 0.6933887004852295Training Epoch: 2 | iteration: 247/262 | Loss: 0.7848353385925293Training Epoch: 2 | iteration: 248/262 | Loss: 0.6687335968017578Training Epoch: 2 | iteration: 249/262 | Loss: 0.6955163478851318Training Epoch: 2 | iteration: 250/262 | Loss: 0.6751608848571777Training Epoch: 2 | iteration: 251/262 | Loss: 0.7292826175689697Training Epoch: 2 | iteration: 252/262 | Loss: 0.7644256949424744Training Epoch: 2 | iteration: 253/262 | Loss: 0.7331236600875854Training Epoch: 2 | iteration: 254/262 | Loss: 0.7196298837661743Training Epoch: 2 | iteration: 255/262 | Loss: 0.7331854104995728Training Epoch: 2 | iteration: 256/262 | Loss: 0.8044850826263428Training Epoch: 2 | iteration: 257/262 | Loss: 0.744745135307312Training Epoch: 2 | iteration: 258/262 | Loss: 0.7183200120925903Training Epoch: 2 | iteration: 259/262 | Loss: 0.7448348999023438Training Epoch: 2 | iteration: 260/262 | Loss: 0.7509284019470215Training Epoch: 2 | iteration: 261/262 | Loss: 0.6859387159347534Validating Epoch: 2 | iteration: 0/66 | Loss: 0.6717746257781982Validating Epoch: 2 | iteration: 1/66 | Loss: 0.7048531770706177Validating Epoch: 2 | iteration: 2/66 | Loss: 0.680648684501648Validating Epoch: 2 | iteration: 3/66 | Loss: 0.661302924156189Validating Epoch: 2 | iteration: 4/66 | Loss: 0.7146809101104736Validating Epoch: 2 | iteration: 5/66 | Loss: 0.6774011850357056Validating Epoch: 2 | iteration: 6/66 | Loss: 0.6791160106658936Validating Epoch: 2 | iteration: 7/66 | Loss: 0.6759436726570129Validating Epoch: 2 | iteration: 8/66 | Loss: 0.6994791030883789Validating Epoch: 2 | iteration: 9/66 | Loss: 0.6380603313446045Validating Epoch: 2 | iteration: 10/66 | Loss: 0.7404767274856567Validating Epoch: 2 | iteration: 11/66 | Loss: 0.7025254368782043Validating Epoch: 2 | iteration: 12/66 | Loss: 0.668174147605896Validating Epoch: 2 | iteration: 13/66 | Loss: 0.6110692024230957Validating Epoch: 2 | iteration: 14/66 | Loss: 0.6749638319015503Validating Epoch: 2 | iteration: 15/66 | Loss: 0.6221754550933838Validating Epoch: 2 | iteration: 16/66 | Loss: 0.673602283000946Validating Epoch: 2 | iteration: 17/66 | Loss: 0.6386783719062805Validating Epoch: 2 | iteration: 18/66 | Loss: 0.6600390672683716Validating Epoch: 2 | iteration: 19/66 | Loss: 0.6853449940681458Validating Epoch: 2 | iteration: 20/66 | Loss: 0.7044267654418945Validating Epoch: 2 | iteration: 21/66 | Loss: 0.7024168968200684Validating Epoch: 2 | iteration: 22/66 | Loss: 0.7010356187820435Validating Epoch: 2 | iteration: 23/66 | Loss: 0.6858469247817993Validating Epoch: 2 | iteration: 24/66 | Loss: 0.66102135181427Validating Epoch: 2 | iteration: 25/66 | Loss: 0.6349267959594727Validating Epoch: 2 | iteration: 26/66 | Loss: 0.6605465412139893Validating Epoch: 2 | iteration: 27/66 | Loss: 0.7800835371017456Validating Epoch: 2 | iteration: 28/66 | Loss: 0.6713322401046753Validating Epoch: 2 | iteration: 29/66 | Loss: 0.6888603568077087Validating Epoch: 2 | iteration: 30/66 | Loss: 0.6791353225708008Validating Epoch: 2 | iteration: 31/66 | Loss: 0.7118028998374939Validating Epoch: 2 | iteration: 32/66 | Loss: 0.6575749516487122Validating Epoch: 2 | iteration: 33/66 | Loss: 0.6555603742599487Validating Epoch: 2 | iteration: 34/66 | Loss: 0.68963623046875Validating Epoch: 2 | iteration: 35/66 | Loss: 0.6006220579147339Validating Epoch: 2 | iteration: 36/66 | Loss: 0.7047045230865479Validating Epoch: 2 | iteration: 37/66 | Loss: 0.6693703532218933Validating Epoch: 2 | iteration: 38/66 | Loss: 0.6624222993850708Validating Epoch: 2 | iteration: 39/66 | Loss: 0.6437811255455017Validating Epoch: 2 | iteration: 40/66 | Loss: 0.6570307612419128Validating Epoch: 2 | iteration: 41/66 | Loss: 0.6770280003547668Validating Epoch: 2 | iteration: 42/66 | Loss: 0.6944930553436279Validating Epoch: 2 | iteration: 43/66 | Loss: 0.6049926280975342Validating Epoch: 2 | iteration: 44/66 | Loss: 0.6899594068527222Validating Epoch: 2 | iteration: 45/66 | Loss: 0.670907199382782Validating Epoch: 2 | iteration: 46/66 | Loss: 0.6770996451377869Validating Epoch: 2 | iteration: 47/66 | Loss: 0.6849958896636963Validating Epoch: 2 | iteration: 48/66 | Loss: 0.660710334777832Validating Epoch: 2 | iteration: 49/66 | Loss: 0.6638401746749878Validating Epoch: 2 | iteration: 50/66 | Loss: 0.5881049036979675Validating Epoch: 2 | iteration: 51/66 | Loss: 0.6452869176864624Validating Epoch: 2 | iteration: 52/66 | Loss: 0.7009304761886597Validating Epoch: 2 | iteration: 53/66 | Loss: 0.7121398448944092Validating Epoch: 2 | iteration: 54/66 | Loss: 0.6442372798919678Validating Epoch: 2 | iteration: 55/66 | Loss: 0.6461165547370911Validating Epoch: 2 | iteration: 56/66 | Loss: 0.7136982083320618Validating Epoch: 2 | iteration: 57/66 | Loss: 0.6584261655807495Validating Epoch: 2 | iteration: 58/66 | Loss: 0.6621294021606445Validating Epoch: 2 | iteration: 59/66 | Loss: 0.6389474868774414Validating Epoch: 2 | iteration: 60/66 | Loss: 0.6880557537078857Validating Epoch: 2 | iteration: 61/66 | Loss: 0.7140253186225891Validating Epoch: 2 | iteration: 62/66 | Loss: 0.6254141330718994Validating Epoch: 2 | iteration: 63/66 | Loss: 0.6068382263183594Validating Epoch: 2 | iteration: 64/66 | Loss: 0.600148618221283Validating Epoch: 2 | iteration: 65/66 | Loss: 0.7104491591453552Temperature:  0.5
Sampling: iteration: 0/51{'Validity': 0.9931640625, 'Novelty': 1.0, 'Uniqueness': 0.9970501474926253}
Training Epoch: 3 | iteration: 0/262 | Loss: 0.6532171964645386Training Epoch: 3 | iteration: 1/262 | Loss: 0.7393921613693237Training Epoch: 3 | iteration: 2/262 | Loss: 0.654593825340271Training Epoch: 3 | iteration: 3/262 | Loss: 0.6841548085212708Training Epoch: 3 | iteration: 4/262 | Loss: 0.6805856227874756Training Epoch: 3 | iteration: 5/262 | Loss: 0.6482795476913452Training Epoch: 3 | iteration: 6/262 | Loss: 0.7253042459487915Training Epoch: 3 | iteration: 7/262 | Loss: 0.7336660623550415Training Epoch: 3 | iteration: 8/262 | Loss: 0.7279003858566284Training Epoch: 3 | iteration: 9/262 | Loss: 0.7222467660903931Training Epoch: 3 | iteration: 10/262 | Loss: 0.6325780153274536Training Epoch: 3 | iteration: 11/262 | Loss: 0.7092160582542419Training Epoch: 3 | iteration: 12/262 | Loss: 0.6664539575576782Training Epoch: 3 | iteration: 13/262 | Loss: 0.6599493026733398Training Epoch: 3 | iteration: 14/262 | Loss: 0.6575040817260742Training Epoch: 3 | iteration: 15/262 | Loss: 0.6452511548995972Training Epoch: 3 | iteration: 16/262 | Loss: 0.6976326704025269Training Epoch: 3 | iteration: 17/262 | Loss: 0.703460693359375Training Epoch: 3 | iteration: 18/262 | Loss: 0.6620452404022217Training Epoch: 3 | iteration: 19/262 | Loss: 0.7219616174697876Training Epoch: 3 | iteration: 20/262 | Loss: 0.6861832737922668Training Epoch: 3 | iteration: 21/262 | Loss: 0.6748117208480835Training Epoch: 3 | iteration: 22/262 | Loss: 0.6947662830352783Training Epoch: 3 | iteration: 23/262 | Loss: 0.6624146699905396Training Epoch: 3 | iteration: 24/262 | Loss: 0.6809223890304565Training Epoch: 3 | iteration: 25/262 | Loss: 0.6792857050895691Training Epoch: 3 | iteration: 26/262 | Loss: 0.7024456858634949Training Epoch: 3 | iteration: 27/262 | Loss: 0.6959686279296875Training Epoch: 3 | iteration: 28/262 | Loss: 0.6889468431472778Training Epoch: 3 | iteration: 29/262 | Loss: 0.6482431888580322Training Epoch: 3 | iteration: 30/262 | Loss: 0.6868235468864441Training Epoch: 3 | iteration: 31/262 | Loss: 0.7281699180603027Training Epoch: 3 | iteration: 32/262 | Loss: 0.639212965965271Training Epoch: 3 | iteration: 33/262 | Loss: 0.6775192022323608Training Epoch: 3 | iteration: 34/262 | Loss: 0.6716009974479675Training Epoch: 3 | iteration: 35/262 | Loss: 0.5914070010185242Training Epoch: 3 | iteration: 36/262 | Loss: 0.6932615637779236Training Epoch: 3 | iteration: 37/262 | Loss: 0.6806471943855286Training Epoch: 3 | iteration: 38/262 | Loss: 0.6599844694137573Training Epoch: 3 | iteration: 39/262 | Loss: 0.6846444010734558Training Epoch: 3 | iteration: 40/262 | Loss: 0.678882896900177Training Epoch: 3 | iteration: 41/262 | Loss: 0.6719592809677124Training Epoch: 3 | iteration: 42/262 | Loss: 0.6580995321273804Training Epoch: 3 | iteration: 43/262 | Loss: 0.634300708770752Training Epoch: 3 | iteration: 44/262 | Loss: 0.6181538105010986Training Epoch: 3 | iteration: 45/262 | Loss: 0.6512731313705444Training Epoch: 3 | iteration: 46/262 | Loss: 0.6243621110916138Training Epoch: 3 | iteration: 47/262 | Loss: 0.8005309104919434Training Epoch: 3 | iteration: 48/262 | Loss: 0.641975998878479Training Epoch: 3 | iteration: 49/262 | Loss: 0.6098513603210449Training Epoch: 3 | iteration: 50/262 | Loss: 0.6098394393920898Training Epoch: 3 | iteration: 51/262 | Loss: 0.6736994981765747Training Epoch: 3 | iteration: 52/262 | Loss: 0.7473787069320679Training Epoch: 3 | iteration: 53/262 | Loss: 0.7386084794998169Training Epoch: 3 | iteration: 54/262 | Loss: 0.7114628553390503Training Epoch: 3 | iteration: 55/262 | Loss: 0.699195921421051Training Epoch: 3 | iteration: 56/262 | Loss: 0.7754043340682983Training Epoch: 3 | iteration: 57/262 | Loss: 0.6678349375724792Training Epoch: 3 | iteration: 58/262 | Loss: 0.6388710737228394Training Epoch: 3 | iteration: 59/262 | Loss: 0.6893361806869507Training Epoch: 3 | iteration: 60/262 | Loss: 0.6639865636825562Training Epoch: 3 | iteration: 61/262 | Loss: 0.6554931402206421Training Epoch: 3 | iteration: 62/262 | Loss: 0.6542066335678101Training Epoch: 3 | iteration: 63/262 | Loss: 0.609830915927887Training Epoch: 3 | iteration: 64/262 | Loss: 0.682878315448761Training Epoch: 3 | iteration: 65/262 | Loss: 0.6795074939727783Training Epoch: 3 | iteration: 66/262 | Loss: 0.6872847080230713Training Epoch: 3 | iteration: 67/262 | Loss: 0.6397028565406799Training Epoch: 3 | iteration: 68/262 | Loss: 0.6499490737915039Training Epoch: 3 | iteration: 69/262 | Loss: 0.6990460753440857Training Epoch: 3 | iteration: 70/262 | Loss: 0.6223986744880676Training Epoch: 3 | iteration: 71/262 | Loss: 0.6848856210708618Training Epoch: 3 | iteration: 72/262 | Loss: 0.7074194550514221Training Epoch: 3 | iteration: 73/262 | Loss: 0.6743154525756836Training Epoch: 3 | iteration: 74/262 | Loss: 0.7132892608642578Training Epoch: 3 | iteration: 75/262 | Loss: 0.6623507142066956Training Epoch: 3 | iteration: 76/262 | Loss: 0.6038380861282349Training Epoch: 3 | iteration: 77/262 | Loss: 0.6453279256820679Training Epoch: 3 | iteration: 78/262 | Loss: 0.6842048168182373Training Epoch: 3 | iteration: 79/262 | Loss: 0.6773288249969482Training Epoch: 3 | iteration: 80/262 | Loss: 0.7215248346328735Training Epoch: 3 | iteration: 81/262 | Loss: 0.7295745611190796Training Epoch: 3 | iteration: 82/262 | Loss: 0.7124716639518738Training Epoch: 3 | iteration: 83/262 | Loss: 0.6571400165557861Training Epoch: 3 | iteration: 84/262 | Loss: 0.6891409158706665Training Epoch: 3 | iteration: 85/262 | Loss: 0.6740894317626953Training Epoch: 3 | iteration: 86/262 | Loss: 0.6503597497940063Training Epoch: 3 | iteration: 87/262 | Loss: 0.6737279891967773Training Epoch: 3 | iteration: 88/262 | Loss: 0.663013219833374Training Epoch: 3 | iteration: 89/262 | Loss: 0.5792572498321533Training Epoch: 3 | iteration: 90/262 | Loss: 0.6087709665298462Training Epoch: 3 | iteration: 91/262 | Loss: 0.7012357711791992Training Epoch: 3 | iteration: 92/262 | Loss: 0.6681421995162964Training Epoch: 3 | iteration: 93/262 | Loss: 0.6243903636932373Training Epoch: 3 | iteration: 94/262 | Loss: 0.7411243915557861Training Epoch: 3 | iteration: 95/262 | Loss: 0.6841161251068115Training Epoch: 3 | iteration: 96/262 | Loss: 0.6533563137054443Training Epoch: 3 | iteration: 97/262 | Loss: 0.6249513030052185Training Epoch: 3 | iteration: 98/262 | Loss: 0.6299745440483093Training Epoch: 3 | iteration: 99/262 | Loss: 0.6689855456352234Training Epoch: 3 | iteration: 100/262 | Loss: 0.6774417161941528Training Epoch: 3 | iteration: 101/262 | Loss: 0.7278132438659668Training Epoch: 3 | iteration: 102/262 | Loss: 0.6408730149269104Training Epoch: 3 | iteration: 103/262 | Loss: 0.7452816963195801Training Epoch: 3 | iteration: 104/262 | Loss: 0.6634018421173096Training Epoch: 3 | iteration: 105/262 | Loss: 0.685485303401947Training Epoch: 3 | iteration: 106/262 | Loss: 0.657325804233551Training Epoch: 3 | iteration: 107/262 | Loss: 0.6945708394050598Training Epoch: 3 | iteration: 108/262 | Loss: 0.7364063262939453Training Epoch: 3 | iteration: 109/262 | Loss: 0.6924464702606201Training Epoch: 3 | iteration: 110/262 | Loss: 0.7149150371551514Training Epoch: 3 | iteration: 111/262 | Loss: 0.6760953664779663Training Epoch: 3 | iteration: 112/262 | Loss: 0.65528404712677Training Epoch: 3 | iteration: 113/262 | Loss: 0.6528171300888062Training Epoch: 3 | iteration: 114/262 | Loss: 0.729039192199707Training Epoch: 3 | iteration: 115/262 | Loss: 0.6615420579910278Training Epoch: 3 | iteration: 116/262 | Loss: 0.6753185987472534Training Epoch: 3 | iteration: 117/262 | Loss: 0.6820862293243408Training Epoch: 3 | iteration: 118/262 | Loss: 0.7865502238273621Training Epoch: 3 | iteration: 119/262 | Loss: 0.6998533010482788Training Epoch: 3 | iteration: 120/262 | Loss: 0.669521152973175Training Epoch: 3 | iteration: 121/262 | Loss: 0.6490610837936401Training Epoch: 3 | iteration: 122/262 | Loss: 0.6591943502426147Training Epoch: 3 | iteration: 123/262 | Loss: 0.6961727738380432Training Epoch: 3 | iteration: 124/262 | Loss: 0.6319619417190552Training Epoch: 3 | iteration: 125/262 | Loss: 0.6848848462104797Training Epoch: 3 | iteration: 126/262 | Loss: 0.737709641456604Training Epoch: 3 | iteration: 127/262 | Loss: 0.6301405429840088Training Epoch: 3 | iteration: 128/262 | Loss: 0.683626651763916Training Epoch: 3 | iteration: 129/262 | Loss: 0.6806210279464722Training Epoch: 3 | iteration: 130/262 | Loss: 0.6670944094657898Training Epoch: 3 | iteration: 131/262 | Loss: 0.6799862384796143Training Epoch: 3 | iteration: 132/262 | Loss: 0.7349172830581665Training Epoch: 3 | iteration: 133/262 | Loss: 0.6763498783111572Training Epoch: 3 | iteration: 134/262 | Loss: 0.6591201424598694Training Epoch: 3 | iteration: 135/262 | Loss: 0.7075924873352051Training Epoch: 3 | iteration: 136/262 | Loss: 0.6296385526657104Training Epoch: 3 | iteration: 137/262 | Loss: 0.6883041858673096Training Epoch: 3 | iteration: 138/262 | Loss: 0.7056411504745483Training Epoch: 3 | iteration: 139/262 | Loss: 0.647371768951416Training Epoch: 3 | iteration: 140/262 | Loss: 0.7160886526107788Training Epoch: 3 | iteration: 141/262 | Loss: 0.701169490814209Training Epoch: 3 | iteration: 142/262 | Loss: 0.6992067098617554Training Epoch: 3 | iteration: 143/262 | Loss: 0.6697307825088501Training Epoch: 3 | iteration: 144/262 | Loss: 0.6067233085632324Training Epoch: 3 | iteration: 145/262 | Loss: 0.6846756935119629Training Epoch: 3 | iteration: 146/262 | Loss: 0.6687742471694946Training Epoch: 3 | iteration: 147/262 | Loss: 0.6541206240653992Training Epoch: 3 | iteration: 148/262 | Loss: 0.7401179075241089Training Epoch: 3 | iteration: 149/262 | Loss: 0.6688086986541748Training Epoch: 3 | iteration: 150/262 | Loss: 0.6920219659805298Training Epoch: 3 | iteration: 151/262 | Loss: 0.7151681184768677Training Epoch: 3 | iteration: 152/262 | Loss: 0.6704302430152893Training Epoch: 3 | iteration: 153/262 | Loss: 0.7064865231513977Training Epoch: 3 | iteration: 154/262 | Loss: 0.6587074995040894Training Epoch: 3 | iteration: 155/262 | Loss: 0.6421046257019043Training Epoch: 3 | iteration: 156/262 | Loss: 0.6241726875305176Training Epoch: 3 | iteration: 157/262 | Loss: 0.707114577293396Training Epoch: 3 | iteration: 158/262 | Loss: 0.6934957504272461Training Epoch: 3 | iteration: 159/262 | Loss: 0.6397789120674133Training Epoch: 3 | iteration: 160/262 | Loss: 0.7056760787963867Training Epoch: 3 | iteration: 161/262 | Loss: 0.6522842645645142Training Epoch: 3 | iteration: 162/262 | Loss: 0.6365599632263184Training Epoch: 3 | iteration: 163/262 | Loss: 0.6415008902549744Training Epoch: 3 | iteration: 164/262 | Loss: 0.640798807144165Training Epoch: 3 | iteration: 165/262 | Loss: 0.6901076436042786Training Epoch: 3 | iteration: 166/262 | Loss: 0.7350747585296631Training Epoch: 3 | iteration: 167/262 | Loss: 0.6481151580810547Training Epoch: 3 | iteration: 168/262 | Loss: 0.6745615005493164Training Epoch: 3 | iteration: 169/262 | Loss: 0.7457702159881592Training Epoch: 3 | iteration: 170/262 | Loss: 0.7199301719665527Training Epoch: 3 | iteration: 171/262 | Loss: 0.6735042929649353Training Epoch: 3 | iteration: 172/262 | Loss: 0.6749168038368225Training Epoch: 3 | iteration: 173/262 | Loss: 0.6515418291091919Training Epoch: 3 | iteration: 174/262 | Loss: 0.6811174154281616Training Epoch: 3 | iteration: 175/262 | Loss: 0.6932166814804077Training Epoch: 3 | iteration: 176/262 | Loss: 0.7143621444702148Training Epoch: 3 | iteration: 177/262 | Loss: 0.7552621960639954Training Epoch: 3 | iteration: 178/262 | Loss: 0.6834869980812073Training Epoch: 3 | iteration: 179/262 | Loss: 0.8077691793441772Training Epoch: 3 | iteration: 180/262 | Loss: 0.6684869527816772Training Epoch: 3 | iteration: 181/262 | Loss: 0.7788428068161011Training Epoch: 3 | iteration: 182/262 | Loss: 0.6166397333145142Training Epoch: 3 | iteration: 183/262 | Loss: 0.7746033668518066Training Epoch: 3 | iteration: 184/262 | Loss: 0.736944854259491Training Epoch: 3 | iteration: 185/262 | Loss: 0.7430980205535889Training Epoch: 3 | iteration: 186/262 | Loss: 0.7240203619003296Training Epoch: 3 | iteration: 187/262 | Loss: 0.6029212474822998Training Epoch: 3 | iteration: 188/262 | Loss: 0.6335479021072388Training Epoch: 3 | iteration: 189/262 | Loss: 0.6648514270782471Training Epoch: 3 | iteration: 190/262 | Loss: 0.682671070098877Training Epoch: 3 | iteration: 191/262 | Loss: 0.6574663519859314Training Epoch: 3 | iteration: 192/262 | Loss: 0.6884740591049194Training Epoch: 3 | iteration: 193/262 | Loss: 0.6508194208145142Training Epoch: 3 | iteration: 194/262 | Loss: 0.690782904624939Training Epoch: 3 | iteration: 195/262 | Loss: 0.6878256797790527Training Epoch: 3 | iteration: 196/262 | Loss: 0.6499906778335571Training Epoch: 3 | iteration: 197/262 | Loss: 0.7125334739685059Training Epoch: 3 | iteration: 198/262 | Loss: 0.668807327747345Training Epoch: 3 | iteration: 199/262 | Loss: 0.6619962453842163Training Epoch: 3 | iteration: 200/262 | Loss: 0.6092045307159424Training Epoch: 3 | iteration: 201/262 | Loss: 0.7000488638877869Training Epoch: 3 | iteration: 202/262 | Loss: 0.681557297706604Training Epoch: 3 | iteration: 203/262 | Loss: 0.6872110366821289Training Epoch: 3 | iteration: 204/262 | Loss: 0.6556668281555176Training Epoch: 3 | iteration: 205/262 | Loss: 0.7200670838356018Training Epoch: 3 | iteration: 206/262 | Loss: 0.716469407081604Training Epoch: 3 | iteration: 207/262 | Loss: 0.6961149573326111Training Epoch: 3 | iteration: 208/262 | Loss: 0.771838903427124Training Epoch: 3 | iteration: 209/262 | Loss: 0.660317063331604Training Epoch: 3 | iteration: 210/262 | Loss: 0.6660329103469849Training Epoch: 3 | iteration: 211/262 | Loss: 0.6374945640563965Training Epoch: 3 | iteration: 212/262 | Loss: 0.6188698410987854Training Epoch: 3 | iteration: 213/262 | Loss: 0.7068216800689697Training Epoch: 3 | iteration: 214/262 | Loss: 0.7380768060684204Training Epoch: 3 | iteration: 215/262 | Loss: 0.6911537051200867Training Epoch: 3 | iteration: 216/262 | Loss: 0.6482689380645752Training Epoch: 3 | iteration: 217/262 | Loss: 0.6574712991714478Training Epoch: 3 | iteration: 218/262 | Loss: 0.6948339343070984Training Epoch: 3 | iteration: 219/262 | Loss: 0.6747962236404419Training Epoch: 3 | iteration: 220/262 | Loss: 0.7065722942352295Training Epoch: 3 | iteration: 221/262 | Loss: 0.7030827403068542Training Epoch: 3 | iteration: 222/262 | Loss: 0.6401830315589905Training Epoch: 3 | iteration: 223/262 | Loss: 0.6921823620796204Training Epoch: 3 | iteration: 224/262 | Loss: 0.7131142616271973Training Epoch: 3 | iteration: 225/262 | Loss: 0.6629568338394165Training Epoch: 3 | iteration: 226/262 | Loss: 0.6946507692337036Training Epoch: 3 | iteration: 227/262 | Loss: 0.6651955842971802Training Epoch: 3 | iteration: 228/262 | Loss: 0.6535985469818115Training Epoch: 3 | iteration: 229/262 | Loss: 0.5972477197647095Training Epoch: 3 | iteration: 230/262 | Loss: 0.715151846408844Training Epoch: 3 | iteration: 231/262 | Loss: 0.6567007303237915Training Epoch: 3 | iteration: 232/262 | Loss: 0.6532168984413147Training Epoch: 3 | iteration: 233/262 | Loss: 0.7214616537094116Training Epoch: 3 | iteration: 234/262 | Loss: 0.6848450899124146Training Epoch: 3 | iteration: 235/262 | Loss: 0.7464810609817505Training Epoch: 3 | iteration: 236/262 | Loss: 0.7529029846191406Training Epoch: 3 | iteration: 237/262 | Loss: 0.607994794845581Training Epoch: 3 | iteration: 238/262 | Loss: 0.6980420351028442Training Epoch: 3 | iteration: 239/262 | Loss: 0.642385721206665Training Epoch: 3 | iteration: 240/262 | Loss: 0.6540330052375793Training Epoch: 3 | iteration: 241/262 | Loss: 0.6396223306655884Training Epoch: 3 | iteration: 242/262 | Loss: 0.6507085561752319Training Epoch: 3 | iteration: 243/262 | Loss: 0.6489521861076355Training Epoch: 3 | iteration: 244/262 | Loss: 0.7853882312774658Training Epoch: 3 | iteration: 245/262 | Loss: 0.683533787727356Training Epoch: 3 | iteration: 246/262 | Loss: 0.6819837689399719Training Epoch: 3 | iteration: 247/262 | Loss: 0.7079107761383057Training Epoch: 3 | iteration: 248/262 | Loss: 0.6914536952972412Training Epoch: 3 | iteration: 249/262 | Loss: 0.7019251585006714Training Epoch: 3 | iteration: 250/262 | Loss: 0.6761167049407959Training Epoch: 3 | iteration: 251/262 | Loss: 0.6544412970542908Training Epoch: 3 | iteration: 252/262 | Loss: 0.6524816751480103Training Epoch: 3 | iteration: 253/262 | Loss: 0.6936855316162109Training Epoch: 3 | iteration: 254/262 | Loss: 0.6867752075195312Training Epoch: 3 | iteration: 255/262 | Loss: 0.644527792930603Training Epoch: 3 | iteration: 256/262 | Loss: 0.7725883722305298Training Epoch: 3 | iteration: 257/262 | Loss: 0.6241325736045837Training Epoch: 3 | iteration: 258/262 | Loss: 0.6394991874694824Training Epoch: 3 | iteration: 259/262 | Loss: 0.6783939599990845Training Epoch: 3 | iteration: 260/262 | Loss: 0.7508693337440491Training Epoch: 3 | iteration: 261/262 | Loss: 0.8236836194992065Validating Epoch: 3 | iteration: 0/66 | Loss: 0.6678170561790466Validating Epoch: 3 | iteration: 1/66 | Loss: 0.7268201112747192Validating Epoch: 3 | iteration: 2/66 | Loss: 0.7111124992370605Validating Epoch: 3 | iteration: 3/66 | Loss: 0.6621285676956177Validating Epoch: 3 | iteration: 4/66 | Loss: 0.706706702709198Validating Epoch: 3 | iteration: 5/66 | Loss: 0.6126168966293335Validating Epoch: 3 | iteration: 6/66 | Loss: 0.6068921685218811Validating Epoch: 3 | iteration: 7/66 | Loss: 0.6534714102745056Validating Epoch: 3 | iteration: 8/66 | Loss: 0.5912488698959351Validating Epoch: 3 | iteration: 9/66 | Loss: 0.686439037322998Validating Epoch: 3 | iteration: 10/66 | Loss: 0.7139752507209778Validating Epoch: 3 | iteration: 11/66 | Loss: 0.7051305174827576Validating Epoch: 3 | iteration: 12/66 | Loss: 0.6189303994178772Validating Epoch: 3 | iteration: 13/66 | Loss: 0.6686080694198608Validating Epoch: 3 | iteration: 14/66 | Loss: 0.6248549222946167Validating Epoch: 3 | iteration: 15/66 | Loss: 0.6473777294158936Validating Epoch: 3 | iteration: 16/66 | Loss: 0.7143840789794922Validating Epoch: 3 | iteration: 17/66 | Loss: 0.7058156728744507Validating Epoch: 3 | iteration: 18/66 | Loss: 0.6144136786460876Validating Epoch: 3 | iteration: 19/66 | Loss: 0.673153817653656Validating Epoch: 3 | iteration: 20/66 | Loss: 0.6436920762062073Validating Epoch: 3 | iteration: 21/66 | Loss: 0.6958281993865967Validating Epoch: 3 | iteration: 22/66 | Loss: 0.7593437433242798Validating Epoch: 3 | iteration: 23/66 | Loss: 0.7523717284202576Validating Epoch: 3 | iteration: 24/66 | Loss: 0.6879096031188965Validating Epoch: 3 | iteration: 25/66 | Loss: 0.7146952152252197Validating Epoch: 3 | iteration: 26/66 | Loss: 0.6567531228065491Validating Epoch: 3 | iteration: 27/66 | Loss: 0.6275843977928162Validating Epoch: 3 | iteration: 28/66 | Loss: 0.7106473445892334Validating Epoch: 3 | iteration: 29/66 | Loss: 0.7435792684555054Validating Epoch: 3 | iteration: 30/66 | Loss: 0.6509126424789429Validating Epoch: 3 | iteration: 31/66 | Loss: 0.7317726612091064Validating Epoch: 3 | iteration: 32/66 | Loss: 0.6372302770614624Validating Epoch: 3 | iteration: 33/66 | Loss: 0.6418319344520569Validating Epoch: 3 | iteration: 34/66 | Loss: 0.6457684636116028Validating Epoch: 3 | iteration: 35/66 | Loss: 0.7195464968681335Validating Epoch: 3 | iteration: 36/66 | Loss: 0.6998533606529236Validating Epoch: 3 | iteration: 37/66 | Loss: 0.5928215384483337Validating Epoch: 3 | iteration: 38/66 | Loss: 0.6403314471244812Validating Epoch: 3 | iteration: 39/66 | Loss: 0.6088325381278992Validating Epoch: 3 | iteration: 40/66 | Loss: 0.6898472309112549Validating Epoch: 3 | iteration: 41/66 | Loss: 0.6191161870956421Validating Epoch: 3 | iteration: 42/66 | Loss: 0.6604983806610107Validating Epoch: 3 | iteration: 43/66 | Loss: 0.6017974019050598Validating Epoch: 3 | iteration: 44/66 | Loss: 0.6416938304901123Validating Epoch: 3 | iteration: 45/66 | Loss: 0.6322771310806274Validating Epoch: 3 | iteration: 46/66 | Loss: 0.6695377826690674Validating Epoch: 3 | iteration: 47/66 | Loss: 0.5952651500701904Validating Epoch: 3 | iteration: 48/66 | Loss: 0.6774187088012695Validating Epoch: 3 | iteration: 49/66 | Loss: 0.6535934209823608Validating Epoch: 3 | iteration: 50/66 | Loss: 0.669680118560791Validating Epoch: 3 | iteration: 51/66 | Loss: 0.6166356801986694Validating Epoch: 3 | iteration: 52/66 | Loss: 0.7322268486022949Validating Epoch: 3 | iteration: 53/66 | Loss: 0.6556028127670288Validating Epoch: 3 | iteration: 54/66 | Loss: 0.6477628946304321Validating Epoch: 3 | iteration: 55/66 | Loss: 0.5932967662811279Validating Epoch: 3 | iteration: 56/66 | Loss: 0.6391394734382629Validating Epoch: 3 | iteration: 57/66 | Loss: 0.6473102569580078Validating Epoch: 3 | iteration: 58/66 | Loss: 0.7087500095367432Validating Epoch: 3 | iteration: 59/66 | Loss: 0.6625780463218689Validating Epoch: 3 | iteration: 60/66 | Loss: 0.6673194169998169Validating Epoch: 3 | iteration: 61/66 | Loss: 0.6679067611694336Validating Epoch: 3 | iteration: 62/66 | Loss: 0.7094817161560059Validating Epoch: 3 | iteration: 63/66 | Loss: 0.7380715608596802Validating Epoch: 3 | iteration: 64/66 | Loss: 0.5984185934066772Validating Epoch: 3 | iteration: 65/66 | Loss: 0.6304786205291748Temperature:  0.5
Sampling: iteration: 0/51{'Validity': 0.98828125, 'Novelty': 1.0, 'Uniqueness': 0.9940711462450593}
Training Epoch: 4 | iteration: 0/262 | Loss: 0.6516420841217041Training Epoch: 4 | iteration: 1/262 | Loss: 0.6210022568702698Training Epoch: 4 | iteration: 2/262 | Loss: 0.6611522436141968Training Epoch: 4 | iteration: 3/262 | Loss: 0.6215549111366272Training Epoch: 4 | iteration: 4/262 | Loss: 0.6372162103652954Training Epoch: 4 | iteration: 5/262 | Loss: 0.6437629461288452Training Epoch: 4 | iteration: 6/262 | Loss: 0.6566087007522583Training Epoch: 4 | iteration: 7/262 | Loss: 0.6295801997184753Training Epoch: 4 | iteration: 8/262 | Loss: 0.5955803394317627Training Epoch: 4 | iteration: 9/262 | Loss: 0.6752954721450806Training Epoch: 4 | iteration: 10/262 | Loss: 0.6621567010879517Training Epoch: 4 | iteration: 11/262 | Loss: 0.6605236530303955Training Epoch: 4 | iteration: 12/262 | Loss: 0.63606858253479Training Epoch: 4 | iteration: 13/262 | Loss: 0.6261440515518188Training Epoch: 4 | iteration: 14/262 | Loss: 0.6525452136993408Training Epoch: 4 | iteration: 15/262 | Loss: 0.6180014610290527Training Epoch: 4 | iteration: 16/262 | Loss: 0.647422194480896Training Epoch: 4 | iteration: 17/262 | Loss: 0.6712550520896912Training Epoch: 4 | iteration: 18/262 | Loss: 0.670075535774231Training Epoch: 4 | iteration: 19/262 | Loss: 0.5699589252471924Training Epoch: 4 | iteration: 20/262 | Loss: 0.6743003129959106Training Epoch: 4 | iteration: 21/262 | Loss: 0.6493772268295288Training Epoch: 4 | iteration: 22/262 | Loss: 0.658676266670227Training Epoch: 4 | iteration: 23/262 | Loss: 0.6547236442565918Training Epoch: 4 | iteration: 24/262 | Loss: 0.6578174829483032Training Epoch: 4 | iteration: 25/262 | Loss: 0.656955361366272Training Epoch: 4 | iteration: 26/262 | Loss: 0.6454390287399292Training Epoch: 4 | iteration: 27/262 | Loss: 0.6702516674995422Training Epoch: 4 | iteration: 28/262 | Loss: 0.6164658069610596Training Epoch: 4 | iteration: 29/262 | Loss: 0.6389685869216919Training Epoch: 4 | iteration: 30/262 | Loss: 0.5740424990653992Training Epoch: 4 | iteration: 31/262 | Loss: 0.693191409111023Training Epoch: 4 | iteration: 32/262 | Loss: 0.697532057762146Training Epoch: 4 | iteration: 33/262 | Loss: 0.5876455307006836Training Epoch: 4 | iteration: 34/262 | Loss: 0.650976300239563Training Epoch: 4 | iteration: 35/262 | Loss: 0.6894751787185669Training Epoch: 4 | iteration: 36/262 | Loss: 0.6077146530151367Training Epoch: 4 | iteration: 37/262 | Loss: 0.6519550085067749Training Epoch: 4 | iteration: 38/262 | Loss: 0.678424060344696Training Epoch: 4 | iteration: 39/262 | Loss: 0.7148228287696838Training Epoch: 4 | iteration: 40/262 | Loss: 0.6658408641815186Training Epoch: 4 | iteration: 41/262 | Loss: 0.6668911576271057Training Epoch: 4 | iteration: 42/262 | Loss: 0.6302186846733093Training Epoch: 4 | iteration: 43/262 | Loss: 0.6621361970901489Training Epoch: 4 | iteration: 44/262 | Loss: 0.6422910690307617Training Epoch: 4 | iteration: 45/262 | Loss: 0.6355916261672974Training Epoch: 4 | iteration: 46/262 | Loss: 0.6610822081565857Training Epoch: 4 | iteration: 47/262 | Loss: 0.6455293893814087Training Epoch: 4 | iteration: 48/262 | Loss: 0.6045406460762024Training Epoch: 4 | iteration: 49/262 | Loss: 0.6725835800170898Training Epoch: 4 | iteration: 50/262 | Loss: 0.5990643501281738Training Epoch: 4 | iteration: 51/262 | Loss: 0.6729559898376465Training Epoch: 4 | iteration: 52/262 | Loss: 0.6400635242462158Training Epoch: 4 | iteration: 53/262 | Loss: 0.6267359852790833Training Epoch: 4 | iteration: 54/262 | Loss: 0.6197047233581543Training Epoch: 4 | iteration: 55/262 | Loss: 0.6022849082946777Training Epoch: 4 | iteration: 56/262 | Loss: 0.7101069092750549Training Epoch: 4 | iteration: 57/262 | Loss: 0.6362711787223816Training Epoch: 4 | iteration: 58/262 | Loss: 0.6914241909980774Training Epoch: 4 | iteration: 59/262 | Loss: 0.6447350978851318Training Epoch: 4 | iteration: 60/262 | Loss: 0.6728176474571228Training Epoch: 4 | iteration: 61/262 | Loss: 0.6442177891731262Training Epoch: 4 | iteration: 62/262 | Loss: 0.6882543563842773Training Epoch: 4 | iteration: 63/262 | Loss: 0.6240008473396301Training Epoch: 4 | iteration: 64/262 | Loss: 0.6642918586730957Training Epoch: 4 | iteration: 65/262 | Loss: 0.5928632020950317Training Epoch: 4 | iteration: 66/262 | Loss: 0.669353187084198Training Epoch: 4 | iteration: 67/262 | Loss: 0.6206966638565063Training Epoch: 4 | iteration: 68/262 | Loss: 0.6711265444755554Training Epoch: 4 | iteration: 69/262 | Loss: 0.658815860748291Training Epoch: 4 | iteration: 70/262 | Loss: 0.6524918079376221Training Epoch: 4 | iteration: 71/262 | Loss: 0.6155246496200562Training Epoch: 4 | iteration: 72/262 | Loss: 0.627894401550293Training Epoch: 4 | iteration: 73/262 | Loss: 0.7000198364257812Training Epoch: 4 | iteration: 74/262 | Loss: 0.6604185104370117Training Epoch: 4 | iteration: 75/262 | Loss: 0.6714420914649963Training Epoch: 4 | iteration: 76/262 | Loss: 0.6270142793655396Training Epoch: 4 | iteration: 77/262 | Loss: 0.7351406812667847Training Epoch: 4 | iteration: 78/262 | Loss: 0.6575167179107666Training Epoch: 4 | iteration: 79/262 | Loss: 0.6364552974700928Training Epoch: 4 | iteration: 80/262 | Loss: 0.6929091215133667Training Epoch: 4 | iteration: 81/262 | Loss: 0.6638814806938171Training Epoch: 4 | iteration: 82/262 | Loss: 0.684352695941925Training Epoch: 4 | iteration: 83/262 | Loss: 0.6191614270210266Training Epoch: 4 | iteration: 84/262 | Loss: 0.6812621355056763Training Epoch: 4 | iteration: 85/262 | Loss: 0.6878145337104797Training Epoch: 4 | iteration: 86/262 | Loss: 0.6547510027885437Training Epoch: 4 | iteration: 87/262 | Loss: 0.7262589335441589Training Epoch: 4 | iteration: 88/262 | Loss: 0.6948623657226562Training Epoch: 4 | iteration: 89/262 | Loss: 0.6197915077209473Training Epoch: 4 | iteration: 90/262 | Loss: 0.6463718414306641Training Epoch: 4 | iteration: 91/262 | Loss: 0.6682853698730469Training Epoch: 4 | iteration: 92/262 | Loss: 0.6586428880691528Training Epoch: 4 | iteration: 93/262 | Loss: 0.633751630783081Training Epoch: 4 | iteration: 94/262 | Loss: 0.6342813968658447Training Epoch: 4 | iteration: 95/262 | Loss: 0.6371617317199707Training Epoch: 4 | iteration: 96/262 | Loss: 0.6018447279930115Training Epoch: 4 | iteration: 97/262 | Loss: 0.6865013837814331Training Epoch: 4 | iteration: 98/262 | Loss: 0.706052303314209Training Epoch: 4 | iteration: 99/262 | Loss: 0.7432572245597839Training Epoch: 4 | iteration: 100/262 | Loss: 0.6845243573188782Training Epoch: 4 | iteration: 101/262 | Loss: 0.6628110408782959Training Epoch: 4 | iteration: 102/262 | Loss: 0.6756139993667603Training Epoch: 4 | iteration: 103/262 | Loss: 0.6705073714256287Training Epoch: 4 | iteration: 104/262 | Loss: 0.5888073444366455Training Epoch: 4 | iteration: 105/262 | Loss: 0.6705691814422607Training Epoch: 4 | iteration: 106/262 | Loss: 0.6827369928359985Training Epoch: 4 | iteration: 107/262 | Loss: 0.6526663303375244Training Epoch: 4 | iteration: 108/262 | Loss: 0.6573567986488342Training Epoch: 4 | iteration: 109/262 | Loss: 0.5736733078956604Training Epoch: 4 | iteration: 110/262 | Loss: 0.5929456949234009Training Epoch: 4 | iteration: 111/262 | Loss: 0.7039012908935547Training Epoch: 4 | iteration: 112/262 | Loss: 0.6375468969345093Training Epoch: 4 | iteration: 113/262 | Loss: 0.6344447135925293Training Epoch: 4 | iteration: 114/262 | Loss: 0.643888533115387Training Epoch: 4 | iteration: 115/262 | Loss: 0.687001645565033Training Epoch: 4 | iteration: 116/262 | Loss: 0.6663196086883545Training Epoch: 4 | iteration: 117/262 | Loss: 0.6344361901283264Training Epoch: 4 | iteration: 118/262 | Loss: 0.6922192573547363Training Epoch: 4 | iteration: 119/262 | Loss: 0.6107301712036133Training Epoch: 4 | iteration: 120/262 | Loss: 0.703703761100769Training Epoch: 4 | iteration: 121/262 | Loss: 0.6423358917236328Training Epoch: 4 | iteration: 122/262 | Loss: 0.6467083096504211Training Epoch: 4 | iteration: 123/262 | Loss: 0.6230072379112244Training Epoch: 4 | iteration: 124/262 | Loss: 0.6448606252670288Training Epoch: 4 | iteration: 125/262 | Loss: 0.6491144299507141Training Epoch: 4 | iteration: 126/262 | Loss: 0.6107699871063232Training Epoch: 4 | iteration: 127/262 | Loss: 0.6829308271408081Training Epoch: 4 | iteration: 128/262 | Loss: 0.5636115074157715Training Epoch: 4 | iteration: 129/262 | Loss: 0.675844669342041Training Epoch: 4 | iteration: 130/262 | Loss: 0.6445979475975037Training Epoch: 4 | iteration: 131/262 | Loss: 0.6427522301673889Training Epoch: 4 | iteration: 132/262 | Loss: 0.6481328010559082Training Epoch: 4 | iteration: 133/262 | Loss: 0.643622875213623Training Epoch: 4 | iteration: 134/262 | Loss: 0.6268073916435242Training Epoch: 4 | iteration: 135/262 | Loss: 0.5989935398101807Training Epoch: 4 | iteration: 136/262 | Loss: 0.6447257995605469Training Epoch: 4 | iteration: 137/262 | Loss: 0.5984152555465698Training Epoch: 4 | iteration: 138/262 | Loss: 0.6181487441062927Training Epoch: 4 | iteration: 139/262 | Loss: 0.7201553583145142Training Epoch: 4 | iteration: 140/262 | Loss: 0.7092482447624207Training Epoch: 4 | iteration: 141/262 | Loss: 0.6320302486419678Training Epoch: 4 | iteration: 142/262 | Loss: 0.6413951516151428Training Epoch: 4 | iteration: 143/262 | Loss: 0.6899215579032898Training Epoch: 4 | iteration: 144/262 | Loss: 0.6991845369338989Training Epoch: 4 | iteration: 145/262 | Loss: 0.6040714979171753Training Epoch: 4 | iteration: 146/262 | Loss: 0.6396377086639404Training Epoch: 4 | iteration: 147/262 | Loss: 0.7386151552200317Training Epoch: 4 | iteration: 148/262 | Loss: 0.648992657661438Training Epoch: 4 | iteration: 149/262 | Loss: 0.6894311904907227Training Epoch: 4 | iteration: 150/262 | Loss: 0.6552268266677856Training Epoch: 4 | iteration: 151/262 | Loss: 0.6762041449546814Training Epoch: 4 | iteration: 152/262 | Loss: 0.6415706276893616Training Epoch: 4 | iteration: 153/262 | Loss: 0.7091084718704224Training Epoch: 4 | iteration: 154/262 | Loss: 0.6888693571090698Training Epoch: 4 | iteration: 155/262 | Loss: 0.6487317085266113Training Epoch: 4 | iteration: 156/262 | Loss: 0.6570418477058411Training Epoch: 4 | iteration: 157/262 | Loss: 0.6807440519332886Training Epoch: 4 | iteration: 158/262 | Loss: 0.6343492865562439Training Epoch: 4 | iteration: 159/262 | Loss: 0.6600243449211121Training Epoch: 4 | iteration: 160/262 | Loss: 0.6621586680412292Training Epoch: 4 | iteration: 161/262 | Loss: 0.619918942451477Training Epoch: 4 | iteration: 162/262 | Loss: 0.6314420700073242Training Epoch: 4 | iteration: 163/262 | Loss: 0.6700388193130493Training Epoch: 4 | iteration: 164/262 | Loss: 0.7060039043426514Training Epoch: 4 | iteration: 165/262 | Loss: 0.6498311758041382Training Epoch: 4 | iteration: 166/262 | Loss: 0.6223056316375732Training Epoch: 4 | iteration: 167/262 | Loss: 0.6109699606895447Training Epoch: 4 | iteration: 168/262 | Loss: 0.6203243732452393Training Epoch: 4 | iteration: 169/262 | Loss: 0.7160856127738953Training Epoch: 4 | iteration: 170/262 | Loss: 0.6633703708648682Training Epoch: 4 | iteration: 171/262 | Loss: 0.6708277463912964Training Epoch: 4 | iteration: 172/262 | Loss: 0.6854064464569092Training Epoch: 4 | iteration: 173/262 | Loss: 0.6770915985107422Training Epoch: 4 | iteration: 174/262 | Loss: 0.6159435510635376Training Epoch: 4 | iteration: 175/262 | Loss: 0.7150148153305054Training Epoch: 4 | iteration: 176/262 | Loss: 0.7220351696014404Training Epoch: 4 | iteration: 177/262 | Loss: 0.6524031162261963Training Epoch: 4 | iteration: 178/262 | Loss: 0.6588255763053894Training Epoch: 4 | iteration: 179/262 | Loss: 0.6437655687332153Training Epoch: 4 | iteration: 180/262 | Loss: 0.6425399780273438Training Epoch: 4 | iteration: 181/262 | Loss: 0.6674167513847351Training Epoch: 4 | iteration: 182/262 | Loss: 0.6820240616798401Training Epoch: 4 | iteration: 183/262 | Loss: 0.6639743447303772Training Epoch: 4 | iteration: 184/262 | Loss: 0.6261484622955322Training Epoch: 4 | iteration: 185/262 | Loss: 0.6367096304893494Training Epoch: 4 | iteration: 186/262 | Loss: 0.6425840258598328Training Epoch: 4 | iteration: 187/262 | Loss: 0.6258193850517273Training Epoch: 4 | iteration: 188/262 | Loss: 0.5780295133590698Training Epoch: 4 | iteration: 189/262 | Loss: 0.7164278030395508Training Epoch: 4 | iteration: 190/262 | Loss: 0.6711248159408569Training Epoch: 4 | iteration: 191/262 | Loss: 0.6349157691001892Training Epoch: 4 | iteration: 192/262 | Loss: 0.6173368692398071Training Epoch: 4 | iteration: 193/262 | Loss: 0.5971696972846985Training Epoch: 4 | iteration: 194/262 | Loss: 0.7128051519393921Training Epoch: 4 | iteration: 195/262 | Loss: 0.6946369409561157Training Epoch: 4 | iteration: 196/262 | Loss: 0.7362107038497925Training Epoch: 4 | iteration: 197/262 | Loss: 0.6776471138000488Training Epoch: 4 | iteration: 198/262 | Loss: 0.6252858638763428Training Epoch: 4 | iteration: 199/262 | Loss: 0.7135441899299622Training Epoch: 4 | iteration: 200/262 | Loss: 0.6227560043334961Training Epoch: 4 | iteration: 201/262 | Loss: 0.7116861343383789Training Epoch: 4 | iteration: 202/262 | Loss: 0.6764441132545471Training Epoch: 4 | iteration: 203/262 | Loss: 0.6068678498268127Training Epoch: 4 | iteration: 204/262 | Loss: 0.6382666826248169Training Epoch: 4 | iteration: 205/262 | Loss: 0.6450405120849609Training Epoch: 4 | iteration: 206/262 | Loss: 0.6761823892593384Training Epoch: 4 | iteration: 207/262 | Loss: 0.6613166928291321Training Epoch: 4 | iteration: 208/262 | Loss: 0.6567370891571045Training Epoch: 4 | iteration: 209/262 | Loss: 0.6840025186538696Training Epoch: 4 | iteration: 210/262 | Loss: 0.6719540357589722Training Epoch: 4 | iteration: 211/262 | Loss: 0.6927983164787292Training Epoch: 4 | iteration: 212/262 | Loss: 0.6420272588729858Training Epoch: 4 | iteration: 213/262 | Loss: 0.6067100167274475Training Epoch: 4 | iteration: 214/262 | Loss: 0.6678493022918701Training Epoch: 4 | iteration: 215/262 | Loss: 0.7604273557662964Training Epoch: 4 | iteration: 216/262 | Loss: 0.6664270162582397Training Epoch: 4 | iteration: 217/262 | Loss: 0.6493186354637146Training Epoch: 4 | iteration: 218/262 | Loss: 0.6646231412887573Training Epoch: 4 | iteration: 219/262 | Loss: 0.6616209745407104Training Epoch: 4 | iteration: 220/262 | Loss: 0.7070192098617554Training Epoch: 4 | iteration: 221/262 | Loss: 0.7159106731414795Training Epoch: 4 | iteration: 222/262 | Loss: 0.6643211245536804Training Epoch: 4 | iteration: 223/262 | Loss: 0.6618282794952393Training Epoch: 4 | iteration: 224/262 | Loss: 0.6220604181289673Training Epoch: 4 | iteration: 225/262 | Loss: 0.7216765880584717Training Epoch: 4 | iteration: 226/262 | Loss: 0.6337944269180298Training Epoch: 4 | iteration: 227/262 | Loss: 0.6734248995780945Training Epoch: 4 | iteration: 228/262 | Loss: 0.6688088178634644Training Epoch: 4 | iteration: 229/262 | Loss: 0.6561120748519897Training Epoch: 4 | iteration: 230/262 | Loss: 0.6919549703598022Training Epoch: 4 | iteration: 231/262 | Loss: 0.6410709023475647Training Epoch: 4 | iteration: 232/262 | Loss: 0.6827545166015625Training Epoch: 4 | iteration: 233/262 | Loss: 0.6361951231956482Training Epoch: 4 | iteration: 234/262 | Loss: 0.581668496131897Training Epoch: 4 | iteration: 235/262 | Loss: 0.7030346393585205Training Epoch: 4 | iteration: 236/262 | Loss: 0.7169779539108276Training Epoch: 4 | iteration: 237/262 | Loss: 0.6487628221511841Training Epoch: 4 | iteration: 238/262 | Loss: 0.653141975402832Training Epoch: 4 | iteration: 239/262 | Loss: 0.6520856618881226Training Epoch: 4 | iteration: 240/262 | Loss: 0.703965425491333Training Epoch: 4 | iteration: 241/262 | Loss: 0.6929935812950134Training Epoch: 4 | iteration: 242/262 | Loss: 0.644900381565094Training Epoch: 4 | iteration: 243/262 | Loss: 0.6860817670822144Training Epoch: 4 | iteration: 244/262 | Loss: 0.7209522724151611Training Epoch: 4 | iteration: 245/262 | Loss: 0.6250232458114624Training Epoch: 4 | iteration: 246/262 | Loss: 0.6449921131134033Training Epoch: 4 | iteration: 247/262 | Loss: 0.7178544998168945Training Epoch: 4 | iteration: 248/262 | Loss: 0.6340042352676392Training Epoch: 4 | iteration: 249/262 | Loss: 0.66514652967453Training Epoch: 4 | iteration: 250/262 | Loss: 0.6242495775222778Training Epoch: 4 | iteration: 251/262 | Loss: 0.6067990660667419Training Epoch: 4 | iteration: 252/262 | Loss: 0.7109516859054565Training Epoch: 4 | iteration: 253/262 | Loss: 0.6276355385780334Training Epoch: 4 | iteration: 254/262 | Loss: 0.6672941446304321Training Epoch: 4 | iteration: 255/262 | Loss: 0.6235392689704895Training Epoch: 4 | iteration: 256/262 | Loss: 0.6397362351417542Training Epoch: 4 | iteration: 257/262 | Loss: 0.675014078617096Training Epoch: 4 | iteration: 258/262 | Loss: 0.6945925951004028Training Epoch: 4 | iteration: 259/262 | Loss: 0.6862332224845886Training Epoch: 4 | iteration: 260/262 | Loss: 0.7075221538543701Training Epoch: 4 | iteration: 261/262 | Loss: 0.6212235689163208Validating Epoch: 4 | iteration: 0/66 | Loss: 0.6673609614372253Validating Epoch: 4 | iteration: 1/66 | Loss: 0.6645382642745972Validating Epoch: 4 | iteration: 2/66 | Loss: 0.6218544840812683Validating Epoch: 4 | iteration: 3/66 | Loss: 0.5845067501068115Validating Epoch: 4 | iteration: 4/66 | Loss: 0.653679370880127Validating Epoch: 4 | iteration: 5/66 | Loss: 0.6793559193611145Validating Epoch: 4 | iteration: 6/66 | Loss: 0.6805027723312378Validating Epoch: 4 | iteration: 7/66 | Loss: 0.6385645270347595Validating Epoch: 4 | iteration: 8/66 | Loss: 0.6937680244445801Validating Epoch: 4 | iteration: 9/66 | Loss: 0.6485440731048584Validating Epoch: 4 | iteration: 10/66 | Loss: 0.6698729991912842Validating Epoch: 4 | iteration: 11/66 | Loss: 0.6432087421417236Validating Epoch: 4 | iteration: 12/66 | Loss: 0.7166534066200256Validating Epoch: 4 | iteration: 13/66 | Loss: 0.7180613875389099Validating Epoch: 4 | iteration: 14/66 | Loss: 0.6292366981506348Validating Epoch: 4 | iteration: 15/66 | Loss: 0.5905106067657471Validating Epoch: 4 | iteration: 16/66 | Loss: 0.6557696461677551Validating Epoch: 4 | iteration: 17/66 | Loss: 0.7006577253341675Validating Epoch: 4 | iteration: 18/66 | Loss: 0.6678031086921692Validating Epoch: 4 | iteration: 19/66 | Loss: 0.7021743059158325Validating Epoch: 4 | iteration: 20/66 | Loss: 0.6195791959762573Validating Epoch: 4 | iteration: 21/66 | Loss: 0.6642223596572876Validating Epoch: 4 | iteration: 22/66 | Loss: 0.6629272699356079Validating Epoch: 4 | iteration: 23/66 | Loss: 0.6604030132293701Validating Epoch: 4 | iteration: 24/66 | Loss: 0.7322371006011963Validating Epoch: 4 | iteration: 25/66 | Loss: 0.6822679042816162Validating Epoch: 4 | iteration: 26/66 | Loss: 0.6306310892105103Validating Epoch: 4 | iteration: 27/66 | Loss: 0.6579657793045044Validating Epoch: 4 | iteration: 28/66 | Loss: 0.701033353805542Validating Epoch: 4 | iteration: 29/66 | Loss: 0.6567045450210571Validating Epoch: 4 | iteration: 30/66 | Loss: 0.6662529706954956Validating Epoch: 4 | iteration: 31/66 | Loss: 0.6738157272338867Validating Epoch: 4 | iteration: 32/66 | Loss: 0.6164438128471375Validating Epoch: 4 | iteration: 33/66 | Loss: 0.6664818525314331Validating Epoch: 4 | iteration: 34/66 | Loss: 0.637926459312439Validating Epoch: 4 | iteration: 35/66 | Loss: 0.6352764368057251Validating Epoch: 4 | iteration: 36/66 | Loss: 0.7431089282035828Validating Epoch: 4 | iteration: 37/66 | Loss: 0.5861508846282959Validating Epoch: 4 | iteration: 38/66 | Loss: 0.663529098033905Validating Epoch: 4 | iteration: 39/66 | Loss: 0.6835470199584961Validating Epoch: 4 | iteration: 40/66 | Loss: 0.6509801745414734Validating Epoch: 4 | iteration: 41/66 | Loss: 0.6604081988334656Validating Epoch: 4 | iteration: 42/66 | Loss: 0.6540083885192871Validating Epoch: 4 | iteration: 43/66 | Loss: 0.6470571756362915Validating Epoch: 4 | iteration: 44/66 | Loss: 0.6376123428344727Validating Epoch: 4 | iteration: 45/66 | Loss: 0.6580905318260193Validating Epoch: 4 | iteration: 46/66 | Loss: 0.6729992628097534Validating Epoch: 4 | iteration: 47/66 | Loss: 0.7448095083236694Validating Epoch: 4 | iteration: 48/66 | Loss: 0.6620413661003113Validating Epoch: 4 | iteration: 49/66 | Loss: 0.6699684262275696Validating Epoch: 4 | iteration: 50/66 | Loss: 0.6153838634490967Validating Epoch: 4 | iteration: 51/66 | Loss: 0.6698203682899475Validating Epoch: 4 | iteration: 52/66 | Loss: 0.7143080830574036Validating Epoch: 4 | iteration: 53/66 | Loss: 0.6907094717025757Validating Epoch: 4 | iteration: 54/66 | Loss: 0.6159360408782959Validating Epoch: 4 | iteration: 55/66 | Loss: 0.6829484701156616Validating Epoch: 4 | iteration: 56/66 | Loss: 0.6820170879364014Validating Epoch: 4 | iteration: 57/66 | Loss: 0.6235966682434082Validating Epoch: 4 | iteration: 58/66 | Loss: 0.7107885479927063Validating Epoch: 4 | iteration: 59/66 | Loss: 0.6295628547668457Validating Epoch: 4 | iteration: 60/66 | Loss: 0.6819610595703125Validating Epoch: 4 | iteration: 61/66 | Loss: 0.6450219750404358Validating Epoch: 4 | iteration: 62/66 | Loss: 0.6600562930107117Validating Epoch: 4 | iteration: 63/66 | Loss: 0.6952241659164429Validating Epoch: 4 | iteration: 64/66 | Loss: 0.6402391195297241Validating Epoch: 4 | iteration: 65/66 | Loss: 0.7774211168289185Temperature:  0.5
Sampling: iteration: 0/51{'Validity': 0.986328125, 'Novelty': 1.0, 'Uniqueness': 0.9900990099009901}
Training Epoch: 5 | iteration: 0/262 | Loss: 0.6565473079681396Training Epoch: 5 | iteration: 1/262 | Loss: 0.6358194351196289Training Epoch: 5 | iteration: 2/262 | Loss: 0.6074121594429016Training Epoch: 5 | iteration: 3/262 | Loss: 0.6034113168716431Training Epoch: 5 | iteration: 4/262 | Loss: 0.5970389246940613Training Epoch: 5 | iteration: 5/262 | Loss: 0.6730008721351624Training Epoch: 5 | iteration: 6/262 | Loss: 0.6200194358825684Training Epoch: 5 | iteration: 7/262 | Loss: 0.6199395656585693Training Epoch: 5 | iteration: 8/262 | Loss: 0.6671261787414551Training Epoch: 5 | iteration: 9/262 | Loss: 0.6196693778038025Training Epoch: 5 | iteration: 10/262 | Loss: 0.5988066792488098Training Epoch: 5 | iteration: 11/262 | Loss: 0.5461888909339905Training Epoch: 5 | iteration: 12/262 | Loss: 0.5873186588287354Training Epoch: 5 | iteration: 13/262 | Loss: 0.6392426490783691Training Epoch: 5 | iteration: 14/262 | Loss: 0.6568537354469299Training Epoch: 5 | iteration: 15/262 | Loss: 0.6324746608734131Training Epoch: 5 | iteration: 16/262 | Loss: 0.6563454866409302Training Epoch: 5 | iteration: 17/262 | Loss: 0.6523906588554382Training Epoch: 5 | iteration: 18/262 | Loss: 0.6205189228057861Training Epoch: 5 | iteration: 19/262 | Loss: 0.6109820604324341Training Epoch: 5 | iteration: 20/262 | Loss: 0.6202351450920105Training Epoch: 5 | iteration: 21/262 | Loss: 0.6038269996643066Training Epoch: 5 | iteration: 22/262 | Loss: 0.615993320941925Training Epoch: 5 | iteration: 23/262 | Loss: 0.5665172338485718Training Epoch: 5 | iteration: 24/262 | Loss: 0.626642107963562Training Epoch: 5 | iteration: 25/262 | Loss: 0.6202564835548401Training Epoch: 5 | iteration: 26/262 | Loss: 0.665753960609436Training Epoch: 5 | iteration: 27/262 | Loss: 0.6382273435592651Training Epoch: 5 | iteration: 28/262 | Loss: 0.6392554044723511Training Epoch: 5 | iteration: 29/262 | Loss: 0.6487616896629333Training Epoch: 5 | iteration: 30/262 | Loss: 0.6728589534759521Training Epoch: 5 | iteration: 31/262 | Loss: 0.6467924118041992Training Epoch: 5 | iteration: 32/262 | Loss: 0.5891092419624329Training Epoch: 5 | iteration: 33/262 | Loss: 0.6873849630355835Training Epoch: 5 | iteration: 34/262 | Loss: 0.6116746068000793Training Epoch: 5 | iteration: 35/262 | Loss: 0.6108121871948242Training Epoch: 5 | iteration: 36/262 | Loss: 0.5803137421607971Training Epoch: 5 | iteration: 37/262 | Loss: 0.6360575556755066Training Epoch: 5 | iteration: 38/262 | Loss: 0.6068695187568665Training Epoch: 5 | iteration: 39/262 | Loss: 0.6585670709609985Training Epoch: 5 | iteration: 40/262 | Loss: 0.6564859747886658Training Epoch: 5 | iteration: 41/262 | Loss: 0.6788508892059326Training Epoch: 5 | iteration: 42/262 | Loss: 0.6354202032089233Training Epoch: 5 | iteration: 43/262 | Loss: 0.6592147350311279Training Epoch: 5 | iteration: 44/262 | Loss: 0.5865718126296997Training Epoch: 5 | iteration: 45/262 | Loss: 0.6146143674850464Training Epoch: 5 | iteration: 46/262 | Loss: 0.6402671337127686Training Epoch: 5 | iteration: 47/262 | Loss: 0.6741861701011658Training Epoch: 5 | iteration: 48/262 | Loss: 0.6381452083587646Training Epoch: 5 | iteration: 49/262 | Loss: 0.6720370054244995Training Epoch: 5 | iteration: 50/262 | Loss: 0.6065447330474854Training Epoch: 5 | iteration: 51/262 | Loss: 0.6525132060050964Training Epoch: 5 | iteration: 52/262 | Loss: 0.6348451972007751Training Epoch: 5 | iteration: 53/262 | Loss: 0.7276886701583862Training Epoch: 5 | iteration: 54/262 | Loss: 0.5824012160301208Training Epoch: 5 | iteration: 55/262 | Loss: 0.6983583569526672Training Epoch: 5 | iteration: 56/262 | Loss: 0.6260755062103271Training Epoch: 5 | iteration: 57/262 | Loss: 0.5963175892829895Training Epoch: 5 | iteration: 58/262 | Loss: 0.6180405616760254Training Epoch: 5 | iteration: 59/262 | Loss: 0.6652545928955078Training Epoch: 5 | iteration: 60/262 | Loss: 0.6043371558189392Training Epoch: 5 | iteration: 61/262 | Loss: 0.6429779529571533Training Epoch: 5 | iteration: 62/262 | Loss: 0.6953294277191162Training Epoch: 5 | iteration: 63/262 | Loss: 0.6336177587509155Training Epoch: 5 | iteration: 64/262 | Loss: 0.5809252858161926Training Epoch: 5 | iteration: 65/262 | Loss: 0.5882811546325684Training Epoch: 5 | iteration: 66/262 | Loss: 0.7119339108467102Training Epoch: 5 | iteration: 67/262 | Loss: 0.6333291530609131Training Epoch: 5 | iteration: 68/262 | Loss: 0.6698979139328003Training Epoch: 5 | iteration: 69/262 | Loss: 0.6424056887626648Training Epoch: 5 | iteration: 70/262 | Loss: 0.655938982963562Training Epoch: 5 | iteration: 71/262 | Loss: 0.604825496673584Training Epoch: 5 | iteration: 72/262 | Loss: 0.6505460739135742Training Epoch: 5 | iteration: 73/262 | Loss: 0.6477126479148865Training Epoch: 5 | iteration: 74/262 | Loss: 0.6699447631835938Training Epoch: 5 | iteration: 75/262 | Loss: 0.6362738013267517Training Epoch: 5 | iteration: 76/262 | Loss: 0.5826454162597656Training Epoch: 5 | iteration: 77/262 | Loss: 0.5986668467521667Training Epoch: 5 | iteration: 78/262 | Loss: 0.6130436658859253Training Epoch: 5 | iteration: 79/262 | Loss: 0.6547994613647461Training Epoch: 5 | iteration: 80/262 | Loss: 0.6325494050979614Training Epoch: 5 | iteration: 81/262 | Loss: 0.6096789836883545Training Epoch: 5 | iteration: 82/262 | Loss: 0.6627621054649353Training Epoch: 5 | iteration: 83/262 | Loss: 0.6317229270935059Training Epoch: 5 | iteration: 84/262 | Loss: 0.5685570240020752Training Epoch: 5 | iteration: 85/262 | Loss: 0.632152795791626Training Epoch: 5 | iteration: 86/262 | Loss: 0.7172002792358398Training Epoch: 5 | iteration: 87/262 | Loss: 0.6419050693511963Training Epoch: 5 | iteration: 88/262 | Loss: 0.6772754192352295Training Epoch: 5 | iteration: 89/262 | Loss: 0.6410714387893677Training Epoch: 5 | iteration: 90/262 | Loss: 0.6302791833877563Training Epoch: 5 | iteration: 91/262 | Loss: 0.5803772211074829Training Epoch: 5 | iteration: 92/262 | Loss: 0.6011416912078857Training Epoch: 5 | iteration: 93/262 | Loss: 0.6916815042495728Training Epoch: 5 | iteration: 94/262 | Loss: 0.685145914554596Training Epoch: 5 | iteration: 95/262 | Loss: 0.5989447236061096Training Epoch: 5 | iteration: 96/262 | Loss: 0.6106559038162231Training Epoch: 5 | iteration: 97/262 | Loss: 0.6392585039138794Training Epoch: 5 | iteration: 98/262 | Loss: 0.6041311621665955Training Epoch: 5 | iteration: 99/262 | Loss: 0.6267428398132324Training Epoch: 5 | iteration: 100/262 | Loss: 0.646090567111969Training Epoch: 5 | iteration: 101/262 | Loss: 0.6342084407806396Training Epoch: 5 | iteration: 102/262 | Loss: 0.6907285451889038Training Epoch: 5 | iteration: 103/262 | Loss: 0.5759003162384033Training Epoch: 5 | iteration: 104/262 | Loss: 0.6220728754997253Training Epoch: 5 | iteration: 105/262 | Loss: 0.6906307339668274Training Epoch: 5 | iteration: 106/262 | Loss: 0.7073229551315308Training Epoch: 5 | iteration: 107/262 | Loss: 0.6562585830688477Training Epoch: 5 | iteration: 108/262 | Loss: 0.6385377645492554Training Epoch: 5 | iteration: 109/262 | Loss: 0.7025508880615234Training Epoch: 5 | iteration: 110/262 | Loss: 0.6302722692489624Training Epoch: 5 | iteration: 111/262 | Loss: 0.5642693042755127Training Epoch: 5 | iteration: 112/262 | Loss: 0.5966396927833557Training Epoch: 5 | iteration: 113/262 | Loss: 0.6529861688613892Training Epoch: 5 | iteration: 114/262 | Loss: 0.6660280823707581Training Epoch: 5 | iteration: 115/262 | Loss: 0.6545077562332153Training Epoch: 5 | iteration: 116/262 | Loss: 0.6759563088417053Training Epoch: 5 | iteration: 117/262 | Loss: 0.5914070010185242Training Epoch: 5 | iteration: 118/262 | Loss: 0.6653949618339539Training Epoch: 5 | iteration: 119/262 | Loss: 0.6005158424377441Training Epoch: 5 | iteration: 120/262 | Loss: 0.6084884405136108Training Epoch: 5 | iteration: 121/262 | Loss: 0.6109077334403992Training Epoch: 5 | iteration: 122/262 | Loss: 0.6252315640449524Training Epoch: 5 | iteration: 123/262 | Loss: 0.6228402853012085Training Epoch: 5 | iteration: 124/262 | Loss: 0.6826432943344116Training Epoch: 5 | iteration: 125/262 | Loss: 0.6412101984024048Training Epoch: 5 | iteration: 126/262 | Loss: 0.666333794593811Training Epoch: 5 | iteration: 127/262 | Loss: 0.6541311740875244Training Epoch: 5 | iteration: 128/262 | Loss: 0.6610383987426758Training Epoch: 5 | iteration: 129/262 | Loss: 0.6519310474395752Training Epoch: 5 | iteration: 130/262 | Loss: 0.6470176577568054Training Epoch: 5 | iteration: 131/262 | Loss: 0.5935888290405273Training Epoch: 5 | iteration: 132/262 | Loss: 0.6275907158851624Training Epoch: 5 | iteration: 133/262 | Loss: 0.7310450077056885Training Epoch: 5 | iteration: 134/262 | Loss: 0.6292933225631714Training Epoch: 5 | iteration: 135/262 | Loss: 0.6146982312202454Training Epoch: 5 | iteration: 136/262 | Loss: 0.6323361396789551Training Epoch: 5 | iteration: 137/262 | Loss: 0.6379795670509338Training Epoch: 5 | iteration: 138/262 | Loss: 0.6215503215789795Training Epoch: 5 | iteration: 139/262 | Loss: 0.6704087853431702Training Epoch: 5 | iteration: 140/262 | Loss: 0.577957034111023Training Epoch: 5 | iteration: 141/262 | Loss: 0.6600456833839417Training Epoch: 5 | iteration: 142/262 | Loss: 0.6494841575622559Training Epoch: 5 | iteration: 143/262 | Loss: 0.6125709414482117Training Epoch: 5 | iteration: 144/262 | Loss: 0.5707416534423828Training Epoch: 5 | iteration: 145/262 | Loss: 0.7030285596847534Training Epoch: 5 | iteration: 146/262 | Loss: 0.6634331345558167Training Epoch: 5 | iteration: 147/262 | Loss: 0.6207334399223328Training Epoch: 5 | iteration: 148/262 | Loss: 0.5929732918739319Training Epoch: 5 | iteration: 149/262 | Loss: 0.589877724647522Training Epoch: 5 | iteration: 150/262 | Loss: 0.6552844047546387Training Epoch: 5 | iteration: 151/262 | Loss: 0.717315137386322Training Epoch: 5 | iteration: 152/262 | Loss: 0.5382963418960571Training Epoch: 5 | iteration: 153/262 | Loss: 0.6469326615333557Training Epoch: 5 | iteration: 154/262 | Loss: 0.5648177266120911Training Epoch: 5 | iteration: 155/262 | Loss: 0.6669890880584717Training Epoch: 5 | iteration: 156/262 | Loss: 0.6326045393943787Training Epoch: 5 | iteration: 157/262 | Loss: 0.6027688384056091Training Epoch: 5 | iteration: 158/262 | Loss: 0.6087507605552673Training Epoch: 5 | iteration: 159/262 | Loss: 0.610548734664917Training Epoch: 5 | iteration: 160/262 | Loss: 0.6196409463882446Training Epoch: 5 | iteration: 161/262 | Loss: 0.6578375697135925Training Epoch: 5 | iteration: 162/262 | Loss: 0.6671960949897766Training Epoch: 5 | iteration: 163/262 | Loss: 0.701231837272644Training Epoch: 5 | iteration: 164/262 | Loss: 0.6141830086708069Training Epoch: 5 | iteration: 165/262 | Loss: 0.5769195556640625Training Epoch: 5 | iteration: 166/262 | Loss: 0.6098540425300598Training Epoch: 5 | iteration: 167/262 | Loss: 0.6136559844017029Training Epoch: 5 | iteration: 168/262 | Loss: 0.6239081621170044Training Epoch: 5 | iteration: 169/262 | Loss: 0.5961359739303589Training Epoch: 5 | iteration: 170/262 | Loss: 0.655578076839447Training Epoch: 5 | iteration: 171/262 | Loss: 0.657748818397522Training Epoch: 5 | iteration: 172/262 | Loss: 0.5958003997802734Training Epoch: 5 | iteration: 173/262 | Loss: 0.6158327460289001Training Epoch: 5 | iteration: 174/262 | Loss: 0.5648736953735352Training Epoch: 5 | iteration: 175/262 | Loss: 0.6638824939727783Training Epoch: 5 | iteration: 176/262 | Loss: 0.6589370369911194Training Epoch: 5 | iteration: 177/262 | Loss: 0.6794559955596924Training Epoch: 5 | iteration: 178/262 | Loss: 0.6406958699226379Training Epoch: 5 | iteration: 179/262 | Loss: 0.7080999612808228Training Epoch: 5 | iteration: 180/262 | Loss: 0.6124097108840942Training Epoch: 5 | iteration: 181/262 | Loss: 0.6681560277938843Training Epoch: 5 | iteration: 182/262 | Loss: 0.6614255905151367Training Epoch: 5 | iteration: 183/262 | Loss: 0.6517124772071838Training Epoch: 5 | iteration: 184/262 | Loss: 0.631523609161377Training Epoch: 5 | iteration: 185/262 | Loss: 0.6155132055282593Training Epoch: 5 | iteration: 186/262 | Loss: 0.6101839542388916Training Epoch: 5 | iteration: 187/262 | Loss: 0.6440216302871704Training Epoch: 5 | iteration: 188/262 | Loss: 0.6575565934181213Training Epoch: 5 | iteration: 189/262 | Loss: 0.6600421667098999Training Epoch: 5 | iteration: 190/262 | Loss: 0.6119306087493896Training Epoch: 5 | iteration: 191/262 | Loss: 0.6332200765609741Training Epoch: 5 | iteration: 192/262 | Loss: 0.6910430192947388Training Epoch: 5 | iteration: 193/262 | Loss: 0.6073846220970154Training Epoch: 5 | iteration: 194/262 | Loss: 0.6422141194343567Training Epoch: 5 | iteration: 195/262 | Loss: 0.5982884168624878Training Epoch: 5 | iteration: 196/262 | Loss: 0.6614479422569275Training Epoch: 5 | iteration: 197/262 | Loss: 0.6457587480545044Training Epoch: 5 | iteration: 198/262 | Loss: 0.5880106687545776Training Epoch: 5 | iteration: 199/262 | Loss: 0.6139878034591675Training Epoch: 5 | iteration: 200/262 | Loss: 0.6202058792114258Training Epoch: 5 | iteration: 201/262 | Loss: 0.6291743516921997Training Epoch: 5 | iteration: 202/262 | Loss: 0.7024126052856445Training Epoch: 5 | iteration: 203/262 | Loss: 0.6019989252090454Training Epoch: 5 | iteration: 204/262 | Loss: 0.6138768792152405Training Epoch: 5 | iteration: 205/262 | Loss: 0.6632652282714844Training Epoch: 5 | iteration: 206/262 | Loss: 0.7422821521759033Training Epoch: 5 | iteration: 207/262 | Loss: 0.5924581289291382Training Epoch: 5 | iteration: 208/262 | Loss: 0.6383994817733765Training Epoch: 5 | iteration: 209/262 | Loss: 0.6248242855072021Training Epoch: 5 | iteration: 210/262 | Loss: 0.6739200353622437Training Epoch: 5 | iteration: 211/262 | Loss: 0.6503344178199768Training Epoch: 5 | iteration: 212/262 | Loss: 0.6493447422981262Training Epoch: 5 | iteration: 213/262 | Loss: 0.6593962907791138Training Epoch: 5 | iteration: 214/262 | Loss: 0.6732435822486877Training Epoch: 5 | iteration: 215/262 | Loss: 0.6322171688079834Training Epoch: 5 | iteration: 216/262 | Loss: 0.5447371006011963Training Epoch: 5 | iteration: 217/262 | Loss: 0.6159717440605164Training Epoch: 5 | iteration: 218/262 | Loss: 0.6218054294586182Training Epoch: 5 | iteration: 219/262 | Loss: 0.6623305678367615Training Epoch: 5 | iteration: 220/262 | Loss: 0.6054185628890991Training Epoch: 5 | iteration: 221/262 | Loss: 0.6567682027816772Training Epoch: 5 | iteration: 222/262 | Loss: 0.6020395755767822Training Epoch: 5 | iteration: 223/262 | Loss: 0.5865437388420105Training Epoch: 5 | iteration: 224/262 | Loss: 0.6575598120689392Training Epoch: 5 | iteration: 225/262 | Loss: 0.6110154390335083Training Epoch: 5 | iteration: 226/262 | Loss: 0.6297802329063416Training Epoch: 5 | iteration: 227/262 | Loss: 0.6405724287033081Training Epoch: 5 | iteration: 228/262 | Loss: 0.6228588819503784Training Epoch: 5 | iteration: 229/262 | Loss: 0.5908134579658508Training Epoch: 5 | iteration: 230/262 | Loss: 0.602104902267456Training Epoch: 5 | iteration: 231/262 | Loss: 0.5561542510986328Training Epoch: 5 | iteration: 232/262 | Loss: 0.6306849718093872Training Epoch: 5 | iteration: 233/262 | Loss: 0.6583641171455383Training Epoch: 5 | iteration: 234/262 | Loss: 0.612769603729248Training Epoch: 5 | iteration: 235/262 | Loss: 0.6125147342681885Training Epoch: 5 | iteration: 236/262 | Loss: 0.5555070042610168Training Epoch: 5 | iteration: 237/262 | Loss: 0.638822078704834Training Epoch: 5 | iteration: 238/262 | Loss: 0.6489200592041016Training Epoch: 5 | iteration: 239/262 | Loss: 0.6382144689559937Training Epoch: 5 | iteration: 240/262 | Loss: 0.6398410797119141Training Epoch: 5 | iteration: 241/262 | Loss: 0.7138018012046814Training Epoch: 5 | iteration: 242/262 | Loss: 0.6086941957473755Training Epoch: 5 | iteration: 243/262 | Loss: 0.6431978940963745Training Epoch: 5 | iteration: 244/262 | Loss: 0.670563817024231Training Epoch: 5 | iteration: 245/262 | Loss: 0.6043031215667725Training Epoch: 5 | iteration: 246/262 | Loss: 0.6377642154693604Training Epoch: 5 | iteration: 247/262 | Loss: 0.6599279046058655Training Epoch: 5 | iteration: 248/262 | Loss: 0.627151608467102Training Epoch: 5 | iteration: 249/262 | Loss: 0.6327664852142334Training Epoch: 5 | iteration: 250/262 | Loss: 0.618721067905426Training Epoch: 5 | iteration: 251/262 | Loss: 0.7222562432289124Training Epoch: 5 | iteration: 252/262 | Loss: 0.6701970100402832Training Epoch: 5 | iteration: 253/262 | Loss: 0.6856340169906616Training Epoch: 5 | iteration: 254/262 | Loss: 0.5988151431083679Training Epoch: 5 | iteration: 255/262 | Loss: 0.7090181112289429Training Epoch: 5 | iteration: 256/262 | Loss: 0.6443458795547485Training Epoch: 5 | iteration: 257/262 | Loss: 0.5533264875411987Training Epoch: 5 | iteration: 258/262 | Loss: 0.66899174451828Training Epoch: 5 | iteration: 259/262 | Loss: 0.6402592062950134Training Epoch: 5 | iteration: 260/262 | Loss: 0.6952607035636902Training Epoch: 5 | iteration: 261/262 | Loss: 0.6553134918212891Validating Epoch: 5 | iteration: 0/66 | Loss: 0.6520512104034424Validating Epoch: 5 | iteration: 1/66 | Loss: 0.630044162273407Validating Epoch: 5 | iteration: 2/66 | Loss: 0.6315872073173523Validating Epoch: 5 | iteration: 3/66 | Loss: 0.6319665908813477Validating Epoch: 5 | iteration: 4/66 | Loss: 0.6417023539543152Validating Epoch: 5 | iteration: 5/66 | Loss: 0.64690762758255Validating Epoch: 5 | iteration: 6/66 | Loss: 0.622942328453064Validating Epoch: 5 | iteration: 7/66 | Loss: 0.6648955345153809Validating Epoch: 5 | iteration: 8/66 | Loss: 0.6291828155517578Validating Epoch: 5 | iteration: 9/66 | Loss: 0.6354708671569824Validating Epoch: 5 | iteration: 10/66 | Loss: 0.5873434543609619Validating Epoch: 5 | iteration: 11/66 | Loss: 0.6369079351425171Validating Epoch: 5 | iteration: 12/66 | Loss: 0.7028504610061646Validating Epoch: 5 | iteration: 13/66 | Loss: 0.6585282683372498Validating Epoch: 5 | iteration: 14/66 | Loss: 0.7585756778717041Validating Epoch: 5 | iteration: 15/66 | Loss: 0.7050873041152954Validating Epoch: 5 | iteration: 16/66 | Loss: 0.607028603553772Validating Epoch: 5 | iteration: 17/66 | Loss: 0.7015913724899292Validating Epoch: 5 | iteration: 18/66 | Loss: 0.7537611722946167Validating Epoch: 5 | iteration: 19/66 | Loss: 0.6539077162742615Validating Epoch: 5 | iteration: 20/66 | Loss: 0.6730109453201294Validating Epoch: 5 | iteration: 21/66 | Loss: 0.6634615659713745Validating Epoch: 5 | iteration: 22/66 | Loss: 0.6166225671768188Validating Epoch: 5 | iteration: 23/66 | Loss: 0.6962646245956421Validating Epoch: 5 | iteration: 24/66 | Loss: 0.6721264123916626Validating Epoch: 5 | iteration: 25/66 | Loss: 0.598446249961853Validating Epoch: 5 | iteration: 26/66 | Loss: 0.717625617980957Validating Epoch: 5 | iteration: 27/66 | Loss: 0.6270071864128113Validating Epoch: 5 | iteration: 28/66 | Loss: 0.6597455739974976Validating Epoch: 5 | iteration: 29/66 | Loss: 0.6303452253341675Validating Epoch: 5 | iteration: 30/66 | Loss: 0.7157719135284424Validating Epoch: 5 | iteration: 31/66 | Loss: 0.6843752264976501Validating Epoch: 5 | iteration: 32/66 | Loss: 0.6675500869750977Validating Epoch: 5 | iteration: 33/66 | Loss: 0.6917712688446045Validating Epoch: 5 | iteration: 34/66 | Loss: 0.6453197002410889Validating Epoch: 5 | iteration: 35/66 | Loss: 0.6445760130882263Validating Epoch: 5 | iteration: 36/66 | Loss: 0.6382873058319092Validating Epoch: 5 | iteration: 37/66 | Loss: 0.6434749960899353Validating Epoch: 5 | iteration: 38/66 | Loss: 0.6970783472061157Validating Epoch: 5 | iteration: 39/66 | Loss: 0.6401617527008057Validating Epoch: 5 | iteration: 40/66 | Loss: 0.6655159592628479Validating Epoch: 5 | iteration: 41/66 | Loss: 0.6960065364837646Validating Epoch: 5 | iteration: 42/66 | Loss: 0.725448727607727Validating Epoch: 5 | iteration: 43/66 | Loss: 0.6511181592941284Validating Epoch: 5 | iteration: 44/66 | Loss: 0.7165521383285522Validating Epoch: 5 | iteration: 45/66 | Loss: 0.6215870380401611Validating Epoch: 5 | iteration: 46/66 | Loss: 0.6521316766738892Validating Epoch: 5 | iteration: 47/66 | Loss: 0.658373236656189Validating Epoch: 5 | iteration: 48/66 | Loss: 0.6761440634727478Validating Epoch: 5 | iteration: 49/66 | Loss: 0.6925725936889648Validating Epoch: 5 | iteration: 50/66 | Loss: 0.6794419288635254Validating Epoch: 5 | iteration: 51/66 | Loss: 0.6263972520828247Validating Epoch: 5 | iteration: 52/66 | Loss: 0.7252368927001953Validating Epoch: 5 | iteration: 53/66 | Loss: 0.6927847862243652Validating Epoch: 5 | iteration: 54/66 | Loss: 0.6138672828674316Validating Epoch: 5 | iteration: 55/66 | Loss: 0.6761770248413086Validating Epoch: 5 | iteration: 56/66 | Loss: 0.7382456064224243Validating Epoch: 5 | iteration: 57/66 | Loss: 0.6981254816055298Validating Epoch: 5 | iteration: 58/66 | Loss: 0.6544421911239624Validating Epoch: 5 | iteration: 59/66 | Loss: 0.6898456811904907Validating Epoch: 5 | iteration: 60/66 | Loss: 0.688531756401062Validating Epoch: 5 | iteration: 61/66 | Loss: 0.635491132736206Validating Epoch: 5 | iteration: 62/66 | Loss: 0.7069835066795349Validating Epoch: 5 | iteration: 63/66 | Loss: 0.6726793050765991Validating Epoch: 5 | iteration: 64/66 | Loss: 0.7050964832305908Validating Epoch: 5 | iteration: 65/66 | Loss: 0.6834875345230103Temperature:  0.5
Sampling: iteration: 0/51{'Validity': 0.96484375, 'Novelty': 1.0, 'Uniqueness': 0.9959514170040485}
Training Epoch: 6 | iteration: 0/262 | Loss: 0.6510229706764221Training Epoch: 6 | iteration: 1/262 | Loss: 0.6531766653060913Training Epoch: 6 | iteration: 2/262 | Loss: 0.6056932806968689Training Epoch: 6 | iteration: 3/262 | Loss: 0.6609950661659241Training Epoch: 6 | iteration: 4/262 | Loss: 0.6127337217330933Training Epoch: 6 | iteration: 5/262 | Loss: 0.5969225764274597Training Epoch: 6 | iteration: 6/262 | Loss: 0.6790862083435059Training Epoch: 6 | iteration: 7/262 | Loss: 0.5687381625175476Training Epoch: 6 | iteration: 8/262 | Loss: 0.6433928608894348Training Epoch: 6 | iteration: 9/262 | Loss: 0.6164857149124146Training Epoch: 6 | iteration: 10/262 | Loss: 0.6372604370117188Training Epoch: 6 | iteration: 11/262 | Loss: 0.6344295740127563Training Epoch: 6 | iteration: 12/262 | Loss: 0.6552242636680603Training Epoch: 6 | iteration: 13/262 | Loss: 0.6344503164291382Training Epoch: 6 | iteration: 14/262 | Loss: 0.5957611799240112Training Epoch: 6 | iteration: 15/262 | Loss: 0.5723709464073181Training Epoch: 6 | iteration: 16/262 | Loss: 0.6002837419509888Training Epoch: 6 | iteration: 17/262 | Loss: 0.62349534034729Training Epoch: 6 | iteration: 18/262 | Loss: 0.6246042251586914Training Epoch: 6 | iteration: 19/262 | Loss: 0.6451613306999207Training Epoch: 6 | iteration: 20/262 | Loss: 0.6134622097015381Training Epoch: 6 | iteration: 21/262 | Loss: 0.6465311050415039Training Epoch: 6 | iteration: 22/262 | Loss: 0.5879430174827576Training Epoch: 6 | iteration: 23/262 | Loss: 0.5690491199493408Training Epoch: 6 | iteration: 24/262 | Loss: 0.6320713758468628Training Epoch: 6 | iteration: 25/262 | Loss: 0.5535273551940918Training Epoch: 6 | iteration: 26/262 | Loss: 0.5912402272224426Training Epoch: 6 | iteration: 27/262 | Loss: 0.605934739112854Training Epoch: 6 | iteration: 28/262 | Loss: 0.5977917909622192Training Epoch: 6 | iteration: 29/262 | Loss: 0.6054812669754028Training Epoch: 6 | iteration: 30/262 | Loss: 0.6099494695663452Training Epoch: 6 | iteration: 31/262 | Loss: 0.5846967101097107Training Epoch: 6 | iteration: 32/262 | Loss: 0.5520952939987183Training Epoch: 6 | iteration: 33/262 | Loss: 0.5871958136558533Training Epoch: 6 | iteration: 34/262 | Loss: 0.5955485105514526Training Epoch: 6 | iteration: 35/262 | Loss: 0.6221415996551514Training Epoch: 6 | iteration: 36/262 | Loss: 0.6072092652320862Training Epoch: 6 | iteration: 37/262 | Loss: 0.637380838394165Training Epoch: 6 | iteration: 38/262 | Loss: 0.6229484677314758Training Epoch: 6 | iteration: 39/262 | Loss: 0.623893141746521Training Epoch: 6 | iteration: 40/262 | Loss: 0.5722182393074036Training Epoch: 6 | iteration: 41/262 | Loss: 0.5609250068664551Training Epoch: 6 | iteration: 42/262 | Loss: 0.6255397796630859Training Epoch: 6 | iteration: 43/262 | Loss: 0.638950526714325Training Epoch: 6 | iteration: 44/262 | Loss: 0.5813256502151489Training Epoch: 6 | iteration: 45/262 | Loss: 0.6656672358512878Training Epoch: 6 | iteration: 46/262 | Loss: 0.6689502000808716Training Epoch: 6 | iteration: 47/262 | Loss: 0.6178001165390015Training Epoch: 6 | iteration: 48/262 | Loss: 0.5978964567184448Training Epoch: 6 | iteration: 49/262 | Loss: 0.6558703184127808Training Epoch: 6 | iteration: 50/262 | Loss: 0.5974876880645752Training Epoch: 6 | iteration: 51/262 | Loss: 0.6144410967826843Training Epoch: 6 | iteration: 52/262 | Loss: 0.624665379524231Training Epoch: 6 | iteration: 53/262 | Loss: 0.5229016542434692Training Epoch: 6 | iteration: 54/262 | Loss: 0.6444044709205627Training Epoch: 6 | iteration: 55/262 | Loss: 0.535028338432312Training Epoch: 6 | iteration: 56/262 | Loss: 0.6800931692123413Training Epoch: 6 | iteration: 57/262 | Loss: 0.6078429222106934Training Epoch: 6 | iteration: 58/262 | Loss: 0.5777410268783569Training Epoch: 6 | iteration: 59/262 | Loss: 0.5660918951034546Training Epoch: 6 | iteration: 60/262 | Loss: 0.6497665643692017Training Epoch: 6 | iteration: 61/262 | Loss: 0.5715501308441162Training Epoch: 6 | iteration: 62/262 | Loss: 0.6112116575241089Training Epoch: 6 | iteration: 63/262 | Loss: 0.6370888352394104Training Epoch: 6 | iteration: 64/262 | Loss: 0.5812776684761047Training Epoch: 6 | iteration: 65/262 | Loss: 0.6573056578636169Training Epoch: 6 | iteration: 66/262 | Loss: 0.650653600692749Training Epoch: 6 | iteration: 67/262 | Loss: 0.6614804267883301Training Epoch: 6 | iteration: 68/262 | Loss: 0.5825350880622864Training Epoch: 6 | iteration: 69/262 | Loss: 0.6338958740234375Training Epoch: 6 | iteration: 70/262 | Loss: 0.622112512588501Training Epoch: 6 | iteration: 71/262 | Loss: 0.5579750537872314Training Epoch: 6 | iteration: 72/262 | Loss: 0.5643725991249084Training Epoch: 6 | iteration: 73/262 | Loss: 0.6509338021278381Training Epoch: 6 | iteration: 74/262 | Loss: 0.5937063694000244Training Epoch: 6 | iteration: 75/262 | Loss: 0.5989735126495361Training Epoch: 6 | iteration: 76/262 | Loss: 0.6349658966064453Training Epoch: 6 | iteration: 77/262 | Loss: 0.5912450551986694Training Epoch: 6 | iteration: 78/262 | Loss: 0.63689124584198Training Epoch: 6 | iteration: 79/262 | Loss: 0.645706832408905Training Epoch: 6 | iteration: 80/262 | Loss: 0.6583622694015503Training Epoch: 6 | iteration: 81/262 | Loss: 0.6147317290306091Training Epoch: 6 | iteration: 82/262 | Loss: 0.6386168599128723Training Epoch: 6 | iteration: 83/262 | Loss: 0.6509270668029785Training Epoch: 6 | iteration: 84/262 | Loss: 0.6596903204917908Training Epoch: 6 | iteration: 85/262 | Loss: 0.6503006219863892Training Epoch: 6 | iteration: 86/262 | Loss: 0.6782786846160889Training Epoch: 6 | iteration: 87/262 | Loss: 0.6769839525222778Training Epoch: 6 | iteration: 88/262 | Loss: 0.5423702001571655Training Epoch: 6 | iteration: 89/262 | Loss: 0.6664778590202332Training Epoch: 6 | iteration: 90/262 | Loss: 0.633402407169342Training Epoch: 6 | iteration: 91/262 | Loss: 0.6424738168716431Training Epoch: 6 | iteration: 92/262 | Loss: 0.6103808879852295Training Epoch: 6 | iteration: 93/262 | Loss: 0.6127538084983826Training Epoch: 6 | iteration: 94/262 | Loss: 0.650290310382843Training Epoch: 6 | iteration: 95/262 | Loss: 0.6563172340393066Training Epoch: 6 | iteration: 96/262 | Loss: 0.5599690675735474Training Epoch: 6 | iteration: 97/262 | Loss: 0.6375266909599304Training Epoch: 6 | iteration: 98/262 | Loss: 0.6179426312446594Training Epoch: 6 | iteration: 99/262 | Loss: 0.6437473297119141Training Epoch: 6 | iteration: 100/262 | Loss: 0.6333662867546082Training Epoch: 6 | iteration: 101/262 | Loss: 0.5990579128265381Training Epoch: 6 | iteration: 102/262 | Loss: 0.6841073036193848Training Epoch: 6 | iteration: 103/262 | Loss: 0.650165855884552Training Epoch: 6 | iteration: 104/262 | Loss: 0.611700177192688Training Epoch: 6 | iteration: 105/262 | Loss: 0.6285238265991211Training Epoch: 6 | iteration: 106/262 | Loss: 0.6129660606384277Training Epoch: 6 | iteration: 107/262 | Loss: 0.5812699794769287Training Epoch: 6 | iteration: 108/262 | Loss: 0.5670607089996338Training Epoch: 6 | iteration: 109/262 | Loss: 0.6188352108001709Training Epoch: 6 | iteration: 110/262 | Loss: 0.5950793623924255Training Epoch: 6 | iteration: 111/262 | Loss: 0.6425230503082275Training Epoch: 6 | iteration: 112/262 | Loss: 0.6579828262329102Training Epoch: 6 | iteration: 113/262 | Loss: 0.6694608926773071Training Epoch: 6 | iteration: 114/262 | Loss: 0.6394894123077393Training Epoch: 6 | iteration: 115/262 | Loss: 0.6051809787750244Training Epoch: 6 | iteration: 116/262 | Loss: 0.5673012733459473Training Epoch: 6 | iteration: 117/262 | Loss: 0.5956873893737793Training Epoch: 6 | iteration: 118/262 | Loss: 0.6287131309509277Training Epoch: 6 | iteration: 119/262 | Loss: 0.6323096752166748Training Epoch: 6 | iteration: 120/262 | Loss: 0.6720738410949707Training Epoch: 6 | iteration: 121/262 | Loss: 0.5573940277099609Training Epoch: 6 | iteration: 122/262 | Loss: 0.5702941417694092Training Epoch: 6 | iteration: 123/262 | Loss: 0.578659176826477Training Epoch: 6 | iteration: 124/262 | Loss: 0.5919135212898254Training Epoch: 6 | iteration: 125/262 | Loss: 0.5961869955062866Training Epoch: 6 | iteration: 126/262 | Loss: 0.6111865043640137Training Epoch: 6 | iteration: 127/262 | Loss: 0.549498438835144Training Epoch: 6 | iteration: 128/262 | Loss: 0.6019495129585266Training Epoch: 6 | iteration: 129/262 | Loss: 0.6087740659713745Training Epoch: 6 | iteration: 130/262 | Loss: 0.5631909370422363Training Epoch: 6 | iteration: 131/262 | Loss: 0.677706778049469Training Epoch: 6 | iteration: 132/262 | Loss: 0.613089919090271Training Epoch: 6 | iteration: 133/262 | Loss: 0.6062058210372925Training Epoch: 6 | iteration: 134/262 | Loss: 0.6208788752555847Training Epoch: 6 | iteration: 135/262 | Loss: 0.578070342540741Training Epoch: 6 | iteration: 136/262 | Loss: 0.6127126216888428Training Epoch: 6 | iteration: 137/262 | Loss: 0.608107328414917Training Epoch: 6 | iteration: 138/262 | Loss: 0.625515878200531Training Epoch: 6 | iteration: 139/262 | Loss: 0.5851386785507202Training Epoch: 6 | iteration: 140/262 | Loss: 0.6242082118988037Training Epoch: 6 | iteration: 141/262 | Loss: 0.6275131702423096Training Epoch: 6 | iteration: 142/262 | Loss: 0.6031314134597778Training Epoch: 6 | iteration: 143/262 | Loss: 0.7115373015403748Training Epoch: 6 | iteration: 144/262 | Loss: 0.6492377519607544Training Epoch: 6 | iteration: 145/262 | Loss: 0.6736804842948914Training Epoch: 6 | iteration: 146/262 | Loss: 0.6453776359558105Training Epoch: 6 | iteration: 147/262 | Loss: 0.6087501049041748Training Epoch: 6 | iteration: 148/262 | Loss: 0.5875906944274902Training Epoch: 6 | iteration: 149/262 | Loss: 0.6315096020698547Training Epoch: 6 | iteration: 150/262 | Loss: 0.6688055992126465Training Epoch: 6 | iteration: 151/262 | Loss: 0.7187375426292419Training Epoch: 6 | iteration: 152/262 | Loss: 0.5971064567565918Training Epoch: 6 | iteration: 153/262 | Loss: 0.6306026577949524Training Epoch: 6 | iteration: 154/262 | Loss: 0.6164520978927612Training Epoch: 6 | iteration: 155/262 | Loss: 0.6098227500915527Training Epoch: 6 | iteration: 156/262 | Loss: 0.6700451970100403Training Epoch: 6 | iteration: 157/262 | Loss: 0.6909797191619873Training Epoch: 6 | iteration: 158/262 | Loss: 0.6076205968856812Training Epoch: 6 | iteration: 159/262 | Loss: 0.5855352878570557Training Epoch: 6 | iteration: 160/262 | Loss: 0.5433398485183716Training Epoch: 6 | iteration: 161/262 | Loss: 0.6745043992996216Training Epoch: 6 | iteration: 162/262 | Loss: 0.6311781406402588Training Epoch: 6 | iteration: 163/262 | Loss: 0.6245455741882324Training Epoch: 6 | iteration: 164/262 | Loss: 0.6485442519187927Training Epoch: 6 | iteration: 165/262 | Loss: 0.6409154534339905Training Epoch: 6 | iteration: 166/262 | Loss: 0.6792364120483398Training Epoch: 6 | iteration: 167/262 | Loss: 0.6122826337814331Training Epoch: 6 | iteration: 168/262 | Loss: 0.6251401305198669Training Epoch: 6 | iteration: 169/262 | Loss: 0.5945963263511658Training Epoch: 6 | iteration: 170/262 | Loss: 0.5669752359390259Training Epoch: 6 | iteration: 171/262 | Loss: 0.5828261375427246Training Epoch: 6 | iteration: 172/262 | Loss: 0.5689173340797424Training Epoch: 6 | iteration: 173/262 | Loss: 0.6002582907676697Training Epoch: 6 | iteration: 174/262 | Loss: 0.6154494285583496Training Epoch: 6 | iteration: 175/262 | Loss: 0.6333271265029907Training Epoch: 6 | iteration: 176/262 | Loss: 0.6549415588378906Training Epoch: 6 | iteration: 177/262 | Loss: 0.5446594953536987Training Epoch: 6 | iteration: 178/262 | Loss: 0.5745755434036255Training Epoch: 6 | iteration: 179/262 | Loss: 0.6348459720611572Training Epoch: 6 | iteration: 180/262 | Loss: 0.6391533613204956Training Epoch: 6 | iteration: 181/262 | Loss: 0.5491829514503479Training Epoch: 6 | iteration: 182/262 | Loss: 0.5919921398162842Training Epoch: 6 | iteration: 183/262 | Loss: 0.6317846775054932Training Epoch: 6 | iteration: 184/262 | Loss: 0.6183218359947205Training Epoch: 6 | iteration: 185/262 | Loss: 0.6602689027786255Training Epoch: 6 | iteration: 186/262 | Loss: 0.6739140152931213Training Epoch: 6 | iteration: 187/262 | Loss: 0.5435358285903931Training Epoch: 6 | iteration: 188/262 | Loss: 0.606776237487793Training Epoch: 6 | iteration: 189/262 | Loss: 0.6099574565887451Training Epoch: 6 | iteration: 190/262 | Loss: 0.5786938667297363Training Epoch: 6 | iteration: 191/262 | Loss: 0.635251522064209Training Epoch: 6 | iteration: 192/262 | Loss: 0.6404446959495544Training Epoch: 6 | iteration: 193/262 | Loss: 0.6287050247192383Training Epoch: 6 | iteration: 194/262 | Loss: 0.6069357395172119Training Epoch: 6 | iteration: 195/262 | Loss: 0.6167280077934265Training Epoch: 6 | iteration: 196/262 | Loss: 0.5704613924026489Training Epoch: 6 | iteration: 197/262 | Loss: 0.6189148426055908Training Epoch: 6 | iteration: 198/262 | Loss: 0.6011170148849487Training Epoch: 6 | iteration: 199/262 | Loss: 0.5874062776565552Training Epoch: 6 | iteration: 200/262 | Loss: 0.650752067565918Training Epoch: 6 | iteration: 201/262 | Loss: 0.558639407157898Training Epoch: 6 | iteration: 202/262 | Loss: 0.6158827543258667Training Epoch: 6 | iteration: 203/262 | Loss: 0.5918586850166321Training Epoch: 6 | iteration: 204/262 | Loss: 0.670157253742218Training Epoch: 6 | iteration: 205/262 | Loss: 0.6165226697921753Training Epoch: 6 | iteration: 206/262 | Loss: 0.6297093629837036Training Epoch: 6 | iteration: 207/262 | Loss: 0.6161800622940063Training Epoch: 6 | iteration: 208/262 | Loss: 0.6252787113189697Training Epoch: 6 | iteration: 209/262 | Loss: 0.586362361907959Training Epoch: 6 | iteration: 210/262 | Loss: 0.5977076292037964Training Epoch: 6 | iteration: 211/262 | Loss: 0.5858315229415894Training Epoch: 6 | iteration: 212/262 | Loss: 0.6133263111114502Training Epoch: 6 | iteration: 213/262 | Loss: 0.6407104730606079Training Epoch: 6 | iteration: 214/262 | Loss: 0.5924203395843506Training Epoch: 6 | iteration: 215/262 | Loss: 0.568699836730957Training Epoch: 6 | iteration: 216/262 | Loss: 0.5984419584274292Training Epoch: 6 | iteration: 217/262 | Loss: 0.6625989079475403Training Epoch: 6 | iteration: 218/262 | Loss: 0.5316170454025269Training Epoch: 6 | iteration: 219/262 | Loss: 0.6193075180053711Training Epoch: 6 | iteration: 220/262 | Loss: 0.5901027321815491Training Epoch: 6 | iteration: 221/262 | Loss: 0.6217218041419983Training Epoch: 6 | iteration: 222/262 | Loss: 0.617077112197876Training Epoch: 6 | iteration: 223/262 | Loss: 0.6200340390205383Training Epoch: 6 | iteration: 224/262 | Loss: 0.597567081451416Training Epoch: 6 | iteration: 225/262 | Loss: 0.6488642692565918Training Epoch: 6 | iteration: 226/262 | Loss: 0.66943359375Training Epoch: 6 | iteration: 227/262 | Loss: 0.5626794099807739Training Epoch: 6 | iteration: 228/262 | Loss: 0.5787753462791443Training Epoch: 6 | iteration: 229/262 | Loss: 0.5970641374588013Training Epoch: 6 | iteration: 230/262 | Loss: 0.5706219673156738Training Epoch: 6 | iteration: 231/262 | Loss: 0.6255502700805664Training Epoch: 6 | iteration: 232/262 | Loss: 0.6549299955368042Training Epoch: 6 | iteration: 233/262 | Loss: 0.6330447196960449Training Epoch: 6 | iteration: 234/262 | Loss: 0.6167682409286499Training Epoch: 6 | iteration: 235/262 | Loss: 0.6183509826660156Training Epoch: 6 | iteration: 236/262 | Loss: 0.6550930142402649Training Epoch: 6 | iteration: 237/262 | Loss: 0.5852938890457153Training Epoch: 6 | iteration: 238/262 | Loss: 0.6250085830688477Training Epoch: 6 | iteration: 239/262 | Loss: 0.5503672957420349Training Epoch: 6 | iteration: 240/262 | Loss: 0.6606365442276001Training Epoch: 6 | iteration: 241/262 | Loss: 0.6382860541343689Training Epoch: 6 | iteration: 242/262 | Loss: 0.7414177060127258Training Epoch: 6 | iteration: 243/262 | Loss: 0.5302222967147827Training Epoch: 6 | iteration: 244/262 | Loss: 0.6298820972442627Training Epoch: 6 | iteration: 245/262 | Loss: 0.6362932920455933Training Epoch: 6 | iteration: 246/262 | Loss: 0.5875102281570435Training Epoch: 6 | iteration: 247/262 | Loss: 0.6104376912117004Training Epoch: 6 | iteration: 248/262 | Loss: 0.5790662169456482Training Epoch: 6 | iteration: 249/262 | Loss: 0.7082637548446655Training Epoch: 6 | iteration: 250/262 | Loss: 0.6774884462356567Training Epoch: 6 | iteration: 251/262 | Loss: 0.5593357086181641Training Epoch: 6 | iteration: 252/262 | Loss: 0.6518242359161377Training Epoch: 6 | iteration: 253/262 | Loss: 0.7092150449752808Training Epoch: 6 | iteration: 254/262 | Loss: 0.5906566977500916Training Epoch: 6 | iteration: 255/262 | Loss: 0.6297670602798462Training Epoch: 6 | iteration: 256/262 | Loss: 0.6437551379203796Training Epoch: 6 | iteration: 257/262 | Loss: 0.6014662981033325Training Epoch: 6 | iteration: 258/262 | Loss: 0.6484769582748413Training Epoch: 6 | iteration: 259/262 | Loss: 0.627632737159729Training Epoch: 6 | iteration: 260/262 | Loss: 0.6696872115135193Training Epoch: 6 | iteration: 261/262 | Loss: 0.6407680511474609Validating Epoch: 6 | iteration: 0/66 | Loss: 0.5966298580169678Validating Epoch: 6 | iteration: 1/66 | Loss: 0.6264830827713013Validating Epoch: 6 | iteration: 2/66 | Loss: 0.6891095042228699Validating Epoch: 6 | iteration: 3/66 | Loss: 0.6241191029548645Validating Epoch: 6 | iteration: 4/66 | Loss: 0.6109195947647095Validating Epoch: 6 | iteration: 5/66 | Loss: 0.6428648233413696Validating Epoch: 6 | iteration: 6/66 | Loss: 0.7043806314468384Validating Epoch: 6 | iteration: 7/66 | Loss: 0.6767699122428894Validating Epoch: 6 | iteration: 8/66 | Loss: 0.6325675845146179Validating Epoch: 6 | iteration: 9/66 | Loss: 0.6010710000991821Validating Epoch: 6 | iteration: 10/66 | Loss: 0.6678025126457214Validating Epoch: 6 | iteration: 11/66 | Loss: 0.6748976707458496Validating Epoch: 6 | iteration: 12/66 | Loss: 0.6307647824287415Validating Epoch: 6 | iteration: 13/66 | Loss: 0.59987473487854Validating Epoch: 6 | iteration: 14/66 | Loss: 0.6153199672698975Validating Epoch: 6 | iteration: 15/66 | Loss: 0.729694128036499Validating Epoch: 6 | iteration: 16/66 | Loss: 0.7173668146133423Validating Epoch: 6 | iteration: 17/66 | Loss: 0.7640647888183594Validating Epoch: 6 | iteration: 18/66 | Loss: 0.6857335567474365Validating Epoch: 6 | iteration: 19/66 | Loss: 0.6226390600204468Validating Epoch: 6 | iteration: 20/66 | Loss: 0.6320499181747437Validating Epoch: 6 | iteration: 21/66 | Loss: 0.636690080165863Validating Epoch: 6 | iteration: 22/66 | Loss: 0.6278300285339355Validating Epoch: 6 | iteration: 23/66 | Loss: 0.6239391565322876Validating Epoch: 6 | iteration: 24/66 | Loss: 0.6702570915222168Validating Epoch: 6 | iteration: 25/66 | Loss: 0.6794685125350952Validating Epoch: 6 | iteration: 26/66 | Loss: 0.6027959585189819Validating Epoch: 6 | iteration: 27/66 | Loss: 0.6102215647697449Validating Epoch: 6 | iteration: 28/66 | Loss: 0.670213520526886Validating Epoch: 6 | iteration: 29/66 | Loss: 0.7012214660644531Validating Epoch: 6 | iteration: 30/66 | Loss: 0.7093460559844971Validating Epoch: 6 | iteration: 31/66 | Loss: 0.6696677207946777Validating Epoch: 6 | iteration: 32/66 | Loss: 0.6690328121185303Validating Epoch: 6 | iteration: 33/66 | Loss: 0.7656633853912354Validating Epoch: 6 | iteration: 34/66 | Loss: 0.7118372321128845Validating Epoch: 6 | iteration: 35/66 | Loss: 0.6477024555206299Validating Epoch: 6 | iteration: 36/66 | Loss: 0.6940670013427734Validating Epoch: 6 | iteration: 37/66 | Loss: 0.6715147495269775Validating Epoch: 6 | iteration: 38/66 | Loss: 0.7405322790145874Validating Epoch: 6 | iteration: 39/66 | Loss: 0.6772338151931763Validating Epoch: 6 | iteration: 40/66 | Loss: 0.7100867033004761Validating Epoch: 6 | iteration: 41/66 | Loss: 0.690373420715332Validating Epoch: 6 | iteration: 42/66 | Loss: 0.6991060972213745Validating Epoch: 6 | iteration: 43/66 | Loss: 0.7072863578796387Validating Epoch: 6 | iteration: 44/66 | Loss: 0.6827453374862671Validating Epoch: 6 | iteration: 45/66 | Loss: 0.617332398891449Validating Epoch: 6 | iteration: 46/66 | Loss: 0.6634559035301208Validating Epoch: 6 | iteration: 47/66 | Loss: 0.686611533164978Validating Epoch: 6 | iteration: 48/66 | Loss: 0.6115665435791016Validating Epoch: 6 | iteration: 49/66 | Loss: 0.5682018995285034Validating Epoch: 6 | iteration: 50/66 | Loss: 0.735318660736084Validating Epoch: 6 | iteration: 51/66 | Loss: 0.6315534114837646Validating Epoch: 6 | iteration: 52/66 | Loss: 0.6550095677375793Validating Epoch: 6 | iteration: 53/66 | Loss: 0.6880937814712524Validating Epoch: 6 | iteration: 54/66 | Loss: 0.6585153937339783Validating Epoch: 6 | iteration: 55/66 | Loss: 0.7029861211776733Validating Epoch: 6 | iteration: 56/66 | Loss: 0.6179549098014832Validating Epoch: 6 | iteration: 57/66 | Loss: 0.6604584455490112Validating Epoch: 6 | iteration: 58/66 | Loss: 0.6378017663955688Validating Epoch: 6 | iteration: 59/66 | Loss: 0.580668568611145Validating Epoch: 6 | iteration: 60/66 | Loss: 0.6377341151237488Validating Epoch: 6 | iteration: 61/66 | Loss: 0.6650558710098267Validating Epoch: 6 | iteration: 62/66 | Loss: 0.6832816004753113Validating Epoch: 6 | iteration: 63/66 | Loss: 0.7496322989463806Validating Epoch: 6 | iteration: 64/66 | Loss: 0.626203179359436Validating Epoch: 6 | iteration: 65/66 | Loss: 0.5197561979293823Temperature:  0.5
Sampling: iteration: 0/51{'Validity': 0.9833984375, 'Novelty': 1.0, 'Uniqueness': 0.9950347567030785}
Training Epoch: 7 | iteration: 0/262 | Loss: 0.624719500541687Training Epoch: 7 | iteration: 1/262 | Loss: 0.6054126620292664Training Epoch: 7 | iteration: 2/262 | Loss: 0.6079960465431213Training Epoch: 7 | iteration: 3/262 | Loss: 0.6168314218521118Training Epoch: 7 | iteration: 4/262 | Loss: 0.6376147270202637Training Epoch: 7 | iteration: 5/262 | Loss: 0.5634702444076538Training Epoch: 7 | iteration: 6/262 | Loss: 0.5647733807563782Training Epoch: 7 | iteration: 7/262 | Loss: 0.5730860829353333Training Epoch: 7 | iteration: 8/262 | Loss: 0.6343502998352051Training Epoch: 7 | iteration: 9/262 | Loss: 0.5689327716827393Training Epoch: 7 | iteration: 10/262 | Loss: 0.6127830147743225Training Epoch: 7 | iteration: 11/262 | Loss: 0.6144167184829712Training Epoch: 7 | iteration: 12/262 | Loss: 0.5857000350952148Training Epoch: 7 | iteration: 13/262 | Loss: 0.6245770454406738Training Epoch: 7 | iteration: 14/262 | Loss: 0.5917519927024841Training Epoch: 7 | iteration: 15/262 | Loss: 0.5738520622253418Training Epoch: 7 | iteration: 16/262 | Loss: 0.5672824382781982Training Epoch: 7 | iteration: 17/262 | Loss: 0.5880902409553528Training Epoch: 7 | iteration: 18/262 | Loss: 0.5593630075454712Training Epoch: 7 | iteration: 19/262 | Loss: 0.6381033658981323Training Epoch: 7 | iteration: 20/262 | Loss: 0.5391186475753784Training Epoch: 7 | iteration: 21/262 | Loss: 0.5590276122093201Training Epoch: 7 | iteration: 22/262 | Loss: 0.5939193964004517Training Epoch: 7 | iteration: 23/262 | Loss: 0.6746517419815063Training Epoch: 7 | iteration: 24/262 | Loss: 0.6640650033950806Training Epoch: 7 | iteration: 25/262 | Loss: 0.6451348066329956Training Epoch: 7 | iteration: 26/262 | Loss: 0.5239722728729248Training Epoch: 7 | iteration: 27/262 | Loss: 0.6582468152046204Training Epoch: 7 | iteration: 28/262 | Loss: 0.5983276963233948Training Epoch: 7 | iteration: 29/262 | Loss: 0.6064387559890747Training Epoch: 7 | iteration: 30/262 | Loss: 0.6696826219558716Training Epoch: 7 | iteration: 31/262 | Loss: 0.6225368976593018Training Epoch: 7 | iteration: 32/262 | Loss: 0.6291060447692871Training Epoch: 7 | iteration: 33/262 | Loss: 0.6008186936378479Training Epoch: 7 | iteration: 34/262 | Loss: 0.5963913202285767Training Epoch: 7 | iteration: 35/262 | Loss: 0.5924098491668701Training Epoch: 7 | iteration: 36/262 | Loss: 0.5872044563293457Training Epoch: 7 | iteration: 37/262 | Loss: 0.5592300891876221Training Epoch: 7 | iteration: 38/262 | Loss: 0.54432213306427Training Epoch: 7 | iteration: 39/262 | Loss: 0.5916045904159546Training Epoch: 7 | iteration: 40/262 | Loss: 0.5909401178359985Training Epoch: 7 | iteration: 41/262 | Loss: 0.6293363571166992Training Epoch: 7 | iteration: 42/262 | Loss: 0.5428696274757385Training Epoch: 7 | iteration: 43/262 | Loss: 0.5620095133781433Training Epoch: 7 | iteration: 44/262 | Loss: 0.5933086276054382Training Epoch: 7 | iteration: 45/262 | Loss: 0.5941308736801147Training Epoch: 7 | iteration: 46/262 | Loss: 0.5777133703231812Training Epoch: 7 | iteration: 47/262 | Loss: 0.594223141670227Training Epoch: 7 | iteration: 48/262 | Loss: 0.6210480332374573Training Epoch: 7 | iteration: 49/262 | Loss: 0.5510225892066956Training Epoch: 7 | iteration: 50/262 | Loss: 0.6055063605308533Training Epoch: 7 | iteration: 51/262 | Loss: 0.5507540106773376Training Epoch: 7 | iteration: 52/262 | Loss: 0.6112144589424133Training Epoch: 7 | iteration: 53/262 | Loss: 0.594048023223877Training Epoch: 7 | iteration: 54/262 | Loss: 0.562373161315918Training Epoch: 7 | iteration: 55/262 | Loss: 0.6087876558303833Training Epoch: 7 | iteration: 56/262 | Loss: 0.6206376552581787Training Epoch: 7 | iteration: 57/262 | Loss: 0.6060733795166016Training Epoch: 7 | iteration: 58/262 | Loss: 0.5946030616760254Training Epoch: 7 | iteration: 59/262 | Loss: 0.6048597693443298Training Epoch: 7 | iteration: 60/262 | Loss: 0.6171393394470215Training Epoch: 7 | iteration: 61/262 | Loss: 0.5629151463508606Training Epoch: 7 | iteration: 62/262 | Loss: 0.5662789344787598Training Epoch: 7 | iteration: 63/262 | Loss: 0.6114938259124756Training Epoch: 7 | iteration: 64/262 | Loss: 0.5890741348266602Training Epoch: 7 | iteration: 65/262 | Loss: 0.5961169004440308Training Epoch: 7 | iteration: 66/262 | Loss: 0.5790708661079407Training Epoch: 7 | iteration: 67/262 | Loss: 0.5837052464485168Training Epoch: 7 | iteration: 68/262 | Loss: 0.5637242794036865Training Epoch: 7 | iteration: 69/262 | Loss: 0.6019985675811768Training Epoch: 7 | iteration: 70/262 | Loss: 0.6159929037094116Training Epoch: 7 | iteration: 71/262 | Loss: 0.6502618789672852Training Epoch: 7 | iteration: 72/262 | Loss: 0.5140849947929382Training Epoch: 7 | iteration: 73/262 | Loss: 0.6148257851600647Training Epoch: 7 | iteration: 74/262 | Loss: 0.6281037330627441Training Epoch: 7 | iteration: 75/262 | Loss: 0.5709097385406494Training Epoch: 7 | iteration: 76/262 | Loss: 0.5906498432159424Training Epoch: 7 | iteration: 77/262 | Loss: 0.5917645692825317Training Epoch: 7 | iteration: 78/262 | Loss: 0.6205303072929382Training Epoch: 7 | iteration: 79/262 | Loss: 0.5159229040145874Training Epoch: 7 | iteration: 80/262 | Loss: 0.5940581560134888Training Epoch: 7 | iteration: 81/262 | Loss: 0.6051788330078125Training Epoch: 7 | iteration: 82/262 | Loss: 0.5720330476760864Training Epoch: 7 | iteration: 83/262 | Loss: 0.5610169172286987Training Epoch: 7 | iteration: 84/262 | Loss: 0.625251293182373Training Epoch: 7 | iteration: 85/262 | Loss: 0.5750229358673096Training Epoch: 7 | iteration: 86/262 | Loss: 0.6332285404205322Training Epoch: 7 | iteration: 87/262 | Loss: 0.5797591805458069Training Epoch: 7 | iteration: 88/262 | Loss: 0.5620086193084717Training Epoch: 7 | iteration: 89/262 | Loss: 0.5537860989570618Training Epoch: 7 | iteration: 90/262 | Loss: 0.5608439445495605Training Epoch: 7 | iteration: 91/262 | Loss: 0.5726298093795776Training Epoch: 7 | iteration: 92/262 | Loss: 0.6015682220458984Training Epoch: 7 | iteration: 93/262 | Loss: 0.6203325986862183Training Epoch: 7 | iteration: 94/262 | Loss: 0.5551384687423706Training Epoch: 7 | iteration: 95/262 | Loss: 0.6771573424339294Training Epoch: 7 | iteration: 96/262 | Loss: 0.5716859102249146Training Epoch: 7 | iteration: 97/262 | Loss: 0.6039206981658936Training Epoch: 7 | iteration: 98/262 | Loss: 0.5870570540428162Training Epoch: 7 | iteration: 99/262 | Loss: 0.5583252310752869Training Epoch: 7 | iteration: 100/262 | Loss: 0.5866453647613525Training Epoch: 7 | iteration: 101/262 | Loss: 0.6786075830459595Training Epoch: 7 | iteration: 102/262 | Loss: 0.5181218385696411Training Epoch: 7 | iteration: 103/262 | Loss: 0.632738471031189Training Epoch: 7 | iteration: 104/262 | Loss: 0.5679614543914795Training Epoch: 7 | iteration: 105/262 | Loss: 0.5868459939956665Training Epoch: 7 | iteration: 106/262 | Loss: 0.5256557464599609Training Epoch: 7 | iteration: 107/262 | Loss: 0.615841269493103Training Epoch: 7 | iteration: 108/262 | Loss: 0.6095297336578369Training Epoch: 7 | iteration: 109/262 | Loss: 0.5767160654067993Training Epoch: 7 | iteration: 110/262 | Loss: 0.5923304557800293Training Epoch: 7 | iteration: 111/262 | Loss: 0.6014400720596313Training Epoch: 7 | iteration: 112/262 | Loss: 0.5818620920181274Training Epoch: 7 | iteration: 113/262 | Loss: 0.6015334725379944Training Epoch: 7 | iteration: 114/262 | Loss: 0.5852527618408203Training Epoch: 7 | iteration: 115/262 | Loss: 0.5703383684158325Training Epoch: 7 | iteration: 116/262 | Loss: 0.6361794471740723Training Epoch: 7 | iteration: 117/262 | Loss: 0.5991108417510986Training Epoch: 7 | iteration: 118/262 | Loss: 0.6047693490982056Training Epoch: 7 | iteration: 119/262 | Loss: 0.640514612197876Training Epoch: 7 | iteration: 120/262 | Loss: 0.6554780006408691Training Epoch: 7 | iteration: 121/262 | Loss: 0.6513745784759521Training Epoch: 7 | iteration: 122/262 | Loss: 0.5434291362762451Training Epoch: 7 | iteration: 123/262 | Loss: 0.5837618708610535Training Epoch: 7 | iteration: 124/262 | Loss: 0.5721149444580078Training Epoch: 7 | iteration: 125/262 | Loss: 0.5796167850494385Training Epoch: 7 | iteration: 126/262 | Loss: 0.5606434345245361Training Epoch: 7 | iteration: 127/262 | Loss: 0.6518636345863342Training Epoch: 7 | iteration: 128/262 | Loss: 0.6100435256958008Training Epoch: 7 | iteration: 129/262 | Loss: 0.6165958642959595Training Epoch: 7 | iteration: 130/262 | Loss: 0.5636616945266724Training Epoch: 7 | iteration: 131/262 | Loss: 0.5967618823051453Training Epoch: 7 | iteration: 132/262 | Loss: 0.6599136590957642Training Epoch: 7 | iteration: 133/262 | Loss: 0.5919455289840698Training Epoch: 7 | iteration: 134/262 | Loss: 0.5588153004646301Training Epoch: 7 | iteration: 135/262 | Loss: 0.6078726053237915Training Epoch: 7 | iteration: 136/262 | Loss: 0.6278692483901978Training Epoch: 7 | iteration: 137/262 | Loss: 0.6105003356933594Training Epoch: 7 | iteration: 138/262 | Loss: 0.6147807836532593Training Epoch: 7 | iteration: 139/262 | Loss: 0.5808577537536621Training Epoch: 7 | iteration: 140/262 | Loss: 0.6201694011688232Training Epoch: 7 | iteration: 141/262 | Loss: 0.6151058077812195Training Epoch: 7 | iteration: 142/262 | Loss: 0.6099876165390015Training Epoch: 7 | iteration: 143/262 | Loss: 0.5934418439865112Training Epoch: 7 | iteration: 144/262 | Loss: 0.5785472989082336Training Epoch: 7 | iteration: 145/262 | Loss: 0.6557842493057251Training Epoch: 7 | iteration: 146/262 | Loss: 0.5478585362434387Training Epoch: 7 | iteration: 147/262 | Loss: 0.5345718264579773Training Epoch: 7 | iteration: 148/262 | Loss: 0.5898075103759766Training Epoch: 7 | iteration: 149/262 | Loss: 0.5900036692619324Training Epoch: 7 | iteration: 150/262 | Loss: 0.6047943830490112Training Epoch: 7 | iteration: 151/262 | Loss: 0.6593250632286072Training Epoch: 7 | iteration: 152/262 | Loss: 0.5621657967567444Training Epoch: 7 | iteration: 153/262 | Loss: 0.6069132089614868Training Epoch: 7 | iteration: 154/262 | Loss: 0.5542874336242676Training Epoch: 7 | iteration: 155/262 | Loss: 0.6309843063354492Training Epoch: 7 | iteration: 156/262 | Loss: 0.6109315156936646Training Epoch: 7 | iteration: 157/262 | Loss: 0.5398936867713928Training Epoch: 7 | iteration: 158/262 | Loss: 0.5884369611740112Training Epoch: 7 | iteration: 159/262 | Loss: 0.6135379076004028Training Epoch: 7 | iteration: 160/262 | Loss: 0.5955812931060791Training Epoch: 7 | iteration: 161/262 | Loss: 0.6063404083251953Training Epoch: 7 | iteration: 162/262 | Loss: 0.6017621159553528Training Epoch: 7 | iteration: 163/262 | Loss: 0.6450401544570923Training Epoch: 7 | iteration: 164/262 | Loss: 0.6119842529296875Training Epoch: 7 | iteration: 165/262 | Loss: 0.5616930723190308Training Epoch: 7 | iteration: 166/262 | Loss: 0.6173908114433289Training Epoch: 7 | iteration: 167/262 | Loss: 0.5887249708175659Training Epoch: 7 | iteration: 168/262 | Loss: 0.7222925424575806Training Epoch: 7 | iteration: 169/262 | Loss: 0.6162511110305786Training Epoch: 7 | iteration: 170/262 | Loss: 0.6193927526473999Training Epoch: 7 | iteration: 171/262 | Loss: 0.5406244993209839Training Epoch: 7 | iteration: 172/262 | Loss: 0.5969797372817993Training Epoch: 7 | iteration: 173/262 | Loss: 0.6005802154541016Training Epoch: 7 | iteration: 174/262 | Loss: 0.5945826172828674Training Epoch: 7 | iteration: 175/262 | Loss: 0.6503380537033081Training Epoch: 7 | iteration: 176/262 | Loss: 0.5876622200012207Training Epoch: 7 | iteration: 177/262 | Loss: 0.61104416847229Training Epoch: 7 | iteration: 178/262 | Loss: 0.6461931467056274Training Epoch: 7 | iteration: 179/262 | Loss: 0.554955244064331Training Epoch: 7 | iteration: 180/262 | Loss: 0.5530109405517578Training Epoch: 7 | iteration: 181/262 | Loss: 0.5840359330177307Training Epoch: 7 | iteration: 182/262 | Loss: 0.6674624681472778Training Epoch: 7 | iteration: 183/262 | Loss: 0.6524398922920227Training Epoch: 7 | iteration: 184/262 | Loss: 0.5966718792915344Training Epoch: 7 | iteration: 185/262 | Loss: 0.5799457430839539Training Epoch: 7 | iteration: 186/262 | Loss: 0.5990434288978577Training Epoch: 7 | iteration: 187/262 | Loss: 0.6232004165649414Training Epoch: 7 | iteration: 188/262 | Loss: 0.5685487389564514Training Epoch: 7 | iteration: 189/262 | Loss: 0.5734447240829468Training Epoch: 7 | iteration: 190/262 | Loss: 0.6386133432388306Training Epoch: 7 | iteration: 191/262 | Loss: 0.604417085647583Training Epoch: 7 | iteration: 192/262 | Loss: 0.5401368141174316Training Epoch: 7 | iteration: 193/262 | Loss: 0.6404666304588318Training Epoch: 7 | iteration: 194/262 | Loss: 0.6414369940757751Training Epoch: 7 | iteration: 195/262 | Loss: 0.5668383836746216Training Epoch: 7 | iteration: 196/262 | Loss: 0.6168868541717529Training Epoch: 7 | iteration: 197/262 | Loss: 0.6156667470932007Training Epoch: 7 | iteration: 198/262 | Loss: 0.5827438831329346Training Epoch: 7 | iteration: 199/262 | Loss: 0.6105444431304932Training Epoch: 7 | iteration: 200/262 | Loss: 0.6437629461288452Training Epoch: 7 | iteration: 201/262 | Loss: 0.5639716386795044Training Epoch: 7 | iteration: 202/262 | Loss: 0.5829465389251709Training Epoch: 7 | iteration: 203/262 | Loss: 0.5594780445098877Training Epoch: 7 | iteration: 204/262 | Loss: 0.5620021820068359Training Epoch: 7 | iteration: 205/262 | Loss: 0.6136713027954102Training Epoch: 7 | iteration: 206/262 | Loss: 0.642299473285675Training Epoch: 7 | iteration: 207/262 | Loss: 0.7061251997947693Training Epoch: 7 | iteration: 208/262 | Loss: 0.5729888677597046Training Epoch: 7 | iteration: 209/262 | Loss: 0.6437437534332275Training Epoch: 7 | iteration: 210/262 | Loss: 0.6026451587677002Training Epoch: 7 | iteration: 211/262 | Loss: 0.6270486116409302Training Epoch: 7 | iteration: 212/262 | Loss: 0.6369366645812988Training Epoch: 7 | iteration: 213/262 | Loss: 0.5607172250747681Training Epoch: 7 | iteration: 214/262 | Loss: 0.6700881719589233Training Epoch: 7 | iteration: 215/262 | Loss: 0.5973324775695801Training Epoch: 7 | iteration: 216/262 | Loss: 0.6633328199386597Training Epoch: 7 | iteration: 217/262 | Loss: 0.580441415309906Training Epoch: 7 | iteration: 218/262 | Loss: 0.6489670872688293Training Epoch: 7 | iteration: 219/262 | Loss: 0.6173362135887146Training Epoch: 7 | iteration: 220/262 | Loss: 0.5487692356109619Training Epoch: 7 | iteration: 221/262 | Loss: 0.5959895849227905Training Epoch: 7 | iteration: 222/262 | Loss: 0.659950852394104Training Epoch: 7 | iteration: 223/262 | Loss: 0.5927078723907471Training Epoch: 7 | iteration: 224/262 | Loss: 0.5876221656799316Training Epoch: 7 | iteration: 225/262 | Loss: 0.569038450717926Training Epoch: 7 | iteration: 226/262 | Loss: 0.5929027795791626Training Epoch: 7 | iteration: 227/262 | Loss: 0.612440824508667Training Epoch: 7 | iteration: 228/262 | Loss: 0.5604555606842041Training Epoch: 7 | iteration: 229/262 | Loss: 0.6411914825439453Training Epoch: 7 | iteration: 230/262 | Loss: 0.5666253566741943Training Epoch: 7 | iteration: 231/262 | Loss: 0.5937888622283936Training Epoch: 7 | iteration: 232/262 | Loss: 0.5844520330429077Training Epoch: 7 | iteration: 233/262 | Loss: 0.5782157182693481Training Epoch: 7 | iteration: 234/262 | Loss: 0.6348258256912231Training Epoch: 7 | iteration: 235/262 | Loss: 0.5695914030075073Training Epoch: 7 | iteration: 236/262 | Loss: 0.5645939111709595Training Epoch: 7 | iteration: 237/262 | Loss: 0.5616081953048706Training Epoch: 7 | iteration: 238/262 | Loss: 0.5724124908447266Training Epoch: 7 | iteration: 239/262 | Loss: 0.6116421818733215Training Epoch: 7 | iteration: 240/262 | Loss: 0.6284607648849487Training Epoch: 7 | iteration: 241/262 | Loss: 0.6008214950561523Training Epoch: 7 | iteration: 242/262 | Loss: 0.6200479865074158Training Epoch: 7 | iteration: 243/262 | Loss: 0.5785195827484131Training Epoch: 7 | iteration: 244/262 | Loss: 0.5579541921615601Training Epoch: 7 | iteration: 245/262 | Loss: 0.5851351022720337Training Epoch: 7 | iteration: 246/262 | Loss: 0.6274349689483643Training Epoch: 7 | iteration: 247/262 | Loss: 0.6304379105567932Training Epoch: 7 | iteration: 248/262 | Loss: 0.5770600438117981Training Epoch: 7 | iteration: 249/262 | Loss: 0.6050459146499634Training Epoch: 7 | iteration: 250/262 | Loss: 0.5933787226676941Training Epoch: 7 | iteration: 251/262 | Loss: 0.5586017370223999Training Epoch: 7 | iteration: 252/262 | Loss: 0.6051561832427979Training Epoch: 7 | iteration: 253/262 | Loss: 0.580655574798584Training Epoch: 7 | iteration: 254/262 | Loss: 0.640791118144989Training Epoch: 7 | iteration: 255/262 | Loss: 0.5796001553535461Training Epoch: 7 | iteration: 256/262 | Loss: 0.605683445930481Training Epoch: 7 | iteration: 257/262 | Loss: 0.6031674146652222Training Epoch: 7 | iteration: 258/262 | Loss: 0.6096821427345276Training Epoch: 7 | iteration: 259/262 | Loss: 0.6630815863609314Training Epoch: 7 | iteration: 260/262 | Loss: 0.6123789548873901Training Epoch: 7 | iteration: 261/262 | Loss: 0.5969308018684387Validating Epoch: 7 | iteration: 0/66 | Loss: 0.7758857607841492Validating Epoch: 7 | iteration: 1/66 | Loss: 0.6438555717468262Validating Epoch: 7 | iteration: 2/66 | Loss: 0.6858056783676147Validating Epoch: 7 | iteration: 3/66 | Loss: 0.7176699638366699Validating Epoch: 7 | iteration: 4/66 | Loss: 0.6024105548858643Validating Epoch: 7 | iteration: 5/66 | Loss: 0.629465639591217Validating Epoch: 7 | iteration: 6/66 | Loss: 0.6617430448532104Validating Epoch: 7 | iteration: 7/66 | Loss: 0.662345290184021Validating Epoch: 7 | iteration: 8/66 | Loss: 0.6445150375366211Validating Epoch: 7 | iteration: 9/66 | Loss: 0.6386560201644897Validating Epoch: 7 | iteration: 10/66 | Loss: 0.6465261578559875Validating Epoch: 7 | iteration: 11/66 | Loss: 0.6491126418113708Validating Epoch: 7 | iteration: 12/66 | Loss: 0.5599634647369385Validating Epoch: 7 | iteration: 13/66 | Loss: 0.6875025033950806Validating Epoch: 7 | iteration: 14/66 | Loss: 0.6352066397666931Validating Epoch: 7 | iteration: 15/66 | Loss: 0.6967495679855347Validating Epoch: 7 | iteration: 16/66 | Loss: 0.617389440536499Validating Epoch: 7 | iteration: 17/66 | Loss: 0.6622397303581238Validating Epoch: 7 | iteration: 18/66 | Loss: 0.7183362245559692Validating Epoch: 7 | iteration: 19/66 | Loss: 0.7529557347297668Validating Epoch: 7 | iteration: 20/66 | Loss: 0.7265582084655762Validating Epoch: 7 | iteration: 21/66 | Loss: 0.6036881804466248Validating Epoch: 7 | iteration: 22/66 | Loss: 0.7189877033233643Validating Epoch: 7 | iteration: 23/66 | Loss: 0.7119759321212769Validating Epoch: 7 | iteration: 24/66 | Loss: 0.6549568176269531Validating Epoch: 7 | iteration: 25/66 | Loss: 0.6791467666625977Validating Epoch: 7 | iteration: 26/66 | Loss: 0.6255365610122681Validating Epoch: 7 | iteration: 27/66 | Loss: 0.61757493019104Validating Epoch: 7 | iteration: 28/66 | Loss: 0.6546972393989563Validating Epoch: 7 | iteration: 29/66 | Loss: 0.6798906326293945Validating Epoch: 7 | iteration: 30/66 | Loss: 0.6964836120605469Validating Epoch: 7 | iteration: 31/66 | Loss: 0.672407865524292Validating Epoch: 7 | iteration: 32/66 | Loss: 0.6889070272445679Validating Epoch: 7 | iteration: 33/66 | Loss: 0.6654634475708008Validating Epoch: 7 | iteration: 34/66 | Loss: 0.7233230471611023Validating Epoch: 7 | iteration: 35/66 | Loss: 0.6232933402061462Validating Epoch: 7 | iteration: 36/66 | Loss: 0.6663573384284973Validating Epoch: 7 | iteration: 37/66 | Loss: 0.6700841188430786Validating Epoch: 7 | iteration: 38/66 | Loss: 0.6764687299728394Validating Epoch: 7 | iteration: 39/66 | Loss: 0.5947152972221375Validating Epoch: 7 | iteration: 40/66 | Loss: 0.6354703903198242Validating Epoch: 7 | iteration: 41/66 | Loss: 0.7241098880767822Validating Epoch: 7 | iteration: 42/66 | Loss: 0.6710495948791504Validating Epoch: 7 | iteration: 43/66 | Loss: 0.5598864555358887Validating Epoch: 7 | iteration: 44/66 | Loss: 0.6632957458496094Validating Epoch: 7 | iteration: 45/66 | Loss: 0.7315421104431152Validating Epoch: 7 | iteration: 46/66 | Loss: 0.6771826148033142Validating Epoch: 7 | iteration: 47/66 | Loss: 0.6636627912521362Validating Epoch: 7 | iteration: 48/66 | Loss: 0.7051119804382324Validating Epoch: 7 | iteration: 49/66 | Loss: 0.6686223745346069Validating Epoch: 7 | iteration: 50/66 | Loss: 0.6897826790809631Validating Epoch: 7 | iteration: 51/66 | Loss: 0.6240048408508301Validating Epoch: 7 | iteration: 52/66 | Loss: 0.711074709892273Validating Epoch: 7 | iteration: 53/66 | Loss: 0.7204233407974243Validating Epoch: 7 | iteration: 54/66 | Loss: 0.7398732304573059Validating Epoch: 7 | iteration: 55/66 | Loss: 0.6471980214118958Validating Epoch: 7 | iteration: 56/66 | Loss: 0.7091013789176941Validating Epoch: 7 | iteration: 57/66 | Loss: 0.6600279808044434Validating Epoch: 7 | iteration: 58/66 | Loss: 0.6645933389663696Validating Epoch: 7 | iteration: 59/66 | Loss: 0.6587170958518982Validating Epoch: 7 | iteration: 60/66 | Loss: 0.6094250679016113Validating Epoch: 7 | iteration: 61/66 | Loss: 0.7276319265365601Validating Epoch: 7 | iteration: 62/66 | Loss: 0.6481483578681946Validating Epoch: 7 | iteration: 63/66 | Loss: 0.6452257633209229Validating Epoch: 7 | iteration: 64/66 | Loss: 0.6666589975357056Validating Epoch: 7 | iteration: 65/66 | Loss: 0.7032263278961182Temperature:  0.5
Sampling: iteration: 0/51{'Validity': 0.982421875, 'Novelty': 1.0, 'Uniqueness': 0.9930417495029821}
Training Epoch: 8 | iteration: 0/262 | Loss: 0.6046195030212402Training Epoch: 8 | iteration: 1/262 | Loss: 0.5776865482330322Training Epoch: 8 | iteration: 2/262 | Loss: 0.5812832117080688Training Epoch: 8 | iteration: 3/262 | Loss: 0.5667846202850342Training Epoch: 8 | iteration: 4/262 | Loss: 0.5998015403747559Training Epoch: 8 | iteration: 5/262 | Loss: 0.5695042610168457Training Epoch: 8 | iteration: 6/262 | Loss: 0.6420044898986816Training Epoch: 8 | iteration: 7/262 | Loss: 0.5140414237976074Training Epoch: 8 | iteration: 8/262 | Loss: 0.5801165103912354Training Epoch: 8 | iteration: 9/262 | Loss: 0.617943286895752Training Epoch: 8 | iteration: 10/262 | Loss: 0.5751314163208008Training Epoch: 8 | iteration: 11/262 | Loss: 0.5665546655654907Training Epoch: 8 | iteration: 12/262 | Loss: 0.5411441922187805Training Epoch: 8 | iteration: 13/262 | Loss: 0.5284123420715332Training Epoch: 8 | iteration: 14/262 | Loss: 0.5387860536575317Training Epoch: 8 | iteration: 15/262 | Loss: 0.6014090180397034Training Epoch: 8 | iteration: 16/262 | Loss: 0.5841444730758667Training Epoch: 8 | iteration: 17/262 | Loss: 0.5410424470901489Training Epoch: 8 | iteration: 18/262 | Loss: 0.5486829280853271Training Epoch: 8 | iteration: 19/262 | Loss: 0.6066970825195312Training Epoch: 8 | iteration: 20/262 | Loss: 0.5626683235168457Training Epoch: 8 | iteration: 21/262 | Loss: 0.5582974553108215Training Epoch: 8 | iteration: 22/262 | Loss: 0.5584101676940918Training Epoch: 8 | iteration: 23/262 | Loss: 0.5628581643104553Training Epoch: 8 | iteration: 24/262 | Loss: 0.6721062064170837Training Epoch: 8 | iteration: 25/262 | Loss: 0.6124312877655029Training Epoch: 8 | iteration: 26/262 | Loss: 0.6159610748291016Training Epoch: 8 | iteration: 27/262 | Loss: 0.5527973175048828Training Epoch: 8 | iteration: 28/262 | Loss: 0.4961639642715454Training Epoch: 8 | iteration: 29/262 | Loss: 0.5795405507087708Training Epoch: 8 | iteration: 30/262 | Loss: 0.6561765670776367Training Epoch: 8 | iteration: 31/262 | Loss: 0.5763917565345764Training Epoch: 8 | iteration: 32/262 | Loss: 0.5757238864898682Training Epoch: 8 | iteration: 33/262 | Loss: 0.6018184423446655Training Epoch: 8 | iteration: 34/262 | Loss: 0.5881090760231018Training Epoch: 8 | iteration: 35/262 | Loss: 0.6019219160079956Training Epoch: 8 | iteration: 36/262 | Loss: 0.594680905342102Training Epoch: 8 | iteration: 37/262 | Loss: 0.5709646940231323Training Epoch: 8 | iteration: 38/262 | Loss: 0.5923301577568054Training Epoch: 8 | iteration: 39/262 | Loss: 0.5392531156539917Training Epoch: 8 | iteration: 40/262 | Loss: 0.6502398252487183Training Epoch: 8 | iteration: 41/262 | Loss: 0.6045478582382202Training Epoch: 8 | iteration: 42/262 | Loss: 0.5669659376144409Training Epoch: 8 | iteration: 43/262 | Loss: 0.5415117740631104Training Epoch: 8 | iteration: 44/262 | Loss: 0.5799819231033325Training Epoch: 8 | iteration: 45/262 | Loss: 0.5981875658035278Training Epoch: 8 | iteration: 46/262 | Loss: 0.5951371192932129Training Epoch: 8 | iteration: 47/262 | Loss: 0.6082659959793091Training Epoch: 8 | iteration: 48/262 | Loss: 0.5841094255447388Training Epoch: 8 | iteration: 49/262 | Loss: 0.5833572149276733Training Epoch: 8 | iteration: 50/262 | Loss: 0.6030842065811157Training Epoch: 8 | iteration: 51/262 | Loss: 0.5437183380126953Training Epoch: 8 | iteration: 52/262 | Loss: 0.5858927965164185Training Epoch: 8 | iteration: 53/262 | Loss: 0.5337876081466675Training Epoch: 8 | iteration: 54/262 | Loss: 0.5682165622711182Training Epoch: 8 | iteration: 55/262 | Loss: 0.5438873767852783Training Epoch: 8 | iteration: 56/262 | Loss: 0.5746092200279236Training Epoch: 8 | iteration: 57/262 | Loss: 0.585655689239502Training Epoch: 8 | iteration: 58/262 | Loss: 0.5810368657112122Training Epoch: 8 | iteration: 59/262 | Loss: 0.6668396592140198Training Epoch: 8 | iteration: 60/262 | Loss: 0.547340452671051Training Epoch: 8 | iteration: 61/262 | Loss: 0.5776908993721008Training Epoch: 8 | iteration: 62/262 | Loss: 0.6145982146263123Training Epoch: 8 | iteration: 63/262 | Loss: 0.535338819026947Training Epoch: 8 | iteration: 64/262 | Loss: 0.5767367482185364Training Epoch: 8 | iteration: 65/262 | Loss: 0.5822182893753052Training Epoch: 8 | iteration: 66/262 | Loss: 0.548540472984314Training Epoch: 8 | iteration: 67/262 | Loss: 0.5700657367706299Training Epoch: 8 | iteration: 68/262 | Loss: 0.6040858030319214Training Epoch: 8 | iteration: 69/262 | Loss: 0.574184775352478Training Epoch: 8 | iteration: 70/262 | Loss: 0.6079176068305969Training Epoch: 8 | iteration: 71/262 | Loss: 0.5731178522109985Training Epoch: 8 | iteration: 72/262 | Loss: 0.5960118174552917Training Epoch: 8 | iteration: 73/262 | Loss: 0.5538707971572876Training Epoch: 8 | iteration: 74/262 | Loss: 0.6305015087127686Training Epoch: 8 | iteration: 75/262 | Loss: 0.5630679726600647Training Epoch: 8 | iteration: 76/262 | Loss: 0.5663927793502808Training Epoch: 8 | iteration: 77/262 | Loss: 0.6170929670333862Training Epoch: 8 | iteration: 78/262 | Loss: 0.5671426057815552Training Epoch: 8 | iteration: 79/262 | Loss: 0.5862013101577759Training Epoch: 8 | iteration: 80/262 | Loss: 0.5824931859970093Training Epoch: 8 | iteration: 81/262 | Loss: 0.6030133962631226Training Epoch: 8 | iteration: 82/262 | Loss: 0.5547702312469482Training Epoch: 8 | iteration: 83/262 | Loss: 0.6112445592880249Training Epoch: 8 | iteration: 84/262 | Loss: 0.5814499855041504Training Epoch: 8 | iteration: 85/262 | Loss: 0.6059525012969971Training Epoch: 8 | iteration: 86/262 | Loss: 0.6395226120948792Training Epoch: 8 | iteration: 87/262 | Loss: 0.5496898889541626Training Epoch: 8 | iteration: 88/262 | Loss: 0.5508929491043091Training Epoch: 8 | iteration: 89/262 | Loss: 0.5588467717170715Training Epoch: 8 | iteration: 90/262 | Loss: 0.5900759696960449Training Epoch: 8 | iteration: 91/262 | Loss: 0.5377119779586792Training Epoch: 8 | iteration: 92/262 | Loss: 0.6647994518280029Training Epoch: 8 | iteration: 93/262 | Loss: 0.5899346470832825Training Epoch: 8 | iteration: 94/262 | Loss: 0.5781582593917847Training Epoch: 8 | iteration: 95/262 | Loss: 0.5974148511886597Training Epoch: 8 | iteration: 96/262 | Loss: 0.6203899383544922Training Epoch: 8 | iteration: 97/262 | Loss: 0.5920605659484863Training Epoch: 8 | iteration: 98/262 | Loss: 0.5718088150024414Training Epoch: 8 | iteration: 99/262 | Loss: 0.5631211996078491Training Epoch: 8 | iteration: 100/262 | Loss: 0.5939196944236755Training Epoch: 8 | iteration: 101/262 | Loss: 0.5393300652503967Training Epoch: 8 | iteration: 102/262 | Loss: 0.5931979417800903Training Epoch: 8 | iteration: 103/262 | Loss: 0.572327733039856Training Epoch: 8 | iteration: 104/262 | Loss: 0.5719224214553833Training Epoch: 8 | iteration: 105/262 | Loss: 0.5208115577697754Training Epoch: 8 | iteration: 106/262 | Loss: 0.5858358144760132Training Epoch: 8 | iteration: 107/262 | Loss: 0.5853322744369507Training Epoch: 8 | iteration: 108/262 | Loss: 0.5931020975112915Training Epoch: 8 | iteration: 109/262 | Loss: 0.5791710615158081Training Epoch: 8 | iteration: 110/262 | Loss: 0.5235131978988647Training Epoch: 8 | iteration: 111/262 | Loss: 0.5857213735580444Training Epoch: 8 | iteration: 112/262 | Loss: 0.5549092888832092Training Epoch: 8 | iteration: 113/262 | Loss: 0.6117807030677795Training Epoch: 8 | iteration: 114/262 | Loss: 0.6240357160568237Training Epoch: 8 | iteration: 115/262 | Loss: 0.6077488660812378Training Epoch: 8 | iteration: 116/262 | Loss: 0.5986835360527039Training Epoch: 8 | iteration: 117/262 | Loss: 0.6111958026885986Training Epoch: 8 | iteration: 118/262 | Loss: 0.5631912350654602Training Epoch: 8 | iteration: 119/262 | Loss: 0.5518575310707092Training Epoch: 8 | iteration: 120/262 | Loss: 0.5907580852508545Training Epoch: 8 | iteration: 121/262 | Loss: 0.6029436588287354Training Epoch: 8 | iteration: 122/262 | Loss: 0.6336370706558228Training Epoch: 8 | iteration: 123/262 | Loss: 0.6531253457069397Training Epoch: 8 | iteration: 124/262 | Loss: 0.5648858547210693Training Epoch: 8 | iteration: 125/262 | Loss: 0.5772131681442261Training Epoch: 8 | iteration: 126/262 | Loss: 0.58852219581604Training Epoch: 8 | iteration: 127/262 | Loss: 0.5495330691337585Training Epoch: 8 | iteration: 128/262 | Loss: 0.6021701097488403Training Epoch: 8 | iteration: 129/262 | Loss: 0.5688304901123047Training Epoch: 8 | iteration: 130/262 | Loss: 0.5570764541625977Training Epoch: 8 | iteration: 131/262 | Loss: 0.5960023999214172Training Epoch: 8 | iteration: 132/262 | Loss: 0.6097452640533447Training Epoch: 8 | iteration: 133/262 | Loss: 0.5973719954490662Training Epoch: 8 | iteration: 134/262 | Loss: 0.6648890972137451Training Epoch: 8 | iteration: 135/262 | Loss: 0.5624656677246094Training Epoch: 8 | iteration: 136/262 | Loss: 0.6247462630271912Training Epoch: 8 | iteration: 137/262 | Loss: 0.5953969955444336Training Epoch: 8 | iteration: 138/262 | Loss: 0.6697781682014465Training Epoch: 8 | iteration: 139/262 | Loss: 0.6058975458145142Training Epoch: 8 | iteration: 140/262 | Loss: 0.6410824656486511Training Epoch: 8 | iteration: 141/262 | Loss: 0.5646635293960571Training Epoch: 8 | iteration: 142/262 | Loss: 0.6350960731506348Training Epoch: 8 | iteration: 143/262 | Loss: 0.558936357498169Training Epoch: 8 | iteration: 144/262 | Loss: 0.5982967615127563Training Epoch: 8 | iteration: 145/262 | Loss: 0.5747990012168884Training Epoch: 8 | iteration: 146/262 | Loss: 0.5265929102897644Training Epoch: 8 | iteration: 147/262 | Loss: 0.5902324914932251Training Epoch: 8 | iteration: 148/262 | Loss: 0.5591738224029541Training Epoch: 8 | iteration: 149/262 | Loss: 0.6531238555908203Training Epoch: 8 | iteration: 150/262 | Loss: 0.6260988712310791Training Epoch: 8 | iteration: 151/262 | Loss: 0.5505876541137695Training Epoch: 8 | iteration: 152/262 | Loss: 0.5944366455078125Training Epoch: 8 | iteration: 153/262 | Loss: 0.5569707155227661Training Epoch: 8 | iteration: 154/262 | Loss: 0.5328047275543213Training Epoch: 8 | iteration: 155/262 | Loss: 0.5663831233978271Training Epoch: 8 | iteration: 156/262 | Loss: 0.6657236814498901Training Epoch: 8 | iteration: 157/262 | Loss: 0.5600087642669678Training Epoch: 8 | iteration: 158/262 | Loss: 0.5850774049758911Training Epoch: 8 | iteration: 159/262 | Loss: 0.594532310962677Training Epoch: 8 | iteration: 160/262 | Loss: 0.6200894117355347Training Epoch: 8 | iteration: 161/262 | Loss: 0.5958815813064575Training Epoch: 8 | iteration: 162/262 | Loss: 0.5389324426651001Training Epoch: 8 | iteration: 163/262 | Loss: 0.6201400756835938Training Epoch: 8 | iteration: 164/262 | Loss: 0.5995035171508789Training Epoch: 8 | iteration: 165/262 | Loss: 0.6361548900604248Training Epoch: 8 | iteration: 166/262 | Loss: 0.6324425935745239Training Epoch: 8 | iteration: 167/262 | Loss: 0.5887643098831177Training Epoch: 8 | iteration: 168/262 | Loss: 0.5802326798439026Training Epoch: 8 | iteration: 169/262 | Loss: 0.5858001708984375Training Epoch: 8 | iteration: 170/262 | Loss: 0.5858439207077026Training Epoch: 8 | iteration: 171/262 | Loss: 0.616935133934021Training Epoch: 8 | iteration: 172/262 | Loss: 0.5604633092880249Training Epoch: 8 | iteration: 173/262 | Loss: 0.5808322429656982Training Epoch: 8 | iteration: 174/262 | Loss: 0.6437168717384338Training Epoch: 8 | iteration: 175/262 | Loss: 0.5840320587158203Training Epoch: 8 | iteration: 176/262 | Loss: 0.5776488780975342Training Epoch: 8 | iteration: 177/262 | Loss: 0.5563241243362427Training Epoch: 8 | iteration: 178/262 | Loss: 0.6364325881004333Training Epoch: 8 | iteration: 179/262 | Loss: 0.6128968000411987Training Epoch: 8 | iteration: 180/262 | Loss: 0.5975924730300903Training Epoch: 8 | iteration: 181/262 | Loss: 0.579131543636322Training Epoch: 8 | iteration: 182/262 | Loss: 0.5786228775978088Training Epoch: 8 | iteration: 183/262 | Loss: 0.6046637892723083Training Epoch: 8 | iteration: 184/262 | Loss: 0.633307933807373Training Epoch: 8 | iteration: 185/262 | Loss: 0.610206127166748Training Epoch: 8 | iteration: 186/262 | Loss: 0.5591791868209839Training Epoch: 8 | iteration: 187/262 | Loss: 0.6188305616378784Training Epoch: 8 | iteration: 188/262 | Loss: 0.6240640878677368Training Epoch: 8 | iteration: 189/262 | Loss: 0.6023697853088379Training Epoch: 8 | iteration: 190/262 | Loss: 0.6428301334381104Training Epoch: 8 | iteration: 191/262 | Loss: 0.5342584848403931Training Epoch: 8 | iteration: 192/262 | Loss: 0.5998433828353882Training Epoch: 8 | iteration: 193/262 | Loss: 0.5894116759300232Training Epoch: 8 | iteration: 194/262 | Loss: 0.578464150428772Training Epoch: 8 | iteration: 195/262 | Loss: 0.5715969800949097Training Epoch: 8 | iteration: 196/262 | Loss: 0.6028955578804016Training Epoch: 8 | iteration: 197/262 | Loss: 0.6090386509895325Training Epoch: 8 | iteration: 198/262 | Loss: 0.5462710857391357Training Epoch: 8 | iteration: 199/262 | Loss: 0.6242260932922363Training Epoch: 8 | iteration: 200/262 | Loss: 0.6155493259429932Training Epoch: 8 | iteration: 201/262 | Loss: 0.5451757907867432Training Epoch: 8 | iteration: 202/262 | Loss: 0.5802096128463745Training Epoch: 8 | iteration: 203/262 | Loss: 0.5476112365722656Training Epoch: 8 | iteration: 204/262 | Loss: 0.6301764249801636Training Epoch: 8 | iteration: 205/262 | Loss: 0.5381274819374084Training Epoch: 8 | iteration: 206/262 | Loss: 0.581570029258728Training Epoch: 8 | iteration: 207/262 | Loss: 0.5712639093399048Training Epoch: 8 | iteration: 208/262 | Loss: 0.6528393626213074Training Epoch: 8 | iteration: 209/262 | Loss: 0.631596565246582Training Epoch: 8 | iteration: 210/262 | Loss: 0.5995526313781738Training Epoch: 8 | iteration: 211/262 | Loss: 0.5466679334640503Training Epoch: 8 | iteration: 212/262 | Loss: 0.5341811180114746Training Epoch: 8 | iteration: 213/262 | Loss: 0.59367835521698Training Epoch: 8 | iteration: 214/262 | Loss: 0.5854592323303223Training Epoch: 8 | iteration: 215/262 | Loss: 0.5549845695495605Training Epoch: 8 | iteration: 216/262 | Loss: 0.5815778970718384Training Epoch: 8 | iteration: 217/262 | Loss: 0.6105222702026367Training Epoch: 8 | iteration: 218/262 | Loss: 0.5707910656929016Training Epoch: 8 | iteration: 219/262 | Loss: 0.5952820181846619Training Epoch: 8 | iteration: 220/262 | Loss: 0.5737990140914917Training Epoch: 8 | iteration: 221/262 | Loss: 0.6552466750144958Training Epoch: 8 | iteration: 222/262 | Loss: 0.6472355723381042Training Epoch: 8 | iteration: 223/262 | Loss: 0.6007719039916992Training Epoch: 8 | iteration: 224/262 | Loss: 0.5957260131835938Training Epoch: 8 | iteration: 225/262 | Loss: 0.5873327255249023Training Epoch: 8 | iteration: 226/262 | Loss: 0.6189532279968262Training Epoch: 8 | iteration: 227/262 | Loss: 0.5619816780090332Training Epoch: 8 | iteration: 228/262 | Loss: 0.5179768204689026Training Epoch: 8 | iteration: 229/262 | Loss: 0.6291027069091797Training Epoch: 8 | iteration: 230/262 | Loss: 0.5525768399238586Training Epoch: 8 | iteration: 231/262 | Loss: 0.5605120658874512Training Epoch: 8 | iteration: 232/262 | Loss: 0.5893876552581787Training Epoch: 8 | iteration: 233/262 | Loss: 0.5763156414031982Training Epoch: 8 | iteration: 234/262 | Loss: 0.5682562589645386Training Epoch: 8 | iteration: 235/262 | Loss: 0.5802301168441772Training Epoch: 8 | iteration: 236/262 | Loss: 0.6420731544494629Training Epoch: 8 | iteration: 237/262 | Loss: 0.5481639504432678Training Epoch: 8 | iteration: 238/262 | Loss: 0.5492523908615112Training Epoch: 8 | iteration: 239/262 | Loss: 0.5365872383117676Training Epoch: 8 | iteration: 240/262 | Loss: 0.5308544635772705Training Epoch: 8 | iteration: 241/262 | Loss: 0.609197199344635Training Epoch: 8 | iteration: 242/262 | Loss: 0.5695300698280334Training Epoch: 8 | iteration: 243/262 | Loss: 0.6425333619117737Training Epoch: 8 | iteration: 244/262 | Loss: 0.5886648893356323Training Epoch: 8 | iteration: 245/262 | Loss: 0.5609503984451294Training Epoch: 8 | iteration: 246/262 | Loss: 0.5409704446792603Training Epoch: 8 | iteration: 247/262 | Loss: 0.5867666602134705Training Epoch: 8 | iteration: 248/262 | Loss: 0.5542373657226562Training Epoch: 8 | iteration: 249/262 | Loss: 0.6010932922363281Training Epoch: 8 | iteration: 250/262 | Loss: 0.521422266960144Training Epoch: 8 | iteration: 251/262 | Loss: 0.6190548539161682Training Epoch: 8 | iteration: 252/262 | Loss: 0.6529741287231445Training Epoch: 8 | iteration: 253/262 | Loss: 0.6154767274856567Training Epoch: 8 | iteration: 254/262 | Loss: 0.5702853202819824Training Epoch: 8 | iteration: 255/262 | Loss: 0.5923351049423218Training Epoch: 8 | iteration: 256/262 | Loss: 0.5931270718574524Training Epoch: 8 | iteration: 257/262 | Loss: 0.6461637020111084Training Epoch: 8 | iteration: 258/262 | Loss: 0.5786668062210083Training Epoch: 8 | iteration: 259/262 | Loss: 0.5490093231201172Training Epoch: 8 | iteration: 260/262 | Loss: 0.5195118188858032Training Epoch: 8 | iteration: 261/262 | Loss: 0.507280707359314Validating Epoch: 8 | iteration: 0/66 | Loss: 0.6872038841247559Validating Epoch: 8 | iteration: 1/66 | Loss: 0.6776828765869141Validating Epoch: 8 | iteration: 2/66 | Loss: 0.7147759795188904Validating Epoch: 8 | iteration: 3/66 | Loss: 0.6890325546264648Validating Epoch: 8 | iteration: 4/66 | Loss: 0.6610347032546997Validating Epoch: 8 | iteration: 5/66 | Loss: 0.6794869899749756Validating Epoch: 8 | iteration: 6/66 | Loss: 0.6145339608192444Validating Epoch: 8 | iteration: 7/66 | Loss: 0.6348196268081665Validating Epoch: 8 | iteration: 8/66 | Loss: 0.6200376152992249Validating Epoch: 8 | iteration: 9/66 | Loss: 0.6612348556518555Validating Epoch: 8 | iteration: 10/66 | Loss: 0.7017756700515747Validating Epoch: 8 | iteration: 11/66 | Loss: 0.6468548774719238Validating Epoch: 8 | iteration: 12/66 | Loss: 0.6035858392715454Validating Epoch: 8 | iteration: 13/66 | Loss: 0.7258973121643066Validating Epoch: 8 | iteration: 14/66 | Loss: 0.6334978342056274Validating Epoch: 8 | iteration: 15/66 | Loss: 0.6760687828063965Validating Epoch: 8 | iteration: 16/66 | Loss: 0.6164190173149109Validating Epoch: 8 | iteration: 17/66 | Loss: 0.7037338614463806Validating Epoch: 8 | iteration: 18/66 | Loss: 0.6259164810180664Validating Epoch: 8 | iteration: 19/66 | Loss: 0.6744673252105713Validating Epoch: 8 | iteration: 20/66 | Loss: 0.7107830047607422Validating Epoch: 8 | iteration: 21/66 | Loss: 0.6612225770950317Validating Epoch: 8 | iteration: 22/66 | Loss: 0.6827206611633301Validating Epoch: 8 | iteration: 23/66 | Loss: 0.620650053024292Validating Epoch: 8 | iteration: 24/66 | Loss: 0.657631516456604Validating Epoch: 8 | iteration: 25/66 | Loss: 0.6517424583435059Validating Epoch: 8 | iteration: 26/66 | Loss: 0.6223978996276855Validating Epoch: 8 | iteration: 27/66 | Loss: 0.6619172096252441Validating Epoch: 8 | iteration: 28/66 | Loss: 0.7401211857795715Validating Epoch: 8 | iteration: 29/66 | Loss: 0.6744776964187622Validating Epoch: 8 | iteration: 30/66 | Loss: 0.6543844938278198Validating Epoch: 8 | iteration: 31/66 | Loss: 0.6894503235816956Validating Epoch: 8 | iteration: 32/66 | Loss: 0.6952823996543884Validating Epoch: 8 | iteration: 33/66 | Loss: 0.6974166631698608Validating Epoch: 8 | iteration: 34/66 | Loss: 0.7417330741882324Validating Epoch: 8 | iteration: 35/66 | Loss: 0.6813602447509766Validating Epoch: 8 | iteration: 36/66 | Loss: 0.5954697132110596Validating Epoch: 8 | iteration: 37/66 | Loss: 0.6902173161506653Validating Epoch: 8 | iteration: 38/66 | Loss: 0.6255538463592529Validating Epoch: 8 | iteration: 39/66 | Loss: 0.6794341802597046Validating Epoch: 8 | iteration: 40/66 | Loss: 0.6358929872512817Validating Epoch: 8 | iteration: 41/66 | Loss: 0.6462863087654114Validating Epoch: 8 | iteration: 42/66 | Loss: 0.6293603181838989Validating Epoch: 8 | iteration: 43/66 | Loss: 0.7119197845458984Validating Epoch: 8 | iteration: 44/66 | Loss: 0.7856810092926025Validating Epoch: 8 | iteration: 45/66 | Loss: 0.6420267224311829Validating Epoch: 8 | iteration: 46/66 | Loss: 0.664018452167511Validating Epoch: 8 | iteration: 47/66 | Loss: 0.6046870946884155Validating Epoch: 8 | iteration: 48/66 | Loss: 0.614497184753418Validating Epoch: 8 | iteration: 49/66 | Loss: 0.667364239692688Validating Epoch: 8 | iteration: 50/66 | Loss: 0.6776489019393921Validating Epoch: 8 | iteration: 51/66 | Loss: 0.7113014459609985Validating Epoch: 8 | iteration: 52/66 | Loss: 0.6507861614227295Validating Epoch: 8 | iteration: 53/66 | Loss: 0.6776840090751648Validating Epoch: 8 | iteration: 54/66 | Loss: 0.7265103459358215Validating Epoch: 8 | iteration: 55/66 | Loss: 0.6832212209701538Validating Epoch: 8 | iteration: 56/66 | Loss: 0.69831383228302Validating Epoch: 8 | iteration: 57/66 | Loss: 0.6996668577194214Validating Epoch: 8 | iteration: 58/66 | Loss: 0.6299118995666504Validating Epoch: 8 | iteration: 59/66 | Loss: 0.6821807622909546Validating Epoch: 8 | iteration: 60/66 | Loss: 0.6232180595397949Validating Epoch: 8 | iteration: 61/66 | Loss: 0.6793184280395508Validating Epoch: 8 | iteration: 62/66 | Loss: 0.6777139902114868Validating Epoch: 8 | iteration: 63/66 | Loss: 0.6876683235168457Validating Epoch: 8 | iteration: 64/66 | Loss: 0.6738741993904114Validating Epoch: 8 | iteration: 65/66 | Loss: 0.7761899828910828Temperature:  0.5
Sampling: iteration: 0/51{'Validity': 0.97265625, 'Novelty': 1.0, 'Uniqueness': 0.9939759036144579}
Training Epoch: 9 | iteration: 0/262 | Loss: 0.5962578654289246Training Epoch: 9 | iteration: 1/262 | Loss: 0.49825286865234375Training Epoch: 9 | iteration: 2/262 | Loss: 0.5438411235809326Training Epoch: 9 | iteration: 3/262 | Loss: 0.5462833046913147Training Epoch: 9 | iteration: 4/262 | Loss: 0.562984824180603Training Epoch: 9 | iteration: 5/262 | Loss: 0.6079385876655579Training Epoch: 9 | iteration: 6/262 | Loss: 0.533444881439209Training Epoch: 9 | iteration: 7/262 | Loss: 0.5328654646873474Training Epoch: 9 | iteration: 8/262 | Loss: 0.5456993579864502Training Epoch: 9 | iteration: 9/262 | Loss: 0.5941836833953857Training Epoch: 9 | iteration: 10/262 | Loss: 0.5485068559646606Training Epoch: 9 | iteration: 11/262 | Loss: 0.5800948143005371Training Epoch: 9 | iteration: 12/262 | Loss: 0.5699014067649841Training Epoch: 9 | iteration: 13/262 | Loss: 0.6007407307624817Training Epoch: 9 | iteration: 14/262 | Loss: 0.6180458068847656Training Epoch: 9 | iteration: 15/262 | Loss: 0.5607155561447144Training Epoch: 9 | iteration: 16/262 | Loss: 0.5765577554702759Training Epoch: 9 | iteration: 17/262 | Loss: 0.5442798733711243Training Epoch: 9 | iteration: 18/262 | Loss: 0.5774186849594116Training Epoch: 9 | iteration: 19/262 | Loss: 0.5776610374450684Training Epoch: 9 | iteration: 20/262 | Loss: 0.5193285942077637Training Epoch: 9 | iteration: 21/262 | Loss: 0.5870068073272705Training Epoch: 9 | iteration: 22/262 | Loss: 0.5886710286140442Training Epoch: 9 | iteration: 23/262 | Loss: 0.5997380018234253Training Epoch: 9 | iteration: 24/262 | Loss: 0.5936809778213501Training Epoch: 9 | iteration: 25/262 | Loss: 0.5487603545188904Training Epoch: 9 | iteration: 26/262 | Loss: 0.6127719879150391Training Epoch: 9 | iteration: 27/262 | Loss: 0.5090256929397583Training Epoch: 9 | iteration: 28/262 | Loss: 0.5573371648788452Training Epoch: 9 | iteration: 29/262 | Loss: 0.5853017568588257Training Epoch: 9 | iteration: 30/262 | Loss: 0.5482242107391357Training Epoch: 9 | iteration: 31/262 | Loss: 0.5940825939178467Training Epoch: 9 | iteration: 32/262 | Loss: 0.5166996717453003Training Epoch: 9 | iteration: 33/262 | Loss: 0.5430550575256348Training Epoch: 9 | iteration: 34/262 | Loss: 0.5303577184677124Training Epoch: 9 | iteration: 35/262 | Loss: 0.5966411828994751Training Epoch: 9 | iteration: 36/262 | Loss: 0.57673579454422Training Epoch: 9 | iteration: 37/262 | Loss: 0.5293374061584473Training Epoch: 9 | iteration: 38/262 | Loss: 0.5651654005050659Training Epoch: 9 | iteration: 39/262 | Loss: 0.5684137344360352Training Epoch: 9 | iteration: 40/262 | Loss: 0.5335952639579773Training Epoch: 9 | iteration: 41/262 | Loss: 0.5914081335067749Training Epoch: 9 | iteration: 42/262 | Loss: 0.5665885806083679Training Epoch: 9 | iteration: 43/262 | Loss: 0.5868671536445618Training Epoch: 9 | iteration: 44/262 | Loss: 0.5871126651763916Training Epoch: 9 | iteration: 45/262 | Loss: 0.5468647480010986Training Epoch: 9 | iteration: 46/262 | Loss: 0.6016939878463745Training Epoch: 9 | iteration: 47/262 | Loss: 0.5527526140213013Training Epoch: 9 | iteration: 48/262 | Loss: 0.5889593362808228Training Epoch: 9 | iteration: 49/262 | Loss: 0.5459418296813965Training Epoch: 9 | iteration: 50/262 | Loss: 0.5654942989349365Training Epoch: 9 | iteration: 51/262 | Loss: 0.5535157322883606Training Epoch: 9 | iteration: 52/262 | Loss: 0.5951258540153503Training Epoch: 9 | iteration: 53/262 | Loss: 0.5971928834915161Training Epoch: 9 | iteration: 54/262 | Loss: 0.5638390779495239Training Epoch: 9 | iteration: 55/262 | Loss: 0.6101441979408264Training Epoch: 9 | iteration: 56/262 | Loss: 0.5746408700942993Training Epoch: 9 | iteration: 57/262 | Loss: 0.5678396821022034Training Epoch: 9 | iteration: 58/262 | Loss: 0.5815274119377136Training Epoch: 9 | iteration: 59/262 | Loss: 0.5722709894180298Training Epoch: 9 | iteration: 60/262 | Loss: 0.5537056922912598Training Epoch: 9 | iteration: 61/262 | Loss: 0.596747636795044Training Epoch: 9 | iteration: 62/262 | Loss: 0.5832667946815491Training Epoch: 9 | iteration: 63/262 | Loss: 0.5854572653770447Training Epoch: 9 | iteration: 64/262 | Loss: 0.6158662438392639Training Epoch: 9 | iteration: 65/262 | Loss: 0.5790586471557617Training Epoch: 9 | iteration: 66/262 | Loss: 0.5664607286453247Training Epoch: 9 | iteration: 67/262 | Loss: 0.5556719899177551Training Epoch: 9 | iteration: 68/262 | Loss: 0.5212924480438232Training Epoch: 9 | iteration: 69/262 | Loss: 0.6505321264266968Training Epoch: 9 | iteration: 70/262 | Loss: 0.5818594694137573Training Epoch: 9 | iteration: 71/262 | Loss: 0.5530633330345154Training Epoch: 9 | iteration: 72/262 | Loss: 0.5586409568786621Training Epoch: 9 | iteration: 73/262 | Loss: 0.5889874696731567Training Epoch: 9 | iteration: 74/262 | Loss: 0.5197657346725464Training Epoch: 9 | iteration: 75/262 | Loss: 0.6176245212554932Training Epoch: 9 | iteration: 76/262 | Loss: 0.5394808053970337Training Epoch: 9 | iteration: 77/262 | Loss: 0.5702254176139832Training Epoch: 9 | iteration: 78/262 | Loss: 0.5627972483634949Training Epoch: 9 | iteration: 79/262 | Loss: 0.5538028478622437Training Epoch: 9 | iteration: 80/262 | Loss: 0.5592658519744873Training Epoch: 9 | iteration: 81/262 | Loss: 0.5890597701072693Training Epoch: 9 | iteration: 82/262 | Loss: 0.5925351977348328Training Epoch: 9 | iteration: 83/262 | Loss: 0.6121194362640381Training Epoch: 9 | iteration: 84/262 | Loss: 0.5742402076721191Training Epoch: 9 | iteration: 85/262 | Loss: 0.5978882312774658Training Epoch: 9 | iteration: 86/262 | Loss: 0.5951376557350159Training Epoch: 9 | iteration: 87/262 | Loss: 0.5379953384399414Training Epoch: 9 | iteration: 88/262 | Loss: 0.5661284923553467Training Epoch: 9 | iteration: 89/262 | Loss: 0.5663130283355713Training Epoch: 9 | iteration: 90/262 | Loss: 0.5593042373657227Training Epoch: 9 | iteration: 91/262 | Loss: 0.543904185295105Training Epoch: 9 | iteration: 92/262 | Loss: 0.5943235158920288Training Epoch: 9 | iteration: 93/262 | Loss: 0.5348646640777588Training Epoch: 9 | iteration: 94/262 | Loss: 0.6132821440696716Training Epoch: 9 | iteration: 95/262 | Loss: 0.5831355452537537Training Epoch: 9 | iteration: 96/262 | Loss: 0.5873286128044128Training Epoch: 9 | iteration: 97/262 | Loss: 0.6261701583862305Training Epoch: 9 | iteration: 98/262 | Loss: 0.5390291213989258Training Epoch: 9 | iteration: 99/262 | Loss: 0.6560854315757751Training Epoch: 9 | iteration: 100/262 | Loss: 0.5993970632553101Training Epoch: 9 | iteration: 101/262 | Loss: 0.6065081357955933Training Epoch: 9 | iteration: 102/262 | Loss: 0.5566401481628418Training Epoch: 9 | iteration: 103/262 | Loss: 0.5542115569114685Training Epoch: 9 | iteration: 104/262 | Loss: 0.6575852632522583Training Epoch: 9 | iteration: 105/262 | Loss: 0.622055172920227Training Epoch: 9 | iteration: 106/262 | Loss: 0.6030369400978088Training Epoch: 9 | iteration: 107/262 | Loss: 0.5839896202087402Training Epoch: 9 | iteration: 108/262 | Loss: 0.5704752802848816Training Epoch: 9 | iteration: 109/262 | Loss: 0.5240868926048279Training Epoch: 9 | iteration: 110/262 | Loss: 0.6082231998443604Training Epoch: 9 | iteration: 111/262 | Loss: 0.5479522943496704Training Epoch: 9 | iteration: 112/262 | Loss: 0.5913783311843872Training Epoch: 9 | iteration: 113/262 | Loss: 0.6008831262588501Training Epoch: 9 | iteration: 114/262 | Loss: 0.5570104718208313Training Epoch: 9 | iteration: 115/262 | Loss: 0.5827654600143433Training Epoch: 9 | iteration: 116/262 | Loss: 0.60399329662323Training Epoch: 9 | iteration: 117/262 | Loss: 0.5472577810287476Training Epoch: 9 | iteration: 118/262 | Loss: 0.5466341376304626Training Epoch: 9 | iteration: 119/262 | Loss: 0.5631304383277893Training Epoch: 9 | iteration: 120/262 | Loss: 0.6144826412200928Training Epoch: 9 | iteration: 121/262 | Loss: 0.5550634264945984Training Epoch: 9 | iteration: 122/262 | Loss: 0.5987418293952942Training Epoch: 9 | iteration: 123/262 | Loss: 0.6008507013320923Training Epoch: 9 | iteration: 124/262 | Loss: 0.568058967590332Training Epoch: 9 | iteration: 125/262 | Loss: 0.5628601312637329Training Epoch: 9 | iteration: 126/262 | Loss: 0.5294809937477112Training Epoch: 9 | iteration: 127/262 | Loss: 0.5272220373153687Training Epoch: 9 | iteration: 128/262 | Loss: 0.6054102778434753Training Epoch: 9 | iteration: 129/262 | Loss: 0.5605213642120361Training Epoch: 9 | iteration: 130/262 | Loss: 0.5768856406211853Training Epoch: 9 | iteration: 131/262 | Loss: 0.5221148133277893Training Epoch: 9 | iteration: 132/262 | Loss: 0.6018304824829102Training Epoch: 9 | iteration: 133/262 | Loss: 0.5557899475097656Training Epoch: 9 | iteration: 134/262 | Loss: 0.5416179299354553Training Epoch: 9 | iteration: 135/262 | Loss: 0.5789480209350586Training Epoch: 9 | iteration: 136/262 | Loss: 0.5914278030395508Training Epoch: 9 | iteration: 137/262 | Loss: 0.5472813844680786Training Epoch: 9 | iteration: 138/262 | Loss: 0.5748553276062012Training Epoch: 9 | iteration: 139/262 | Loss: 0.5517607927322388Training Epoch: 9 | iteration: 140/262 | Loss: 0.5967081785202026Training Epoch: 9 | iteration: 141/262 | Loss: 0.570766806602478Training Epoch: 9 | iteration: 142/262 | Loss: 0.6219990849494934Training Epoch: 9 | iteration: 143/262 | Loss: 0.5663536787033081Training Epoch: 9 | iteration: 144/262 | Loss: 0.5437186360359192Training Epoch: 9 | iteration: 145/262 | Loss: 0.5828115344047546Training Epoch: 9 | iteration: 146/262 | Loss: 0.5170924663543701Training Epoch: 9 | iteration: 147/262 | Loss: 0.6212161779403687Training Epoch: 9 | iteration: 148/262 | Loss: 0.5387691855430603Training Epoch: 9 | iteration: 149/262 | Loss: 0.5487512350082397Training Epoch: 9 | iteration: 150/262 | Loss: 0.5730708837509155Training Epoch: 9 | iteration: 151/262 | Loss: 0.6281924247741699Training Epoch: 9 | iteration: 152/262 | Loss: 0.5721613168716431Training Epoch: 9 | iteration: 153/262 | Loss: 0.5676164627075195Training Epoch: 9 | iteration: 154/262 | Loss: 0.6032218933105469Training Epoch: 9 | iteration: 155/262 | Loss: 0.6221072673797607Training Epoch: 9 | iteration: 156/262 | Loss: 0.5966322422027588Training Epoch: 9 | iteration: 157/262 | Loss: 0.5790386199951172Training Epoch: 9 | iteration: 158/262 | Loss: 0.5751233100891113Training Epoch: 9 | iteration: 159/262 | Loss: 0.5865781307220459Training Epoch: 9 | iteration: 160/262 | Loss: 0.5583745241165161Training Epoch: 9 | iteration: 161/262 | Loss: 0.5278092622756958Training Epoch: 9 | iteration: 162/262 | Loss: 0.5961642265319824Training Epoch: 9 | iteration: 163/262 | Loss: 0.5875207185745239Training Epoch: 9 | iteration: 164/262 | Loss: 0.5949554443359375Training Epoch: 9 | iteration: 165/262 | Loss: 0.62531578540802Training Epoch: 9 | iteration: 166/262 | Loss: 0.5766035914421082Training Epoch: 9 | iteration: 167/262 | Loss: 0.5891537666320801Training Epoch: 9 | iteration: 168/262 | Loss: 0.5487885475158691Training Epoch: 9 | iteration: 169/262 | Loss: 0.5439020991325378Training Epoch: 9 | iteration: 170/262 | Loss: 0.5812830924987793Training Epoch: 9 | iteration: 171/262 | Loss: 0.6037570834159851Training Epoch: 9 | iteration: 172/262 | Loss: 0.5501053333282471Training Epoch: 9 | iteration: 173/262 | Loss: 0.5923920273780823Training Epoch: 9 | iteration: 174/262 | Loss: 0.5323822498321533Training Epoch: 9 | iteration: 175/262 | Loss: 0.6253874897956848Training Epoch: 9 | iteration: 176/262 | Loss: 0.6082101464271545Training Epoch: 9 | iteration: 177/262 | Loss: 0.6269097328186035Training Epoch: 9 | iteration: 178/262 | Loss: 0.5319488048553467Training Epoch: 9 | iteration: 179/262 | Loss: 0.5745497941970825Training Epoch: 9 | iteration: 180/262 | Loss: 0.5586316585540771Training Epoch: 9 | iteration: 181/262 | Loss: 0.6105926036834717Training Epoch: 9 | iteration: 182/262 | Loss: 0.5562889575958252Training Epoch: 9 | iteration: 183/262 | Loss: 0.6362274885177612Training Epoch: 9 | iteration: 184/262 | Loss: 0.6154409050941467Training Epoch: 9 | iteration: 185/262 | Loss: 0.5730916261672974Training Epoch: 9 | iteration: 186/262 | Loss: 0.5891557931900024Training Epoch: 9 | iteration: 187/262 | Loss: 0.5589510798454285Training Epoch: 9 | iteration: 188/262 | Loss: 0.5516753196716309Training Epoch: 9 | iteration: 189/262 | Loss: 0.5716017484664917Training Epoch: 9 | iteration: 190/262 | Loss: 0.6039025187492371Training Epoch: 9 | iteration: 191/262 | Loss: 0.5545276403427124Training Epoch: 9 | iteration: 192/262 | Loss: 0.5245897769927979Training Epoch: 9 | iteration: 193/262 | Loss: 0.5746458768844604Training Epoch: 9 | iteration: 194/262 | Loss: 0.5531501770019531Training Epoch: 9 | iteration: 195/262 | Loss: 0.579157292842865Training Epoch: 9 | iteration: 196/262 | Loss: 0.5624752044677734Training Epoch: 9 | iteration: 197/262 | Loss: 0.590073823928833Training Epoch: 9 | iteration: 198/262 | Loss: 0.5975750088691711Training Epoch: 9 | iteration: 199/262 | Loss: 0.5989398956298828Training Epoch: 9 | iteration: 200/262 | Loss: 0.533882200717926Training Epoch: 9 | iteration: 201/262 | Loss: 0.5751714706420898Training Epoch: 9 | iteration: 202/262 | Loss: 0.5384851694107056Training Epoch: 9 | iteration: 203/262 | Loss: 0.5567771196365356Training Epoch: 9 | iteration: 204/262 | Loss: 0.5415321588516235Training Epoch: 9 | iteration: 205/262 | Loss: 0.5752618312835693Training Epoch: 9 | iteration: 206/262 | Loss: 0.5432066917419434Training Epoch: 9 | iteration: 207/262 | Loss: 0.5549435615539551Training Epoch: 9 | iteration: 208/262 | Loss: 0.5449953079223633Training Epoch: 9 | iteration: 209/262 | Loss: 0.6103118658065796Training Epoch: 9 | iteration: 210/262 | Loss: 0.5691078305244446Training Epoch: 9 | iteration: 211/262 | Loss: 0.5235151052474976Training Epoch: 9 | iteration: 212/262 | Loss: 0.5401673316955566Training Epoch: 9 | iteration: 213/262 | Loss: 0.6046023368835449Training Epoch: 9 | iteration: 214/262 | Loss: 0.5985546112060547Training Epoch: 9 | iteration: 215/262 | Loss: 0.5332220792770386Training Epoch: 9 | iteration: 216/262 | Loss: 0.5673007965087891Training Epoch: 9 | iteration: 217/262 | Loss: 0.5580919981002808Training Epoch: 9 | iteration: 218/262 | Loss: 0.491217702627182Training Epoch: 9 | iteration: 219/262 | Loss: 0.584107518196106Training Epoch: 9 | iteration: 220/262 | Loss: 0.5984376072883606Training Epoch: 9 | iteration: 221/262 | Loss: 0.5472897887229919Training Epoch: 9 | iteration: 222/262 | Loss: 0.5892858505249023Training Epoch: 9 | iteration: 223/262 | Loss: 0.5579218864440918Training Epoch: 9 | iteration: 224/262 | Loss: 0.6283292770385742Training Epoch: 9 | iteration: 225/262 | Loss: 0.5950403809547424Training Epoch: 9 | iteration: 226/262 | Loss: 0.6177114844322205Training Epoch: 9 | iteration: 227/262 | Loss: 0.5527336597442627Training Epoch: 9 | iteration: 228/262 | Loss: 0.5503803491592407Training Epoch: 9 | iteration: 229/262 | Loss: 0.5487439036369324Training Epoch: 9 | iteration: 230/262 | Loss: 0.5390850901603699Training Epoch: 9 | iteration: 231/262 | Loss: 0.6206198930740356Training Epoch: 9 | iteration: 232/262 | Loss: 0.594533383846283Training Epoch: 9 | iteration: 233/262 | Loss: 0.5737504363059998Training Epoch: 9 | iteration: 234/262 | Loss: 0.5332744121551514Training Epoch: 9 | iteration: 235/262 | Loss: 0.5651063919067383Training Epoch: 9 | iteration: 236/262 | Loss: 0.5678789019584656Training Epoch: 9 | iteration: 237/262 | Loss: 0.6175683736801147Training Epoch: 9 | iteration: 238/262 | Loss: 0.5653172731399536Training Epoch: 9 | iteration: 239/262 | Loss: 0.5572376251220703Training Epoch: 9 | iteration: 240/262 | Loss: 0.5357345342636108Training Epoch: 9 | iteration: 241/262 | Loss: 0.5619534254074097Training Epoch: 9 | iteration: 242/262 | Loss: 0.5866864919662476Training Epoch: 9 | iteration: 243/262 | Loss: 0.5802419185638428Training Epoch: 9 | iteration: 244/262 | Loss: 0.5947234630584717Training Epoch: 9 | iteration: 245/262 | Loss: 0.5747883319854736Training Epoch: 9 | iteration: 246/262 | Loss: 0.538800060749054Training Epoch: 9 | iteration: 247/262 | Loss: 0.5649425983428955Training Epoch: 9 | iteration: 248/262 | Loss: 0.5772892236709595Training Epoch: 9 | iteration: 249/262 | Loss: 0.5730143785476685Training Epoch: 9 | iteration: 250/262 | Loss: 0.5462385416030884Training Epoch: 9 | iteration: 251/262 | Loss: 0.6026293039321899Training Epoch: 9 | iteration: 252/262 | Loss: 0.5571380853652954Training Epoch: 9 | iteration: 253/262 | Loss: 0.5707937479019165Training Epoch: 9 | iteration: 254/262 | Loss: 0.5056705474853516Training Epoch: 9 | iteration: 255/262 | Loss: 0.5572784543037415Training Epoch: 9 | iteration: 256/262 | Loss: 0.6167359352111816Training Epoch: 9 | iteration: 257/262 | Loss: 0.5613198280334473Training Epoch: 9 | iteration: 258/262 | Loss: 0.6062369346618652Training Epoch: 9 | iteration: 259/262 | Loss: 0.6147359013557434Training Epoch: 9 | iteration: 260/262 | Loss: 0.5477491617202759Training Epoch: 9 | iteration: 261/262 | Loss: 0.4683515429496765Validating Epoch: 9 | iteration: 0/66 | Loss: 0.6208344101905823Validating Epoch: 9 | iteration: 1/66 | Loss: 0.6481425762176514Validating Epoch: 9 | iteration: 2/66 | Loss: 0.6336712837219238Validating Epoch: 9 | iteration: 3/66 | Loss: 0.6464011669158936Validating Epoch: 9 | iteration: 4/66 | Loss: 0.6321792602539062Validating Epoch: 9 | iteration: 5/66 | Loss: 0.7373444437980652Validating Epoch: 9 | iteration: 6/66 | Loss: 0.5898200869560242Validating Epoch: 9 | iteration: 7/66 | Loss: 0.6893157958984375Validating Epoch: 9 | iteration: 8/66 | Loss: 0.6753007173538208Validating Epoch: 9 | iteration: 9/66 | Loss: 0.6928409337997437Validating Epoch: 9 | iteration: 10/66 | Loss: 0.7257903814315796Validating Epoch: 9 | iteration: 11/66 | Loss: 0.6805550456047058Validating Epoch: 9 | iteration: 12/66 | Loss: 0.676537811756134Validating Epoch: 9 | iteration: 13/66 | Loss: 0.6922433972358704Validating Epoch: 9 | iteration: 14/66 | Loss: 0.6371480226516724Validating Epoch: 9 | iteration: 15/66 | Loss: 0.7355406284332275Validating Epoch: 9 | iteration: 16/66 | Loss: 0.6382365226745605Validating Epoch: 9 | iteration: 17/66 | Loss: 0.6683453321456909Validating Epoch: 9 | iteration: 18/66 | Loss: 0.6700581312179565Validating Epoch: 9 | iteration: 19/66 | Loss: 0.6771842241287231Validating Epoch: 9 | iteration: 20/66 | Loss: 0.6177528500556946Validating Epoch: 9 | iteration: 21/66 | Loss: 0.7042167782783508Validating Epoch: 9 | iteration: 22/66 | Loss: 0.6940977573394775Validating Epoch: 9 | iteration: 23/66 | Loss: 0.7048537731170654Validating Epoch: 9 | iteration: 24/66 | Loss: 0.6602545976638794Validating Epoch: 9 | iteration: 25/66 | Loss: 0.7058442831039429Validating Epoch: 9 | iteration: 26/66 | Loss: 0.7016626596450806Validating Epoch: 9 | iteration: 27/66 | Loss: 0.6838045120239258Validating Epoch: 9 | iteration: 28/66 | Loss: 0.6769604682922363Validating Epoch: 9 | iteration: 29/66 | Loss: 0.6317418813705444Validating Epoch: 9 | iteration: 30/66 | Loss: 0.6896611452102661Validating Epoch: 9 | iteration: 31/66 | Loss: 0.7536911964416504Validating Epoch: 9 | iteration: 32/66 | Loss: 0.6443233489990234Validating Epoch: 9 | iteration: 33/66 | Loss: 0.814752995967865Validating Epoch: 9 | iteration: 34/66 | Loss: 0.6474325656890869Validating Epoch: 9 | iteration: 35/66 | Loss: 0.6191697120666504Validating Epoch: 9 | iteration: 36/66 | Loss: 0.6396753787994385Validating Epoch: 9 | iteration: 37/66 | Loss: 0.6923959255218506Validating Epoch: 9 | iteration: 38/66 | Loss: 0.6876028776168823Validating Epoch: 9 | iteration: 39/66 | Loss: 0.6815922856330872Validating Epoch: 9 | iteration: 40/66 | Loss: 0.7131887674331665Validating Epoch: 9 | iteration: 41/66 | Loss: 0.6630207300186157Validating Epoch: 9 | iteration: 42/66 | Loss: 0.6758012771606445Validating Epoch: 9 | iteration: 43/66 | Loss: 0.6392467021942139Validating Epoch: 9 | iteration: 44/66 | Loss: 0.6186516880989075Validating Epoch: 9 | iteration: 45/66 | Loss: 0.7032235264778137Validating Epoch: 9 | iteration: 46/66 | Loss: 0.6434580087661743Validating Epoch: 9 | iteration: 47/66 | Loss: 0.5803448557853699Validating Epoch: 9 | iteration: 48/66 | Loss: 0.732017993927002Validating Epoch: 9 | iteration: 49/66 | Loss: 0.6566234827041626Validating Epoch: 9 | iteration: 50/66 | Loss: 0.6595931053161621Validating Epoch: 9 | iteration: 51/66 | Loss: 0.6600613594055176Validating Epoch: 9 | iteration: 52/66 | Loss: 0.6973975300788879Validating Epoch: 9 | iteration: 53/66 | Loss: 0.6430433988571167Validating Epoch: 9 | iteration: 54/66 | Loss: 0.6408863067626953Validating Epoch: 9 | iteration: 55/66 | Loss: 0.7023831605911255Validating Epoch: 9 | iteration: 56/66 | Loss: 0.7055604457855225Validating Epoch: 9 | iteration: 57/66 | Loss: 0.6551073789596558Validating Epoch: 9 | iteration: 58/66 | Loss: 0.6834052205085754Validating Epoch: 9 | iteration: 59/66 | Loss: 0.6263384819030762Validating Epoch: 9 | iteration: 60/66 | Loss: 0.6526911854743958Validating Epoch: 9 | iteration: 61/66 | Loss: 0.6150441765785217Validating Epoch: 9 | iteration: 62/66 | Loss: 0.6784838438034058Validating Epoch: 9 | iteration: 63/66 | Loss: 0.6187492609024048Validating Epoch: 9 | iteration: 64/66 | Loss: 0.6483505368232727Validating Epoch: 9 | iteration: 65/66 | Loss: 0.7364678382873535No of GPUs available 4

==================================================
Generating molecules with target properties...
==================================================

Generating for -10_2...
Generating for -10_4...
Generating for -9_2...
Generating for -9_4...
Generating for -8_2...
Generating for -8_4...
Generating for -7_2...
Generating for -7_4...
Generating for -6_2...
Generating for -6_4...

==================================================
Generated molecules saved to: ../checkpoints/DPO_Finetuning_BETA_0.11_epochs_10_LCK_DOCKSTRING_FAST_ACTUAL_affinity_sas_DPO_pref_affinity/generated_molecules.pkl
To analyze and plot results, run:
python analyze_generated_molecules.py --checkpoint_dir ../checkpoints/DPO_Finetuning_BETA_0.11_epochs_10_LCK_DOCKSTRING_FAST_ACTUAL_affinity_sas_DPO_pref_affinity --properties affinity sas
==================================================

[1;34mwandb[0m: 
[1;34mwandb[0m:  View run [33mDPO_Finetuning_BETA_0.11_epochs_10_LCK_DOCKSTRING_FAST_ACTUAL_affinity_sas_DPO_pref_affinity[0m at: [34mhttps://wandb.ai/bhuvan-kapur1-iiith/molgpt2.0%20FINAL/runs/xm78m3gw[0m
[1;34mwandb[0m: Find logs at: [1;35mwandb/run-20260201_032913-xm78m3gw/logs[0m
