Starting ...
cuda
Model properties:  ['affinity', 'tpsas']
Preference properties:  ['affinity']
Building Vocab
{'batch_size': 128, 'd_model': 512, 'n_heads': 8, 'n_layers': 6, 'hidden_units': 512, 'lr': 1e-05, 'epochs': 265, 'properties': ['affinity', 'logps', 'qeds', 'sas', 'tpsas'], 'model_properties': ['affinity', 'tpsas'], 'run_name': 'LCK_DOCKSTRING_FAST_ACTUAL_affinity_tpsas'}
length of preference data : 41795
Sample preference_data[0]: ['O1C(C(=O)N2CCN(CC2)C3=CC=C(N(=O)=O)C=C3)=C(C=4C1=CC=CC4)C', array([0.47959185, 0.53255087, 0.54797947, 0.15855041, 0.16055913]), [np.str_('O1C(=NC2=C(C1=O)C=CC=C2)C3=C(C(=O)NCCC4=CC=CC=C4)C=CC(OC)=C3'), np.float64(0.9873459430403937), array([0.52040816, 0.54211667, 0.55725834, 0.15815913, 0.16377715])], [np.str_('O1C(=NC2=C(C1=O)C=CC=C2)C3=CC(OC(=O)C=4C=CC=CC4)=C(OC)C=C3'), np.float64(0.9574398041990725), array([0.43877551, 0.55263869, 0.41284505, 0.14420153, 0.15814562])]]
LCK_DOCKSTRING_FAST_ACTUAL_affinity_tpsas
len(target_smiles): 41800
len(data): 41800
LCK_DOCKSTRING_FAST_ACTUAL_affinity_tpsas
Loading affinity-only preference data from: ../checkpoints/LCK_DOCKSTRING_FAST_ACTUAL_affinity_logps_qeds_sas_tpsas/PreferenceData_affinity.pkl
dataset built
cuda
True
2.7.1+cu118
No of GPUs available 4
No of GPUs available 4
{'batch_size': 128, 'd_model': 512, 'n_heads': 8, 'n_layers': 6, 'hidden_units': 512, 'lr': 1e-05, 'epochs': 10, 'model_properties': ['affinity', 'tpsas'], 'preference_properties': ['affinity'], 'ipo': False, 'run_name': 'DPO_Finetuning_BETA_0.11_epochs_10_LCK_DOCKSTRING_FAST_ACTUAL_affinity_tpsas_DPO_pref_affinity', 'beta': 0.11}
Temperature:  0.5
Sampling: iteration: 0/51{'Validity': 0.99609375, 'Novelty': 1.0, 'Uniqueness': 0.996078431372549}
Training Epoch: 0 | iteration: 0/262 | Loss: 1.0216383934020996Training Epoch: 0 | iteration: 1/262 | Loss: 0.9336475133895874Training Epoch: 0 | iteration: 2/262 | Loss: 0.9506531953811646Training Epoch: 0 | iteration: 3/262 | Loss: 0.9528887271881104Training Epoch: 0 | iteration: 4/262 | Loss: 0.9300087094306946Training Epoch: 0 | iteration: 5/262 | Loss: 0.9377349615097046Training Epoch: 0 | iteration: 6/262 | Loss: 0.8895305395126343Training Epoch: 0 | iteration: 7/262 | Loss: 0.799234926700592Training Epoch: 0 | iteration: 8/262 | Loss: 0.8603264689445496Training Epoch: 0 | iteration: 9/262 | Loss: 0.9766194820404053Training Epoch: 0 | iteration: 10/262 | Loss: 1.0017955303192139Training Epoch: 0 | iteration: 11/262 | Loss: 0.9272040128707886Training Epoch: 0 | iteration: 12/262 | Loss: 0.898357629776001Training Epoch: 0 | iteration: 13/262 | Loss: 0.9288209080696106Training Epoch: 0 | iteration: 14/262 | Loss: 0.8404661417007446Training Epoch: 0 | iteration: 15/262 | Loss: 1.0137660503387451Training Epoch: 0 | iteration: 16/262 | Loss: 0.952286958694458Training Epoch: 0 | iteration: 17/262 | Loss: 0.8289235234260559Training Epoch: 0 | iteration: 18/262 | Loss: 0.9594689607620239Training Epoch: 0 | iteration: 19/262 | Loss: 0.9007806777954102Training Epoch: 0 | iteration: 20/262 | Loss: 0.8001880645751953Training Epoch: 0 | iteration: 21/262 | Loss: 1.0439532995224Training Epoch: 0 | iteration: 22/262 | Loss: 0.9961091876029968Training Epoch: 0 | iteration: 23/262 | Loss: 1.0175021886825562Training Epoch: 0 | iteration: 24/262 | Loss: 0.9563353061676025Training Epoch: 0 | iteration: 25/262 | Loss: 0.8232857584953308Training Epoch: 0 | iteration: 26/262 | Loss: 0.9687631130218506Training Epoch: 0 | iteration: 27/262 | Loss: 0.8818795084953308Training Epoch: 0 | iteration: 28/262 | Loss: 0.942767322063446Training Epoch: 0 | iteration: 29/262 | Loss: 0.9039536118507385Training Epoch: 0 | iteration: 30/262 | Loss: 0.883097767829895Training Epoch: 0 | iteration: 31/262 | Loss: 0.9213604927062988Training Epoch: 0 | iteration: 32/262 | Loss: 0.9654048681259155Training Epoch: 0 | iteration: 33/262 | Loss: 0.9859700202941895Training Epoch: 0 | iteration: 34/262 | Loss: 0.8559503555297852Training Epoch: 0 | iteration: 35/262 | Loss: 0.8386022448539734Training Epoch: 0 | iteration: 36/262 | Loss: 0.8663895130157471Training Epoch: 0 | iteration: 37/262 | Loss: 0.8990303874015808Training Epoch: 0 | iteration: 38/262 | Loss: 0.9489302635192871Training Epoch: 0 | iteration: 39/262 | Loss: 0.8334866762161255Training Epoch: 0 | iteration: 40/262 | Loss: 0.8921371102333069Training Epoch: 0 | iteration: 41/262 | Loss: 0.9253222942352295Training Epoch: 0 | iteration: 42/262 | Loss: 0.8065991401672363Training Epoch: 0 | iteration: 43/262 | Loss: 0.9864484071731567Training Epoch: 0 | iteration: 44/262 | Loss: 0.9488969445228577Training Epoch: 0 | iteration: 45/262 | Loss: 0.8758042454719543Training Epoch: 0 | iteration: 46/262 | Loss: 0.8510960936546326Training Epoch: 0 | iteration: 47/262 | Loss: 0.8841689229011536Training Epoch: 0 | iteration: 48/262 | Loss: 0.9298275709152222Training Epoch: 0 | iteration: 49/262 | Loss: 0.9392217397689819Training Epoch: 0 | iteration: 50/262 | Loss: 0.853115439414978Training Epoch: 0 | iteration: 51/262 | Loss: 0.9650306105613708Training Epoch: 0 | iteration: 52/262 | Loss: 0.8843971490859985Training Epoch: 0 | iteration: 53/262 | Loss: 0.9241495132446289Training Epoch: 0 | iteration: 54/262 | Loss: 0.9396130442619324Training Epoch: 0 | iteration: 55/262 | Loss: 0.8064706921577454Training Epoch: 0 | iteration: 56/262 | Loss: 0.8847790360450745Training Epoch: 0 | iteration: 57/262 | Loss: 0.8630698919296265Training Epoch: 0 | iteration: 58/262 | Loss: 0.9616280198097229Training Epoch: 0 | iteration: 59/262 | Loss: 0.8845040798187256Training Epoch: 0 | iteration: 60/262 | Loss: 0.9086480140686035Training Epoch: 0 | iteration: 61/262 | Loss: 0.914723813533783Training Epoch: 0 | iteration: 62/262 | Loss: 0.9485296010971069Training Epoch: 0 | iteration: 63/262 | Loss: 0.9849989414215088Training Epoch: 0 | iteration: 64/262 | Loss: 0.8816595077514648Training Epoch: 0 | iteration: 65/262 | Loss: 0.7890415191650391Training Epoch: 0 | iteration: 66/262 | Loss: 0.8594517707824707Training Epoch: 0 | iteration: 67/262 | Loss: 0.8548113107681274Training Epoch: 0 | iteration: 68/262 | Loss: 0.9344169497489929Training Epoch: 0 | iteration: 69/262 | Loss: 0.8493930101394653Training Epoch: 0 | iteration: 70/262 | Loss: 0.8888527154922485Training Epoch: 0 | iteration: 71/262 | Loss: 0.9064671993255615Training Epoch: 0 | iteration: 72/262 | Loss: 0.736315131187439Training Epoch: 0 | iteration: 73/262 | Loss: 0.862354040145874Training Epoch: 0 | iteration: 74/262 | Loss: 0.8170768022537231Training Epoch: 0 | iteration: 75/262 | Loss: 0.813427746295929Training Epoch: 0 | iteration: 76/262 | Loss: 0.9208757281303406Training Epoch: 0 | iteration: 77/262 | Loss: 0.9094825387001038Training Epoch: 0 | iteration: 78/262 | Loss: 0.8590954542160034Training Epoch: 0 | iteration: 79/262 | Loss: 0.8805326223373413Training Epoch: 0 | iteration: 80/262 | Loss: 0.9627223014831543Training Epoch: 0 | iteration: 81/262 | Loss: 0.941875696182251Training Epoch: 0 | iteration: 82/262 | Loss: 0.8817137479782104Training Epoch: 0 | iteration: 83/262 | Loss: 0.9795290231704712Training Epoch: 0 | iteration: 84/262 | Loss: 0.9181822538375854Training Epoch: 0 | iteration: 85/262 | Loss: 0.8785828351974487Training Epoch: 0 | iteration: 86/262 | Loss: 0.9675955772399902Training Epoch: 0 | iteration: 87/262 | Loss: 0.8670169115066528Training Epoch: 0 | iteration: 88/262 | Loss: 0.8305139541625977Training Epoch: 0 | iteration: 89/262 | Loss: 0.9296237826347351Training Epoch: 0 | iteration: 90/262 | Loss: 0.8414945602416992Training Epoch: 0 | iteration: 91/262 | Loss: 0.8649019002914429Training Epoch: 0 | iteration: 92/262 | Loss: 0.9035020470619202Training Epoch: 0 | iteration: 93/262 | Loss: 0.9354913830757141Training Epoch: 0 | iteration: 94/262 | Loss: 0.9571281671524048Training Epoch: 0 | iteration: 95/262 | Loss: 0.9483227133750916Training Epoch: 0 | iteration: 96/262 | Loss: 0.8868573904037476Training Epoch: 0 | iteration: 97/262 | Loss: 0.8727968335151672Training Epoch: 0 | iteration: 98/262 | Loss: 0.8419176340103149Training Epoch: 0 | iteration: 99/262 | Loss: 0.8522363901138306Training Epoch: 0 | iteration: 100/262 | Loss: 0.8256592154502869Training Epoch: 0 | iteration: 101/262 | Loss: 0.8221117854118347Training Epoch: 0 | iteration: 102/262 | Loss: 0.9078197479248047Training Epoch: 0 | iteration: 103/262 | Loss: 0.7536836862564087Training Epoch: 0 | iteration: 104/262 | Loss: 0.8671936392784119Training Epoch: 0 | iteration: 105/262 | Loss: 0.8540037870407104Training Epoch: 0 | iteration: 106/262 | Loss: 0.9038223028182983Training Epoch: 0 | iteration: 107/262 | Loss: 0.8318190574645996Training Epoch: 0 | iteration: 108/262 | Loss: 0.9685010313987732Training Epoch: 0 | iteration: 109/262 | Loss: 0.9574170112609863Training Epoch: 0 | iteration: 110/262 | Loss: 0.9025558233261108Training Epoch: 0 | iteration: 111/262 | Loss: 0.7592970728874207Training Epoch: 0 | iteration: 112/262 | Loss: 0.8503228425979614Training Epoch: 0 | iteration: 113/262 | Loss: 0.769281268119812Training Epoch: 0 | iteration: 114/262 | Loss: 0.9174914360046387Training Epoch: 0 | iteration: 115/262 | Loss: 0.8884472846984863Training Epoch: 0 | iteration: 116/262 | Loss: 0.7959427833557129Training Epoch: 0 | iteration: 117/262 | Loss: 0.9121901988983154Training Epoch: 0 | iteration: 118/262 | Loss: 0.8998069763183594Training Epoch: 0 | iteration: 119/262 | Loss: 0.8732609152793884Training Epoch: 0 | iteration: 120/262 | Loss: 0.8906410932540894Training Epoch: 0 | iteration: 121/262 | Loss: 0.7737467288970947Training Epoch: 0 | iteration: 122/262 | Loss: 0.9310189485549927Training Epoch: 0 | iteration: 123/262 | Loss: 0.8184235095977783Training Epoch: 0 | iteration: 124/262 | Loss: 0.8165866136550903Training Epoch: 0 | iteration: 125/262 | Loss: 0.9471126794815063Training Epoch: 0 | iteration: 126/262 | Loss: 0.8930724859237671Training Epoch: 0 | iteration: 127/262 | Loss: 0.8354697227478027Training Epoch: 0 | iteration: 128/262 | Loss: 0.8615875244140625Training Epoch: 0 | iteration: 129/262 | Loss: 0.9093530178070068Training Epoch: 0 | iteration: 130/262 | Loss: 0.8697130680084229Training Epoch: 0 | iteration: 131/262 | Loss: 0.8851465582847595Training Epoch: 0 | iteration: 132/262 | Loss: 0.8889871835708618Training Epoch: 0 | iteration: 133/262 | Loss: 0.8477667570114136Training Epoch: 0 | iteration: 134/262 | Loss: 0.830331563949585Training Epoch: 0 | iteration: 135/262 | Loss: 0.8557940721511841Training Epoch: 0 | iteration: 136/262 | Loss: 0.8484972715377808Training Epoch: 0 | iteration: 137/262 | Loss: 0.8301477432250977Training Epoch: 0 | iteration: 138/262 | Loss: 0.9417723417282104Training Epoch: 0 | iteration: 139/262 | Loss: 0.985910177230835Training Epoch: 0 | iteration: 140/262 | Loss: 0.9267256259918213Training Epoch: 0 | iteration: 141/262 | Loss: 0.9437690377235413Training Epoch: 0 | iteration: 142/262 | Loss: 0.8699827194213867Training Epoch: 0 | iteration: 143/262 | Loss: 0.7894067764282227Training Epoch: 0 | iteration: 144/262 | Loss: 0.8936981558799744Training Epoch: 0 | iteration: 145/262 | Loss: 0.8151739239692688Training Epoch: 0 | iteration: 146/262 | Loss: 0.9225502014160156Training Epoch: 0 | iteration: 147/262 | Loss: 0.8341405987739563Training Epoch: 0 | iteration: 148/262 | Loss: 0.8102267980575562Training Epoch: 0 | iteration: 149/262 | Loss: 0.8705909848213196Training Epoch: 0 | iteration: 150/262 | Loss: 0.9236429929733276Training Epoch: 0 | iteration: 151/262 | Loss: 0.8435876369476318Training Epoch: 0 | iteration: 152/262 | Loss: 0.937532901763916Training Epoch: 0 | iteration: 153/262 | Loss: 0.8402842283248901Training Epoch: 0 | iteration: 154/262 | Loss: 0.8199074268341064Training Epoch: 0 | iteration: 155/262 | Loss: 0.7948964238166809Training Epoch: 0 | iteration: 156/262 | Loss: 0.8299134969711304Training Epoch: 0 | iteration: 157/262 | Loss: 0.8611854314804077Training Epoch: 0 | iteration: 158/262 | Loss: 0.8145933151245117Training Epoch: 0 | iteration: 159/262 | Loss: 0.8339106440544128Training Epoch: 0 | iteration: 160/262 | Loss: 0.8697526454925537Training Epoch: 0 | iteration: 161/262 | Loss: 0.9345539808273315Training Epoch: 0 | iteration: 162/262 | Loss: 0.7545996308326721Training Epoch: 0 | iteration: 163/262 | Loss: 0.7884421944618225Training Epoch: 0 | iteration: 164/262 | Loss: 0.7277913689613342Training Epoch: 0 | iteration: 165/262 | Loss: 0.8082101941108704Training Epoch: 0 | iteration: 166/262 | Loss: 0.9040956497192383Training Epoch: 0 | iteration: 167/262 | Loss: 0.8039641380310059Training Epoch: 0 | iteration: 168/262 | Loss: 0.8454896211624146Training Epoch: 0 | iteration: 169/262 | Loss: 0.8781988620758057Training Epoch: 0 | iteration: 170/262 | Loss: 0.8206193447113037Training Epoch: 0 | iteration: 171/262 | Loss: 0.8892490863800049Training Epoch: 0 | iteration: 172/262 | Loss: 0.7493762373924255Training Epoch: 0 | iteration: 173/262 | Loss: 0.9364181756973267Training Epoch: 0 | iteration: 174/262 | Loss: 0.8675311803817749Training Epoch: 0 | iteration: 175/262 | Loss: 0.9408718347549438Training Epoch: 0 | iteration: 176/262 | Loss: 0.8266072273254395Training Epoch: 0 | iteration: 177/262 | Loss: 0.8524360656738281Training Epoch: 0 | iteration: 178/262 | Loss: 0.8782706260681152Training Epoch: 0 | iteration: 179/262 | Loss: 0.7794604897499084Training Epoch: 0 | iteration: 180/262 | Loss: 0.8033684492111206Training Epoch: 0 | iteration: 181/262 | Loss: 0.8406165838241577Training Epoch: 0 | iteration: 182/262 | Loss: 0.8566548824310303Training Epoch: 0 | iteration: 183/262 | Loss: 0.8608055114746094Training Epoch: 0 | iteration: 184/262 | Loss: 0.9123754501342773Training Epoch: 0 | iteration: 185/262 | Loss: 0.8671053051948547Training Epoch: 0 | iteration: 186/262 | Loss: 0.7893699407577515Training Epoch: 0 | iteration: 187/262 | Loss: 0.772700309753418Training Epoch: 0 | iteration: 188/262 | Loss: 0.8267720937728882Training Epoch: 0 | iteration: 189/262 | Loss: 0.8899523019790649Training Epoch: 0 | iteration: 190/262 | Loss: 0.8772594928741455Training Epoch: 0 | iteration: 191/262 | Loss: 0.8027765154838562Training Epoch: 0 | iteration: 192/262 | Loss: 0.8484572172164917Training Epoch: 0 | iteration: 193/262 | Loss: 0.917510986328125Training Epoch: 0 | iteration: 194/262 | Loss: 0.8496725559234619Training Epoch: 0 | iteration: 195/262 | Loss: 0.9098235368728638Training Epoch: 0 | iteration: 196/262 | Loss: 0.8193007707595825Training Epoch: 0 | iteration: 197/262 | Loss: 0.8258392214775085Training Epoch: 0 | iteration: 198/262 | Loss: 0.8531776666641235Training Epoch: 0 | iteration: 199/262 | Loss: 0.865239143371582Training Epoch: 0 | iteration: 200/262 | Loss: 0.8280548453330994Training Epoch: 0 | iteration: 201/262 | Loss: 0.7923955917358398Training Epoch: 0 | iteration: 202/262 | Loss: 0.8407907485961914Training Epoch: 0 | iteration: 203/262 | Loss: 0.7957364916801453Training Epoch: 0 | iteration: 204/262 | Loss: 0.889847457408905Training Epoch: 0 | iteration: 205/262 | Loss: 0.9169673919677734Training Epoch: 0 | iteration: 206/262 | Loss: 0.8565222024917603Training Epoch: 0 | iteration: 207/262 | Loss: 0.9786195755004883Training Epoch: 0 | iteration: 208/262 | Loss: 0.8207329511642456Training Epoch: 0 | iteration: 209/262 | Loss: 0.808684229850769Training Epoch: 0 | iteration: 210/262 | Loss: 0.9541152119636536Training Epoch: 0 | iteration: 211/262 | Loss: 0.8886200189590454Training Epoch: 0 | iteration: 212/262 | Loss: 0.7817436456680298Training Epoch: 0 | iteration: 213/262 | Loss: 0.9575360417366028Training Epoch: 0 | iteration: 214/262 | Loss: 0.8671302199363708Training Epoch: 0 | iteration: 215/262 | Loss: 0.8601583242416382Training Epoch: 0 | iteration: 216/262 | Loss: 0.8995605707168579Training Epoch: 0 | iteration: 217/262 | Loss: 0.9038635492324829Training Epoch: 0 | iteration: 218/262 | Loss: 0.8901523947715759Training Epoch: 0 | iteration: 219/262 | Loss: 0.8868478536605835Training Epoch: 0 | iteration: 220/262 | Loss: 0.7910019159317017Training Epoch: 0 | iteration: 221/262 | Loss: 0.8559907674789429Training Epoch: 0 | iteration: 222/262 | Loss: 0.9039816856384277Training Epoch: 0 | iteration: 223/262 | Loss: 0.8343915939331055Training Epoch: 0 | iteration: 224/262 | Loss: 0.9448286294937134Training Epoch: 0 | iteration: 225/262 | Loss: 0.7515181303024292Training Epoch: 0 | iteration: 226/262 | Loss: 0.7807849049568176Training Epoch: 0 | iteration: 227/262 | Loss: 0.864105761051178Training Epoch: 0 | iteration: 228/262 | Loss: 0.8515332341194153Training Epoch: 0 | iteration: 229/262 | Loss: 0.8539983034133911Training Epoch: 0 | iteration: 230/262 | Loss: 0.9368197917938232Training Epoch: 0 | iteration: 231/262 | Loss: 0.890518069267273Training Epoch: 0 | iteration: 232/262 | Loss: 0.9363836050033569Training Epoch: 0 | iteration: 233/262 | Loss: 0.8345407247543335Training Epoch: 0 | iteration: 234/262 | Loss: 0.8813055157661438Training Epoch: 0 | iteration: 235/262 | Loss: 0.8808707594871521Training Epoch: 0 | iteration: 236/262 | Loss: 0.7725468873977661Training Epoch: 0 | iteration: 237/262 | Loss: 0.870604395866394Training Epoch: 0 | iteration: 238/262 | Loss: 0.8145134449005127Training Epoch: 0 | iteration: 239/262 | Loss: 0.8121696710586548Training Epoch: 0 | iteration: 240/262 | Loss: 0.7636162042617798Training Epoch: 0 | iteration: 241/262 | Loss: 0.8474869728088379Training Epoch: 0 | iteration: 242/262 | Loss: 0.8036525249481201Training Epoch: 0 | iteration: 243/262 | Loss: 0.8223621845245361Training Epoch: 0 | iteration: 244/262 | Loss: 0.8129746913909912Training Epoch: 0 | iteration: 245/262 | Loss: 0.9137758016586304Training Epoch: 0 | iteration: 246/262 | Loss: 0.7793003916740417Training Epoch: 0 | iteration: 247/262 | Loss: 0.8511016964912415Training Epoch: 0 | iteration: 248/262 | Loss: 0.8381954431533813Training Epoch: 0 | iteration: 249/262 | Loss: 0.9036209583282471Training Epoch: 0 | iteration: 250/262 | Loss: 0.9435613751411438Training Epoch: 0 | iteration: 251/262 | Loss: 0.830460250377655Training Epoch: 0 | iteration: 252/262 | Loss: 0.8610146045684814Training Epoch: 0 | iteration: 253/262 | Loss: 0.849233865737915Training Epoch: 0 | iteration: 254/262 | Loss: 0.8311634063720703Training Epoch: 0 | iteration: 255/262 | Loss: 0.8080418109893799Training Epoch: 0 | iteration: 256/262 | Loss: 0.8691425919532776Training Epoch: 0 | iteration: 257/262 | Loss: 0.7408237457275391Training Epoch: 0 | iteration: 258/262 | Loss: 0.8578744530677795Training Epoch: 0 | iteration: 259/262 | Loss: 0.942950963973999Training Epoch: 0 | iteration: 260/262 | Loss: 0.9066576361656189Training Epoch: 0 | iteration: 261/262 | Loss: 0.8033927083015442Validating Epoch: 0 | iteration: 0/66 | Loss: 0.7641960978507996Validating Epoch: 0 | iteration: 1/66 | Loss: 0.7325785160064697Validating Epoch: 0 | iteration: 2/66 | Loss: 0.739876389503479Validating Epoch: 0 | iteration: 3/66 | Loss: 0.6946752071380615Validating Epoch: 0 | iteration: 4/66 | Loss: 0.6597291231155396Validating Epoch: 0 | iteration: 5/66 | Loss: 0.6909870505332947Validating Epoch: 0 | iteration: 6/66 | Loss: 0.7340924143791199Validating Epoch: 0 | iteration: 7/66 | Loss: 0.6899427175521851Validating Epoch: 0 | iteration: 8/66 | Loss: 0.8341760039329529Validating Epoch: 0 | iteration: 9/66 | Loss: 0.7659525275230408Validating Epoch: 0 | iteration: 10/66 | Loss: 0.7379174828529358Validating Epoch: 0 | iteration: 11/66 | Loss: 0.735431432723999Validating Epoch: 0 | iteration: 12/66 | Loss: 0.6861017346382141Validating Epoch: 0 | iteration: 13/66 | Loss: 0.6943552494049072Validating Epoch: 0 | iteration: 14/66 | Loss: 0.6976581811904907Validating Epoch: 0 | iteration: 15/66 | Loss: 0.7166599631309509Validating Epoch: 0 | iteration: 16/66 | Loss: 0.6472290754318237Validating Epoch: 0 | iteration: 17/66 | Loss: 0.7012767791748047Validating Epoch: 0 | iteration: 18/66 | Loss: 0.6851094365119934Validating Epoch: 0 | iteration: 19/66 | Loss: 0.8265683650970459Validating Epoch: 0 | iteration: 20/66 | Loss: 0.7220665216445923Validating Epoch: 0 | iteration: 21/66 | Loss: 0.7018671035766602Validating Epoch: 0 | iteration: 22/66 | Loss: 0.6465185880661011Validating Epoch: 0 | iteration: 23/66 | Loss: 0.7072721123695374Validating Epoch: 0 | iteration: 24/66 | Loss: 0.6695988774299622Validating Epoch: 0 | iteration: 25/66 | Loss: 0.7032209038734436Validating Epoch: 0 | iteration: 26/66 | Loss: 0.7740695476531982Validating Epoch: 0 | iteration: 27/66 | Loss: 0.7250996828079224Validating Epoch: 0 | iteration: 28/66 | Loss: 0.7110564112663269Validating Epoch: 0 | iteration: 29/66 | Loss: 0.670850396156311Validating Epoch: 0 | iteration: 30/66 | Loss: 0.6882508993148804Validating Epoch: 0 | iteration: 31/66 | Loss: 0.6922330856323242Validating Epoch: 0 | iteration: 32/66 | Loss: 0.7274662256240845Validating Epoch: 0 | iteration: 33/66 | Loss: 0.7205244898796082Validating Epoch: 0 | iteration: 34/66 | Loss: 0.6738358736038208Validating Epoch: 0 | iteration: 35/66 | Loss: 0.6839907169342041Validating Epoch: 0 | iteration: 36/66 | Loss: 0.6936450600624084Validating Epoch: 0 | iteration: 37/66 | Loss: 0.683907151222229Validating Epoch: 0 | iteration: 38/66 | Loss: 0.6970847845077515Validating Epoch: 0 | iteration: 39/66 | Loss: 0.7147085666656494Validating Epoch: 0 | iteration: 40/66 | Loss: 0.6778548955917358Validating Epoch: 0 | iteration: 41/66 | Loss: 0.7466016411781311Validating Epoch: 0 | iteration: 42/66 | Loss: 0.7322887182235718Validating Epoch: 0 | iteration: 43/66 | Loss: 0.6996190547943115Validating Epoch: 0 | iteration: 44/66 | Loss: 0.7066519856452942Validating Epoch: 0 | iteration: 45/66 | Loss: 0.7093231678009033Validating Epoch: 0 | iteration: 46/66 | Loss: 0.7291143536567688Validating Epoch: 0 | iteration: 47/66 | Loss: 0.7397773861885071Validating Epoch: 0 | iteration: 48/66 | Loss: 0.7431771755218506Validating Epoch: 0 | iteration: 49/66 | Loss: 0.6743093729019165Validating Epoch: 0 | iteration: 50/66 | Loss: 0.7464070916175842Validating Epoch: 0 | iteration: 51/66 | Loss: 0.7166440486907959Validating Epoch: 0 | iteration: 52/66 | Loss: 0.7085810899734497Validating Epoch: 0 | iteration: 53/66 | Loss: 0.7177305221557617Validating Epoch: 0 | iteration: 54/66 | Loss: 0.7772795557975769Validating Epoch: 0 | iteration: 55/66 | Loss: 0.756432294845581Validating Epoch: 0 | iteration: 56/66 | Loss: 0.7014663219451904Validating Epoch: 0 | iteration: 57/66 | Loss: 0.711821436882019Validating Epoch: 0 | iteration: 58/66 | Loss: 0.7375568151473999Validating Epoch: 0 | iteration: 59/66 | Loss: 0.7530452013015747Validating Epoch: 0 | iteration: 60/66 | Loss: 0.7903062105178833Validating Epoch: 0 | iteration: 61/66 | Loss: 0.7435352802276611Validating Epoch: 0 | iteration: 62/66 | Loss: 0.7032892107963562Validating Epoch: 0 | iteration: 63/66 | Loss: 0.7611985802650452Validating Epoch: 0 | iteration: 64/66 | Loss: 0.7252063155174255Validating Epoch: 0 | iteration: 65/66 | Loss: 0.5983192324638367Temperature:  0.5
Sampling: iteration: 0/51{'Validity': 0.9833984375, 'Novelty': 1.0, 'Uniqueness': 0.9950347567030785}
Training Epoch: 1 | iteration: 0/262 | Loss: 0.78786301612854Training Epoch: 1 | iteration: 1/262 | Loss: 0.7918834686279297Training Epoch: 1 | iteration: 2/262 | Loss: 0.7645639181137085Training Epoch: 1 | iteration: 3/262 | Loss: 0.7659199237823486Training Epoch: 1 | iteration: 4/262 | Loss: 0.8354549407958984Training Epoch: 1 | iteration: 5/262 | Loss: 0.7673057913780212Training Epoch: 1 | iteration: 6/262 | Loss: 0.7502208948135376Training Epoch: 1 | iteration: 7/262 | Loss: 0.7973536252975464Training Epoch: 1 | iteration: 8/262 | Loss: 0.752562403678894Training Epoch: 1 | iteration: 9/262 | Loss: 0.8240183591842651Training Epoch: 1 | iteration: 10/262 | Loss: 0.842186689376831Training Epoch: 1 | iteration: 11/262 | Loss: 0.9948948621749878Training Epoch: 1 | iteration: 12/262 | Loss: 0.8667123317718506Training Epoch: 1 | iteration: 13/262 | Loss: 0.7948631048202515Training Epoch: 1 | iteration: 14/262 | Loss: 0.7408413887023926Training Epoch: 1 | iteration: 15/262 | Loss: 0.7911733388900757Training Epoch: 1 | iteration: 16/262 | Loss: 0.7887481451034546Training Epoch: 1 | iteration: 17/262 | Loss: 0.8110561370849609Training Epoch: 1 | iteration: 18/262 | Loss: 0.7633428573608398Training Epoch: 1 | iteration: 19/262 | Loss: 0.7622493505477905Training Epoch: 1 | iteration: 20/262 | Loss: 0.836459755897522Training Epoch: 1 | iteration: 21/262 | Loss: 0.8230975866317749Training Epoch: 1 | iteration: 22/262 | Loss: 0.8958492875099182Training Epoch: 1 | iteration: 23/262 | Loss: 0.8000231981277466Training Epoch: 1 | iteration: 24/262 | Loss: 0.8579543232917786Training Epoch: 1 | iteration: 25/262 | Loss: 0.8439045548439026Training Epoch: 1 | iteration: 26/262 | Loss: 0.8304675221443176Training Epoch: 1 | iteration: 27/262 | Loss: 0.7663416266441345Training Epoch: 1 | iteration: 28/262 | Loss: 0.8260756731033325Training Epoch: 1 | iteration: 29/262 | Loss: 0.8906435966491699Training Epoch: 1 | iteration: 30/262 | Loss: 0.7278918027877808Training Epoch: 1 | iteration: 31/262 | Loss: 0.8424863815307617Training Epoch: 1 | iteration: 32/262 | Loss: 0.7366512417793274Training Epoch: 1 | iteration: 33/262 | Loss: 0.8506507277488708Training Epoch: 1 | iteration: 34/262 | Loss: 0.8773152828216553Training Epoch: 1 | iteration: 35/262 | Loss: 0.7963352799415588Training Epoch: 1 | iteration: 36/262 | Loss: 0.7740787267684937Training Epoch: 1 | iteration: 37/262 | Loss: 0.8802656531333923Training Epoch: 1 | iteration: 38/262 | Loss: 0.8073825240135193Training Epoch: 1 | iteration: 39/262 | Loss: 0.8182346820831299Training Epoch: 1 | iteration: 40/262 | Loss: 0.7764251232147217Training Epoch: 1 | iteration: 41/262 | Loss: 0.8077477812767029Training Epoch: 1 | iteration: 42/262 | Loss: 0.8114724159240723Training Epoch: 1 | iteration: 43/262 | Loss: 0.890790581703186Training Epoch: 1 | iteration: 44/262 | Loss: 0.7865311503410339Training Epoch: 1 | iteration: 45/262 | Loss: 0.8520510792732239Training Epoch: 1 | iteration: 46/262 | Loss: 0.8509852886199951Training Epoch: 1 | iteration: 47/262 | Loss: 0.7963488101959229Training Epoch: 1 | iteration: 48/262 | Loss: 0.7475981712341309Training Epoch: 1 | iteration: 49/262 | Loss: 0.7589462995529175Training Epoch: 1 | iteration: 50/262 | Loss: 0.8170208930969238Training Epoch: 1 | iteration: 51/262 | Loss: 0.7872495651245117Training Epoch: 1 | iteration: 52/262 | Loss: 0.7880649566650391Training Epoch: 1 | iteration: 53/262 | Loss: 0.8191715478897095Training Epoch: 1 | iteration: 54/262 | Loss: 0.8563307523727417Training Epoch: 1 | iteration: 55/262 | Loss: 0.7681956887245178Training Epoch: 1 | iteration: 56/262 | Loss: 0.8259910345077515Training Epoch: 1 | iteration: 57/262 | Loss: 0.8560222387313843Training Epoch: 1 | iteration: 58/262 | Loss: 0.7351875305175781Training Epoch: 1 | iteration: 59/262 | Loss: 0.7967605590820312Training Epoch: 1 | iteration: 60/262 | Loss: 0.8554421663284302Training Epoch: 1 | iteration: 61/262 | Loss: 0.8397687673568726Training Epoch: 1 | iteration: 62/262 | Loss: 0.7993489503860474Training Epoch: 1 | iteration: 63/262 | Loss: 0.7255173325538635Training Epoch: 1 | iteration: 64/262 | Loss: 0.8745228052139282Training Epoch: 1 | iteration: 65/262 | Loss: 0.7713054418563843Training Epoch: 1 | iteration: 66/262 | Loss: 0.7993789315223694Training Epoch: 1 | iteration: 67/262 | Loss: 0.7690976858139038Training Epoch: 1 | iteration: 68/262 | Loss: 0.9202117919921875Training Epoch: 1 | iteration: 69/262 | Loss: 0.9483093023300171Training Epoch: 1 | iteration: 70/262 | Loss: 0.7688509225845337Training Epoch: 1 | iteration: 71/262 | Loss: 0.8258382678031921Training Epoch: 1 | iteration: 72/262 | Loss: 0.8313436508178711Training Epoch: 1 | iteration: 73/262 | Loss: 0.7454671859741211Training Epoch: 1 | iteration: 74/262 | Loss: 0.7271723747253418Training Epoch: 1 | iteration: 75/262 | Loss: 0.7901188135147095Training Epoch: 1 | iteration: 76/262 | Loss: 0.9009221792221069Training Epoch: 1 | iteration: 77/262 | Loss: 0.7789786458015442Training Epoch: 1 | iteration: 78/262 | Loss: 0.9112943410873413Training Epoch: 1 | iteration: 79/262 | Loss: 0.8970494270324707Training Epoch: 1 | iteration: 80/262 | Loss: 0.7974190711975098Training Epoch: 1 | iteration: 81/262 | Loss: 0.7752491235733032Training Epoch: 1 | iteration: 82/262 | Loss: 0.8037992715835571Training Epoch: 1 | iteration: 83/262 | Loss: 0.8430328369140625Training Epoch: 1 | iteration: 84/262 | Loss: 0.8525199294090271Training Epoch: 1 | iteration: 85/262 | Loss: 0.7692956328392029Training Epoch: 1 | iteration: 86/262 | Loss: 0.8078169822692871Training Epoch: 1 | iteration: 87/262 | Loss: 0.7873252630233765Training Epoch: 1 | iteration: 88/262 | Loss: 0.8018748760223389Training Epoch: 1 | iteration: 89/262 | Loss: 0.7102890014648438Training Epoch: 1 | iteration: 90/262 | Loss: 0.7817749381065369Training Epoch: 1 | iteration: 91/262 | Loss: 0.8062900304794312Training Epoch: 1 | iteration: 92/262 | Loss: 0.7988883852958679Training Epoch: 1 | iteration: 93/262 | Loss: 0.7298420667648315Training Epoch: 1 | iteration: 94/262 | Loss: 0.8767818212509155Training Epoch: 1 | iteration: 95/262 | Loss: 0.7370840907096863Training Epoch: 1 | iteration: 96/262 | Loss: 0.7929160594940186Training Epoch: 1 | iteration: 97/262 | Loss: 0.7695114016532898Training Epoch: 1 | iteration: 98/262 | Loss: 0.7478086948394775Training Epoch: 1 | iteration: 99/262 | Loss: 0.8517859578132629Training Epoch: 1 | iteration: 100/262 | Loss: 0.6781599521636963Training Epoch: 1 | iteration: 101/262 | Loss: 0.7526487708091736Training Epoch: 1 | iteration: 102/262 | Loss: 0.863499104976654Training Epoch: 1 | iteration: 103/262 | Loss: 0.8704920411109924Training Epoch: 1 | iteration: 104/262 | Loss: 0.8628767132759094Training Epoch: 1 | iteration: 105/262 | Loss: 0.7906891107559204Training Epoch: 1 | iteration: 106/262 | Loss: 0.831663966178894Training Epoch: 1 | iteration: 107/262 | Loss: 0.766790509223938Training Epoch: 1 | iteration: 108/262 | Loss: 0.8841120004653931Training Epoch: 1 | iteration: 109/262 | Loss: 0.8333467841148376Training Epoch: 1 | iteration: 110/262 | Loss: 0.7680407166481018Training Epoch: 1 | iteration: 111/262 | Loss: 0.8544778227806091Training Epoch: 1 | iteration: 112/262 | Loss: 0.8535323739051819Training Epoch: 1 | iteration: 113/262 | Loss: 0.7719355821609497Training Epoch: 1 | iteration: 114/262 | Loss: 0.7450704574584961Training Epoch: 1 | iteration: 115/262 | Loss: 0.7855318784713745Training Epoch: 1 | iteration: 116/262 | Loss: 0.8194319605827332Training Epoch: 1 | iteration: 117/262 | Loss: 0.7955178022384644Training Epoch: 1 | iteration: 118/262 | Loss: 0.8712269067764282Training Epoch: 1 | iteration: 119/262 | Loss: 0.7814918756484985Training Epoch: 1 | iteration: 120/262 | Loss: 0.8688664436340332Training Epoch: 1 | iteration: 121/262 | Loss: 0.8405633568763733Training Epoch: 1 | iteration: 122/262 | Loss: 0.82298743724823Training Epoch: 1 | iteration: 123/262 | Loss: 0.7963683605194092Training Epoch: 1 | iteration: 124/262 | Loss: 0.8074904680252075Training Epoch: 1 | iteration: 125/262 | Loss: 0.7654373049736023Training Epoch: 1 | iteration: 126/262 | Loss: 0.8176783323287964Training Epoch: 1 | iteration: 127/262 | Loss: 0.8141465187072754Training Epoch: 1 | iteration: 128/262 | Loss: 0.8103541135787964Training Epoch: 1 | iteration: 129/262 | Loss: 0.858277440071106Training Epoch: 1 | iteration: 130/262 | Loss: 0.7777343988418579Training Epoch: 1 | iteration: 131/262 | Loss: 0.8183311223983765Training Epoch: 1 | iteration: 132/262 | Loss: 0.801315188407898Training Epoch: 1 | iteration: 133/262 | Loss: 0.7970645427703857Training Epoch: 1 | iteration: 134/262 | Loss: 0.7230854034423828Training Epoch: 1 | iteration: 135/262 | Loss: 0.8267993927001953Training Epoch: 1 | iteration: 136/262 | Loss: 0.7156153917312622Training Epoch: 1 | iteration: 137/262 | Loss: 0.7862481474876404Training Epoch: 1 | iteration: 138/262 | Loss: 0.8332862854003906Training Epoch: 1 | iteration: 139/262 | Loss: 0.7345089912414551Training Epoch: 1 | iteration: 140/262 | Loss: 0.7588943839073181Training Epoch: 1 | iteration: 141/262 | Loss: 0.838463306427002Training Epoch: 1 | iteration: 142/262 | Loss: 0.804238498210907Training Epoch: 1 | iteration: 143/262 | Loss: 0.775312066078186Training Epoch: 1 | iteration: 144/262 | Loss: 0.869114875793457Training Epoch: 1 | iteration: 145/262 | Loss: 0.7810646891593933Training Epoch: 1 | iteration: 146/262 | Loss: 0.8914135694503784Training Epoch: 1 | iteration: 147/262 | Loss: 0.8507711291313171Training Epoch: 1 | iteration: 148/262 | Loss: 0.8053841590881348Training Epoch: 1 | iteration: 149/262 | Loss: 0.7548216581344604Training Epoch: 1 | iteration: 150/262 | Loss: 0.8365730047225952Training Epoch: 1 | iteration: 151/262 | Loss: 0.7494034767150879Training Epoch: 1 | iteration: 152/262 | Loss: 0.8326363563537598Training Epoch: 1 | iteration: 153/262 | Loss: 0.867996335029602Training Epoch: 1 | iteration: 154/262 | Loss: 0.6976770758628845Training Epoch: 1 | iteration: 155/262 | Loss: 0.7752647399902344Training Epoch: 1 | iteration: 156/262 | Loss: 0.8496304750442505Training Epoch: 1 | iteration: 157/262 | Loss: 0.8228789567947388Training Epoch: 1 | iteration: 158/262 | Loss: 0.818352460861206Training Epoch: 1 | iteration: 159/262 | Loss: 0.8549710512161255Training Epoch: 1 | iteration: 160/262 | Loss: 0.80242919921875Training Epoch: 1 | iteration: 161/262 | Loss: 0.8162989616394043Training Epoch: 1 | iteration: 162/262 | Loss: 0.8513122200965881Training Epoch: 1 | iteration: 163/262 | Loss: 0.7564713954925537Training Epoch: 1 | iteration: 164/262 | Loss: 0.7466951608657837Training Epoch: 1 | iteration: 165/262 | Loss: 0.7554112672805786Training Epoch: 1 | iteration: 166/262 | Loss: 0.7914053797721863Training Epoch: 1 | iteration: 167/262 | Loss: 0.8400273323059082Training Epoch: 1 | iteration: 168/262 | Loss: 0.788023829460144Training Epoch: 1 | iteration: 169/262 | Loss: 0.8138801455497742Training Epoch: 1 | iteration: 170/262 | Loss: 0.8263481855392456Training Epoch: 1 | iteration: 171/262 | Loss: 0.6865324974060059Training Epoch: 1 | iteration: 172/262 | Loss: 0.8060377836227417Training Epoch: 1 | iteration: 173/262 | Loss: 0.9061322808265686Training Epoch: 1 | iteration: 174/262 | Loss: 0.7841249108314514Training Epoch: 1 | iteration: 175/262 | Loss: 0.8421236276626587Training Epoch: 1 | iteration: 176/262 | Loss: 0.7980242967605591Training Epoch: 1 | iteration: 177/262 | Loss: 0.7374438047409058Training Epoch: 1 | iteration: 178/262 | Loss: 0.702387809753418Training Epoch: 1 | iteration: 179/262 | Loss: 0.7113100290298462Training Epoch: 1 | iteration: 180/262 | Loss: 0.8278603553771973Training Epoch: 1 | iteration: 181/262 | Loss: 0.7717584371566772Training Epoch: 1 | iteration: 182/262 | Loss: 0.8899999856948853Training Epoch: 1 | iteration: 183/262 | Loss: 0.7258869409561157Training Epoch: 1 | iteration: 184/262 | Loss: 0.7208489179611206Training Epoch: 1 | iteration: 185/262 | Loss: 0.9393917322158813Training Epoch: 1 | iteration: 186/262 | Loss: 0.8179381489753723Training Epoch: 1 | iteration: 187/262 | Loss: 0.8299386501312256Training Epoch: 1 | iteration: 188/262 | Loss: 0.7283090353012085Training Epoch: 1 | iteration: 189/262 | Loss: 0.8131868243217468Training Epoch: 1 | iteration: 190/262 | Loss: 0.8648394346237183Training Epoch: 1 | iteration: 191/262 | Loss: 0.7354334592819214Training Epoch: 1 | iteration: 192/262 | Loss: 0.7417129278182983Training Epoch: 1 | iteration: 193/262 | Loss: 0.7688562870025635Training Epoch: 1 | iteration: 194/262 | Loss: 0.8470948934555054Training Epoch: 1 | iteration: 195/262 | Loss: 0.8480214476585388Training Epoch: 1 | iteration: 196/262 | Loss: 0.8818607330322266Training Epoch: 1 | iteration: 197/262 | Loss: 0.8376408219337463Training Epoch: 1 | iteration: 198/262 | Loss: 0.7981041669845581Training Epoch: 1 | iteration: 199/262 | Loss: 0.9106581211090088Training Epoch: 1 | iteration: 200/262 | Loss: 0.746403694152832Training Epoch: 1 | iteration: 201/262 | Loss: 0.767375111579895Training Epoch: 1 | iteration: 202/262 | Loss: 0.9002863168716431Training Epoch: 1 | iteration: 203/262 | Loss: 0.7741870880126953Training Epoch: 1 | iteration: 204/262 | Loss: 0.8847422003746033Training Epoch: 1 | iteration: 205/262 | Loss: 0.7985132336616516Training Epoch: 1 | iteration: 206/262 | Loss: 0.7920703291893005Training Epoch: 1 | iteration: 207/262 | Loss: 0.8497870564460754Training Epoch: 1 | iteration: 208/262 | Loss: 0.7816945314407349Training Epoch: 1 | iteration: 209/262 | Loss: 0.7418844103813171Training Epoch: 1 | iteration: 210/262 | Loss: 0.7534939646720886Training Epoch: 1 | iteration: 211/262 | Loss: 0.6896357536315918Training Epoch: 1 | iteration: 212/262 | Loss: 0.8144731521606445Training Epoch: 1 | iteration: 213/262 | Loss: 0.7463797330856323Training Epoch: 1 | iteration: 214/262 | Loss: 0.8871200084686279Training Epoch: 1 | iteration: 215/262 | Loss: 0.7984914779663086Training Epoch: 1 | iteration: 216/262 | Loss: 0.7997655868530273Training Epoch: 1 | iteration: 217/262 | Loss: 0.8593355417251587Training Epoch: 1 | iteration: 218/262 | Loss: 0.8540205359458923Training Epoch: 1 | iteration: 219/262 | Loss: 0.7843198776245117Training Epoch: 1 | iteration: 220/262 | Loss: 0.7733141779899597Training Epoch: 1 | iteration: 221/262 | Loss: 0.8100265264511108Training Epoch: 1 | iteration: 222/262 | Loss: 0.7626885175704956Training Epoch: 1 | iteration: 223/262 | Loss: 0.8138739466667175Training Epoch: 1 | iteration: 224/262 | Loss: 0.8462265729904175Training Epoch: 1 | iteration: 225/262 | Loss: 0.8250590562820435Training Epoch: 1 | iteration: 226/262 | Loss: 0.811389148235321Training Epoch: 1 | iteration: 227/262 | Loss: 0.7094479203224182Training Epoch: 1 | iteration: 228/262 | Loss: 0.6160920262336731Training Epoch: 1 | iteration: 229/262 | Loss: 0.8233007192611694Training Epoch: 1 | iteration: 230/262 | Loss: 0.8897764682769775Training Epoch: 1 | iteration: 231/262 | Loss: 0.9160851240158081Training Epoch: 1 | iteration: 232/262 | Loss: 0.827107310295105Training Epoch: 1 | iteration: 233/262 | Loss: 0.797644853591919Training Epoch: 1 | iteration: 234/262 | Loss: 0.7903008460998535Training Epoch: 1 | iteration: 235/262 | Loss: 0.7640170454978943Training Epoch: 1 | iteration: 236/262 | Loss: 0.7490744590759277Training Epoch: 1 | iteration: 237/262 | Loss: 0.8015127182006836Training Epoch: 1 | iteration: 238/262 | Loss: 0.7733415365219116Training Epoch: 1 | iteration: 239/262 | Loss: 0.7452822923660278Training Epoch: 1 | iteration: 240/262 | Loss: 0.9880238771438599Training Epoch: 1 | iteration: 241/262 | Loss: 0.791520357131958Training Epoch: 1 | iteration: 242/262 | Loss: 0.7554223537445068Training Epoch: 1 | iteration: 243/262 | Loss: 0.8583225011825562Training Epoch: 1 | iteration: 244/262 | Loss: 0.7641462683677673Training Epoch: 1 | iteration: 245/262 | Loss: 0.8098176121711731Training Epoch: 1 | iteration: 246/262 | Loss: 0.7421619296073914Training Epoch: 1 | iteration: 247/262 | Loss: 0.8882699012756348Training Epoch: 1 | iteration: 248/262 | Loss: 0.7891950607299805Training Epoch: 1 | iteration: 249/262 | Loss: 0.8287218809127808Training Epoch: 1 | iteration: 250/262 | Loss: 0.7763476371765137Training Epoch: 1 | iteration: 251/262 | Loss: 0.7655853033065796Training Epoch: 1 | iteration: 252/262 | Loss: 0.7097074389457703Training Epoch: 1 | iteration: 253/262 | Loss: 0.7687221765518188Training Epoch: 1 | iteration: 254/262 | Loss: 0.8407307863235474Training Epoch: 1 | iteration: 255/262 | Loss: 0.7228777408599854Training Epoch: 1 | iteration: 256/262 | Loss: 0.7423968315124512Training Epoch: 1 | iteration: 257/262 | Loss: 0.7898712158203125Training Epoch: 1 | iteration: 258/262 | Loss: 0.7583000659942627Training Epoch: 1 | iteration: 259/262 | Loss: 0.7936643362045288Training Epoch: 1 | iteration: 260/262 | Loss: 0.6891717910766602Training Epoch: 1 | iteration: 261/262 | Loss: 0.7159565687179565Validating Epoch: 1 | iteration: 0/66 | Loss: 0.660393476486206Validating Epoch: 1 | iteration: 1/66 | Loss: 0.7024816870689392Validating Epoch: 1 | iteration: 2/66 | Loss: 0.7013260722160339Validating Epoch: 1 | iteration: 3/66 | Loss: 0.7120270729064941Validating Epoch: 1 | iteration: 4/66 | Loss: 0.6700584888458252Validating Epoch: 1 | iteration: 5/66 | Loss: 0.6879194974899292Validating Epoch: 1 | iteration: 6/66 | Loss: 0.693790078163147Validating Epoch: 1 | iteration: 7/66 | Loss: 0.6833250522613525Validating Epoch: 1 | iteration: 8/66 | Loss: 0.6418417692184448Validating Epoch: 1 | iteration: 9/66 | Loss: 0.6621062755584717Validating Epoch: 1 | iteration: 10/66 | Loss: 0.6790975332260132Validating Epoch: 1 | iteration: 11/66 | Loss: 0.7424693703651428Validating Epoch: 1 | iteration: 12/66 | Loss: 0.6481218934059143Validating Epoch: 1 | iteration: 13/66 | Loss: 0.7266550064086914Validating Epoch: 1 | iteration: 14/66 | Loss: 0.712814450263977Validating Epoch: 1 | iteration: 15/66 | Loss: 0.6915193796157837Validating Epoch: 1 | iteration: 16/66 | Loss: 0.7443351149559021Validating Epoch: 1 | iteration: 17/66 | Loss: 0.7333052754402161Validating Epoch: 1 | iteration: 18/66 | Loss: 0.6783140897750854Validating Epoch: 1 | iteration: 19/66 | Loss: 0.6474257111549377Validating Epoch: 1 | iteration: 20/66 | Loss: 0.6939469575881958Validating Epoch: 1 | iteration: 21/66 | Loss: 0.7150145769119263Validating Epoch: 1 | iteration: 22/66 | Loss: 0.7223916053771973Validating Epoch: 1 | iteration: 23/66 | Loss: 0.6521127223968506Validating Epoch: 1 | iteration: 24/66 | Loss: 0.7670159935951233Validating Epoch: 1 | iteration: 25/66 | Loss: 0.6908540725708008Validating Epoch: 1 | iteration: 26/66 | Loss: 0.7209622859954834Validating Epoch: 1 | iteration: 27/66 | Loss: 0.6824440360069275Validating Epoch: 1 | iteration: 28/66 | Loss: 0.6982160210609436Validating Epoch: 1 | iteration: 29/66 | Loss: 0.7057211399078369Validating Epoch: 1 | iteration: 30/66 | Loss: 0.7057733535766602Validating Epoch: 1 | iteration: 31/66 | Loss: 0.6805019974708557Validating Epoch: 1 | iteration: 32/66 | Loss: 0.7356238961219788Validating Epoch: 1 | iteration: 33/66 | Loss: 0.7147545218467712Validating Epoch: 1 | iteration: 34/66 | Loss: 0.6659572720527649Validating Epoch: 1 | iteration: 35/66 | Loss: 0.7008234262466431Validating Epoch: 1 | iteration: 36/66 | Loss: 0.7417987585067749Validating Epoch: 1 | iteration: 37/66 | Loss: 0.7051562070846558Validating Epoch: 1 | iteration: 38/66 | Loss: 0.6734581589698792Validating Epoch: 1 | iteration: 39/66 | Loss: 0.7574936747550964Validating Epoch: 1 | iteration: 40/66 | Loss: 0.7699378728866577Validating Epoch: 1 | iteration: 41/66 | Loss: 0.7251102924346924Validating Epoch: 1 | iteration: 42/66 | Loss: 0.7089680433273315Validating Epoch: 1 | iteration: 43/66 | Loss: 0.7372307777404785Validating Epoch: 1 | iteration: 44/66 | Loss: 0.7158492207527161Validating Epoch: 1 | iteration: 45/66 | Loss: 0.706195592880249Validating Epoch: 1 | iteration: 46/66 | Loss: 0.7635184526443481Validating Epoch: 1 | iteration: 47/66 | Loss: 0.7548617720603943Validating Epoch: 1 | iteration: 48/66 | Loss: 0.6985912322998047Validating Epoch: 1 | iteration: 49/66 | Loss: 0.748638391494751Validating Epoch: 1 | iteration: 50/66 | Loss: 0.6727966070175171Validating Epoch: 1 | iteration: 51/66 | Loss: 0.7077047824859619Validating Epoch: 1 | iteration: 52/66 | Loss: 0.736575722694397Validating Epoch: 1 | iteration: 53/66 | Loss: 0.6964946985244751Validating Epoch: 1 | iteration: 54/66 | Loss: 0.7730897665023804Validating Epoch: 1 | iteration: 55/66 | Loss: 0.7294934988021851Validating Epoch: 1 | iteration: 56/66 | Loss: 0.6901581883430481Validating Epoch: 1 | iteration: 57/66 | Loss: 0.7144778370857239Validating Epoch: 1 | iteration: 58/66 | Loss: 0.710185170173645Validating Epoch: 1 | iteration: 59/66 | Loss: 0.7360703945159912Validating Epoch: 1 | iteration: 60/66 | Loss: 0.7147563695907593Validating Epoch: 1 | iteration: 61/66 | Loss: 0.6865164637565613Validating Epoch: 1 | iteration: 62/66 | Loss: 0.7571535110473633Validating Epoch: 1 | iteration: 63/66 | Loss: 0.7016932964324951Validating Epoch: 1 | iteration: 64/66 | Loss: 0.6785659790039062Validating Epoch: 1 | iteration: 65/66 | Loss: 0.6236384510993958Temperature:  0.5
Sampling: iteration: 0/51{'Validity': 0.998046875, 'Novelty': 1.0, 'Uniqueness': 0.9980430528375733}
Training Epoch: 2 | iteration: 0/262 | Loss: 0.7350258827209473Training Epoch: 2 | iteration: 1/262 | Loss: 0.7466015815734863Training Epoch: 2 | iteration: 2/262 | Loss: 0.779003381729126Training Epoch: 2 | iteration: 3/262 | Loss: 0.7873080968856812Training Epoch: 2 | iteration: 4/262 | Loss: 0.7912975549697876Training Epoch: 2 | iteration: 5/262 | Loss: 0.7606717944145203Training Epoch: 2 | iteration: 6/262 | Loss: 0.7564821243286133Training Epoch: 2 | iteration: 7/262 | Loss: 0.7727423906326294Training Epoch: 2 | iteration: 8/262 | Loss: 0.7434998154640198Training Epoch: 2 | iteration: 9/262 | Loss: 0.7484225034713745Training Epoch: 2 | iteration: 10/262 | Loss: 0.6885831356048584Training Epoch: 2 | iteration: 11/262 | Loss: 0.8264627456665039Training Epoch: 2 | iteration: 12/262 | Loss: 0.8006176352500916Training Epoch: 2 | iteration: 13/262 | Loss: 0.7415474653244019Training Epoch: 2 | iteration: 14/262 | Loss: 0.7533398270606995Training Epoch: 2 | iteration: 15/262 | Loss: 0.786900520324707Training Epoch: 2 | iteration: 16/262 | Loss: 0.7820498943328857Training Epoch: 2 | iteration: 17/262 | Loss: 0.826725959777832Training Epoch: 2 | iteration: 18/262 | Loss: 0.7328594923019409Training Epoch: 2 | iteration: 19/262 | Loss: 0.7491941452026367Training Epoch: 2 | iteration: 20/262 | Loss: 0.7852895259857178Training Epoch: 2 | iteration: 21/262 | Loss: 0.7605103850364685Training Epoch: 2 | iteration: 22/262 | Loss: 0.8635788559913635Training Epoch: 2 | iteration: 23/262 | Loss: 0.7143071889877319Training Epoch: 2 | iteration: 24/262 | Loss: 0.7442485094070435Training Epoch: 2 | iteration: 25/262 | Loss: 0.7754226326942444Training Epoch: 2 | iteration: 26/262 | Loss: 0.7503331899642944Training Epoch: 2 | iteration: 27/262 | Loss: 0.724132239818573Training Epoch: 2 | iteration: 28/262 | Loss: 0.7942574620246887Training Epoch: 2 | iteration: 29/262 | Loss: 0.7701317667961121Training Epoch: 2 | iteration: 30/262 | Loss: 0.7666967511177063Training Epoch: 2 | iteration: 31/262 | Loss: 0.7353904247283936Training Epoch: 2 | iteration: 32/262 | Loss: 0.7737429738044739Training Epoch: 2 | iteration: 33/262 | Loss: 0.8210940361022949Training Epoch: 2 | iteration: 34/262 | Loss: 0.7237000465393066Training Epoch: 2 | iteration: 35/262 | Loss: 0.784085750579834Training Epoch: 2 | iteration: 36/262 | Loss: 0.7546091079711914Training Epoch: 2 | iteration: 37/262 | Loss: 0.7824456691741943Training Epoch: 2 | iteration: 38/262 | Loss: 0.6624923944473267Training Epoch: 2 | iteration: 39/262 | Loss: 0.7626972198486328Training Epoch: 2 | iteration: 40/262 | Loss: 0.7910335063934326Training Epoch: 2 | iteration: 41/262 | Loss: 0.842235267162323Training Epoch: 2 | iteration: 42/262 | Loss: 0.7291465997695923Training Epoch: 2 | iteration: 43/262 | Loss: 0.7058782577514648Training Epoch: 2 | iteration: 44/262 | Loss: 0.770926833152771Training Epoch: 2 | iteration: 45/262 | Loss: 0.7457582950592041Training Epoch: 2 | iteration: 46/262 | Loss: 0.6878696084022522Training Epoch: 2 | iteration: 47/262 | Loss: 0.7583020329475403Training Epoch: 2 | iteration: 48/262 | Loss: 0.756188154220581Training Epoch: 2 | iteration: 49/262 | Loss: 0.8041355013847351Training Epoch: 2 | iteration: 50/262 | Loss: 0.7542062997817993Training Epoch: 2 | iteration: 51/262 | Loss: 0.7370510101318359Training Epoch: 2 | iteration: 52/262 | Loss: 0.6739922761917114Training Epoch: 2 | iteration: 53/262 | Loss: 0.8055094480514526Training Epoch: 2 | iteration: 54/262 | Loss: 0.8233882188796997Training Epoch: 2 | iteration: 55/262 | Loss: 0.8247008323669434Training Epoch: 2 | iteration: 56/262 | Loss: 0.7980546355247498Training Epoch: 2 | iteration: 57/262 | Loss: 0.7496408820152283Training Epoch: 2 | iteration: 58/262 | Loss: 0.8163377046585083Training Epoch: 2 | iteration: 59/262 | Loss: 0.7700833678245544Training Epoch: 2 | iteration: 60/262 | Loss: 0.7755274772644043Training Epoch: 2 | iteration: 61/262 | Loss: 0.68204665184021Training Epoch: 2 | iteration: 62/262 | Loss: 0.7898277044296265Training Epoch: 2 | iteration: 63/262 | Loss: 0.7285981178283691Training Epoch: 2 | iteration: 64/262 | Loss: 0.8011924028396606Training Epoch: 2 | iteration: 65/262 | Loss: 0.6997705698013306Training Epoch: 2 | iteration: 66/262 | Loss: 0.6291214227676392Training Epoch: 2 | iteration: 67/262 | Loss: 0.7030933499336243Training Epoch: 2 | iteration: 68/262 | Loss: 0.7118797302246094Training Epoch: 2 | iteration: 69/262 | Loss: 0.7975262999534607Training Epoch: 2 | iteration: 70/262 | Loss: 0.7829565405845642Training Epoch: 2 | iteration: 71/262 | Loss: 0.8419908285140991Training Epoch: 2 | iteration: 72/262 | Loss: 0.830673336982727Training Epoch: 2 | iteration: 73/262 | Loss: 0.7751372456550598Training Epoch: 2 | iteration: 74/262 | Loss: 0.7167959213256836Training Epoch: 2 | iteration: 75/262 | Loss: 0.7360007762908936Training Epoch: 2 | iteration: 76/262 | Loss: 0.7212674617767334Training Epoch: 2 | iteration: 77/262 | Loss: 0.7626920342445374Training Epoch: 2 | iteration: 78/262 | Loss: 0.7242053747177124Training Epoch: 2 | iteration: 79/262 | Loss: 0.8135281205177307Training Epoch: 2 | iteration: 80/262 | Loss: 0.8590527176856995Training Epoch: 2 | iteration: 81/262 | Loss: 0.7694324254989624Training Epoch: 2 | iteration: 82/262 | Loss: 0.6892780065536499Training Epoch: 2 | iteration: 83/262 | Loss: 0.6960883140563965Training Epoch: 2 | iteration: 84/262 | Loss: 0.7853609323501587Training Epoch: 2 | iteration: 85/262 | Loss: 0.7313061952590942Training Epoch: 2 | iteration: 86/262 | Loss: 0.6821427345275879Training Epoch: 2 | iteration: 87/262 | Loss: 0.7480125427246094Training Epoch: 2 | iteration: 88/262 | Loss: 0.7694559693336487Training Epoch: 2 | iteration: 89/262 | Loss: 0.7367807030677795Training Epoch: 2 | iteration: 90/262 | Loss: 0.7992779612541199Training Epoch: 2 | iteration: 91/262 | Loss: 0.7502589821815491Training Epoch: 2 | iteration: 92/262 | Loss: 0.653990626335144Training Epoch: 2 | iteration: 93/262 | Loss: 0.7208664417266846Training Epoch: 2 | iteration: 94/262 | Loss: 0.7531628608703613Training Epoch: 2 | iteration: 95/262 | Loss: 0.763624906539917Training Epoch: 2 | iteration: 96/262 | Loss: 0.681688666343689Training Epoch: 2 | iteration: 97/262 | Loss: 0.8189248442649841Training Epoch: 2 | iteration: 98/262 | Loss: 0.6958823800086975Training Epoch: 2 | iteration: 99/262 | Loss: 0.8344705700874329Training Epoch: 2 | iteration: 100/262 | Loss: 0.6498987674713135Training Epoch: 2 | iteration: 101/262 | Loss: 0.7279905676841736Training Epoch: 2 | iteration: 102/262 | Loss: 0.7141472101211548Training Epoch: 2 | iteration: 103/262 | Loss: 0.7967681884765625Training Epoch: 2 | iteration: 104/262 | Loss: 0.8033971786499023Training Epoch: 2 | iteration: 105/262 | Loss: 0.6938592791557312Training Epoch: 2 | iteration: 106/262 | Loss: 0.7513546943664551Training Epoch: 2 | iteration: 107/262 | Loss: 0.7298585176467896Training Epoch: 2 | iteration: 108/262 | Loss: 0.6976337432861328Training Epoch: 2 | iteration: 109/262 | Loss: 0.704063892364502Training Epoch: 2 | iteration: 110/262 | Loss: 0.8326559066772461Training Epoch: 2 | iteration: 111/262 | Loss: 0.8096276521682739Training Epoch: 2 | iteration: 112/262 | Loss: 0.7790396213531494Training Epoch: 2 | iteration: 113/262 | Loss: 0.8329119682312012Training Epoch: 2 | iteration: 114/262 | Loss: 0.7824488878250122Training Epoch: 2 | iteration: 115/262 | Loss: 0.7832202911376953Training Epoch: 2 | iteration: 116/262 | Loss: 0.8378872275352478Training Epoch: 2 | iteration: 117/262 | Loss: 0.7378015518188477Training Epoch: 2 | iteration: 118/262 | Loss: 0.7631852626800537Training Epoch: 2 | iteration: 119/262 | Loss: 0.7867527604103088Training Epoch: 2 | iteration: 120/262 | Loss: 0.741199254989624Training Epoch: 2 | iteration: 121/262 | Loss: 0.7829532623291016Training Epoch: 2 | iteration: 122/262 | Loss: 0.762381911277771Training Epoch: 2 | iteration: 123/262 | Loss: 0.7576738595962524Training Epoch: 2 | iteration: 124/262 | Loss: 0.6912810802459717Training Epoch: 2 | iteration: 125/262 | Loss: 0.8304015398025513Training Epoch: 2 | iteration: 126/262 | Loss: 0.7212147116661072Training Epoch: 2 | iteration: 127/262 | Loss: 0.7701476216316223Training Epoch: 2 | iteration: 128/262 | Loss: 0.7486258745193481Training Epoch: 2 | iteration: 129/262 | Loss: 0.8909421563148499Training Epoch: 2 | iteration: 130/262 | Loss: 0.7747364044189453Training Epoch: 2 | iteration: 131/262 | Loss: 0.8221263885498047Training Epoch: 2 | iteration: 132/262 | Loss: 0.7068942785263062Training Epoch: 2 | iteration: 133/262 | Loss: 0.7528642416000366Training Epoch: 2 | iteration: 134/262 | Loss: 0.7618476152420044Training Epoch: 2 | iteration: 135/262 | Loss: 0.7656620144844055Training Epoch: 2 | iteration: 136/262 | Loss: 0.71570885181427Training Epoch: 2 | iteration: 137/262 | Loss: 0.7507481575012207Training Epoch: 2 | iteration: 138/262 | Loss: 0.7385870218276978Training Epoch: 2 | iteration: 139/262 | Loss: 0.7270121574401855Training Epoch: 2 | iteration: 140/262 | Loss: 0.7658314108848572Training Epoch: 2 | iteration: 141/262 | Loss: 0.7964160442352295Training Epoch: 2 | iteration: 142/262 | Loss: 0.8120336532592773Training Epoch: 2 | iteration: 143/262 | Loss: 0.7336562871932983Training Epoch: 2 | iteration: 144/262 | Loss: 0.6960641741752625Training Epoch: 2 | iteration: 145/262 | Loss: 0.6551101207733154Training Epoch: 2 | iteration: 146/262 | Loss: 0.7943000197410583Training Epoch: 2 | iteration: 147/262 | Loss: 0.805992841720581Training Epoch: 2 | iteration: 148/262 | Loss: 0.7191997170448303Training Epoch: 2 | iteration: 149/262 | Loss: 0.7509045004844666Training Epoch: 2 | iteration: 150/262 | Loss: 0.7702211737632751Training Epoch: 2 | iteration: 151/262 | Loss: 0.759518563747406Training Epoch: 2 | iteration: 152/262 | Loss: 0.758519172668457Training Epoch: 2 | iteration: 153/262 | Loss: 0.7235810160636902Training Epoch: 2 | iteration: 154/262 | Loss: 0.7855515480041504Training Epoch: 2 | iteration: 155/262 | Loss: 0.6604052782058716Training Epoch: 2 | iteration: 156/262 | Loss: 0.721254825592041Training Epoch: 2 | iteration: 157/262 | Loss: 0.6909006834030151Training Epoch: 2 | iteration: 158/262 | Loss: 0.848099410533905Training Epoch: 2 | iteration: 159/262 | Loss: 0.7330513000488281Training Epoch: 2 | iteration: 160/262 | Loss: 0.7353497743606567Training Epoch: 2 | iteration: 161/262 | Loss: 0.7722777128219604Training Epoch: 2 | iteration: 162/262 | Loss: 0.8151853084564209Training Epoch: 2 | iteration: 163/262 | Loss: 0.7872600555419922Training Epoch: 2 | iteration: 164/262 | Loss: 0.7685519456863403Training Epoch: 2 | iteration: 165/262 | Loss: 0.7948088049888611Training Epoch: 2 | iteration: 166/262 | Loss: 0.7542727589607239Training Epoch: 2 | iteration: 167/262 | Loss: 0.715707540512085Training Epoch: 2 | iteration: 168/262 | Loss: 0.735363781452179Training Epoch: 2 | iteration: 169/262 | Loss: 0.868059515953064Training Epoch: 2 | iteration: 170/262 | Loss: 0.7152920961380005Training Epoch: 2 | iteration: 171/262 | Loss: 0.7895079851150513Training Epoch: 2 | iteration: 172/262 | Loss: 0.7019407749176025Training Epoch: 2 | iteration: 173/262 | Loss: 0.6644704937934875Training Epoch: 2 | iteration: 174/262 | Loss: 0.7273733615875244Training Epoch: 2 | iteration: 175/262 | Loss: 0.7452483773231506Training Epoch: 2 | iteration: 176/262 | Loss: 0.7561831474304199Training Epoch: 2 | iteration: 177/262 | Loss: 0.7889653444290161Training Epoch: 2 | iteration: 178/262 | Loss: 0.7052170038223267Training Epoch: 2 | iteration: 179/262 | Loss: 0.7058649659156799Training Epoch: 2 | iteration: 180/262 | Loss: 0.7305580377578735Training Epoch: 2 | iteration: 181/262 | Loss: 0.7639442682266235Training Epoch: 2 | iteration: 182/262 | Loss: 0.7274597883224487Training Epoch: 2 | iteration: 183/262 | Loss: 0.6553889513015747Training Epoch: 2 | iteration: 184/262 | Loss: 0.7975039482116699Training Epoch: 2 | iteration: 185/262 | Loss: 0.7134861946105957Training Epoch: 2 | iteration: 186/262 | Loss: 0.8088480830192566Training Epoch: 2 | iteration: 187/262 | Loss: 0.7534422874450684Training Epoch: 2 | iteration: 188/262 | Loss: 0.7825754284858704Training Epoch: 2 | iteration: 189/262 | Loss: 0.8113031387329102Training Epoch: 2 | iteration: 190/262 | Loss: 0.7686046957969666Training Epoch: 2 | iteration: 191/262 | Loss: 0.7985470294952393Training Epoch: 2 | iteration: 192/262 | Loss: 0.7933828830718994Training Epoch: 2 | iteration: 193/262 | Loss: 0.6575385332107544Training Epoch: 2 | iteration: 194/262 | Loss: 0.6965979933738708Training Epoch: 2 | iteration: 195/262 | Loss: 0.7059890627861023Training Epoch: 2 | iteration: 196/262 | Loss: 0.7485204935073853Training Epoch: 2 | iteration: 197/262 | Loss: 0.8009790182113647Training Epoch: 2 | iteration: 198/262 | Loss: 0.8281729817390442Training Epoch: 2 | iteration: 199/262 | Loss: 0.7145664691925049Training Epoch: 2 | iteration: 200/262 | Loss: 0.7794520258903503Training Epoch: 2 | iteration: 201/262 | Loss: 0.7964828610420227Training Epoch: 2 | iteration: 202/262 | Loss: 0.8275551199913025Training Epoch: 2 | iteration: 203/262 | Loss: 0.791383683681488Training Epoch: 2 | iteration: 204/262 | Loss: 0.807628870010376Training Epoch: 2 | iteration: 205/262 | Loss: 0.7106770277023315Training Epoch: 2 | iteration: 206/262 | Loss: 0.7507869005203247Training Epoch: 2 | iteration: 207/262 | Loss: 0.668071985244751Training Epoch: 2 | iteration: 208/262 | Loss: 0.7769752740859985Training Epoch: 2 | iteration: 209/262 | Loss: 0.9076235890388489Training Epoch: 2 | iteration: 210/262 | Loss: 0.6925198435783386Training Epoch: 2 | iteration: 211/262 | Loss: 0.7279964685440063Training Epoch: 2 | iteration: 212/262 | Loss: 0.700637698173523Training Epoch: 2 | iteration: 213/262 | Loss: 0.7562063932418823Training Epoch: 2 | iteration: 214/262 | Loss: 0.8082805871963501Training Epoch: 2 | iteration: 215/262 | Loss: 0.7170003652572632Training Epoch: 2 | iteration: 216/262 | Loss: 0.7934564352035522Training Epoch: 2 | iteration: 217/262 | Loss: 0.8468230962753296Training Epoch: 2 | iteration: 218/262 | Loss: 0.7110832333564758Training Epoch: 2 | iteration: 219/262 | Loss: 0.7516480684280396Training Epoch: 2 | iteration: 220/262 | Loss: 0.6744992733001709Training Epoch: 2 | iteration: 221/262 | Loss: 0.7218285799026489Training Epoch: 2 | iteration: 222/262 | Loss: 0.7256277799606323Training Epoch: 2 | iteration: 223/262 | Loss: 0.7199819087982178Training Epoch: 2 | iteration: 224/262 | Loss: 0.7423030138015747Training Epoch: 2 | iteration: 225/262 | Loss: 0.8320260643959045Training Epoch: 2 | iteration: 226/262 | Loss: 0.8803738355636597Training Epoch: 2 | iteration: 227/262 | Loss: 0.7861099243164062Training Epoch: 2 | iteration: 228/262 | Loss: 0.7609294652938843Training Epoch: 2 | iteration: 229/262 | Loss: 0.688373863697052Training Epoch: 2 | iteration: 230/262 | Loss: 0.7071034908294678Training Epoch: 2 | iteration: 231/262 | Loss: 0.8216213583946228Training Epoch: 2 | iteration: 232/262 | Loss: 0.8200244903564453Training Epoch: 2 | iteration: 233/262 | Loss: 0.800355076789856Training Epoch: 2 | iteration: 234/262 | Loss: 0.7151282429695129Training Epoch: 2 | iteration: 235/262 | Loss: 0.793671727180481Training Epoch: 2 | iteration: 236/262 | Loss: 0.6849386096000671Training Epoch: 2 | iteration: 237/262 | Loss: 0.7285498976707458Training Epoch: 2 | iteration: 238/262 | Loss: 0.7651638984680176Training Epoch: 2 | iteration: 239/262 | Loss: 0.7178056240081787Training Epoch: 2 | iteration: 240/262 | Loss: 0.7636576890945435Training Epoch: 2 | iteration: 241/262 | Loss: 0.7763023376464844Training Epoch: 2 | iteration: 242/262 | Loss: 0.6815630793571472Training Epoch: 2 | iteration: 243/262 | Loss: 0.6954696178436279Training Epoch: 2 | iteration: 244/262 | Loss: 0.7925081253051758Training Epoch: 2 | iteration: 245/262 | Loss: 0.7898279428482056Training Epoch: 2 | iteration: 246/262 | Loss: 0.6722675561904907Training Epoch: 2 | iteration: 247/262 | Loss: 0.6975024938583374Training Epoch: 2 | iteration: 248/262 | Loss: 0.8461465835571289Training Epoch: 2 | iteration: 249/262 | Loss: 0.7111674547195435Training Epoch: 2 | iteration: 250/262 | Loss: 0.7471402883529663Training Epoch: 2 | iteration: 251/262 | Loss: 0.7123475670814514Training Epoch: 2 | iteration: 252/262 | Loss: 0.738395094871521Training Epoch: 2 | iteration: 253/262 | Loss: 0.6644777655601501Training Epoch: 2 | iteration: 254/262 | Loss: 0.8064166903495789Training Epoch: 2 | iteration: 255/262 | Loss: 0.7339807152748108Training Epoch: 2 | iteration: 256/262 | Loss: 0.7764467000961304Training Epoch: 2 | iteration: 257/262 | Loss: 0.7287132143974304Training Epoch: 2 | iteration: 258/262 | Loss: 0.8001562356948853Training Epoch: 2 | iteration: 259/262 | Loss: 0.7811496257781982Training Epoch: 2 | iteration: 260/262 | Loss: 0.8440288305282593Training Epoch: 2 | iteration: 261/262 | Loss: 0.6976091861724854Validating Epoch: 2 | iteration: 0/66 | Loss: 0.6399878859519958Validating Epoch: 2 | iteration: 1/66 | Loss: 0.714121401309967Validating Epoch: 2 | iteration: 2/66 | Loss: 0.7463569045066833Validating Epoch: 2 | iteration: 3/66 | Loss: 0.7637661695480347Validating Epoch: 2 | iteration: 4/66 | Loss: 0.6661192178726196Validating Epoch: 2 | iteration: 5/66 | Loss: 0.6542861461639404Validating Epoch: 2 | iteration: 6/66 | Loss: 0.7138329744338989Validating Epoch: 2 | iteration: 7/66 | Loss: 0.7284179925918579Validating Epoch: 2 | iteration: 8/66 | Loss: 0.7455551624298096Validating Epoch: 2 | iteration: 9/66 | Loss: 0.7297003865242004Validating Epoch: 2 | iteration: 10/66 | Loss: 0.6682159900665283Validating Epoch: 2 | iteration: 11/66 | Loss: 0.7249748706817627Validating Epoch: 2 | iteration: 12/66 | Loss: 0.7356846332550049Validating Epoch: 2 | iteration: 13/66 | Loss: 0.6620804071426392Validating Epoch: 2 | iteration: 14/66 | Loss: 0.7680152654647827Validating Epoch: 2 | iteration: 15/66 | Loss: 0.6451432108879089Validating Epoch: 2 | iteration: 16/66 | Loss: 0.7098687887191772Validating Epoch: 2 | iteration: 17/66 | Loss: 0.6784700751304626Validating Epoch: 2 | iteration: 18/66 | Loss: 0.7255967855453491Validating Epoch: 2 | iteration: 19/66 | Loss: 0.6455427408218384Validating Epoch: 2 | iteration: 20/66 | Loss: 0.6718044281005859Validating Epoch: 2 | iteration: 21/66 | Loss: 0.6050167679786682Validating Epoch: 2 | iteration: 22/66 | Loss: 0.7427465319633484Validating Epoch: 2 | iteration: 23/66 | Loss: 0.6699207425117493Validating Epoch: 2 | iteration: 24/66 | Loss: 0.7349907159805298Validating Epoch: 2 | iteration: 25/66 | Loss: 0.7131537795066833Validating Epoch: 2 | iteration: 26/66 | Loss: 0.6863162517547607Validating Epoch: 2 | iteration: 27/66 | Loss: 0.7735053896903992Validating Epoch: 2 | iteration: 28/66 | Loss: 0.6845222115516663Validating Epoch: 2 | iteration: 29/66 | Loss: 0.6423273682594299Validating Epoch: 2 | iteration: 30/66 | Loss: 0.7259049415588379Validating Epoch: 2 | iteration: 31/66 | Loss: 0.6817359328269958Validating Epoch: 2 | iteration: 32/66 | Loss: 0.7332750558853149Validating Epoch: 2 | iteration: 33/66 | Loss: 0.7141982316970825Validating Epoch: 2 | iteration: 34/66 | Loss: 0.6639365553855896Validating Epoch: 2 | iteration: 35/66 | Loss: 0.7117812037467957Validating Epoch: 2 | iteration: 36/66 | Loss: 0.6347856521606445Validating Epoch: 2 | iteration: 37/66 | Loss: 0.7107924222946167Validating Epoch: 2 | iteration: 38/66 | Loss: 0.6576172709465027Validating Epoch: 2 | iteration: 39/66 | Loss: 0.7009543180465698Validating Epoch: 2 | iteration: 40/66 | Loss: 0.7390229105949402Validating Epoch: 2 | iteration: 41/66 | Loss: 0.6986433863639832Validating Epoch: 2 | iteration: 42/66 | Loss: 0.7200688123703003Validating Epoch: 2 | iteration: 43/66 | Loss: 0.6810834407806396Validating Epoch: 2 | iteration: 44/66 | Loss: 0.7132430076599121Validating Epoch: 2 | iteration: 45/66 | Loss: 0.6757791638374329Validating Epoch: 2 | iteration: 46/66 | Loss: 0.6791999936103821Validating Epoch: 2 | iteration: 47/66 | Loss: 0.7649374604225159Validating Epoch: 2 | iteration: 48/66 | Loss: 0.7444093823432922Validating Epoch: 2 | iteration: 49/66 | Loss: 0.7427091598510742Validating Epoch: 2 | iteration: 50/66 | Loss: 0.7737615704536438Validating Epoch: 2 | iteration: 51/66 | Loss: 0.7607752084732056Validating Epoch: 2 | iteration: 52/66 | Loss: 0.7527904510498047Validating Epoch: 2 | iteration: 53/66 | Loss: 0.6886986494064331Validating Epoch: 2 | iteration: 54/66 | Loss: 0.7249280214309692Validating Epoch: 2 | iteration: 55/66 | Loss: 0.7227920293807983Validating Epoch: 2 | iteration: 56/66 | Loss: 0.7589749097824097Validating Epoch: 2 | iteration: 57/66 | Loss: 0.6655229926109314Validating Epoch: 2 | iteration: 58/66 | Loss: 0.7421672344207764Validating Epoch: 2 | iteration: 59/66 | Loss: 0.7113655805587769Validating Epoch: 2 | iteration: 60/66 | Loss: 0.6977165937423706Validating Epoch: 2 | iteration: 61/66 | Loss: 0.7331647872924805Validating Epoch: 2 | iteration: 62/66 | Loss: 0.6697108745574951Validating Epoch: 2 | iteration: 63/66 | Loss: 0.7107093334197998Validating Epoch: 2 | iteration: 64/66 | Loss: 0.7156554460525513Validating Epoch: 2 | iteration: 65/66 | Loss: 0.8330450654029846Temperature:  0.5
Sampling: iteration: 0/51{'Validity': 0.9951171875, 'Novelty': 1.0, 'Uniqueness': 0.9970559371933267}
Training Epoch: 3 | iteration: 0/262 | Loss: 0.7188010215759277Training Epoch: 3 | iteration: 1/262 | Loss: 0.7315064668655396Training Epoch: 3 | iteration: 2/262 | Loss: 0.7445960640907288Training Epoch: 3 | iteration: 3/262 | Loss: 0.713428258895874Training Epoch: 3 | iteration: 4/262 | Loss: 0.6980644464492798Training Epoch: 3 | iteration: 5/262 | Loss: 0.7043224573135376Training Epoch: 3 | iteration: 6/262 | Loss: 0.7700150012969971Training Epoch: 3 | iteration: 7/262 | Loss: 0.7435899376869202Training Epoch: 3 | iteration: 8/262 | Loss: 0.7862175107002258Training Epoch: 3 | iteration: 9/262 | Loss: 0.7383714318275452Training Epoch: 3 | iteration: 10/262 | Loss: 0.7841541767120361Training Epoch: 3 | iteration: 11/262 | Loss: 0.7227471470832825Training Epoch: 3 | iteration: 12/262 | Loss: 0.7669551372528076Training Epoch: 3 | iteration: 13/262 | Loss: 0.6027994155883789Training Epoch: 3 | iteration: 14/262 | Loss: 0.7402387857437134Training Epoch: 3 | iteration: 15/262 | Loss: 0.7378184199333191Training Epoch: 3 | iteration: 16/262 | Loss: 0.6751851439476013Training Epoch: 3 | iteration: 17/262 | Loss: 0.6616091728210449Training Epoch: 3 | iteration: 18/262 | Loss: 0.6938036680221558Training Epoch: 3 | iteration: 19/262 | Loss: 0.7095555663108826Training Epoch: 3 | iteration: 20/262 | Loss: 0.6449719667434692Training Epoch: 3 | iteration: 21/262 | Loss: 0.714598536491394Training Epoch: 3 | iteration: 22/262 | Loss: 0.7111020684242249Training Epoch: 3 | iteration: 23/262 | Loss: 0.7106494903564453Training Epoch: 3 | iteration: 24/262 | Loss: 0.6418682336807251Training Epoch: 3 | iteration: 25/262 | Loss: 0.6708170175552368Training Epoch: 3 | iteration: 26/262 | Loss: 0.7586325407028198Training Epoch: 3 | iteration: 27/262 | Loss: 0.707322359085083Training Epoch: 3 | iteration: 28/262 | Loss: 0.6204588413238525Training Epoch: 3 | iteration: 29/262 | Loss: 0.7268587350845337Training Epoch: 3 | iteration: 30/262 | Loss: 0.7381076216697693Training Epoch: 3 | iteration: 31/262 | Loss: 0.6966423988342285Training Epoch: 3 | iteration: 32/262 | Loss: 0.8515568971633911Training Epoch: 3 | iteration: 33/262 | Loss: 0.7195095419883728Training Epoch: 3 | iteration: 34/262 | Loss: 0.7167233228683472Training Epoch: 3 | iteration: 35/262 | Loss: 0.7244952917098999Training Epoch: 3 | iteration: 36/262 | Loss: 0.6902671456336975Training Epoch: 3 | iteration: 37/262 | Loss: 0.7882460355758667Training Epoch: 3 | iteration: 38/262 | Loss: 0.7194629311561584Training Epoch: 3 | iteration: 39/262 | Loss: 0.8758754730224609Training Epoch: 3 | iteration: 40/262 | Loss: 0.7977664470672607Training Epoch: 3 | iteration: 41/262 | Loss: 0.6992224454879761Training Epoch: 3 | iteration: 42/262 | Loss: 0.7421518564224243Training Epoch: 3 | iteration: 43/262 | Loss: 0.6805463433265686Training Epoch: 3 | iteration: 44/262 | Loss: 0.7753555774688721Training Epoch: 3 | iteration: 45/262 | Loss: 0.6917286515235901Training Epoch: 3 | iteration: 46/262 | Loss: 0.697712779045105Training Epoch: 3 | iteration: 47/262 | Loss: 0.7462031841278076Training Epoch: 3 | iteration: 48/262 | Loss: 0.7585468292236328Training Epoch: 3 | iteration: 49/262 | Loss: 0.7743841409683228Training Epoch: 3 | iteration: 50/262 | Loss: 0.7838221192359924Training Epoch: 3 | iteration: 51/262 | Loss: 0.7238032817840576Training Epoch: 3 | iteration: 52/262 | Loss: 0.6469942927360535Training Epoch: 3 | iteration: 53/262 | Loss: 0.7132605314254761Training Epoch: 3 | iteration: 54/262 | Loss: 0.7666614651679993Training Epoch: 3 | iteration: 55/262 | Loss: 0.7500596046447754Training Epoch: 3 | iteration: 56/262 | Loss: 0.7512616515159607Training Epoch: 3 | iteration: 57/262 | Loss: 0.6942302584648132Training Epoch: 3 | iteration: 58/262 | Loss: 0.8326314687728882Training Epoch: 3 | iteration: 59/262 | Loss: 0.735045850276947Training Epoch: 3 | iteration: 60/262 | Loss: 0.6454272270202637Training Epoch: 3 | iteration: 61/262 | Loss: 0.6180201768875122Training Epoch: 3 | iteration: 62/262 | Loss: 0.693884015083313Training Epoch: 3 | iteration: 63/262 | Loss: 0.6868200302124023Training Epoch: 3 | iteration: 64/262 | Loss: 0.6917600631713867Training Epoch: 3 | iteration: 65/262 | Loss: 0.6833905577659607Training Epoch: 3 | iteration: 66/262 | Loss: 0.6666078567504883Training Epoch: 3 | iteration: 67/262 | Loss: 0.6594291925430298Training Epoch: 3 | iteration: 68/262 | Loss: 0.719235360622406Training Epoch: 3 | iteration: 69/262 | Loss: 0.7471567392349243Training Epoch: 3 | iteration: 70/262 | Loss: 0.697526216506958Training Epoch: 3 | iteration: 71/262 | Loss: 0.7546762824058533Training Epoch: 3 | iteration: 72/262 | Loss: 0.6936031579971313Training Epoch: 3 | iteration: 73/262 | Loss: 0.7222427725791931Training Epoch: 3 | iteration: 74/262 | Loss: 0.7164243459701538Training Epoch: 3 | iteration: 75/262 | Loss: 0.6466916799545288Training Epoch: 3 | iteration: 76/262 | Loss: 0.6725397706031799Training Epoch: 3 | iteration: 77/262 | Loss: 0.7419233322143555Training Epoch: 3 | iteration: 78/262 | Loss: 0.7192433476448059Training Epoch: 3 | iteration: 79/262 | Loss: 0.6821966171264648Training Epoch: 3 | iteration: 80/262 | Loss: 0.6498934030532837Training Epoch: 3 | iteration: 81/262 | Loss: 0.7129197120666504Training Epoch: 3 | iteration: 82/262 | Loss: 0.7262454032897949Training Epoch: 3 | iteration: 83/262 | Loss: 0.7437596321105957Training Epoch: 3 | iteration: 84/262 | Loss: 0.7330663204193115Training Epoch: 3 | iteration: 85/262 | Loss: 0.6922234296798706Training Epoch: 3 | iteration: 86/262 | Loss: 0.7828940153121948Training Epoch: 3 | iteration: 87/262 | Loss: 0.7051314115524292Training Epoch: 3 | iteration: 88/262 | Loss: 0.7241315841674805Training Epoch: 3 | iteration: 89/262 | Loss: 0.7632964253425598Training Epoch: 3 | iteration: 90/262 | Loss: 0.7556709051132202Training Epoch: 3 | iteration: 91/262 | Loss: 0.6762840747833252Training Epoch: 3 | iteration: 92/262 | Loss: 0.7479161024093628Training Epoch: 3 | iteration: 93/262 | Loss: 0.6845513582229614Training Epoch: 3 | iteration: 94/262 | Loss: 0.7099485993385315Training Epoch: 3 | iteration: 95/262 | Loss: 0.7649803161621094Training Epoch: 3 | iteration: 96/262 | Loss: 0.7094091176986694Training Epoch: 3 | iteration: 97/262 | Loss: 0.7593373656272888Training Epoch: 3 | iteration: 98/262 | Loss: 0.6954984068870544Training Epoch: 3 | iteration: 99/262 | Loss: 0.7193748950958252Training Epoch: 3 | iteration: 100/262 | Loss: 0.7408539056777954Training Epoch: 3 | iteration: 101/262 | Loss: 0.7584141492843628Training Epoch: 3 | iteration: 102/262 | Loss: 0.7431645393371582Training Epoch: 3 | iteration: 103/262 | Loss: 0.7544155120849609Training Epoch: 3 | iteration: 104/262 | Loss: 0.9109898209571838Training Epoch: 3 | iteration: 105/262 | Loss: 0.7486445903778076Training Epoch: 3 | iteration: 106/262 | Loss: 0.6999979019165039Training Epoch: 3 | iteration: 107/262 | Loss: 0.7655150294303894Training Epoch: 3 | iteration: 108/262 | Loss: 0.7537659406661987Training Epoch: 3 | iteration: 109/262 | Loss: 0.8172566890716553Training Epoch: 3 | iteration: 110/262 | Loss: 0.7399533987045288Training Epoch: 3 | iteration: 111/262 | Loss: 0.6924903988838196Training Epoch: 3 | iteration: 112/262 | Loss: 0.7456828355789185Training Epoch: 3 | iteration: 113/262 | Loss: 0.791195809841156Training Epoch: 3 | iteration: 114/262 | Loss: 0.7634280323982239Training Epoch: 3 | iteration: 115/262 | Loss: 0.7962554097175598Training Epoch: 3 | iteration: 116/262 | Loss: 0.739067018032074Training Epoch: 3 | iteration: 117/262 | Loss: 0.7447234392166138Training Epoch: 3 | iteration: 118/262 | Loss: 0.7141934037208557Training Epoch: 3 | iteration: 119/262 | Loss: 0.6870797872543335Training Epoch: 3 | iteration: 120/262 | Loss: 0.802546501159668Training Epoch: 3 | iteration: 121/262 | Loss: 0.7608673572540283Training Epoch: 3 | iteration: 122/262 | Loss: 0.7182806134223938Training Epoch: 3 | iteration: 123/262 | Loss: 0.7972650527954102Training Epoch: 3 | iteration: 124/262 | Loss: 0.6873620748519897Training Epoch: 3 | iteration: 125/262 | Loss: 0.7868454456329346Training Epoch: 3 | iteration: 126/262 | Loss: 0.7119845151901245Training Epoch: 3 | iteration: 127/262 | Loss: 0.652824878692627Training Epoch: 3 | iteration: 128/262 | Loss: 0.649041473865509Training Epoch: 3 | iteration: 129/262 | Loss: 0.7309389114379883Training Epoch: 3 | iteration: 130/262 | Loss: 0.7771661281585693Training Epoch: 3 | iteration: 131/262 | Loss: 0.7113921046257019Training Epoch: 3 | iteration: 132/262 | Loss: 0.7166489362716675Training Epoch: 3 | iteration: 133/262 | Loss: 0.7039529085159302Training Epoch: 3 | iteration: 134/262 | Loss: 0.6778501272201538Training Epoch: 3 | iteration: 135/262 | Loss: 0.7218805551528931Training Epoch: 3 | iteration: 136/262 | Loss: 0.7024080753326416Training Epoch: 3 | iteration: 137/262 | Loss: 0.6378118991851807Training Epoch: 3 | iteration: 138/262 | Loss: 0.7299667596817017Training Epoch: 3 | iteration: 139/262 | Loss: 0.7753835916519165Training Epoch: 3 | iteration: 140/262 | Loss: 0.7243362069129944Training Epoch: 3 | iteration: 141/262 | Loss: 0.664060115814209Training Epoch: 3 | iteration: 142/262 | Loss: 0.6394236087799072Training Epoch: 3 | iteration: 143/262 | Loss: 0.7800096273422241Training Epoch: 3 | iteration: 144/262 | Loss: 0.6611332893371582Training Epoch: 3 | iteration: 145/262 | Loss: 0.8061236143112183Training Epoch: 3 | iteration: 146/262 | Loss: 0.7386144995689392Training Epoch: 3 | iteration: 147/262 | Loss: 0.7537640333175659Training Epoch: 3 | iteration: 148/262 | Loss: 0.6669321060180664Training Epoch: 3 | iteration: 149/262 | Loss: 0.7093390226364136Training Epoch: 3 | iteration: 150/262 | Loss: 0.6796635389328003Training Epoch: 3 | iteration: 151/262 | Loss: 0.7395198941230774Training Epoch: 3 | iteration: 152/262 | Loss: 0.6238369345664978Training Epoch: 3 | iteration: 153/262 | Loss: 0.803147554397583Training Epoch: 3 | iteration: 154/262 | Loss: 0.6990420818328857Training Epoch: 3 | iteration: 155/262 | Loss: 0.7514972686767578Training Epoch: 3 | iteration: 156/262 | Loss: 0.7248305678367615Training Epoch: 3 | iteration: 157/262 | Loss: 0.7200107574462891Training Epoch: 3 | iteration: 158/262 | Loss: 0.7366647720336914Training Epoch: 3 | iteration: 159/262 | Loss: 0.727943480014801Training Epoch: 3 | iteration: 160/262 | Loss: 0.6319102644920349Training Epoch: 3 | iteration: 161/262 | Loss: 0.6920567154884338Training Epoch: 3 | iteration: 162/262 | Loss: 0.6734551191329956Training Epoch: 3 | iteration: 163/262 | Loss: 0.7353956699371338Training Epoch: 3 | iteration: 164/262 | Loss: 0.6881974935531616Training Epoch: 3 | iteration: 165/262 | Loss: 0.708411693572998Training Epoch: 3 | iteration: 166/262 | Loss: 0.7480122447013855Training Epoch: 3 | iteration: 167/262 | Loss: 0.6824275255203247Training Epoch: 3 | iteration: 168/262 | Loss: 0.7343820333480835Training Epoch: 3 | iteration: 169/262 | Loss: 0.729365348815918Training Epoch: 3 | iteration: 170/262 | Loss: 0.7622784376144409Training Epoch: 3 | iteration: 171/262 | Loss: 0.7322372198104858Training Epoch: 3 | iteration: 172/262 | Loss: 0.7342669367790222Training Epoch: 3 | iteration: 173/262 | Loss: 0.7521263360977173Training Epoch: 3 | iteration: 174/262 | Loss: 0.6994739770889282Training Epoch: 3 | iteration: 175/262 | Loss: 0.7012488842010498Training Epoch: 3 | iteration: 176/262 | Loss: 0.7376440167427063Training Epoch: 3 | iteration: 177/262 | Loss: 0.7032116651535034Training Epoch: 3 | iteration: 178/262 | Loss: 0.665276050567627Training Epoch: 3 | iteration: 179/262 | Loss: 0.6525352597236633Training Epoch: 3 | iteration: 180/262 | Loss: 0.7891156077384949Training Epoch: 3 | iteration: 181/262 | Loss: 0.8190931677818298Training Epoch: 3 | iteration: 182/262 | Loss: 0.6805838346481323Training Epoch: 3 | iteration: 183/262 | Loss: 0.7956092357635498Training Epoch: 3 | iteration: 184/262 | Loss: 0.6815131902694702Training Epoch: 3 | iteration: 185/262 | Loss: 0.743330717086792Training Epoch: 3 | iteration: 186/262 | Loss: 0.8231955170631409Training Epoch: 3 | iteration: 187/262 | Loss: 0.8134734630584717Training Epoch: 3 | iteration: 188/262 | Loss: 0.823750376701355Training Epoch: 3 | iteration: 189/262 | Loss: 0.7527766823768616Training Epoch: 3 | iteration: 190/262 | Loss: 0.6794918775558472Training Epoch: 3 | iteration: 191/262 | Loss: 0.665438175201416Training Epoch: 3 | iteration: 192/262 | Loss: 0.7351896166801453Training Epoch: 3 | iteration: 193/262 | Loss: 0.6964055299758911Training Epoch: 3 | iteration: 194/262 | Loss: 0.7734009027481079Training Epoch: 3 | iteration: 195/262 | Loss: 0.7583903074264526Training Epoch: 3 | iteration: 196/262 | Loss: 0.849907398223877Training Epoch: 3 | iteration: 197/262 | Loss: 0.7102743983268738Training Epoch: 3 | iteration: 198/262 | Loss: 0.7580726146697998Training Epoch: 3 | iteration: 199/262 | Loss: 0.7555879354476929Training Epoch: 3 | iteration: 200/262 | Loss: 0.7905622124671936Training Epoch: 3 | iteration: 201/262 | Loss: 0.6817415952682495Training Epoch: 3 | iteration: 202/262 | Loss: 0.7292858362197876Training Epoch: 3 | iteration: 203/262 | Loss: 0.7246603965759277Training Epoch: 3 | iteration: 204/262 | Loss: 0.6428787708282471Training Epoch: 3 | iteration: 205/262 | Loss: 0.7299373149871826Training Epoch: 3 | iteration: 206/262 | Loss: 0.6532663106918335Training Epoch: 3 | iteration: 207/262 | Loss: 0.7267531156539917Training Epoch: 3 | iteration: 208/262 | Loss: 0.7794941663742065Training Epoch: 3 | iteration: 209/262 | Loss: 0.7133638262748718Training Epoch: 3 | iteration: 210/262 | Loss: 0.7184606790542603Training Epoch: 3 | iteration: 211/262 | Loss: 0.6989372968673706Training Epoch: 3 | iteration: 212/262 | Loss: 0.7069627642631531Training Epoch: 3 | iteration: 213/262 | Loss: 0.7435003519058228Training Epoch: 3 | iteration: 214/262 | Loss: 0.720287561416626Training Epoch: 3 | iteration: 215/262 | Loss: 0.7621005773544312Training Epoch: 3 | iteration: 216/262 | Loss: 0.8124616146087646Training Epoch: 3 | iteration: 217/262 | Loss: 0.6797847747802734Training Epoch: 3 | iteration: 218/262 | Loss: 0.6931082606315613Training Epoch: 3 | iteration: 219/262 | Loss: 0.7190769910812378Training Epoch: 3 | iteration: 220/262 | Loss: 0.7604438066482544Training Epoch: 3 | iteration: 221/262 | Loss: 0.6731261014938354Training Epoch: 3 | iteration: 222/262 | Loss: 0.7269548177719116Training Epoch: 3 | iteration: 223/262 | Loss: 0.7108322978019714Training Epoch: 3 | iteration: 224/262 | Loss: 0.7715975046157837Training Epoch: 3 | iteration: 225/262 | Loss: 0.7585479021072388Training Epoch: 3 | iteration: 226/262 | Loss: 0.7488828897476196Training Epoch: 3 | iteration: 227/262 | Loss: 0.7077698707580566Training Epoch: 3 | iteration: 228/262 | Loss: 0.688823938369751Training Epoch: 3 | iteration: 229/262 | Loss: 0.6961226463317871Training Epoch: 3 | iteration: 230/262 | Loss: 0.6779084205627441Training Epoch: 3 | iteration: 231/262 | Loss: 0.7349430322647095Training Epoch: 3 | iteration: 232/262 | Loss: 0.6823647618293762Training Epoch: 3 | iteration: 233/262 | Loss: 0.7748909592628479Training Epoch: 3 | iteration: 234/262 | Loss: 0.7373721599578857Training Epoch: 3 | iteration: 235/262 | Loss: 0.6711108088493347Training Epoch: 3 | iteration: 236/262 | Loss: 0.7044183015823364Training Epoch: 3 | iteration: 237/262 | Loss: 0.6702882647514343Training Epoch: 3 | iteration: 238/262 | Loss: 0.7745009660720825Training Epoch: 3 | iteration: 239/262 | Loss: 0.6988236904144287Training Epoch: 3 | iteration: 240/262 | Loss: 0.7896453142166138Training Epoch: 3 | iteration: 241/262 | Loss: 0.696505069732666Training Epoch: 3 | iteration: 242/262 | Loss: 0.725782036781311Training Epoch: 3 | iteration: 243/262 | Loss: 0.7351927757263184Training Epoch: 3 | iteration: 244/262 | Loss: 0.6907472610473633Training Epoch: 3 | iteration: 245/262 | Loss: 0.6605557203292847Training Epoch: 3 | iteration: 246/262 | Loss: 0.7861844301223755Training Epoch: 3 | iteration: 247/262 | Loss: 0.6972135305404663Training Epoch: 3 | iteration: 248/262 | Loss: 0.6648819446563721Training Epoch: 3 | iteration: 249/262 | Loss: 0.7202602624893188Training Epoch: 3 | iteration: 250/262 | Loss: 0.7920467257499695Training Epoch: 3 | iteration: 251/262 | Loss: 0.7145811319351196Training Epoch: 3 | iteration: 252/262 | Loss: 0.6903931498527527Training Epoch: 3 | iteration: 253/262 | Loss: 0.7781937718391418Training Epoch: 3 | iteration: 254/262 | Loss: 0.6199870109558105Training Epoch: 3 | iteration: 255/262 | Loss: 0.6562644243240356Training Epoch: 3 | iteration: 256/262 | Loss: 0.756175696849823Training Epoch: 3 | iteration: 257/262 | Loss: 0.7169484496116638Training Epoch: 3 | iteration: 258/262 | Loss: 0.7162495851516724Training Epoch: 3 | iteration: 259/262 | Loss: 0.704079270362854Training Epoch: 3 | iteration: 260/262 | Loss: 0.714492917060852Training Epoch: 3 | iteration: 261/262 | Loss: 0.9205441474914551Validating Epoch: 3 | iteration: 0/66 | Loss: 0.7545934319496155Validating Epoch: 3 | iteration: 1/66 | Loss: 0.7990127801895142Validating Epoch: 3 | iteration: 2/66 | Loss: 0.6868637800216675Validating Epoch: 3 | iteration: 3/66 | Loss: 0.699089765548706Validating Epoch: 3 | iteration: 4/66 | Loss: 0.7653327584266663Validating Epoch: 3 | iteration: 5/66 | Loss: 0.6279940605163574Validating Epoch: 3 | iteration: 6/66 | Loss: 0.6483421921730042Validating Epoch: 3 | iteration: 7/66 | Loss: 0.6312674283981323Validating Epoch: 3 | iteration: 8/66 | Loss: 0.7187641859054565Validating Epoch: 3 | iteration: 9/66 | Loss: 0.7069522738456726Validating Epoch: 3 | iteration: 10/66 | Loss: 0.7092893719673157Validating Epoch: 3 | iteration: 11/66 | Loss: 0.7103238701820374Validating Epoch: 3 | iteration: 12/66 | Loss: 0.6948959827423096Validating Epoch: 3 | iteration: 13/66 | Loss: 0.768191933631897Validating Epoch: 3 | iteration: 14/66 | Loss: 0.6628048419952393Validating Epoch: 3 | iteration: 15/66 | Loss: 0.7034282684326172Validating Epoch: 3 | iteration: 16/66 | Loss: 0.6852765083312988Validating Epoch: 3 | iteration: 17/66 | Loss: 0.7688851952552795Validating Epoch: 3 | iteration: 18/66 | Loss: 0.6722420454025269Validating Epoch: 3 | iteration: 19/66 | Loss: 0.6766232848167419Validating Epoch: 3 | iteration: 20/66 | Loss: 0.6778004765510559Validating Epoch: 3 | iteration: 21/66 | Loss: 0.635377049446106Validating Epoch: 3 | iteration: 22/66 | Loss: 0.7731232643127441Validating Epoch: 3 | iteration: 23/66 | Loss: 0.8168525695800781Validating Epoch: 3 | iteration: 24/66 | Loss: 0.6530200839042664Validating Epoch: 3 | iteration: 25/66 | Loss: 0.6990882158279419Validating Epoch: 3 | iteration: 26/66 | Loss: 0.632274866104126Validating Epoch: 3 | iteration: 27/66 | Loss: 0.699671745300293Validating Epoch: 3 | iteration: 28/66 | Loss: 0.7370945811271667Validating Epoch: 3 | iteration: 29/66 | Loss: 0.6960028409957886Validating Epoch: 3 | iteration: 30/66 | Loss: 0.7887266874313354Validating Epoch: 3 | iteration: 31/66 | Loss: 0.7639176249504089Validating Epoch: 3 | iteration: 32/66 | Loss: 0.6380460262298584Validating Epoch: 3 | iteration: 33/66 | Loss: 0.6714438796043396Validating Epoch: 3 | iteration: 34/66 | Loss: 0.7743964791297913Validating Epoch: 3 | iteration: 35/66 | Loss: 0.7140892744064331Validating Epoch: 3 | iteration: 36/66 | Loss: 0.702088475227356Validating Epoch: 3 | iteration: 37/66 | Loss: 0.7641639709472656Validating Epoch: 3 | iteration: 38/66 | Loss: 0.6828629970550537Validating Epoch: 3 | iteration: 39/66 | Loss: 0.7190151214599609Validating Epoch: 3 | iteration: 40/66 | Loss: 0.6698857545852661Validating Epoch: 3 | iteration: 41/66 | Loss: 0.6997250318527222Validating Epoch: 3 | iteration: 42/66 | Loss: 0.7208039164543152Validating Epoch: 3 | iteration: 43/66 | Loss: 0.7281582355499268Validating Epoch: 3 | iteration: 44/66 | Loss: 0.6355289220809937Validating Epoch: 3 | iteration: 45/66 | Loss: 0.7386643886566162Validating Epoch: 3 | iteration: 46/66 | Loss: 0.6753398776054382Validating Epoch: 3 | iteration: 47/66 | Loss: 0.6707487106323242Validating Epoch: 3 | iteration: 48/66 | Loss: 0.6945042610168457Validating Epoch: 3 | iteration: 49/66 | Loss: 0.735999584197998Validating Epoch: 3 | iteration: 50/66 | Loss: 0.6693702936172485Validating Epoch: 3 | iteration: 51/66 | Loss: 0.6834809184074402Validating Epoch: 3 | iteration: 52/66 | Loss: 0.6382635235786438Validating Epoch: 3 | iteration: 53/66 | Loss: 0.7330690622329712Validating Epoch: 3 | iteration: 54/66 | Loss: 0.649584949016571Validating Epoch: 3 | iteration: 55/66 | Loss: 0.6477715969085693Validating Epoch: 3 | iteration: 56/66 | Loss: 0.7122247815132141Validating Epoch: 3 | iteration: 57/66 | Loss: 0.6193745732307434Validating Epoch: 3 | iteration: 58/66 | Loss: 0.6890481114387512Validating Epoch: 3 | iteration: 59/66 | Loss: 0.6309401392936707Validating Epoch: 3 | iteration: 60/66 | Loss: 0.7632050514221191Validating Epoch: 3 | iteration: 61/66 | Loss: 0.636981725692749Validating Epoch: 3 | iteration: 62/66 | Loss: 0.6908738017082214Validating Epoch: 3 | iteration: 63/66 | Loss: 0.6462612748146057Validating Epoch: 3 | iteration: 64/66 | Loss: 0.6926315426826477Validating Epoch: 3 | iteration: 65/66 | Loss: 0.7346956729888916Temperature:  0.5
Sampling: iteration: 0/51{'Validity': 0.9912109375, 'Novelty': 1.0, 'Uniqueness': 0.993103448275862}
Training Epoch: 4 | iteration: 0/262 | Loss: 0.7997968792915344Training Epoch: 4 | iteration: 1/262 | Loss: 0.6989407539367676Training Epoch: 4 | iteration: 2/262 | Loss: 0.6812736988067627Training Epoch: 4 | iteration: 3/262 | Loss: 0.786445140838623Training Epoch: 4 | iteration: 4/262 | Loss: 0.6939236521720886Training Epoch: 4 | iteration: 5/262 | Loss: 0.6944797039031982Training Epoch: 4 | iteration: 6/262 | Loss: 0.6530425548553467Training Epoch: 4 | iteration: 7/262 | Loss: 0.7765194177627563Training Epoch: 4 | iteration: 8/262 | Loss: 0.6306424140930176Training Epoch: 4 | iteration: 9/262 | Loss: 0.6264379620552063Training Epoch: 4 | iteration: 10/262 | Loss: 0.6758637428283691Training Epoch: 4 | iteration: 11/262 | Loss: 0.681713879108429Training Epoch: 4 | iteration: 12/262 | Loss: 0.7234720587730408Training Epoch: 4 | iteration: 13/262 | Loss: 0.7017837762832642Training Epoch: 4 | iteration: 14/262 | Loss: 0.7014531493186951Training Epoch: 4 | iteration: 15/262 | Loss: 0.7842786908149719Training Epoch: 4 | iteration: 16/262 | Loss: 0.6995612382888794Training Epoch: 4 | iteration: 17/262 | Loss: 0.671013593673706Training Epoch: 4 | iteration: 18/262 | Loss: 0.6470508575439453Training Epoch: 4 | iteration: 19/262 | Loss: 0.7672147750854492Training Epoch: 4 | iteration: 20/262 | Loss: 0.6985281705856323Training Epoch: 4 | iteration: 21/262 | Loss: 0.6875649094581604Training Epoch: 4 | iteration: 22/262 | Loss: 0.7299472689628601Training Epoch: 4 | iteration: 23/262 | Loss: 0.6808404326438904Training Epoch: 4 | iteration: 24/262 | Loss: 0.6524825096130371Training Epoch: 4 | iteration: 25/262 | Loss: 0.7680865526199341Training Epoch: 4 | iteration: 26/262 | Loss: 0.7073054909706116Training Epoch: 4 | iteration: 27/262 | Loss: 0.602445125579834Training Epoch: 4 | iteration: 28/262 | Loss: 0.7913344502449036Training Epoch: 4 | iteration: 29/262 | Loss: 0.6022571921348572Training Epoch: 4 | iteration: 30/262 | Loss: 0.6350752115249634Training Epoch: 4 | iteration: 31/262 | Loss: 0.7514687776565552Training Epoch: 4 | iteration: 32/262 | Loss: 0.7019170522689819Training Epoch: 4 | iteration: 33/262 | Loss: 0.6656088829040527Training Epoch: 4 | iteration: 34/262 | Loss: 0.6479104161262512Training Epoch: 4 | iteration: 35/262 | Loss: 0.7558039426803589Training Epoch: 4 | iteration: 36/262 | Loss: 0.7213671207427979Training Epoch: 4 | iteration: 37/262 | Loss: 0.6568410992622375Training Epoch: 4 | iteration: 38/262 | Loss: 0.6870954036712646Training Epoch: 4 | iteration: 39/262 | Loss: 0.7471001148223877Training Epoch: 4 | iteration: 40/262 | Loss: 0.6773080229759216Training Epoch: 4 | iteration: 41/262 | Loss: 0.7034813761711121Training Epoch: 4 | iteration: 42/262 | Loss: 0.674546480178833Training Epoch: 4 | iteration: 43/262 | Loss: 0.7216373682022095Training Epoch: 4 | iteration: 44/262 | Loss: 0.6686025857925415Training Epoch: 4 | iteration: 45/262 | Loss: 0.7220941781997681Training Epoch: 4 | iteration: 46/262 | Loss: 0.7025219202041626Training Epoch: 4 | iteration: 47/262 | Loss: 0.5735037326812744Training Epoch: 4 | iteration: 48/262 | Loss: 0.7331402897834778Training Epoch: 4 | iteration: 49/262 | Loss: 0.7428321838378906Training Epoch: 4 | iteration: 50/262 | Loss: 0.7487492561340332Training Epoch: 4 | iteration: 51/262 | Loss: 0.7319959402084351Training Epoch: 4 | iteration: 52/262 | Loss: 0.6722326874732971Training Epoch: 4 | iteration: 53/262 | Loss: 0.6371880769729614Training Epoch: 4 | iteration: 54/262 | Loss: 0.7587651014328003Training Epoch: 4 | iteration: 55/262 | Loss: 0.7435344457626343Training Epoch: 4 | iteration: 56/262 | Loss: 0.6596845388412476Training Epoch: 4 | iteration: 57/262 | Loss: 0.7612528204917908Training Epoch: 4 | iteration: 58/262 | Loss: 0.6526790261268616Training Epoch: 4 | iteration: 59/262 | Loss: 0.6502950191497803Training Epoch: 4 | iteration: 60/262 | Loss: 0.629817008972168Training Epoch: 4 | iteration: 61/262 | Loss: 0.645039439201355Training Epoch: 4 | iteration: 62/262 | Loss: 0.6905046105384827Training Epoch: 4 | iteration: 63/262 | Loss: 0.7347393035888672Training Epoch: 4 | iteration: 64/262 | Loss: 0.6321355104446411Training Epoch: 4 | iteration: 65/262 | Loss: 0.7040106058120728Training Epoch: 4 | iteration: 66/262 | Loss: 0.6727843284606934Training Epoch: 4 | iteration: 67/262 | Loss: 0.6926815509796143Training Epoch: 4 | iteration: 68/262 | Loss: 0.8598504066467285Training Epoch: 4 | iteration: 69/262 | Loss: 0.6568140387535095Training Epoch: 4 | iteration: 70/262 | Loss: 0.6531400680541992Training Epoch: 4 | iteration: 71/262 | Loss: 0.6598869562149048Training Epoch: 4 | iteration: 72/262 | Loss: 0.6309128403663635Training Epoch: 4 | iteration: 73/262 | Loss: 0.7404286861419678Training Epoch: 4 | iteration: 74/262 | Loss: 0.743602454662323Training Epoch: 4 | iteration: 75/262 | Loss: 0.6674303412437439Training Epoch: 4 | iteration: 76/262 | Loss: 0.6345517635345459Training Epoch: 4 | iteration: 77/262 | Loss: 0.8172690868377686Training Epoch: 4 | iteration: 78/262 | Loss: 0.7018427848815918Training Epoch: 4 | iteration: 79/262 | Loss: 0.6298367977142334Training Epoch: 4 | iteration: 80/262 | Loss: 0.6805391311645508Training Epoch: 4 | iteration: 81/262 | Loss: 0.736965000629425Training Epoch: 4 | iteration: 82/262 | Loss: 0.6670907735824585Training Epoch: 4 | iteration: 83/262 | Loss: 0.6329925656318665Training Epoch: 4 | iteration: 84/262 | Loss: 0.6791197657585144Training Epoch: 4 | iteration: 85/262 | Loss: 0.6758270263671875Training Epoch: 4 | iteration: 86/262 | Loss: 0.6598490476608276Training Epoch: 4 | iteration: 87/262 | Loss: 0.7167282700538635Training Epoch: 4 | iteration: 88/262 | Loss: 0.6167656779289246Training Epoch: 4 | iteration: 89/262 | Loss: 0.7439529895782471Training Epoch: 4 | iteration: 90/262 | Loss: 0.7113645076751709Training Epoch: 4 | iteration: 91/262 | Loss: 0.7210357785224915Training Epoch: 4 | iteration: 92/262 | Loss: 0.7308198809623718Training Epoch: 4 | iteration: 93/262 | Loss: 0.7201735377311707Training Epoch: 4 | iteration: 94/262 | Loss: 0.7646917104721069Training Epoch: 4 | iteration: 95/262 | Loss: 0.6630078554153442Training Epoch: 4 | iteration: 96/262 | Loss: 0.6868412494659424Training Epoch: 4 | iteration: 97/262 | Loss: 0.6366128921508789Training Epoch: 4 | iteration: 98/262 | Loss: 0.6664510369300842Training Epoch: 4 | iteration: 99/262 | Loss: 0.7518905401229858Training Epoch: 4 | iteration: 100/262 | Loss: 0.7197147011756897Training Epoch: 4 | iteration: 101/262 | Loss: 0.6367248296737671Training Epoch: 4 | iteration: 102/262 | Loss: 0.6671242713928223Training Epoch: 4 | iteration: 103/262 | Loss: 0.76483154296875Training Epoch: 4 | iteration: 104/262 | Loss: 0.678912878036499Training Epoch: 4 | iteration: 105/262 | Loss: 0.7066900730133057Training Epoch: 4 | iteration: 106/262 | Loss: 0.777915358543396Training Epoch: 4 | iteration: 107/262 | Loss: 0.699546754360199Training Epoch: 4 | iteration: 108/262 | Loss: 0.6836962103843689Training Epoch: 4 | iteration: 109/262 | Loss: 0.7507859468460083Training Epoch: 4 | iteration: 110/262 | Loss: 0.7062331438064575Training Epoch: 4 | iteration: 111/262 | Loss: 0.7516177892684937Training Epoch: 4 | iteration: 112/262 | Loss: 0.6927784085273743Training Epoch: 4 | iteration: 113/262 | Loss: 0.7133375406265259Training Epoch: 4 | iteration: 114/262 | Loss: 0.7418737411499023Training Epoch: 4 | iteration: 115/262 | Loss: 0.7245527505874634Training Epoch: 4 | iteration: 116/262 | Loss: 0.6596119403839111Training Epoch: 4 | iteration: 117/262 | Loss: 0.7448794841766357Training Epoch: 4 | iteration: 118/262 | Loss: 0.716282844543457Training Epoch: 4 | iteration: 119/262 | Loss: 0.7000220417976379Training Epoch: 4 | iteration: 120/262 | Loss: 0.6368518471717834Training Epoch: 4 | iteration: 121/262 | Loss: 0.648552656173706Training Epoch: 4 | iteration: 122/262 | Loss: 0.7406938076019287Training Epoch: 4 | iteration: 123/262 | Loss: 0.7091723680496216Training Epoch: 4 | iteration: 124/262 | Loss: 0.7897051572799683Training Epoch: 4 | iteration: 125/262 | Loss: 0.7705088257789612Training Epoch: 4 | iteration: 126/262 | Loss: 0.6649948358535767Training Epoch: 4 | iteration: 127/262 | Loss: 0.7149382829666138Training Epoch: 4 | iteration: 128/262 | Loss: 0.6925410032272339Training Epoch: 4 | iteration: 129/262 | Loss: 0.6572552919387817Training Epoch: 4 | iteration: 130/262 | Loss: 0.7044777870178223Training Epoch: 4 | iteration: 131/262 | Loss: 0.750758171081543Training Epoch: 4 | iteration: 132/262 | Loss: 0.727567732334137Training Epoch: 4 | iteration: 133/262 | Loss: 0.6555424928665161Training Epoch: 4 | iteration: 134/262 | Loss: 0.6830142736434937Training Epoch: 4 | iteration: 135/262 | Loss: 0.7355787754058838Training Epoch: 4 | iteration: 136/262 | Loss: 0.765228271484375Training Epoch: 4 | iteration: 137/262 | Loss: 0.6166305541992188Training Epoch: 4 | iteration: 138/262 | Loss: 0.6745915412902832Training Epoch: 4 | iteration: 139/262 | Loss: 0.663068950176239Training Epoch: 4 | iteration: 140/262 | Loss: 0.7158974409103394Training Epoch: 4 | iteration: 141/262 | Loss: 0.749720573425293Training Epoch: 4 | iteration: 142/262 | Loss: 0.7613358497619629Training Epoch: 4 | iteration: 143/262 | Loss: 0.7217398881912231Training Epoch: 4 | iteration: 144/262 | Loss: 0.6424400806427002Training Epoch: 4 | iteration: 145/262 | Loss: 0.7236175537109375Training Epoch: 4 | iteration: 146/262 | Loss: 0.7091916799545288Training Epoch: 4 | iteration: 147/262 | Loss: 0.6687722206115723Training Epoch: 4 | iteration: 148/262 | Loss: 0.6580017805099487Training Epoch: 4 | iteration: 149/262 | Loss: 0.7394653558731079Training Epoch: 4 | iteration: 150/262 | Loss: 0.628464937210083Training Epoch: 4 | iteration: 151/262 | Loss: 0.72271728515625Training Epoch: 4 | iteration: 152/262 | Loss: 0.7251754403114319Training Epoch: 4 | iteration: 153/262 | Loss: 0.7193092107772827Training Epoch: 4 | iteration: 154/262 | Loss: 0.6465702652931213Training Epoch: 4 | iteration: 155/262 | Loss: 0.7777771949768066Training Epoch: 4 | iteration: 156/262 | Loss: 0.7074843645095825Training Epoch: 4 | iteration: 157/262 | Loss: 0.7199720144271851Training Epoch: 4 | iteration: 158/262 | Loss: 0.7439610958099365Training Epoch: 4 | iteration: 159/262 | Loss: 0.7258225083351135Training Epoch: 4 | iteration: 160/262 | Loss: 0.7496563196182251Training Epoch: 4 | iteration: 161/262 | Loss: 0.6781334280967712Training Epoch: 4 | iteration: 162/262 | Loss: 0.7884498238563538Training Epoch: 4 | iteration: 163/262 | Loss: 0.6762976050376892Training Epoch: 4 | iteration: 164/262 | Loss: 0.7488515377044678Training Epoch: 4 | iteration: 165/262 | Loss: 0.6543245315551758Training Epoch: 4 | iteration: 166/262 | Loss: 0.6532716155052185Training Epoch: 4 | iteration: 167/262 | Loss: 0.7690010070800781Training Epoch: 4 | iteration: 168/262 | Loss: 0.6372005939483643Training Epoch: 4 | iteration: 169/262 | Loss: 0.7220003604888916Training Epoch: 4 | iteration: 170/262 | Loss: 0.7133530378341675Training Epoch: 4 | iteration: 171/262 | Loss: 0.6892667412757874Training Epoch: 4 | iteration: 172/262 | Loss: 0.6602176427841187Training Epoch: 4 | iteration: 173/262 | Loss: 0.6183149814605713Training Epoch: 4 | iteration: 174/262 | Loss: 0.7050958871841431Training Epoch: 4 | iteration: 175/262 | Loss: 0.753223180770874Training Epoch: 4 | iteration: 176/262 | Loss: 0.6526171565055847Training Epoch: 4 | iteration: 177/262 | Loss: 0.6558433771133423Training Epoch: 4 | iteration: 178/262 | Loss: 0.8285011053085327Training Epoch: 4 | iteration: 179/262 | Loss: 0.7407670617103577Training Epoch: 4 | iteration: 180/262 | Loss: 0.6653843522071838Training Epoch: 4 | iteration: 181/262 | Loss: 0.7242039442062378Training Epoch: 4 | iteration: 182/262 | Loss: 0.7393741011619568Training Epoch: 4 | iteration: 183/262 | Loss: 0.7569820880889893Training Epoch: 4 | iteration: 184/262 | Loss: 0.7549138069152832Training Epoch: 4 | iteration: 185/262 | Loss: 0.6237552165985107Training Epoch: 4 | iteration: 186/262 | Loss: 0.7412853240966797Training Epoch: 4 | iteration: 187/262 | Loss: 0.7218496203422546Training Epoch: 4 | iteration: 188/262 | Loss: 0.7213369607925415Training Epoch: 4 | iteration: 189/262 | Loss: 0.5965045690536499Training Epoch: 4 | iteration: 190/262 | Loss: 0.7488565444946289Training Epoch: 4 | iteration: 191/262 | Loss: 0.7784292101860046Training Epoch: 4 | iteration: 192/262 | Loss: 0.6761398315429688Training Epoch: 4 | iteration: 193/262 | Loss: 0.7592791318893433Training Epoch: 4 | iteration: 194/262 | Loss: 0.7178783416748047Training Epoch: 4 | iteration: 195/262 | Loss: 0.7459478378295898Training Epoch: 4 | iteration: 196/262 | Loss: 0.7016396522521973Training Epoch: 4 | iteration: 197/262 | Loss: 0.6914543509483337Training Epoch: 4 | iteration: 198/262 | Loss: 0.7479519844055176Training Epoch: 4 | iteration: 199/262 | Loss: 0.6224936842918396Training Epoch: 4 | iteration: 200/262 | Loss: 0.7279598712921143Training Epoch: 4 | iteration: 201/262 | Loss: 0.7667603492736816Training Epoch: 4 | iteration: 202/262 | Loss: 0.6583398580551147Training Epoch: 4 | iteration: 203/262 | Loss: 0.7353981137275696Training Epoch: 4 | iteration: 204/262 | Loss: 0.6905379891395569Training Epoch: 4 | iteration: 205/262 | Loss: 0.6648786067962646Training Epoch: 4 | iteration: 206/262 | Loss: 0.6644623875617981Training Epoch: 4 | iteration: 207/262 | Loss: 0.7696497440338135Training Epoch: 4 | iteration: 208/262 | Loss: 0.624582827091217Training Epoch: 4 | iteration: 209/262 | Loss: 0.6061376333236694Training Epoch: 4 | iteration: 210/262 | Loss: 0.6836452484130859Training Epoch: 4 | iteration: 211/262 | Loss: 0.6725866794586182Training Epoch: 4 | iteration: 212/262 | Loss: 0.655609667301178Training Epoch: 4 | iteration: 213/262 | Loss: 0.7022395133972168Training Epoch: 4 | iteration: 214/262 | Loss: 0.6624072194099426Training Epoch: 4 | iteration: 215/262 | Loss: 0.7717515826225281Training Epoch: 4 | iteration: 216/262 | Loss: 0.7357221841812134Training Epoch: 4 | iteration: 217/262 | Loss: 0.712914764881134Training Epoch: 4 | iteration: 218/262 | Loss: 0.7538261413574219Training Epoch: 4 | iteration: 219/262 | Loss: 0.7507292032241821Training Epoch: 4 | iteration: 220/262 | Loss: 0.7652378082275391Training Epoch: 4 | iteration: 221/262 | Loss: 0.7589607238769531Training Epoch: 4 | iteration: 222/262 | Loss: 0.7313542366027832Training Epoch: 4 | iteration: 223/262 | Loss: 0.6840283870697021Training Epoch: 4 | iteration: 224/262 | Loss: 0.6931384801864624Training Epoch: 4 | iteration: 225/262 | Loss: 0.7672743797302246Training Epoch: 4 | iteration: 226/262 | Loss: 0.6803374886512756Training Epoch: 4 | iteration: 227/262 | Loss: 0.738219141960144Training Epoch: 4 | iteration: 228/262 | Loss: 0.6640760898590088Training Epoch: 4 | iteration: 229/262 | Loss: 0.7408252954483032Training Epoch: 4 | iteration: 230/262 | Loss: 0.7167315483093262Training Epoch: 4 | iteration: 231/262 | Loss: 0.6813902854919434Training Epoch: 4 | iteration: 232/262 | Loss: 0.6932424306869507Training Epoch: 4 | iteration: 233/262 | Loss: 0.6482635736465454Training Epoch: 4 | iteration: 234/262 | Loss: 0.7107782363891602Training Epoch: 4 | iteration: 235/262 | Loss: 0.7022510766983032Training Epoch: 4 | iteration: 236/262 | Loss: 0.7775310277938843Training Epoch: 4 | iteration: 237/262 | Loss: 0.7035133838653564Training Epoch: 4 | iteration: 238/262 | Loss: 0.7376995086669922Training Epoch: 4 | iteration: 239/262 | Loss: 0.6374722123146057Training Epoch: 4 | iteration: 240/262 | Loss: 0.6645674705505371Training Epoch: 4 | iteration: 241/262 | Loss: 0.6952391862869263Training Epoch: 4 | iteration: 242/262 | Loss: 0.6734613180160522Training Epoch: 4 | iteration: 243/262 | Loss: 0.6798744201660156Training Epoch: 4 | iteration: 244/262 | Loss: 0.7260667085647583Training Epoch: 4 | iteration: 245/262 | Loss: 0.6278647184371948Training Epoch: 4 | iteration: 246/262 | Loss: 0.6969273686408997Training Epoch: 4 | iteration: 247/262 | Loss: 0.7470502257347107Training Epoch: 4 | iteration: 248/262 | Loss: 0.6808315515518188Training Epoch: 4 | iteration: 249/262 | Loss: 0.6906522512435913Training Epoch: 4 | iteration: 250/262 | Loss: 0.6384523510932922Training Epoch: 4 | iteration: 251/262 | Loss: 0.7091870903968811Training Epoch: 4 | iteration: 252/262 | Loss: 0.6712629199028015Training Epoch: 4 | iteration: 253/262 | Loss: 0.6924142837524414Training Epoch: 4 | iteration: 254/262 | Loss: 0.7133347988128662Training Epoch: 4 | iteration: 255/262 | Loss: 0.680504560470581Training Epoch: 4 | iteration: 256/262 | Loss: 0.691760778427124Training Epoch: 4 | iteration: 257/262 | Loss: 0.6916537880897522Training Epoch: 4 | iteration: 258/262 | Loss: 0.6729713678359985Training Epoch: 4 | iteration: 259/262 | Loss: 0.6755019426345825Training Epoch: 4 | iteration: 260/262 | Loss: 0.6187103986740112Training Epoch: 4 | iteration: 261/262 | Loss: 0.5312409400939941Validating Epoch: 4 | iteration: 0/66 | Loss: 0.6775321960449219Validating Epoch: 4 | iteration: 1/66 | Loss: 0.7278116345405579Validating Epoch: 4 | iteration: 2/66 | Loss: 0.7066134810447693Validating Epoch: 4 | iteration: 3/66 | Loss: 0.6966214776039124Validating Epoch: 4 | iteration: 4/66 | Loss: 0.7081053853034973Validating Epoch: 4 | iteration: 5/66 | Loss: 0.6724826693534851Validating Epoch: 4 | iteration: 6/66 | Loss: 0.7719669938087463Validating Epoch: 4 | iteration: 7/66 | Loss: 0.7136001586914062Validating Epoch: 4 | iteration: 8/66 | Loss: 0.703947901725769Validating Epoch: 4 | iteration: 9/66 | Loss: 0.6834332942962646Validating Epoch: 4 | iteration: 10/66 | Loss: 0.6772904396057129Validating Epoch: 4 | iteration: 11/66 | Loss: 0.701066255569458Validating Epoch: 4 | iteration: 12/66 | Loss: 0.7592164278030396Validating Epoch: 4 | iteration: 13/66 | Loss: 0.6762176752090454Validating Epoch: 4 | iteration: 14/66 | Loss: 0.6579161882400513Validating Epoch: 4 | iteration: 15/66 | Loss: 0.7923327684402466Validating Epoch: 4 | iteration: 16/66 | Loss: 0.7094132900238037Validating Epoch: 4 | iteration: 17/66 | Loss: 0.6826666593551636Validating Epoch: 4 | iteration: 18/66 | Loss: 0.6590172052383423Validating Epoch: 4 | iteration: 19/66 | Loss: 0.7384316325187683Validating Epoch: 4 | iteration: 20/66 | Loss: 0.6792190670967102Validating Epoch: 4 | iteration: 21/66 | Loss: 0.6640256643295288Validating Epoch: 4 | iteration: 22/66 | Loss: 0.8022217750549316Validating Epoch: 4 | iteration: 23/66 | Loss: 0.6374596953392029Validating Epoch: 4 | iteration: 24/66 | Loss: 0.6327914595603943Validating Epoch: 4 | iteration: 25/66 | Loss: 0.7258212566375732Validating Epoch: 4 | iteration: 26/66 | Loss: 0.6915977001190186Validating Epoch: 4 | iteration: 27/66 | Loss: 0.6949447393417358Validating Epoch: 4 | iteration: 28/66 | Loss: 0.6678192615509033Validating Epoch: 4 | iteration: 29/66 | Loss: 0.6721149682998657Validating Epoch: 4 | iteration: 30/66 | Loss: 0.6405643224716187Validating Epoch: 4 | iteration: 31/66 | Loss: 0.7468091249465942Validating Epoch: 4 | iteration: 32/66 | Loss: 0.6671807765960693Validating Epoch: 4 | iteration: 33/66 | Loss: 0.6891013979911804Validating Epoch: 4 | iteration: 34/66 | Loss: 0.6570313572883606Validating Epoch: 4 | iteration: 35/66 | Loss: 0.7041720151901245Validating Epoch: 4 | iteration: 36/66 | Loss: 0.6961030960083008Validating Epoch: 4 | iteration: 37/66 | Loss: 0.6664394736289978Validating Epoch: 4 | iteration: 38/66 | Loss: 0.7143747806549072Validating Epoch: 4 | iteration: 39/66 | Loss: 0.6931717395782471Validating Epoch: 4 | iteration: 40/66 | Loss: 0.691929817199707Validating Epoch: 4 | iteration: 41/66 | Loss: 0.772863507270813Validating Epoch: 4 | iteration: 42/66 | Loss: 0.670738160610199Validating Epoch: 4 | iteration: 43/66 | Loss: 0.7033495903015137Validating Epoch: 4 | iteration: 44/66 | Loss: 0.6845017671585083Validating Epoch: 4 | iteration: 45/66 | Loss: 0.6765358448028564Validating Epoch: 4 | iteration: 46/66 | Loss: 0.7880964279174805Validating Epoch: 4 | iteration: 47/66 | Loss: 0.6781002879142761Validating Epoch: 4 | iteration: 48/66 | Loss: 0.7229771614074707Validating Epoch: 4 | iteration: 49/66 | Loss: 0.6891821622848511Validating Epoch: 4 | iteration: 50/66 | Loss: 0.721563458442688Validating Epoch: 4 | iteration: 51/66 | Loss: 0.6623331308364868Validating Epoch: 4 | iteration: 52/66 | Loss: 0.7175003290176392Validating Epoch: 4 | iteration: 53/66 | Loss: 0.6698994636535645Validating Epoch: 4 | iteration: 54/66 | Loss: 0.7347531318664551Validating Epoch: 4 | iteration: 55/66 | Loss: 0.6387538909912109Validating Epoch: 4 | iteration: 56/66 | Loss: 0.7072582244873047Validating Epoch: 4 | iteration: 57/66 | Loss: 0.6903756260871887Validating Epoch: 4 | iteration: 58/66 | Loss: 0.7296975255012512Validating Epoch: 4 | iteration: 59/66 | Loss: 0.7515168190002441Validating Epoch: 4 | iteration: 60/66 | Loss: 0.6836960315704346Validating Epoch: 4 | iteration: 61/66 | Loss: 0.6587537527084351Validating Epoch: 4 | iteration: 62/66 | Loss: 0.7072061896324158Validating Epoch: 4 | iteration: 63/66 | Loss: 0.7176744341850281Validating Epoch: 4 | iteration: 64/66 | Loss: 0.6476317048072815Validating Epoch: 4 | iteration: 65/66 | Loss: 0.7325975298881531Temperature:  0.5
Sampling: iteration: 0/51{'Validity': 0.994140625, 'Novelty': 1.0, 'Uniqueness': 0.9950884086444007}
Training Epoch: 5 | iteration: 0/262 | Loss: 0.6858700513839722Training Epoch: 5 | iteration: 1/262 | Loss: 0.6322445869445801Training Epoch: 5 | iteration: 2/262 | Loss: 0.6303324699401855Training Epoch: 5 | iteration: 3/262 | Loss: 0.638998806476593Training Epoch: 5 | iteration: 4/262 | Loss: 0.6512875556945801Training Epoch: 5 | iteration: 5/262 | Loss: 0.6305917501449585Training Epoch: 5 | iteration: 6/262 | Loss: 0.6366268992424011Training Epoch: 5 | iteration: 7/262 | Loss: 0.6680600047111511Training Epoch: 5 | iteration: 8/262 | Loss: 0.6488199234008789Training Epoch: 5 | iteration: 9/262 | Loss: 0.7569231390953064Training Epoch: 5 | iteration: 10/262 | Loss: 0.6634081602096558Training Epoch: 5 | iteration: 11/262 | Loss: 0.6379193067550659Training Epoch: 5 | iteration: 12/262 | Loss: 0.6753004789352417Training Epoch: 5 | iteration: 13/262 | Loss: 0.6023234128952026Training Epoch: 5 | iteration: 14/262 | Loss: 0.657481849193573Training Epoch: 5 | iteration: 15/262 | Loss: 0.6679006218910217Training Epoch: 5 | iteration: 16/262 | Loss: 0.6181211471557617Training Epoch: 5 | iteration: 17/262 | Loss: 0.7159761190414429Training Epoch: 5 | iteration: 18/262 | Loss: 0.6726062893867493Training Epoch: 5 | iteration: 19/262 | Loss: 0.6546481847763062Training Epoch: 5 | iteration: 20/262 | Loss: 0.7057872414588928Training Epoch: 5 | iteration: 21/262 | Loss: 0.6738066673278809Training Epoch: 5 | iteration: 22/262 | Loss: 0.7262111306190491Training Epoch: 5 | iteration: 23/262 | Loss: 0.654644250869751Training Epoch: 5 | iteration: 24/262 | Loss: 0.7101789712905884Training Epoch: 5 | iteration: 25/262 | Loss: 0.6682961583137512Training Epoch: 5 | iteration: 26/262 | Loss: 0.6178187727928162Training Epoch: 5 | iteration: 27/262 | Loss: 0.7522010803222656Training Epoch: 5 | iteration: 28/262 | Loss: 0.6671960353851318Training Epoch: 5 | iteration: 29/262 | Loss: 0.6621826887130737Training Epoch: 5 | iteration: 30/262 | Loss: 0.6423025131225586Training Epoch: 5 | iteration: 31/262 | Loss: 0.6684311628341675Training Epoch: 5 | iteration: 32/262 | Loss: 0.7469834685325623Training Epoch: 5 | iteration: 33/262 | Loss: 0.6129260063171387Training Epoch: 5 | iteration: 34/262 | Loss: 0.6187953948974609Training Epoch: 5 | iteration: 35/262 | Loss: 0.6701346635818481Training Epoch: 5 | iteration: 36/262 | Loss: 0.6425864696502686Training Epoch: 5 | iteration: 37/262 | Loss: 0.6671415567398071Training Epoch: 5 | iteration: 38/262 | Loss: 0.6828010678291321Training Epoch: 5 | iteration: 39/262 | Loss: 0.7149465084075928Training Epoch: 5 | iteration: 40/262 | Loss: 0.6660621762275696Training Epoch: 5 | iteration: 41/262 | Loss: 0.7372411489486694Training Epoch: 5 | iteration: 42/262 | Loss: 0.6919949054718018Training Epoch: 5 | iteration: 43/262 | Loss: 0.6951154470443726Training Epoch: 5 | iteration: 44/262 | Loss: 0.6912802457809448Training Epoch: 5 | iteration: 45/262 | Loss: 0.712165355682373Training Epoch: 5 | iteration: 46/262 | Loss: 0.697170615196228Training Epoch: 5 | iteration: 47/262 | Loss: 0.7305759787559509Training Epoch: 5 | iteration: 48/262 | Loss: 0.717062771320343Training Epoch: 5 | iteration: 49/262 | Loss: 0.6669533252716064Training Epoch: 5 | iteration: 50/262 | Loss: 0.688539981842041Training Epoch: 5 | iteration: 51/262 | Loss: 0.6187620162963867Training Epoch: 5 | iteration: 52/262 | Loss: 0.5926212072372437Training Epoch: 5 | iteration: 53/262 | Loss: 0.7169563174247742Training Epoch: 5 | iteration: 54/262 | Loss: 0.6463741064071655Training Epoch: 5 | iteration: 55/262 | Loss: 0.6790651082992554Training Epoch: 5 | iteration: 56/262 | Loss: 0.5894302725791931Training Epoch: 5 | iteration: 57/262 | Loss: 0.7271268367767334Training Epoch: 5 | iteration: 58/262 | Loss: 0.6845786571502686Training Epoch: 5 | iteration: 59/262 | Loss: 0.7122052907943726Training Epoch: 5 | iteration: 60/262 | Loss: 0.6979433298110962Training Epoch: 5 | iteration: 61/262 | Loss: 0.7079317569732666Training Epoch: 5 | iteration: 62/262 | Loss: 0.5879039168357849Training Epoch: 5 | iteration: 63/262 | Loss: 0.6628109216690063Training Epoch: 5 | iteration: 64/262 | Loss: 0.629940390586853Training Epoch: 5 | iteration: 65/262 | Loss: 0.5947610139846802Training Epoch: 5 | iteration: 66/262 | Loss: 0.6904253363609314Training Epoch: 5 | iteration: 67/262 | Loss: 0.7003828287124634Training Epoch: 5 | iteration: 68/262 | Loss: 0.6437076926231384Training Epoch: 5 | iteration: 69/262 | Loss: 0.746870219707489Training Epoch: 5 | iteration: 70/262 | Loss: 0.782286524772644Training Epoch: 5 | iteration: 71/262 | Loss: 0.7351136207580566Training Epoch: 5 | iteration: 72/262 | Loss: 0.674092710018158Training Epoch: 5 | iteration: 73/262 | Loss: 0.7392110228538513Training Epoch: 5 | iteration: 74/262 | Loss: 0.6986863017082214Training Epoch: 5 | iteration: 75/262 | Loss: 0.6744595766067505Training Epoch: 5 | iteration: 76/262 | Loss: 0.65718674659729Training Epoch: 5 | iteration: 77/262 | Loss: 0.6382488012313843Training Epoch: 5 | iteration: 78/262 | Loss: 0.7626568078994751Training Epoch: 5 | iteration: 79/262 | Loss: 0.6690018177032471Training Epoch: 5 | iteration: 80/262 | Loss: 0.7080302238464355Training Epoch: 5 | iteration: 81/262 | Loss: 0.6825138330459595Training Epoch: 5 | iteration: 82/262 | Loss: 0.5987102389335632Training Epoch: 5 | iteration: 83/262 | Loss: 0.7242838144302368Training Epoch: 5 | iteration: 84/262 | Loss: 0.7041988372802734Training Epoch: 5 | iteration: 85/262 | Loss: 0.662203311920166Training Epoch: 5 | iteration: 86/262 | Loss: 0.7300417423248291Training Epoch: 5 | iteration: 87/262 | Loss: 0.6432748436927795Training Epoch: 5 | iteration: 88/262 | Loss: 0.6626118421554565Training Epoch: 5 | iteration: 89/262 | Loss: 0.7487239837646484Training Epoch: 5 | iteration: 90/262 | Loss: 0.736280083656311Training Epoch: 5 | iteration: 91/262 | Loss: 0.6898761987686157Training Epoch: 5 | iteration: 92/262 | Loss: 0.6267764568328857Training Epoch: 5 | iteration: 93/262 | Loss: 0.6462680697441101Training Epoch: 5 | iteration: 94/262 | Loss: 0.715531587600708Training Epoch: 5 | iteration: 95/262 | Loss: 0.6199989914894104Training Epoch: 5 | iteration: 96/262 | Loss: 0.6058189868927002Training Epoch: 5 | iteration: 97/262 | Loss: 0.7214691638946533Training Epoch: 5 | iteration: 98/262 | Loss: 0.6691048741340637Training Epoch: 5 | iteration: 99/262 | Loss: 0.7015910148620605Training Epoch: 5 | iteration: 100/262 | Loss: 0.673633337020874Training Epoch: 5 | iteration: 101/262 | Loss: 0.6538889408111572Training Epoch: 5 | iteration: 102/262 | Loss: 0.6351320147514343Training Epoch: 5 | iteration: 103/262 | Loss: 0.6746399402618408Training Epoch: 5 | iteration: 104/262 | Loss: 0.7060115337371826Training Epoch: 5 | iteration: 105/262 | Loss: 0.7173818349838257Training Epoch: 5 | iteration: 106/262 | Loss: 0.6324239373207092Training Epoch: 5 | iteration: 107/262 | Loss: 0.6743896007537842Training Epoch: 5 | iteration: 108/262 | Loss: 0.7253897190093994Training Epoch: 5 | iteration: 109/262 | Loss: 0.7114740610122681Training Epoch: 5 | iteration: 110/262 | Loss: 0.6667197942733765Training Epoch: 5 | iteration: 111/262 | Loss: 0.6140941381454468Training Epoch: 5 | iteration: 112/262 | Loss: 0.6515964865684509Training Epoch: 5 | iteration: 113/262 | Loss: 0.7758423686027527Training Epoch: 5 | iteration: 114/262 | Loss: 0.662432074546814Training Epoch: 5 | iteration: 115/262 | Loss: 0.7080903053283691Training Epoch: 5 | iteration: 116/262 | Loss: 0.5682849287986755Training Epoch: 5 | iteration: 117/262 | Loss: 0.6430927515029907Training Epoch: 5 | iteration: 118/262 | Loss: 0.6595296263694763Training Epoch: 5 | iteration: 119/262 | Loss: 0.7063751220703125Training Epoch: 5 | iteration: 120/262 | Loss: 0.660403847694397Training Epoch: 5 | iteration: 121/262 | Loss: 0.7061619758605957Training Epoch: 5 | iteration: 122/262 | Loss: 0.6488168835639954Training Epoch: 5 | iteration: 123/262 | Loss: 0.5738539695739746Training Epoch: 5 | iteration: 124/262 | Loss: 0.6937142610549927Training Epoch: 5 | iteration: 125/262 | Loss: 0.7091882228851318Training Epoch: 5 | iteration: 126/262 | Loss: 0.6471751928329468Training Epoch: 5 | iteration: 127/262 | Loss: 0.6437587738037109Training Epoch: 5 | iteration: 128/262 | Loss: 0.7210876941680908Training Epoch: 5 | iteration: 129/262 | Loss: 0.6342030763626099Training Epoch: 5 | iteration: 130/262 | Loss: 0.7395858764648438Training Epoch: 5 | iteration: 131/262 | Loss: 0.6794997453689575Training Epoch: 5 | iteration: 132/262 | Loss: 0.6293999552726746Training Epoch: 5 | iteration: 133/262 | Loss: 0.6460155844688416Training Epoch: 5 | iteration: 134/262 | Loss: 0.6786807775497437Training Epoch: 5 | iteration: 135/262 | Loss: 0.659966766834259Training Epoch: 5 | iteration: 136/262 | Loss: 0.608579158782959Training Epoch: 5 | iteration: 137/262 | Loss: 0.6991798877716064Training Epoch: 5 | iteration: 138/262 | Loss: 0.6666625738143921Training Epoch: 5 | iteration: 139/262 | Loss: 0.6078527569770813Training Epoch: 5 | iteration: 140/262 | Loss: 0.7022672891616821Training Epoch: 5 | iteration: 141/262 | Loss: 0.597756028175354Training Epoch: 5 | iteration: 142/262 | Loss: 0.6749985218048096Training Epoch: 5 | iteration: 143/262 | Loss: 0.7090996503829956Training Epoch: 5 | iteration: 144/262 | Loss: 0.7302892208099365Training Epoch: 5 | iteration: 145/262 | Loss: 0.6556906700134277Training Epoch: 5 | iteration: 146/262 | Loss: 0.7400232553482056Training Epoch: 5 | iteration: 147/262 | Loss: 0.6252007484436035Training Epoch: 5 | iteration: 148/262 | Loss: 0.7441064715385437Training Epoch: 5 | iteration: 149/262 | Loss: 0.7374387979507446Training Epoch: 5 | iteration: 150/262 | Loss: 0.6360427141189575Training Epoch: 5 | iteration: 151/262 | Loss: 0.6340761184692383Training Epoch: 5 | iteration: 152/262 | Loss: 0.7047116756439209Training Epoch: 5 | iteration: 153/262 | Loss: 0.6554005146026611Training Epoch: 5 | iteration: 154/262 | Loss: 0.6185150742530823Training Epoch: 5 | iteration: 155/262 | Loss: 0.7260501384735107Training Epoch: 5 | iteration: 156/262 | Loss: 0.7151466608047485Training Epoch: 5 | iteration: 157/262 | Loss: 0.6633558869361877Training Epoch: 5 | iteration: 158/262 | Loss: 0.6708759665489197Training Epoch: 5 | iteration: 159/262 | Loss: 0.6641760468482971Training Epoch: 5 | iteration: 160/262 | Loss: 0.6974501609802246Training Epoch: 5 | iteration: 161/262 | Loss: 0.6738777160644531Training Epoch: 5 | iteration: 162/262 | Loss: 0.7031327486038208Training Epoch: 5 | iteration: 163/262 | Loss: 0.6932998895645142Training Epoch: 5 | iteration: 164/262 | Loss: 0.6052072048187256Training Epoch: 5 | iteration: 165/262 | Loss: 0.6245195269584656Training Epoch: 5 | iteration: 166/262 | Loss: 0.625095009803772Training Epoch: 5 | iteration: 167/262 | Loss: 0.6062160730361938Training Epoch: 5 | iteration: 168/262 | Loss: 0.7104150652885437Training Epoch: 5 | iteration: 169/262 | Loss: 0.7038216590881348Training Epoch: 5 | iteration: 170/262 | Loss: 0.7057311534881592Training Epoch: 5 | iteration: 171/262 | Loss: 0.6498339176177979Training Epoch: 5 | iteration: 172/262 | Loss: 0.647499144077301Training Epoch: 5 | iteration: 173/262 | Loss: 0.6902309656143188Training Epoch: 5 | iteration: 174/262 | Loss: 0.6935226917266846Training Epoch: 5 | iteration: 175/262 | Loss: 0.7184483408927917Training Epoch: 5 | iteration: 176/262 | Loss: 0.7066107392311096Training Epoch: 5 | iteration: 177/262 | Loss: 0.7171578407287598Training Epoch: 5 | iteration: 178/262 | Loss: 0.7009103298187256Training Epoch: 5 | iteration: 179/262 | Loss: 0.6801577210426331Training Epoch: 5 | iteration: 180/262 | Loss: 0.6780036687850952Training Epoch: 5 | iteration: 181/262 | Loss: 0.7137173414230347Training Epoch: 5 | iteration: 182/262 | Loss: 0.7722364664077759Training Epoch: 5 | iteration: 183/262 | Loss: 0.7532938718795776Training Epoch: 5 | iteration: 184/262 | Loss: 0.7434500455856323Training Epoch: 5 | iteration: 185/262 | Loss: 0.787519097328186Training Epoch: 5 | iteration: 186/262 | Loss: 0.6736037731170654Training Epoch: 5 | iteration: 187/262 | Loss: 0.6222101449966431Training Epoch: 5 | iteration: 188/262 | Loss: 0.6393640041351318Training Epoch: 5 | iteration: 189/262 | Loss: 0.6355037689208984Training Epoch: 5 | iteration: 190/262 | Loss: 0.6971726417541504Training Epoch: 5 | iteration: 191/262 | Loss: 0.6740981340408325Training Epoch: 5 | iteration: 192/262 | Loss: 0.7495741248130798Training Epoch: 5 | iteration: 193/262 | Loss: 0.6815232038497925Training Epoch: 5 | iteration: 194/262 | Loss: 0.685937762260437Training Epoch: 5 | iteration: 195/262 | Loss: 0.7146188020706177Training Epoch: 5 | iteration: 196/262 | Loss: 0.7491908669471741Training Epoch: 5 | iteration: 197/262 | Loss: 0.690875768661499Training Epoch: 5 | iteration: 198/262 | Loss: 0.7095154523849487Training Epoch: 5 | iteration: 199/262 | Loss: 0.6323784589767456Training Epoch: 5 | iteration: 200/262 | Loss: 0.6847063302993774Training Epoch: 5 | iteration: 201/262 | Loss: 0.6575835943222046Training Epoch: 5 | iteration: 202/262 | Loss: 0.676744818687439Training Epoch: 5 | iteration: 203/262 | Loss: 0.6959675550460815Training Epoch: 5 | iteration: 204/262 | Loss: 0.6879698038101196Training Epoch: 5 | iteration: 205/262 | Loss: 0.6363081336021423Training Epoch: 5 | iteration: 206/262 | Loss: 0.6920100450515747Training Epoch: 5 | iteration: 207/262 | Loss: 0.7812420725822449Training Epoch: 5 | iteration: 208/262 | Loss: 0.6157283782958984Training Epoch: 5 | iteration: 209/262 | Loss: 0.6634750366210938Training Epoch: 5 | iteration: 210/262 | Loss: 0.712835431098938Training Epoch: 5 | iteration: 211/262 | Loss: 0.7058488130569458Training Epoch: 5 | iteration: 212/262 | Loss: 0.7890294790267944Training Epoch: 5 | iteration: 213/262 | Loss: 0.703243613243103Training Epoch: 5 | iteration: 214/262 | Loss: 0.6809760332107544Training Epoch: 5 | iteration: 215/262 | Loss: 0.6287343502044678Training Epoch: 5 | iteration: 216/262 | Loss: 0.685776948928833Training Epoch: 5 | iteration: 217/262 | Loss: 0.6529597043991089Training Epoch: 5 | iteration: 218/262 | Loss: 0.7247994542121887Training Epoch: 5 | iteration: 219/262 | Loss: 0.5961767435073853Training Epoch: 5 | iteration: 220/262 | Loss: 0.7453895211219788Training Epoch: 5 | iteration: 221/262 | Loss: 0.7351927757263184Training Epoch: 5 | iteration: 222/262 | Loss: 0.7015358209609985Training Epoch: 5 | iteration: 223/262 | Loss: 0.6729190349578857Training Epoch: 5 | iteration: 224/262 | Loss: 0.6551429033279419Training Epoch: 5 | iteration: 225/262 | Loss: 0.6908818483352661Training Epoch: 5 | iteration: 226/262 | Loss: 0.6602342128753662Training Epoch: 5 | iteration: 227/262 | Loss: 0.6518619060516357Training Epoch: 5 | iteration: 228/262 | Loss: 0.6322101950645447Training Epoch: 5 | iteration: 229/262 | Loss: 0.727361798286438Training Epoch: 5 | iteration: 230/262 | Loss: 0.6557657718658447Training Epoch: 5 | iteration: 231/262 | Loss: 0.6425508856773376Training Epoch: 5 | iteration: 232/262 | Loss: 0.7522712349891663Training Epoch: 5 | iteration: 233/262 | Loss: 0.6676605939865112Training Epoch: 5 | iteration: 234/262 | Loss: 0.6538878679275513Training Epoch: 5 | iteration: 235/262 | Loss: 0.7188177108764648Training Epoch: 5 | iteration: 236/262 | Loss: 0.6706655025482178Training Epoch: 5 | iteration: 237/262 | Loss: 0.7728894948959351Training Epoch: 5 | iteration: 238/262 | Loss: 0.6647945642471313Training Epoch: 5 | iteration: 239/262 | Loss: 0.7201030254364014Training Epoch: 5 | iteration: 240/262 | Loss: 0.6788241863250732Training Epoch: 5 | iteration: 241/262 | Loss: 0.7419352531433105Training Epoch: 5 | iteration: 242/262 | Loss: 0.6262080669403076Training Epoch: 5 | iteration: 243/262 | Loss: 0.7465718984603882Training Epoch: 5 | iteration: 244/262 | Loss: 0.7244812250137329Training Epoch: 5 | iteration: 245/262 | Loss: 0.6832600235939026Training Epoch: 5 | iteration: 246/262 | Loss: 0.6579245328903198Training Epoch: 5 | iteration: 247/262 | Loss: 0.6957103610038757Training Epoch: 5 | iteration: 248/262 | Loss: 0.721167802810669Training Epoch: 5 | iteration: 249/262 | Loss: 0.7011610865592957Training Epoch: 5 | iteration: 250/262 | Loss: 0.7509292960166931Training Epoch: 5 | iteration: 251/262 | Loss: 0.6799046397209167Training Epoch: 5 | iteration: 252/262 | Loss: 0.6486274003982544Training Epoch: 5 | iteration: 253/262 | Loss: 0.6668305993080139Training Epoch: 5 | iteration: 254/262 | Loss: 0.6961761713027954Training Epoch: 5 | iteration: 255/262 | Loss: 0.6830899715423584Training Epoch: 5 | iteration: 256/262 | Loss: 0.6990359425544739Training Epoch: 5 | iteration: 257/262 | Loss: 0.7082476615905762Training Epoch: 5 | iteration: 258/262 | Loss: 0.7249937653541565Training Epoch: 5 | iteration: 259/262 | Loss: 0.6369039416313171Training Epoch: 5 | iteration: 260/262 | Loss: 0.6188346743583679Training Epoch: 5 | iteration: 261/262 | Loss: 0.8635003566741943Validating Epoch: 5 | iteration: 0/66 | Loss: 0.7910172939300537Validating Epoch: 5 | iteration: 1/66 | Loss: 0.6573066711425781Validating Epoch: 5 | iteration: 2/66 | Loss: 0.6610288023948669Validating Epoch: 5 | iteration: 3/66 | Loss: 0.6385518908500671Validating Epoch: 5 | iteration: 4/66 | Loss: 0.7094293236732483Validating Epoch: 5 | iteration: 5/66 | Loss: 0.7752716541290283Validating Epoch: 5 | iteration: 6/66 | Loss: 0.6942635774612427Validating Epoch: 5 | iteration: 7/66 | Loss: 0.7760181427001953Validating Epoch: 5 | iteration: 8/66 | Loss: 0.8077930808067322Validating Epoch: 5 | iteration: 9/66 | Loss: 0.698693573474884Validating Epoch: 5 | iteration: 10/66 | Loss: 0.6467220783233643Validating Epoch: 5 | iteration: 11/66 | Loss: 0.7116302847862244Validating Epoch: 5 | iteration: 12/66 | Loss: 0.736503541469574Validating Epoch: 5 | iteration: 13/66 | Loss: 0.7142360210418701Validating Epoch: 5 | iteration: 14/66 | Loss: 0.7283356785774231Validating Epoch: 5 | iteration: 15/66 | Loss: 0.6809035539627075Validating Epoch: 5 | iteration: 16/66 | Loss: 0.6667975187301636Validating Epoch: 5 | iteration: 17/66 | Loss: 0.7628229856491089Validating Epoch: 5 | iteration: 18/66 | Loss: 0.7135345935821533Validating Epoch: 5 | iteration: 19/66 | Loss: 0.707413375377655Validating Epoch: 5 | iteration: 20/66 | Loss: 0.7202086448669434Validating Epoch: 5 | iteration: 21/66 | Loss: 0.7010213136672974Validating Epoch: 5 | iteration: 22/66 | Loss: 0.7188352942466736Validating Epoch: 5 | iteration: 23/66 | Loss: 0.6526798009872437Validating Epoch: 5 | iteration: 24/66 | Loss: 0.6960611343383789Validating Epoch: 5 | iteration: 25/66 | Loss: 0.6393814086914062Validating Epoch: 5 | iteration: 26/66 | Loss: 0.668380618095398Validating Epoch: 5 | iteration: 27/66 | Loss: 0.7375967502593994Validating Epoch: 5 | iteration: 28/66 | Loss: 0.693879246711731Validating Epoch: 5 | iteration: 29/66 | Loss: 0.755287766456604Validating Epoch: 5 | iteration: 30/66 | Loss: 0.7213674783706665Validating Epoch: 5 | iteration: 31/66 | Loss: 0.6330529451370239Validating Epoch: 5 | iteration: 32/66 | Loss: 0.797744870185852Validating Epoch: 5 | iteration: 33/66 | Loss: 0.7032699584960938Validating Epoch: 5 | iteration: 34/66 | Loss: 0.6486013531684875Validating Epoch: 5 | iteration: 35/66 | Loss: 0.7066109776496887Validating Epoch: 5 | iteration: 36/66 | Loss: 0.6517515182495117Validating Epoch: 5 | iteration: 37/66 | Loss: 0.6557987928390503Validating Epoch: 5 | iteration: 38/66 | Loss: 0.6610654592514038Validating Epoch: 5 | iteration: 39/66 | Loss: 0.702013373374939Validating Epoch: 5 | iteration: 40/66 | Loss: 0.6829748153686523Validating Epoch: 5 | iteration: 41/66 | Loss: 0.7531960010528564Validating Epoch: 5 | iteration: 42/66 | Loss: 0.6347901821136475Validating Epoch: 5 | iteration: 43/66 | Loss: 0.7624902725219727Validating Epoch: 5 | iteration: 44/66 | Loss: 0.6643695831298828Validating Epoch: 5 | iteration: 45/66 | Loss: 0.6546372175216675Validating Epoch: 5 | iteration: 46/66 | Loss: 0.64842689037323Validating Epoch: 5 | iteration: 47/66 | Loss: 0.7035751342773438Validating Epoch: 5 | iteration: 48/66 | Loss: 0.6256083250045776Validating Epoch: 5 | iteration: 49/66 | Loss: 0.7148206233978271Validating Epoch: 5 | iteration: 50/66 | Loss: 0.7122464179992676Validating Epoch: 5 | iteration: 51/66 | Loss: 0.709313154220581Validating Epoch: 5 | iteration: 52/66 | Loss: 0.6410269141197205Validating Epoch: 5 | iteration: 53/66 | Loss: 0.6673002243041992Validating Epoch: 5 | iteration: 54/66 | Loss: 0.7029320001602173Validating Epoch: 5 | iteration: 55/66 | Loss: 0.6200571656227112Validating Epoch: 5 | iteration: 56/66 | Loss: 0.6609761714935303Validating Epoch: 5 | iteration: 57/66 | Loss: 0.6987095475196838Validating Epoch: 5 | iteration: 58/66 | Loss: 0.6878523826599121Validating Epoch: 5 | iteration: 59/66 | Loss: 0.6601678729057312Validating Epoch: 5 | iteration: 60/66 | Loss: 0.7042382955551147Validating Epoch: 5 | iteration: 61/66 | Loss: 0.7370501756668091Validating Epoch: 5 | iteration: 62/66 | Loss: 0.7608633041381836Validating Epoch: 5 | iteration: 63/66 | Loss: 0.7023630142211914Validating Epoch: 5 | iteration: 64/66 | Loss: 0.6677881479263306Validating Epoch: 5 | iteration: 65/66 | Loss: 0.6983733773231506Temperature:  0.5
Sampling: iteration: 0/51{'Validity': 0.9833984375, 'Novelty': 1.0, 'Uniqueness': 0.9980139026812314}
Training Epoch: 6 | iteration: 0/262 | Loss: 0.5803840160369873Training Epoch: 6 | iteration: 1/262 | Loss: 0.6921836733818054Training Epoch: 6 | iteration: 2/262 | Loss: 0.6501674652099609Training Epoch: 6 | iteration: 3/262 | Loss: 0.6725404262542725Training Epoch: 6 | iteration: 4/262 | Loss: 0.6984363794326782Training Epoch: 6 | iteration: 5/262 | Loss: 0.5962971448898315Training Epoch: 6 | iteration: 6/262 | Loss: 0.6645746231079102Training Epoch: 6 | iteration: 7/262 | Loss: 0.6790435314178467Training Epoch: 6 | iteration: 8/262 | Loss: 0.682275652885437Training Epoch: 6 | iteration: 9/262 | Loss: 0.6528148651123047Training Epoch: 6 | iteration: 10/262 | Loss: 0.6300541162490845Training Epoch: 6 | iteration: 11/262 | Loss: 0.6907259821891785Training Epoch: 6 | iteration: 12/262 | Loss: 0.6891618967056274Training Epoch: 6 | iteration: 13/262 | Loss: 0.6115151643753052Training Epoch: 6 | iteration: 14/262 | Loss: 0.6398150324821472Training Epoch: 6 | iteration: 15/262 | Loss: 0.638725221157074Training Epoch: 6 | iteration: 16/262 | Loss: 0.5988357067108154Training Epoch: 6 | iteration: 17/262 | Loss: 0.6378214359283447Training Epoch: 6 | iteration: 18/262 | Loss: 0.5509494543075562Training Epoch: 6 | iteration: 19/262 | Loss: 0.6462967395782471Training Epoch: 6 | iteration: 20/262 | Loss: 0.7334628701210022Training Epoch: 6 | iteration: 21/262 | Loss: 0.635645866394043Training Epoch: 6 | iteration: 22/262 | Loss: 0.6542344093322754Training Epoch: 6 | iteration: 23/262 | Loss: 0.6244088411331177Training Epoch: 6 | iteration: 24/262 | Loss: 0.6322489976882935Training Epoch: 6 | iteration: 25/262 | Loss: 0.6220701336860657Training Epoch: 6 | iteration: 26/262 | Loss: 0.5777281522750854Training Epoch: 6 | iteration: 27/262 | Loss: 0.6318736672401428Training Epoch: 6 | iteration: 28/262 | Loss: 0.6722985506057739Training Epoch: 6 | iteration: 29/262 | Loss: 0.6004915237426758Training Epoch: 6 | iteration: 30/262 | Loss: 0.6455250978469849Training Epoch: 6 | iteration: 31/262 | Loss: 0.7121548056602478Training Epoch: 6 | iteration: 32/262 | Loss: 0.6133617758750916Training Epoch: 6 | iteration: 33/262 | Loss: 0.7056562900543213Training Epoch: 6 | iteration: 34/262 | Loss: 0.6620855331420898Training Epoch: 6 | iteration: 35/262 | Loss: 0.6795012950897217Training Epoch: 6 | iteration: 36/262 | Loss: 0.6241057515144348Training Epoch: 6 | iteration: 37/262 | Loss: 0.6571567058563232Training Epoch: 6 | iteration: 38/262 | Loss: 0.6218394041061401Training Epoch: 6 | iteration: 39/262 | Loss: 0.6809212565422058Training Epoch: 6 | iteration: 40/262 | Loss: 0.6426572799682617Training Epoch: 6 | iteration: 41/262 | Loss: 0.6344825625419617Training Epoch: 6 | iteration: 42/262 | Loss: 0.6513421535491943Training Epoch: 6 | iteration: 43/262 | Loss: 0.6611221432685852Training Epoch: 6 | iteration: 44/262 | Loss: 0.6378680467605591Training Epoch: 6 | iteration: 45/262 | Loss: 0.6154763698577881Training Epoch: 6 | iteration: 46/262 | Loss: 0.6559312343597412Training Epoch: 6 | iteration: 47/262 | Loss: 0.7173888683319092Training Epoch: 6 | iteration: 48/262 | Loss: 0.7078533172607422Training Epoch: 6 | iteration: 49/262 | Loss: 0.5989380478858948Training Epoch: 6 | iteration: 50/262 | Loss: 0.6437025666236877Training Epoch: 6 | iteration: 51/262 | Loss: 0.6482410430908203Training Epoch: 6 | iteration: 52/262 | Loss: 0.5738038420677185Training Epoch: 6 | iteration: 53/262 | Loss: 0.5629896521568298Training Epoch: 6 | iteration: 54/262 | Loss: 0.5915093421936035Training Epoch: 6 | iteration: 55/262 | Loss: 0.6806620955467224Training Epoch: 6 | iteration: 56/262 | Loss: 0.6371879577636719Training Epoch: 6 | iteration: 57/262 | Loss: 0.707412838935852Training Epoch: 6 | iteration: 58/262 | Loss: 0.6600069999694824Training Epoch: 6 | iteration: 59/262 | Loss: 0.6364426016807556Training Epoch: 6 | iteration: 60/262 | Loss: 0.6474189162254333Training Epoch: 6 | iteration: 61/262 | Loss: 0.6972484588623047Training Epoch: 6 | iteration: 62/262 | Loss: 0.6361136436462402Training Epoch: 6 | iteration: 63/262 | Loss: 0.769275426864624Training Epoch: 6 | iteration: 64/262 | Loss: 0.624224066734314Training Epoch: 6 | iteration: 65/262 | Loss: 0.6655634045600891Training Epoch: 6 | iteration: 66/262 | Loss: 0.6498517394065857Training Epoch: 6 | iteration: 67/262 | Loss: 0.6792960166931152Training Epoch: 6 | iteration: 68/262 | Loss: 0.687481701374054Training Epoch: 6 | iteration: 69/262 | Loss: 0.743710458278656Training Epoch: 6 | iteration: 70/262 | Loss: 0.6656110286712646Training Epoch: 6 | iteration: 71/262 | Loss: 0.6897067427635193Training Epoch: 6 | iteration: 72/262 | Loss: 0.753319501876831Training Epoch: 6 | iteration: 73/262 | Loss: 0.6209999322891235Training Epoch: 6 | iteration: 74/262 | Loss: 0.6112731695175171Training Epoch: 6 | iteration: 75/262 | Loss: 0.6412612199783325Training Epoch: 6 | iteration: 76/262 | Loss: 0.66634202003479Training Epoch: 6 | iteration: 77/262 | Loss: 0.7116188406944275Training Epoch: 6 | iteration: 78/262 | Loss: 0.6217823028564453Training Epoch: 6 | iteration: 79/262 | Loss: 0.6778209209442139Training Epoch: 6 | iteration: 80/262 | Loss: 0.6600528955459595Training Epoch: 6 | iteration: 81/262 | Loss: 0.7467689514160156Training Epoch: 6 | iteration: 82/262 | Loss: 0.5934945344924927Training Epoch: 6 | iteration: 83/262 | Loss: 0.6719214916229248Training Epoch: 6 | iteration: 84/262 | Loss: 0.6447354555130005Training Epoch: 6 | iteration: 85/262 | Loss: 0.6747376918792725Training Epoch: 6 | iteration: 86/262 | Loss: 0.6485768556594849Training Epoch: 6 | iteration: 87/262 | Loss: 0.6915294528007507Training Epoch: 6 | iteration: 88/262 | Loss: 0.7132499814033508Training Epoch: 6 | iteration: 89/262 | Loss: 0.6779972314834595Training Epoch: 6 | iteration: 90/262 | Loss: 0.71978759765625Training Epoch: 6 | iteration: 91/262 | Loss: 0.6079145669937134Training Epoch: 6 | iteration: 92/262 | Loss: 0.7973873615264893Training Epoch: 6 | iteration: 93/262 | Loss: 0.6577166318893433Training Epoch: 6 | iteration: 94/262 | Loss: 0.6539455056190491Training Epoch: 6 | iteration: 95/262 | Loss: 0.6352776885032654Training Epoch: 6 | iteration: 96/262 | Loss: 0.6753360629081726Training Epoch: 6 | iteration: 97/262 | Loss: 0.6377924680709839Training Epoch: 6 | iteration: 98/262 | Loss: 0.6644262671470642Training Epoch: 6 | iteration: 99/262 | Loss: 0.6170608401298523Training Epoch: 6 | iteration: 100/262 | Loss: 0.6493465900421143Training Epoch: 6 | iteration: 101/262 | Loss: 0.6088703870773315Training Epoch: 6 | iteration: 102/262 | Loss: 0.7212079763412476Training Epoch: 6 | iteration: 103/262 | Loss: 0.7037379741668701Training Epoch: 6 | iteration: 104/262 | Loss: 0.6734881401062012Training Epoch: 6 | iteration: 105/262 | Loss: 0.6211970448493958Training Epoch: 6 | iteration: 106/262 | Loss: 0.6513686180114746Training Epoch: 6 | iteration: 107/262 | Loss: 0.6458101272583008Training Epoch: 6 | iteration: 108/262 | Loss: 0.6221157312393188Training Epoch: 6 | iteration: 109/262 | Loss: 0.6073657274246216Training Epoch: 6 | iteration: 110/262 | Loss: 0.7068251967430115Training Epoch: 6 | iteration: 111/262 | Loss: 0.6393932104110718Training Epoch: 6 | iteration: 112/262 | Loss: 0.6045071482658386Training Epoch: 6 | iteration: 113/262 | Loss: 0.6941192150115967Training Epoch: 6 | iteration: 114/262 | Loss: 0.6833120584487915Training Epoch: 6 | iteration: 115/262 | Loss: 0.7277591228485107Training Epoch: 6 | iteration: 116/262 | Loss: 0.6980171203613281Training Epoch: 6 | iteration: 117/262 | Loss: 0.5852574110031128Training Epoch: 6 | iteration: 118/262 | Loss: 0.6234880685806274Training Epoch: 6 | iteration: 119/262 | Loss: 0.6646612882614136Training Epoch: 6 | iteration: 120/262 | Loss: 0.6771149635314941Training Epoch: 6 | iteration: 121/262 | Loss: 0.7170759439468384Training Epoch: 6 | iteration: 122/262 | Loss: 0.6056622862815857Training Epoch: 6 | iteration: 123/262 | Loss: 0.6367514729499817Training Epoch: 6 | iteration: 124/262 | Loss: 0.7294610738754272Training Epoch: 6 | iteration: 125/262 | Loss: 0.737710177898407Training Epoch: 6 | iteration: 126/262 | Loss: 0.6317884922027588Training Epoch: 6 | iteration: 127/262 | Loss: 0.6568245887756348Training Epoch: 6 | iteration: 128/262 | Loss: 0.7122884392738342Training Epoch: 6 | iteration: 129/262 | Loss: 0.7189648747444153Training Epoch: 6 | iteration: 130/262 | Loss: 0.7305353283882141Training Epoch: 6 | iteration: 131/262 | Loss: 0.6475654244422913Training Epoch: 6 | iteration: 132/262 | Loss: 0.626021683216095Training Epoch: 6 | iteration: 133/262 | Loss: 0.659092128276825Training Epoch: 6 | iteration: 134/262 | Loss: 0.6480216383934021Training Epoch: 6 | iteration: 135/262 | Loss: 0.623144268989563Training Epoch: 6 | iteration: 136/262 | Loss: 0.7404153943061829Training Epoch: 6 | iteration: 137/262 | Loss: 0.6106319427490234Training Epoch: 6 | iteration: 138/262 | Loss: 0.644133448600769Training Epoch: 6 | iteration: 139/262 | Loss: 0.6876959204673767Training Epoch: 6 | iteration: 140/262 | Loss: 0.6374956369400024Training Epoch: 6 | iteration: 141/262 | Loss: 0.6216323375701904Training Epoch: 6 | iteration: 142/262 | Loss: 0.591418445110321Training Epoch: 6 | iteration: 143/262 | Loss: 0.6702418327331543Training Epoch: 6 | iteration: 144/262 | Loss: 0.6935131549835205Training Epoch: 6 | iteration: 145/262 | Loss: 0.6273346543312073Training Epoch: 6 | iteration: 146/262 | Loss: 0.7139967679977417Training Epoch: 6 | iteration: 147/262 | Loss: 0.6554714441299438Training Epoch: 6 | iteration: 148/262 | Loss: 0.6890579462051392Training Epoch: 6 | iteration: 149/262 | Loss: 0.6169313192367554Training Epoch: 6 | iteration: 150/262 | Loss: 0.67476487159729Training Epoch: 6 | iteration: 151/262 | Loss: 0.644199013710022Training Epoch: 6 | iteration: 152/262 | Loss: 0.6103476285934448Training Epoch: 6 | iteration: 153/262 | Loss: 0.7094538807868958Training Epoch: 6 | iteration: 154/262 | Loss: 0.5739027261734009Training Epoch: 6 | iteration: 155/262 | Loss: 0.594207763671875Training Epoch: 6 | iteration: 156/262 | Loss: 0.6732602715492249Training Epoch: 6 | iteration: 157/262 | Loss: 0.6281756162643433Training Epoch: 6 | iteration: 158/262 | Loss: 0.6206731796264648Training Epoch: 6 | iteration: 159/262 | Loss: 0.6198309659957886Training Epoch: 6 | iteration: 160/262 | Loss: 0.6621080636978149Training Epoch: 6 | iteration: 161/262 | Loss: 0.6606245040893555Training Epoch: 6 | iteration: 162/262 | Loss: 0.6426106095314026Training Epoch: 6 | iteration: 163/262 | Loss: 0.7144870162010193Training Epoch: 6 | iteration: 164/262 | Loss: 0.6235285401344299Training Epoch: 6 | iteration: 165/262 | Loss: 0.6792951822280884Training Epoch: 6 | iteration: 166/262 | Loss: 0.6187419891357422Training Epoch: 6 | iteration: 167/262 | Loss: 0.746681809425354Training Epoch: 6 | iteration: 168/262 | Loss: 0.6511996388435364Training Epoch: 6 | iteration: 169/262 | Loss: 0.6235674619674683Training Epoch: 6 | iteration: 170/262 | Loss: 0.6228530406951904Training Epoch: 6 | iteration: 171/262 | Loss: 0.6985604763031006Training Epoch: 6 | iteration: 172/262 | Loss: 0.6180857419967651Training Epoch: 6 | iteration: 173/262 | Loss: 0.563992977142334Training Epoch: 6 | iteration: 174/262 | Loss: 0.5547175407409668Training Epoch: 6 | iteration: 175/262 | Loss: 0.5919629335403442Training Epoch: 6 | iteration: 176/262 | Loss: 0.6421374678611755Training Epoch: 6 | iteration: 177/262 | Loss: 0.6406853199005127Training Epoch: 6 | iteration: 178/262 | Loss: 0.6399713754653931Training Epoch: 6 | iteration: 179/262 | Loss: 0.6685836315155029Training Epoch: 6 | iteration: 180/262 | Loss: 0.6902651786804199Training Epoch: 6 | iteration: 181/262 | Loss: 0.6244025230407715Training Epoch: 6 | iteration: 182/262 | Loss: 0.700410008430481Training Epoch: 6 | iteration: 183/262 | Loss: 0.6642257571220398Training Epoch: 6 | iteration: 184/262 | Loss: 0.6007668375968933Training Epoch: 6 | iteration: 185/262 | Loss: 0.6350119709968567Training Epoch: 6 | iteration: 186/262 | Loss: 0.6313140392303467Training Epoch: 6 | iteration: 187/262 | Loss: 0.6574411392211914Training Epoch: 6 | iteration: 188/262 | Loss: 0.6611937880516052Training Epoch: 6 | iteration: 189/262 | Loss: 0.8041602969169617Training Epoch: 6 | iteration: 190/262 | Loss: 0.689626932144165Training Epoch: 6 | iteration: 191/262 | Loss: 0.6234898567199707Training Epoch: 6 | iteration: 192/262 | Loss: 0.7386812567710876Training Epoch: 6 | iteration: 193/262 | Loss: 0.6841322183609009Training Epoch: 6 | iteration: 194/262 | Loss: 0.7511359453201294Training Epoch: 6 | iteration: 195/262 | Loss: 0.6866533756256104Training Epoch: 6 | iteration: 196/262 | Loss: 0.720780074596405Training Epoch: 6 | iteration: 197/262 | Loss: 0.6712595224380493Training Epoch: 6 | iteration: 198/262 | Loss: 0.6491019129753113Training Epoch: 6 | iteration: 199/262 | Loss: 0.6624890565872192Training Epoch: 6 | iteration: 200/262 | Loss: 0.7089314460754395Training Epoch: 6 | iteration: 201/262 | Loss: 0.7090281844139099Training Epoch: 6 | iteration: 202/262 | Loss: 0.6097445487976074Training Epoch: 6 | iteration: 203/262 | Loss: 0.7384100556373596Training Epoch: 6 | iteration: 204/262 | Loss: 0.6678667068481445Training Epoch: 6 | iteration: 205/262 | Loss: 0.7207555174827576Training Epoch: 6 | iteration: 206/262 | Loss: 0.640126645565033Training Epoch: 6 | iteration: 207/262 | Loss: 0.6291460990905762Training Epoch: 6 | iteration: 208/262 | Loss: 0.6785506010055542Training Epoch: 6 | iteration: 209/262 | Loss: 0.7058383226394653Training Epoch: 6 | iteration: 210/262 | Loss: 0.5847010612487793Training Epoch: 6 | iteration: 211/262 | Loss: 0.6496678590774536Training Epoch: 6 | iteration: 212/262 | Loss: 0.6498339176177979Training Epoch: 6 | iteration: 213/262 | Loss: 0.5892571210861206Training Epoch: 6 | iteration: 214/262 | Loss: 0.6242731213569641Training Epoch: 6 | iteration: 215/262 | Loss: 0.6536599397659302Training Epoch: 6 | iteration: 216/262 | Loss: 0.7362247705459595Training Epoch: 6 | iteration: 217/262 | Loss: 0.7052943706512451Training Epoch: 6 | iteration: 218/262 | Loss: 0.639796793460846Training Epoch: 6 | iteration: 219/262 | Loss: 0.6987561583518982Training Epoch: 6 | iteration: 220/262 | Loss: 0.7767719626426697Training Epoch: 6 | iteration: 221/262 | Loss: 0.7007855176925659Training Epoch: 6 | iteration: 222/262 | Loss: 0.634091854095459Training Epoch: 6 | iteration: 223/262 | Loss: 0.5661609172821045Training Epoch: 6 | iteration: 224/262 | Loss: 0.6787861585617065Training Epoch: 6 | iteration: 225/262 | Loss: 0.6492515206336975Training Epoch: 6 | iteration: 226/262 | Loss: 0.6207997798919678Training Epoch: 6 | iteration: 227/262 | Loss: 0.6815987229347229Training Epoch: 6 | iteration: 228/262 | Loss: 0.701741099357605Training Epoch: 6 | iteration: 229/262 | Loss: 0.6782498359680176Training Epoch: 6 | iteration: 230/262 | Loss: 0.6482039093971252Training Epoch: 6 | iteration: 231/262 | Loss: 0.6469780206680298Training Epoch: 6 | iteration: 232/262 | Loss: 0.6934394836425781Training Epoch: 6 | iteration: 233/262 | Loss: 0.6544737219810486Training Epoch: 6 | iteration: 234/262 | Loss: 0.6378481388092041Training Epoch: 6 | iteration: 235/262 | Loss: 0.6682690382003784Training Epoch: 6 | iteration: 236/262 | Loss: 0.7420135736465454Training Epoch: 6 | iteration: 237/262 | Loss: 0.6726230382919312Training Epoch: 6 | iteration: 238/262 | Loss: 0.6920733451843262Training Epoch: 6 | iteration: 239/262 | Loss: 0.6698833107948303Training Epoch: 6 | iteration: 240/262 | Loss: 0.6372641921043396Training Epoch: 6 | iteration: 241/262 | Loss: 0.7009990215301514Training Epoch: 6 | iteration: 242/262 | Loss: 0.6708763837814331Training Epoch: 6 | iteration: 243/262 | Loss: 0.7211911678314209Training Epoch: 6 | iteration: 244/262 | Loss: 0.6638680696487427Training Epoch: 6 | iteration: 245/262 | Loss: 0.6545839309692383Training Epoch: 6 | iteration: 246/262 | Loss: 0.7026283740997314Training Epoch: 6 | iteration: 247/262 | Loss: 0.7133170366287231Training Epoch: 6 | iteration: 248/262 | Loss: 0.6165186166763306Training Epoch: 6 | iteration: 249/262 | Loss: 0.6449755430221558Training Epoch: 6 | iteration: 250/262 | Loss: 0.7290012836456299Training Epoch: 6 | iteration: 251/262 | Loss: 0.7168369293212891Training Epoch: 6 | iteration: 252/262 | Loss: 0.723103940486908Training Epoch: 6 | iteration: 253/262 | Loss: 0.5723884105682373Training Epoch: 6 | iteration: 254/262 | Loss: 0.62287837266922Training Epoch: 6 | iteration: 255/262 | Loss: 0.6418396234512329Training Epoch: 6 | iteration: 256/262 | Loss: 0.6077126264572144Training Epoch: 6 | iteration: 257/262 | Loss: 0.6884760856628418Training Epoch: 6 | iteration: 258/262 | Loss: 0.6636682748794556Training Epoch: 6 | iteration: 259/262 | Loss: 0.6402655839920044Training Epoch: 6 | iteration: 260/262 | Loss: 0.6490505933761597Training Epoch: 6 | iteration: 261/262 | Loss: 0.6444589495658875Validating Epoch: 6 | iteration: 0/66 | Loss: 0.738014280796051Validating Epoch: 6 | iteration: 1/66 | Loss: 0.6778222322463989Validating Epoch: 6 | iteration: 2/66 | Loss: 0.7198483943939209Validating Epoch: 6 | iteration: 3/66 | Loss: 0.6479371190071106Validating Epoch: 6 | iteration: 4/66 | Loss: 0.7092561721801758Validating Epoch: 6 | iteration: 5/66 | Loss: 0.7059811949729919Validating Epoch: 6 | iteration: 6/66 | Loss: 0.7725070118904114Validating Epoch: 6 | iteration: 7/66 | Loss: 0.6379004716873169Validating Epoch: 6 | iteration: 8/66 | Loss: 0.654491662979126Validating Epoch: 6 | iteration: 9/66 | Loss: 0.6012709140777588Validating Epoch: 6 | iteration: 10/66 | Loss: 0.6928921937942505Validating Epoch: 6 | iteration: 11/66 | Loss: 0.6699868440628052Validating Epoch: 6 | iteration: 12/66 | Loss: 0.7131571769714355Validating Epoch: 6 | iteration: 13/66 | Loss: 0.6539626717567444Validating Epoch: 6 | iteration: 14/66 | Loss: 0.7030383944511414Validating Epoch: 6 | iteration: 15/66 | Loss: 0.7054281234741211Validating Epoch: 6 | iteration: 16/66 | Loss: 0.652542769908905Validating Epoch: 6 | iteration: 17/66 | Loss: 0.7388473749160767Validating Epoch: 6 | iteration: 18/66 | Loss: 0.6509593725204468Validating Epoch: 6 | iteration: 19/66 | Loss: 0.7038492560386658Validating Epoch: 6 | iteration: 20/66 | Loss: 0.6915668249130249Validating Epoch: 6 | iteration: 21/66 | Loss: 0.7598709464073181Validating Epoch: 6 | iteration: 22/66 | Loss: 0.720000147819519Validating Epoch: 6 | iteration: 23/66 | Loss: 0.6760635375976562Validating Epoch: 6 | iteration: 24/66 | Loss: 0.735919713973999Validating Epoch: 6 | iteration: 25/66 | Loss: 0.6665949821472168Validating Epoch: 6 | iteration: 26/66 | Loss: 0.7319619059562683Validating Epoch: 6 | iteration: 27/66 | Loss: 0.7100347876548767Validating Epoch: 6 | iteration: 28/66 | Loss: 0.6272875070571899Validating Epoch: 6 | iteration: 29/66 | Loss: 0.6275199055671692Validating Epoch: 6 | iteration: 30/66 | Loss: 0.7743543386459351Validating Epoch: 6 | iteration: 31/66 | Loss: 0.697683572769165Validating Epoch: 6 | iteration: 32/66 | Loss: 0.6859854459762573Validating Epoch: 6 | iteration: 33/66 | Loss: 0.6294865608215332Validating Epoch: 6 | iteration: 34/66 | Loss: 0.6982772946357727Validating Epoch: 6 | iteration: 35/66 | Loss: 0.6680819392204285Validating Epoch: 6 | iteration: 36/66 | Loss: 0.7134331464767456Validating Epoch: 6 | iteration: 37/66 | Loss: 0.671576976776123Validating Epoch: 6 | iteration: 38/66 | Loss: 0.7251347303390503Validating Epoch: 6 | iteration: 39/66 | Loss: 0.6978180408477783Validating Epoch: 6 | iteration: 40/66 | Loss: 0.6955676078796387Validating Epoch: 6 | iteration: 41/66 | Loss: 0.6870849132537842Validating Epoch: 6 | iteration: 42/66 | Loss: 0.7238327860832214Validating Epoch: 6 | iteration: 43/66 | Loss: 0.6617860198020935Validating Epoch: 6 | iteration: 44/66 | Loss: 0.7185912132263184Validating Epoch: 6 | iteration: 45/66 | Loss: 0.6966116428375244Validating Epoch: 6 | iteration: 46/66 | Loss: 0.7389756441116333Validating Epoch: 6 | iteration: 47/66 | Loss: 0.7278108596801758Validating Epoch: 6 | iteration: 48/66 | Loss: 0.7656540870666504Validating Epoch: 6 | iteration: 49/66 | Loss: 0.7182382941246033Validating Epoch: 6 | iteration: 50/66 | Loss: 0.6303735375404358Validating Epoch: 6 | iteration: 51/66 | Loss: 0.7027532458305359Validating Epoch: 6 | iteration: 52/66 | Loss: 0.6678186655044556Validating Epoch: 6 | iteration: 53/66 | Loss: 0.7203483581542969Validating Epoch: 6 | iteration: 54/66 | Loss: 0.6747415065765381Validating Epoch: 6 | iteration: 55/66 | Loss: 0.713603675365448Validating Epoch: 6 | iteration: 56/66 | Loss: 0.7050060033798218Validating Epoch: 6 | iteration: 57/66 | Loss: 0.6825402975082397Validating Epoch: 6 | iteration: 58/66 | Loss: 0.6508910059928894Validating Epoch: 6 | iteration: 59/66 | Loss: 0.7161275744438171Validating Epoch: 6 | iteration: 60/66 | Loss: 0.702885091304779Validating Epoch: 6 | iteration: 61/66 | Loss: 0.7063260078430176Validating Epoch: 6 | iteration: 62/66 | Loss: 0.6271423101425171Validating Epoch: 6 | iteration: 63/66 | Loss: 0.6973242163658142Validating Epoch: 6 | iteration: 64/66 | Loss: 0.6391506195068359Validating Epoch: 6 | iteration: 65/66 | Loss: 0.7526882886886597Temperature:  0.5
Sampling: iteration: 0/51{'Validity': 0.9892578125, 'Novelty': 1.0, 'Uniqueness': 0.998025666337611}
Training Epoch: 7 | iteration: 0/262 | Loss: 0.6651774644851685Training Epoch: 7 | iteration: 1/262 | Loss: 0.6991300582885742Training Epoch: 7 | iteration: 2/262 | Loss: 0.6258307695388794Training Epoch: 7 | iteration: 3/262 | Loss: 0.6896905899047852Training Epoch: 7 | iteration: 4/262 | Loss: 0.602251410484314Training Epoch: 7 | iteration: 5/262 | Loss: 0.6963951587677002Training Epoch: 7 | iteration: 6/262 | Loss: 0.6003559231758118Training Epoch: 7 | iteration: 7/262 | Loss: 0.6021252870559692Training Epoch: 7 | iteration: 8/262 | Loss: 0.623997688293457Training Epoch: 7 | iteration: 9/262 | Loss: 0.660916805267334Training Epoch: 7 | iteration: 10/262 | Loss: 0.6732293367385864Training Epoch: 7 | iteration: 11/262 | Loss: 0.6450977325439453Training Epoch: 7 | iteration: 12/262 | Loss: 0.5747275352478027Training Epoch: 7 | iteration: 13/262 | Loss: 0.6136243939399719Training Epoch: 7 | iteration: 14/262 | Loss: 0.6698540449142456Training Epoch: 7 | iteration: 15/262 | Loss: 0.633800745010376Training Epoch: 7 | iteration: 16/262 | Loss: 0.634524941444397Training Epoch: 7 | iteration: 17/262 | Loss: 0.6875691413879395Training Epoch: 7 | iteration: 18/262 | Loss: 0.6304317712783813Training Epoch: 7 | iteration: 19/262 | Loss: 0.633800745010376Training Epoch: 7 | iteration: 20/262 | Loss: 0.6524366736412048Training Epoch: 7 | iteration: 21/262 | Loss: 0.642853856086731Training Epoch: 7 | iteration: 22/262 | Loss: 0.5964994430541992Training Epoch: 7 | iteration: 23/262 | Loss: 0.6664324402809143Training Epoch: 7 | iteration: 24/262 | Loss: 0.6821917295455933Training Epoch: 7 | iteration: 25/262 | Loss: 0.6427421569824219Training Epoch: 7 | iteration: 26/262 | Loss: 0.6755568981170654Training Epoch: 7 | iteration: 27/262 | Loss: 0.645561695098877Training Epoch: 7 | iteration: 28/262 | Loss: 0.7025904655456543Training Epoch: 7 | iteration: 29/262 | Loss: 0.57869553565979Training Epoch: 7 | iteration: 30/262 | Loss: 0.666499674320221Training Epoch: 7 | iteration: 31/262 | Loss: 0.5844724178314209Training Epoch: 7 | iteration: 32/262 | Loss: 0.6803672313690186Training Epoch: 7 | iteration: 33/262 | Loss: 0.6710085868835449Training Epoch: 7 | iteration: 34/262 | Loss: 0.7097766995429993Training Epoch: 7 | iteration: 35/262 | Loss: 0.6383217573165894Training Epoch: 7 | iteration: 36/262 | Loss: 0.6598462462425232Training Epoch: 7 | iteration: 37/262 | Loss: 0.6336759328842163Training Epoch: 7 | iteration: 38/262 | Loss: 0.5759276151657104Training Epoch: 7 | iteration: 39/262 | Loss: 0.6802126169204712Training Epoch: 7 | iteration: 40/262 | Loss: 0.6639282703399658Training Epoch: 7 | iteration: 41/262 | Loss: 0.6455250978469849Training Epoch: 7 | iteration: 42/262 | Loss: 0.6579961180686951Training Epoch: 7 | iteration: 43/262 | Loss: 0.6067238450050354Training Epoch: 7 | iteration: 44/262 | Loss: 0.6638402342796326Training Epoch: 7 | iteration: 45/262 | Loss: 0.6555628180503845Training Epoch: 7 | iteration: 46/262 | Loss: 0.597062885761261Training Epoch: 7 | iteration: 47/262 | Loss: 0.6028831005096436Training Epoch: 7 | iteration: 48/262 | Loss: 0.70421302318573Training Epoch: 7 | iteration: 49/262 | Loss: 0.6762224435806274Training Epoch: 7 | iteration: 50/262 | Loss: 0.6592828035354614Training Epoch: 7 | iteration: 51/262 | Loss: 0.6223094463348389Training Epoch: 7 | iteration: 52/262 | Loss: 0.589445173740387Training Epoch: 7 | iteration: 53/262 | Loss: 0.6268253326416016Training Epoch: 7 | iteration: 54/262 | Loss: 0.65804123878479Training Epoch: 7 | iteration: 55/262 | Loss: 0.6117298603057861Training Epoch: 7 | iteration: 56/262 | Loss: 0.6282774209976196Training Epoch: 7 | iteration: 57/262 | Loss: 0.6126691102981567Training Epoch: 7 | iteration: 58/262 | Loss: 0.6441836357116699Training Epoch: 7 | iteration: 59/262 | Loss: 0.5945037007331848Training Epoch: 7 | iteration: 60/262 | Loss: 0.6663345098495483Training Epoch: 7 | iteration: 61/262 | Loss: 0.6081480979919434Training Epoch: 7 | iteration: 62/262 | Loss: 0.6932586431503296Training Epoch: 7 | iteration: 63/262 | Loss: 0.7217996120452881Training Epoch: 7 | iteration: 64/262 | Loss: 0.638990581035614Training Epoch: 7 | iteration: 65/262 | Loss: 0.6366848945617676Training Epoch: 7 | iteration: 66/262 | Loss: 0.5529699325561523Training Epoch: 7 | iteration: 67/262 | Loss: 0.635123610496521Training Epoch: 7 | iteration: 68/262 | Loss: 0.6428136825561523Training Epoch: 7 | iteration: 69/262 | Loss: 0.7079892754554749Training Epoch: 7 | iteration: 70/262 | Loss: 0.5810390710830688Training Epoch: 7 | iteration: 71/262 | Loss: 0.5925114154815674Training Epoch: 7 | iteration: 72/262 | Loss: 0.6319888830184937Training Epoch: 7 | iteration: 73/262 | Loss: 0.6807443499565125Training Epoch: 7 | iteration: 74/262 | Loss: 0.6124489307403564Training Epoch: 7 | iteration: 75/262 | Loss: 0.5844078660011292Training Epoch: 7 | iteration: 76/262 | Loss: 0.6682780385017395Training Epoch: 7 | iteration: 77/262 | Loss: 0.652198314666748Training Epoch: 7 | iteration: 78/262 | Loss: 0.6717879772186279Training Epoch: 7 | iteration: 79/262 | Loss: 0.6643664836883545Training Epoch: 7 | iteration: 80/262 | Loss: 0.6513038873672485Training Epoch: 7 | iteration: 81/262 | Loss: 0.5823888182640076Training Epoch: 7 | iteration: 82/262 | Loss: 0.6136445999145508Training Epoch: 7 | iteration: 83/262 | Loss: 0.6493752002716064Training Epoch: 7 | iteration: 84/262 | Loss: 0.6155151128768921Training Epoch: 7 | iteration: 85/262 | Loss: 0.6899698376655579Training Epoch: 7 | iteration: 86/262 | Loss: 0.6545581221580505Training Epoch: 7 | iteration: 87/262 | Loss: 0.612334132194519Training Epoch: 7 | iteration: 88/262 | Loss: 0.5892438888549805Training Epoch: 7 | iteration: 89/262 | Loss: 0.7092692255973816Training Epoch: 7 | iteration: 90/262 | Loss: 0.6424973607063293Training Epoch: 7 | iteration: 91/262 | Loss: 0.6486420631408691Training Epoch: 7 | iteration: 92/262 | Loss: 0.6088950634002686Training Epoch: 7 | iteration: 93/262 | Loss: 0.6174335479736328Training Epoch: 7 | iteration: 94/262 | Loss: 0.6214960813522339Training Epoch: 7 | iteration: 95/262 | Loss: 0.6637821793556213Training Epoch: 7 | iteration: 96/262 | Loss: 0.6454969048500061Training Epoch: 7 | iteration: 97/262 | Loss: 0.6684876680374146Training Epoch: 7 | iteration: 98/262 | Loss: 0.5942336916923523Training Epoch: 7 | iteration: 99/262 | Loss: 0.7350618243217468Training Epoch: 7 | iteration: 100/262 | Loss: 0.6754559278488159Training Epoch: 7 | iteration: 101/262 | Loss: 0.5741788148880005Training Epoch: 7 | iteration: 102/262 | Loss: 0.6478918790817261Training Epoch: 7 | iteration: 103/262 | Loss: 0.648083508014679Training Epoch: 7 | iteration: 104/262 | Loss: 0.6649465560913086Training Epoch: 7 | iteration: 105/262 | Loss: 0.6460575461387634Training Epoch: 7 | iteration: 106/262 | Loss: 0.5870261788368225Training Epoch: 7 | iteration: 107/262 | Loss: 0.6339360475540161Training Epoch: 7 | iteration: 108/262 | Loss: 0.6240935921669006Training Epoch: 7 | iteration: 109/262 | Loss: 0.6500511765480042Training Epoch: 7 | iteration: 110/262 | Loss: 0.6020710468292236Training Epoch: 7 | iteration: 111/262 | Loss: 0.6777031421661377Training Epoch: 7 | iteration: 112/262 | Loss: 0.6489694118499756Training Epoch: 7 | iteration: 113/262 | Loss: 0.6596086621284485Training Epoch: 7 | iteration: 114/262 | Loss: 0.612510085105896Training Epoch: 7 | iteration: 115/262 | Loss: 0.6470890045166016Training Epoch: 7 | iteration: 116/262 | Loss: 0.6367426514625549Training Epoch: 7 | iteration: 117/262 | Loss: 0.6550426483154297Training Epoch: 7 | iteration: 118/262 | Loss: 0.5716586112976074Training Epoch: 7 | iteration: 119/262 | Loss: 0.6254197359085083Training Epoch: 7 | iteration: 120/262 | Loss: 0.687939465045929Training Epoch: 7 | iteration: 121/262 | Loss: 0.6438629031181335Training Epoch: 7 | iteration: 122/262 | Loss: 0.6672038435935974Training Epoch: 7 | iteration: 123/262 | Loss: 0.6465988755226135Training Epoch: 7 | iteration: 124/262 | Loss: 0.6794813275337219Training Epoch: 7 | iteration: 125/262 | Loss: 0.624602198600769Training Epoch: 7 | iteration: 126/262 | Loss: 0.707596480846405Training Epoch: 7 | iteration: 127/262 | Loss: 0.6392776966094971Training Epoch: 7 | iteration: 128/262 | Loss: 0.7586521506309509Training Epoch: 7 | iteration: 129/262 | Loss: 0.638297438621521Training Epoch: 7 | iteration: 130/262 | Loss: 0.6559442281723022Training Epoch: 7 | iteration: 131/262 | Loss: 0.6809331178665161Training Epoch: 7 | iteration: 132/262 | Loss: 0.6316981315612793Training Epoch: 7 | iteration: 133/262 | Loss: 0.6747234463691711Training Epoch: 7 | iteration: 134/262 | Loss: 0.7213137149810791Training Epoch: 7 | iteration: 135/262 | Loss: 0.5965327620506287Training Epoch: 7 | iteration: 136/262 | Loss: 0.5893537402153015Training Epoch: 7 | iteration: 137/262 | Loss: 0.5862183570861816Training Epoch: 7 | iteration: 138/262 | Loss: 0.6863231658935547Training Epoch: 7 | iteration: 139/262 | Loss: 0.662244439125061Training Epoch: 7 | iteration: 140/262 | Loss: 0.6921064853668213Training Epoch: 7 | iteration: 141/262 | Loss: 0.627886950969696Training Epoch: 7 | iteration: 142/262 | Loss: 0.6456688642501831Training Epoch: 7 | iteration: 143/262 | Loss: 0.6479102373123169Training Epoch: 7 | iteration: 144/262 | Loss: 0.6473249793052673Training Epoch: 7 | iteration: 145/262 | Loss: 0.5807206630706787Training Epoch: 7 | iteration: 146/262 | Loss: 0.6637595891952515Training Epoch: 7 | iteration: 147/262 | Loss: 0.6512688398361206Training Epoch: 7 | iteration: 148/262 | Loss: 0.6661853790283203Training Epoch: 7 | iteration: 149/262 | Loss: 0.583616316318512Training Epoch: 7 | iteration: 150/262 | Loss: 0.6270514726638794Training Epoch: 7 | iteration: 151/262 | Loss: 0.6467195153236389Training Epoch: 7 | iteration: 152/262 | Loss: 0.6091581583023071Training Epoch: 7 | iteration: 153/262 | Loss: 0.6417057514190674Training Epoch: 7 | iteration: 154/262 | Loss: 0.6442140340805054Training Epoch: 7 | iteration: 155/262 | Loss: 0.7275617718696594Training Epoch: 7 | iteration: 156/262 | Loss: 0.6200321316719055Training Epoch: 7 | iteration: 157/262 | Loss: 0.6094495058059692Training Epoch: 7 | iteration: 158/262 | Loss: 0.7031041979789734Training Epoch: 7 | iteration: 159/262 | Loss: 0.7018297910690308Training Epoch: 7 | iteration: 160/262 | Loss: 0.6373961567878723Training Epoch: 7 | iteration: 161/262 | Loss: 0.740713894367218Training Epoch: 7 | iteration: 162/262 | Loss: 0.6349643468856812Training Epoch: 7 | iteration: 163/262 | Loss: 0.6070652008056641Training Epoch: 7 | iteration: 164/262 | Loss: 0.6587347984313965Training Epoch: 7 | iteration: 165/262 | Loss: 0.6010997295379639Training Epoch: 7 | iteration: 166/262 | Loss: 0.6276165246963501Training Epoch: 7 | iteration: 167/262 | Loss: 0.6027114987373352Training Epoch: 7 | iteration: 168/262 | Loss: 0.6415411233901978Training Epoch: 7 | iteration: 169/262 | Loss: 0.7097729444503784Training Epoch: 7 | iteration: 170/262 | Loss: 0.5739855766296387Training Epoch: 7 | iteration: 171/262 | Loss: 0.7282429933547974Training Epoch: 7 | iteration: 172/262 | Loss: 0.5801370143890381Training Epoch: 7 | iteration: 173/262 | Loss: 0.6116087436676025Training Epoch: 7 | iteration: 174/262 | Loss: 0.575232982635498Training Epoch: 7 | iteration: 175/262 | Loss: 0.6847027540206909Training Epoch: 7 | iteration: 176/262 | Loss: 0.6265394687652588Training Epoch: 7 | iteration: 177/262 | Loss: 0.5463232398033142Training Epoch: 7 | iteration: 178/262 | Loss: 0.6187547445297241Training Epoch: 7 | iteration: 179/262 | Loss: 0.6529154181480408Training Epoch: 7 | iteration: 180/262 | Loss: 0.5891215801239014Training Epoch: 7 | iteration: 181/262 | Loss: 0.631269633769989Training Epoch: 7 | iteration: 182/262 | Loss: 0.5972011685371399Training Epoch: 7 | iteration: 183/262 | Loss: 0.6077528595924377Training Epoch: 7 | iteration: 184/262 | Loss: 0.622894287109375Training Epoch: 7 | iteration: 185/262 | Loss: 0.6416271924972534Training Epoch: 7 | iteration: 186/262 | Loss: 0.6459705233573914Training Epoch: 7 | iteration: 187/262 | Loss: 0.6434691548347473Training Epoch: 7 | iteration: 188/262 | Loss: 0.7059874534606934Training Epoch: 7 | iteration: 189/262 | Loss: 0.6116801500320435Training Epoch: 7 | iteration: 190/262 | Loss: 0.6795520782470703Training Epoch: 7 | iteration: 191/262 | Loss: 0.6626368761062622Training Epoch: 7 | iteration: 192/262 | Loss: 0.5989648103713989Training Epoch: 7 | iteration: 193/262 | Loss: 0.6932612657546997Training Epoch: 7 | iteration: 194/262 | Loss: 0.5931302309036255Training Epoch: 7 | iteration: 195/262 | Loss: 0.6295403838157654Training Epoch: 7 | iteration: 196/262 | Loss: 0.6622000932693481Training Epoch: 7 | iteration: 197/262 | Loss: 0.623977541923523Training Epoch: 7 | iteration: 198/262 | Loss: 0.6962682604789734Training Epoch: 7 | iteration: 199/262 | Loss: 0.6321414709091187Training Epoch: 7 | iteration: 200/262 | Loss: 0.6795272827148438Training Epoch: 7 | iteration: 201/262 | Loss: 0.6700589656829834Training Epoch: 7 | iteration: 202/262 | Loss: 0.6677367091178894Training Epoch: 7 | iteration: 203/262 | Loss: 0.6054501533508301Training Epoch: 7 | iteration: 204/262 | Loss: 0.6737872362136841Training Epoch: 7 | iteration: 205/262 | Loss: 0.6315974593162537Training Epoch: 7 | iteration: 206/262 | Loss: 0.6166083812713623Training Epoch: 7 | iteration: 207/262 | Loss: 0.6274847388267517Training Epoch: 7 | iteration: 208/262 | Loss: 0.6449315547943115Training Epoch: 7 | iteration: 209/262 | Loss: 0.6667176485061646Training Epoch: 7 | iteration: 210/262 | Loss: 0.6649453639984131Training Epoch: 7 | iteration: 211/262 | Loss: 0.657650887966156Training Epoch: 7 | iteration: 212/262 | Loss: 0.6198115348815918Training Epoch: 7 | iteration: 213/262 | Loss: 0.6055623292922974Training Epoch: 7 | iteration: 214/262 | Loss: 0.6222620606422424Training Epoch: 7 | iteration: 215/262 | Loss: 0.6160329580307007Training Epoch: 7 | iteration: 216/262 | Loss: 0.6747031807899475Training Epoch: 7 | iteration: 217/262 | Loss: 0.5884500741958618Training Epoch: 7 | iteration: 218/262 | Loss: 0.6827857494354248Training Epoch: 7 | iteration: 219/262 | Loss: 0.7305117249488831Training Epoch: 7 | iteration: 220/262 | Loss: 0.6153421401977539Training Epoch: 7 | iteration: 221/262 | Loss: 0.6175665855407715Training Epoch: 7 | iteration: 222/262 | Loss: 0.649936318397522Training Epoch: 7 | iteration: 223/262 | Loss: 0.660963773727417Training Epoch: 7 | iteration: 224/262 | Loss: 0.6721884608268738Training Epoch: 7 | iteration: 225/262 | Loss: 0.6332366466522217Training Epoch: 7 | iteration: 226/262 | Loss: 0.6279520988464355Training Epoch: 7 | iteration: 227/262 | Loss: 0.6709017157554626Training Epoch: 7 | iteration: 228/262 | Loss: 0.6758886575698853Training Epoch: 7 | iteration: 229/262 | Loss: 0.6244402527809143Training Epoch: 7 | iteration: 230/262 | Loss: 0.6408476829528809Training Epoch: 7 | iteration: 231/262 | Loss: 0.592238187789917Training Epoch: 7 | iteration: 232/262 | Loss: 0.6171783208847046Training Epoch: 7 | iteration: 233/262 | Loss: 0.6303200125694275Training Epoch: 7 | iteration: 234/262 | Loss: 0.5873098373413086Training Epoch: 7 | iteration: 235/262 | Loss: 0.7113453149795532Training Epoch: 7 | iteration: 236/262 | Loss: 0.6559852957725525Training Epoch: 7 | iteration: 237/262 | Loss: 0.6061254739761353Training Epoch: 7 | iteration: 238/262 | Loss: 0.6457241773605347Training Epoch: 7 | iteration: 239/262 | Loss: 0.6676396727561951Training Epoch: 7 | iteration: 240/262 | Loss: 0.7148600816726685Training Epoch: 7 | iteration: 241/262 | Loss: 0.6879605054855347Training Epoch: 7 | iteration: 242/262 | Loss: 0.6225061416625977Training Epoch: 7 | iteration: 243/262 | Loss: 0.5930653810501099Training Epoch: 7 | iteration: 244/262 | Loss: 0.6789686679840088Training Epoch: 7 | iteration: 245/262 | Loss: 0.6509294509887695Training Epoch: 7 | iteration: 246/262 | Loss: 0.7129532098770142Training Epoch: 7 | iteration: 247/262 | Loss: 0.697359561920166Training Epoch: 7 | iteration: 248/262 | Loss: 0.704359769821167Training Epoch: 7 | iteration: 249/262 | Loss: 0.6293467283248901Training Epoch: 7 | iteration: 250/262 | Loss: 0.5722230672836304Training Epoch: 7 | iteration: 251/262 | Loss: 0.5654357075691223Training Epoch: 7 | iteration: 252/262 | Loss: 0.6658722162246704Training Epoch: 7 | iteration: 253/262 | Loss: 0.6027841567993164Training Epoch: 7 | iteration: 254/262 | Loss: 0.5995473265647888Training Epoch: 7 | iteration: 255/262 | Loss: 0.6683632135391235Training Epoch: 7 | iteration: 256/262 | Loss: 0.6495445370674133Training Epoch: 7 | iteration: 257/262 | Loss: 0.6409780979156494Training Epoch: 7 | iteration: 258/262 | Loss: 0.6849508285522461Training Epoch: 7 | iteration: 259/262 | Loss: 0.6703596115112305Training Epoch: 7 | iteration: 260/262 | Loss: 0.6438874006271362Training Epoch: 7 | iteration: 261/262 | Loss: 0.49755099415779114Validating Epoch: 7 | iteration: 0/66 | Loss: 0.6648926734924316Validating Epoch: 7 | iteration: 1/66 | Loss: 0.7590416669845581Validating Epoch: 7 | iteration: 2/66 | Loss: 0.6907047033309937Validating Epoch: 7 | iteration: 3/66 | Loss: 0.6725482940673828Validating Epoch: 7 | iteration: 4/66 | Loss: 0.6647080183029175Validating Epoch: 7 | iteration: 5/66 | Loss: 0.7260191440582275Validating Epoch: 7 | iteration: 6/66 | Loss: 0.5956228375434875Validating Epoch: 7 | iteration: 7/66 | Loss: 0.7483362555503845Validating Epoch: 7 | iteration: 8/66 | Loss: 0.6867948770523071Validating Epoch: 7 | iteration: 9/66 | Loss: 0.7068771719932556Validating Epoch: 7 | iteration: 10/66 | Loss: 0.7676050662994385Validating Epoch: 7 | iteration: 11/66 | Loss: 0.6723219752311707Validating Epoch: 7 | iteration: 12/66 | Loss: 0.700287938117981Validating Epoch: 7 | iteration: 13/66 | Loss: 0.7334784269332886Validating Epoch: 7 | iteration: 14/66 | Loss: 0.6578402519226074Validating Epoch: 7 | iteration: 15/66 | Loss: 0.7451349496841431Validating Epoch: 7 | iteration: 16/66 | Loss: 0.7478148937225342Validating Epoch: 7 | iteration: 17/66 | Loss: 0.6851220726966858Validating Epoch: 7 | iteration: 18/66 | Loss: 0.6349939703941345Validating Epoch: 7 | iteration: 19/66 | Loss: 0.7459218502044678Validating Epoch: 7 | iteration: 20/66 | Loss: 0.7289817333221436Validating Epoch: 7 | iteration: 21/66 | Loss: 0.6381438970565796Validating Epoch: 7 | iteration: 22/66 | Loss: 0.806157112121582Validating Epoch: 7 | iteration: 23/66 | Loss: 0.7142958641052246Validating Epoch: 7 | iteration: 24/66 | Loss: 0.7061845660209656Validating Epoch: 7 | iteration: 25/66 | Loss: 0.7310876250267029Validating Epoch: 7 | iteration: 26/66 | Loss: 0.7466797232627869Validating Epoch: 7 | iteration: 27/66 | Loss: 0.7295353412628174Validating Epoch: 7 | iteration: 28/66 | Loss: 0.7036017775535583Validating Epoch: 7 | iteration: 29/66 | Loss: 0.7499475479125977Validating Epoch: 7 | iteration: 30/66 | Loss: 0.6217408776283264Validating Epoch: 7 | iteration: 31/66 | Loss: 0.7631887197494507Validating Epoch: 7 | iteration: 32/66 | Loss: 0.6267426013946533Validating Epoch: 7 | iteration: 33/66 | Loss: 0.6570203304290771Validating Epoch: 7 | iteration: 34/66 | Loss: 0.7176641225814819Validating Epoch: 7 | iteration: 35/66 | Loss: 0.6664429903030396Validating Epoch: 7 | iteration: 36/66 | Loss: 0.7248104214668274Validating Epoch: 7 | iteration: 37/66 | Loss: 0.6781122088432312Validating Epoch: 7 | iteration: 38/66 | Loss: 0.7397019863128662Validating Epoch: 7 | iteration: 39/66 | Loss: 0.7558476328849792Validating Epoch: 7 | iteration: 40/66 | Loss: 0.6936179995536804Validating Epoch: 7 | iteration: 41/66 | Loss: 0.6660782694816589Validating Epoch: 7 | iteration: 42/66 | Loss: 0.6716456413269043Validating Epoch: 7 | iteration: 43/66 | Loss: 0.7100409269332886Validating Epoch: 7 | iteration: 44/66 | Loss: 0.653795063495636Validating Epoch: 7 | iteration: 45/66 | Loss: 0.6432535648345947Validating Epoch: 7 | iteration: 46/66 | Loss: 0.6880079507827759Validating Epoch: 7 | iteration: 47/66 | Loss: 0.6715512871742249Validating Epoch: 7 | iteration: 48/66 | Loss: 0.6400924921035767Validating Epoch: 7 | iteration: 49/66 | Loss: 0.6645135283470154Validating Epoch: 7 | iteration: 50/66 | Loss: 0.7481356263160706Validating Epoch: 7 | iteration: 51/66 | Loss: 0.650215744972229Validating Epoch: 7 | iteration: 52/66 | Loss: 0.7013390064239502Validating Epoch: 7 | iteration: 53/66 | Loss: 0.6496193408966064Validating Epoch: 7 | iteration: 54/66 | Loss: 0.69560706615448Validating Epoch: 7 | iteration: 55/66 | Loss: 0.7015846967697144Validating Epoch: 7 | iteration: 56/66 | Loss: 0.7753077745437622Validating Epoch: 7 | iteration: 57/66 | Loss: 0.7741040587425232Validating Epoch: 7 | iteration: 58/66 | Loss: 0.7543327212333679Validating Epoch: 7 | iteration: 59/66 | Loss: 0.7111586332321167Validating Epoch: 7 | iteration: 60/66 | Loss: 0.7146191596984863Validating Epoch: 7 | iteration: 61/66 | Loss: 0.7121573090553284Validating Epoch: 7 | iteration: 62/66 | Loss: 0.7810643911361694Validating Epoch: 7 | iteration: 63/66 | Loss: 0.7375110387802124Validating Epoch: 7 | iteration: 64/66 | Loss: 0.7498815059661865Validating Epoch: 7 | iteration: 65/66 | Loss: 0.6831148266792297Temperature:  0.5
Sampling: iteration: 0/51{'Validity': 0.98828125, 'Novelty': 1.0, 'Uniqueness': 0.9970355731225297}
Training Epoch: 8 | iteration: 0/262 | Loss: 0.61210036277771Training Epoch: 8 | iteration: 1/262 | Loss: 0.6166424751281738Training Epoch: 8 | iteration: 2/262 | Loss: 0.5900934934616089Training Epoch: 8 | iteration: 3/262 | Loss: 0.5963243842124939Training Epoch: 8 | iteration: 4/262 | Loss: 0.5950387716293335Training Epoch: 8 | iteration: 5/262 | Loss: 0.5853112936019897Training Epoch: 8 | iteration: 6/262 | Loss: 0.6857194900512695Training Epoch: 8 | iteration: 7/262 | Loss: 0.5845766067504883Training Epoch: 8 | iteration: 8/262 | Loss: 0.6715461015701294Training Epoch: 8 | iteration: 9/262 | Loss: 0.6064295172691345Training Epoch: 8 | iteration: 10/262 | Loss: 0.6172491312026978Training Epoch: 8 | iteration: 11/262 | Loss: 0.6241945624351501Training Epoch: 8 | iteration: 12/262 | Loss: 0.6443268060684204Training Epoch: 8 | iteration: 13/262 | Loss: 0.6141420602798462Training Epoch: 8 | iteration: 14/262 | Loss: 0.6915475130081177Training Epoch: 8 | iteration: 15/262 | Loss: 0.559752881526947Training Epoch: 8 | iteration: 16/262 | Loss: 0.6525051593780518Training Epoch: 8 | iteration: 17/262 | Loss: 0.6006301641464233Training Epoch: 8 | iteration: 18/262 | Loss: 0.6031366586685181Training Epoch: 8 | iteration: 19/262 | Loss: 0.6491469144821167Training Epoch: 8 | iteration: 20/262 | Loss: 0.5541412830352783Training Epoch: 8 | iteration: 21/262 | Loss: 0.6061221361160278Training Epoch: 8 | iteration: 22/262 | Loss: 0.5827937126159668Training Epoch: 8 | iteration: 23/262 | Loss: 0.5706781148910522Training Epoch: 8 | iteration: 24/262 | Loss: 0.6716295480728149Training Epoch: 8 | iteration: 25/262 | Loss: 0.6205437779426575Training Epoch: 8 | iteration: 26/262 | Loss: 0.6580096483230591Training Epoch: 8 | iteration: 27/262 | Loss: 0.593566358089447Training Epoch: 8 | iteration: 28/262 | Loss: 0.5780736207962036Training Epoch: 8 | iteration: 29/262 | Loss: 0.6375352144241333Training Epoch: 8 | iteration: 30/262 | Loss: 0.5400491952896118Training Epoch: 8 | iteration: 31/262 | Loss: 0.5521314144134521Training Epoch: 8 | iteration: 32/262 | Loss: 0.5945337414741516Training Epoch: 8 | iteration: 33/262 | Loss: 0.6156334280967712Training Epoch: 8 | iteration: 34/262 | Loss: 0.711650013923645Training Epoch: 8 | iteration: 35/262 | Loss: 0.619072675704956Training Epoch: 8 | iteration: 36/262 | Loss: 0.619414210319519Training Epoch: 8 | iteration: 37/262 | Loss: 0.586268961429596Training Epoch: 8 | iteration: 38/262 | Loss: 0.6549020409584045Training Epoch: 8 | iteration: 39/262 | Loss: 0.5959200859069824Training Epoch: 8 | iteration: 40/262 | Loss: 0.674147367477417Training Epoch: 8 | iteration: 41/262 | Loss: 0.6479484438896179Training Epoch: 8 | iteration: 42/262 | Loss: 0.6489365696907043Training Epoch: 8 | iteration: 43/262 | Loss: 0.6571537256240845Training Epoch: 8 | iteration: 44/262 | Loss: 0.6005831956863403Training Epoch: 8 | iteration: 45/262 | Loss: 0.5669236779212952Training Epoch: 8 | iteration: 46/262 | Loss: 0.5516730546951294Training Epoch: 8 | iteration: 47/262 | Loss: 0.5822331309318542Training Epoch: 8 | iteration: 48/262 | Loss: 0.6317424774169922Training Epoch: 8 | iteration: 49/262 | Loss: 0.6659220457077026Training Epoch: 8 | iteration: 50/262 | Loss: 0.5267857909202576Training Epoch: 8 | iteration: 51/262 | Loss: 0.6304215788841248Training Epoch: 8 | iteration: 52/262 | Loss: 0.5674163103103638Training Epoch: 8 | iteration: 53/262 | Loss: 0.6390668153762817Training Epoch: 8 | iteration: 54/262 | Loss: 0.5957027077674866Training Epoch: 8 | iteration: 55/262 | Loss: 0.6154193878173828Training Epoch: 8 | iteration: 56/262 | Loss: 0.673009991645813Training Epoch: 8 | iteration: 57/262 | Loss: 0.5857604742050171Training Epoch: 8 | iteration: 58/262 | Loss: 0.6773890256881714Training Epoch: 8 | iteration: 59/262 | Loss: 0.6907287836074829Training Epoch: 8 | iteration: 60/262 | Loss: 0.6003037691116333Training Epoch: 8 | iteration: 61/262 | Loss: 0.5738571882247925Training Epoch: 8 | iteration: 62/262 | Loss: 0.5816627740859985Training Epoch: 8 | iteration: 63/262 | Loss: 0.5996465086936951Training Epoch: 8 | iteration: 64/262 | Loss: 0.6224708557128906Training Epoch: 8 | iteration: 65/262 | Loss: 0.6513612270355225Training Epoch: 8 | iteration: 66/262 | Loss: 0.6762545108795166Training Epoch: 8 | iteration: 67/262 | Loss: 0.6057840585708618Training Epoch: 8 | iteration: 68/262 | Loss: 0.6967791318893433Training Epoch: 8 | iteration: 69/262 | Loss: 0.6344467401504517Training Epoch: 8 | iteration: 70/262 | Loss: 0.5709266662597656Training Epoch: 8 | iteration: 71/262 | Loss: 0.6164437532424927Training Epoch: 8 | iteration: 72/262 | Loss: 0.63686203956604Training Epoch: 8 | iteration: 73/262 | Loss: 0.5739408135414124Training Epoch: 8 | iteration: 74/262 | Loss: 0.6108893156051636Training Epoch: 8 | iteration: 75/262 | Loss: 0.6596595048904419Training Epoch: 8 | iteration: 76/262 | Loss: 0.5972248315811157Training Epoch: 8 | iteration: 77/262 | Loss: 0.5638045072555542Training Epoch: 8 | iteration: 78/262 | Loss: 0.6910212635993958Training Epoch: 8 | iteration: 79/262 | Loss: 0.5620336532592773Training Epoch: 8 | iteration: 80/262 | Loss: 0.6550000905990601Training Epoch: 8 | iteration: 81/262 | Loss: 0.6494114398956299Training Epoch: 8 | iteration: 82/262 | Loss: 0.5641189813613892Training Epoch: 8 | iteration: 83/262 | Loss: 0.5952513217926025Training Epoch: 8 | iteration: 84/262 | Loss: 0.5813133716583252Training Epoch: 8 | iteration: 85/262 | Loss: 0.6463191509246826Training Epoch: 8 | iteration: 86/262 | Loss: 0.6642539501190186Training Epoch: 8 | iteration: 87/262 | Loss: 0.603615403175354Training Epoch: 8 | iteration: 88/262 | Loss: 0.6734548807144165Training Epoch: 8 | iteration: 89/262 | Loss: 0.6897349953651428Training Epoch: 8 | iteration: 90/262 | Loss: 0.6584746837615967Training Epoch: 8 | iteration: 91/262 | Loss: 0.7631264925003052Training Epoch: 8 | iteration: 92/262 | Loss: 0.5764771699905396Training Epoch: 8 | iteration: 93/262 | Loss: 0.6535034775733948Training Epoch: 8 | iteration: 94/262 | Loss: 0.6539921760559082Training Epoch: 8 | iteration: 95/262 | Loss: 0.6583119630813599Training Epoch: 8 | iteration: 96/262 | Loss: 0.5837011337280273Training Epoch: 8 | iteration: 97/262 | Loss: 0.6100989580154419Training Epoch: 8 | iteration: 98/262 | Loss: 0.5968835353851318Training Epoch: 8 | iteration: 99/262 | Loss: 0.6440435647964478Training Epoch: 8 | iteration: 100/262 | Loss: 0.6312843561172485Training Epoch: 8 | iteration: 101/262 | Loss: 0.6321978569030762Training Epoch: 8 | iteration: 102/262 | Loss: 0.5713854432106018Training Epoch: 8 | iteration: 103/262 | Loss: 0.6598466634750366Training Epoch: 8 | iteration: 104/262 | Loss: 0.6663264632225037Training Epoch: 8 | iteration: 105/262 | Loss: 0.5934238433837891Training Epoch: 8 | iteration: 106/262 | Loss: 0.6549732685089111Training Epoch: 8 | iteration: 107/262 | Loss: 0.6123781800270081Training Epoch: 8 | iteration: 108/262 | Loss: 0.6054565906524658Training Epoch: 8 | iteration: 109/262 | Loss: 0.5292790532112122Training Epoch: 8 | iteration: 110/262 | Loss: 0.6812084913253784Training Epoch: 8 | iteration: 111/262 | Loss: 0.6071906685829163Training Epoch: 8 | iteration: 112/262 | Loss: 0.6168463230133057Training Epoch: 8 | iteration: 113/262 | Loss: 0.6094776391983032Training Epoch: 8 | iteration: 114/262 | Loss: 0.6106547117233276Training Epoch: 8 | iteration: 115/262 | Loss: 0.6816344857215881Training Epoch: 8 | iteration: 116/262 | Loss: 0.6866167783737183Training Epoch: 8 | iteration: 117/262 | Loss: 0.6579579710960388Training Epoch: 8 | iteration: 118/262 | Loss: 0.6231005191802979Training Epoch: 8 | iteration: 119/262 | Loss: 0.5898589491844177Training Epoch: 8 | iteration: 120/262 | Loss: 0.6540338397026062Training Epoch: 8 | iteration: 121/262 | Loss: 0.6850390434265137Training Epoch: 8 | iteration: 122/262 | Loss: 0.5943657159805298Training Epoch: 8 | iteration: 123/262 | Loss: 0.6142865419387817Training Epoch: 8 | iteration: 124/262 | Loss: 0.5685453414916992Training Epoch: 8 | iteration: 125/262 | Loss: 0.6234661340713501Training Epoch: 8 | iteration: 126/262 | Loss: 0.7109938859939575Training Epoch: 8 | iteration: 127/262 | Loss: 0.5985617637634277Training Epoch: 8 | iteration: 128/262 | Loss: 0.6123603582382202Training Epoch: 8 | iteration: 129/262 | Loss: 0.5930865406990051Training Epoch: 8 | iteration: 130/262 | Loss: 0.6177486181259155Training Epoch: 8 | iteration: 131/262 | Loss: 0.6127101182937622Training Epoch: 8 | iteration: 132/262 | Loss: 0.6128371953964233Training Epoch: 8 | iteration: 133/262 | Loss: 0.6317404508590698Training Epoch: 8 | iteration: 134/262 | Loss: 0.5635169148445129Training Epoch: 8 | iteration: 135/262 | Loss: 0.6537493467330933Training Epoch: 8 | iteration: 136/262 | Loss: 0.5950487852096558Training Epoch: 8 | iteration: 137/262 | Loss: 0.6761554479598999Training Epoch: 8 | iteration: 138/262 | Loss: 0.5705797672271729Training Epoch: 8 | iteration: 139/262 | Loss: 0.6118396520614624Training Epoch: 8 | iteration: 140/262 | Loss: 0.5883569717407227Training Epoch: 8 | iteration: 141/262 | Loss: 0.636481523513794Training Epoch: 8 | iteration: 142/262 | Loss: 0.6813987493515015Training Epoch: 8 | iteration: 143/262 | Loss: 0.5709397792816162Training Epoch: 8 | iteration: 144/262 | Loss: 0.6425362825393677Training Epoch: 8 | iteration: 145/262 | Loss: 0.6508090496063232Training Epoch: 8 | iteration: 146/262 | Loss: 0.6141251921653748Training Epoch: 8 | iteration: 147/262 | Loss: 0.5627509355545044Training Epoch: 8 | iteration: 148/262 | Loss: 0.6342692375183105Training Epoch: 8 | iteration: 149/262 | Loss: 0.7224059700965881Training Epoch: 8 | iteration: 150/262 | Loss: 0.6398142576217651Training Epoch: 8 | iteration: 151/262 | Loss: 0.648298978805542Training Epoch: 8 | iteration: 152/262 | Loss: 0.6778898239135742Training Epoch: 8 | iteration: 153/262 | Loss: 0.6596727967262268Training Epoch: 8 | iteration: 154/262 | Loss: 0.5729243755340576Training Epoch: 8 | iteration: 155/262 | Loss: 0.6472184658050537Training Epoch: 8 | iteration: 156/262 | Loss: 0.6920437216758728Training Epoch: 8 | iteration: 157/262 | Loss: 0.6755778193473816Training Epoch: 8 | iteration: 158/262 | Loss: 0.6315004229545593Training Epoch: 8 | iteration: 159/262 | Loss: 0.6485929489135742Training Epoch: 8 | iteration: 160/262 | Loss: 0.6617817878723145Training Epoch: 8 | iteration: 161/262 | Loss: 0.6118661761283875Training Epoch: 8 | iteration: 162/262 | Loss: 0.6103878617286682Training Epoch: 8 | iteration: 163/262 | Loss: 0.6503686904907227Training Epoch: 8 | iteration: 164/262 | Loss: 0.6938531398773193Training Epoch: 8 | iteration: 165/262 | Loss: 0.6264004707336426Training Epoch: 8 | iteration: 166/262 | Loss: 0.6730955839157104Training Epoch: 8 | iteration: 167/262 | Loss: 0.6497048139572144Training Epoch: 8 | iteration: 168/262 | Loss: 0.7277959585189819Training Epoch: 8 | iteration: 169/262 | Loss: 0.6285666227340698Training Epoch: 8 | iteration: 170/262 | Loss: 0.65911465883255Training Epoch: 8 | iteration: 171/262 | Loss: 0.5630383491516113Training Epoch: 8 | iteration: 172/262 | Loss: 0.644952654838562Training Epoch: 8 | iteration: 173/262 | Loss: 0.650864839553833Training Epoch: 8 | iteration: 174/262 | Loss: 0.6514946818351746Training Epoch: 8 | iteration: 175/262 | Loss: 0.7146786451339722Training Epoch: 8 | iteration: 176/262 | Loss: 0.6431095600128174Training Epoch: 8 | iteration: 177/262 | Loss: 0.622132420539856Training Epoch: 8 | iteration: 178/262 | Loss: 0.6735897064208984Training Epoch: 8 | iteration: 179/262 | Loss: 0.6800897121429443Training Epoch: 8 | iteration: 180/262 | Loss: 0.6478766798973083Training Epoch: 8 | iteration: 181/262 | Loss: 0.6247797012329102Training Epoch: 8 | iteration: 182/262 | Loss: 0.5978032350540161Training Epoch: 8 | iteration: 183/262 | Loss: 0.598511815071106Training Epoch: 8 | iteration: 184/262 | Loss: 0.6518296003341675Training Epoch: 8 | iteration: 185/262 | Loss: 0.5712581872940063Training Epoch: 8 | iteration: 186/262 | Loss: 0.6808065176010132Training Epoch: 8 | iteration: 187/262 | Loss: 0.6609270572662354Training Epoch: 8 | iteration: 188/262 | Loss: 0.5696495771408081Training Epoch: 8 | iteration: 189/262 | Loss: 0.6123078465461731Training Epoch: 8 | iteration: 190/262 | Loss: 0.6049902439117432Training Epoch: 8 | iteration: 191/262 | Loss: 0.598264217376709Training Epoch: 8 | iteration: 192/262 | Loss: 0.5715755224227905Training Epoch: 8 | iteration: 193/262 | Loss: 0.6622074842453003Training Epoch: 8 | iteration: 194/262 | Loss: 0.5998939275741577Training Epoch: 8 | iteration: 195/262 | Loss: 0.6466840505599976Training Epoch: 8 | iteration: 196/262 | Loss: 0.5769482851028442Training Epoch: 8 | iteration: 197/262 | Loss: 0.5930585861206055Training Epoch: 8 | iteration: 198/262 | Loss: 0.6322116851806641Training Epoch: 8 | iteration: 199/262 | Loss: 0.589430570602417Training Epoch: 8 | iteration: 200/262 | Loss: 0.5919255018234253Training Epoch: 8 | iteration: 201/262 | Loss: 0.596576452255249Training Epoch: 8 | iteration: 202/262 | Loss: 0.5248461365699768Training Epoch: 8 | iteration: 203/262 | Loss: 0.633752703666687Training Epoch: 8 | iteration: 204/262 | Loss: 0.5554817914962769Training Epoch: 8 | iteration: 205/262 | Loss: 0.5878438949584961Training Epoch: 8 | iteration: 206/262 | Loss: 0.6672008037567139Training Epoch: 8 | iteration: 207/262 | Loss: 0.5571753978729248Training Epoch: 8 | iteration: 208/262 | Loss: 0.6181227564811707Training Epoch: 8 | iteration: 209/262 | Loss: 0.5888403654098511Training Epoch: 8 | iteration: 210/262 | Loss: 0.6029137372970581Training Epoch: 8 | iteration: 211/262 | Loss: 0.6327958703041077Training Epoch: 8 | iteration: 212/262 | Loss: 0.6792066097259521Training Epoch: 8 | iteration: 213/262 | Loss: 0.5576671957969666Training Epoch: 8 | iteration: 214/262 | Loss: 0.6095485091209412Training Epoch: 8 | iteration: 215/262 | Loss: 0.5752296447753906Training Epoch: 8 | iteration: 216/262 | Loss: 0.573696494102478Training Epoch: 8 | iteration: 217/262 | Loss: 0.706002414226532Training Epoch: 8 | iteration: 218/262 | Loss: 0.6194626092910767Training Epoch: 8 | iteration: 219/262 | Loss: 0.6475843191146851Training Epoch: 8 | iteration: 220/262 | Loss: 0.5771849751472473Training Epoch: 8 | iteration: 221/262 | Loss: 0.6352593302726746Training Epoch: 8 | iteration: 222/262 | Loss: 0.6007809042930603Training Epoch: 8 | iteration: 223/262 | Loss: 0.6300504207611084Training Epoch: 8 | iteration: 224/262 | Loss: 0.6048778891563416Training Epoch: 8 | iteration: 225/262 | Loss: 0.6484549641609192Training Epoch: 8 | iteration: 226/262 | Loss: 0.636070728302002Training Epoch: 8 | iteration: 227/262 | Loss: 0.6286748647689819Training Epoch: 8 | iteration: 228/262 | Loss: 0.6263692378997803Training Epoch: 8 | iteration: 229/262 | Loss: 0.6176016330718994Training Epoch: 8 | iteration: 230/262 | Loss: 0.5943431854248047Training Epoch: 8 | iteration: 231/262 | Loss: 0.6794384717941284Training Epoch: 8 | iteration: 232/262 | Loss: 0.6019596457481384Training Epoch: 8 | iteration: 233/262 | Loss: 0.684301495552063Training Epoch: 8 | iteration: 234/262 | Loss: 0.5510568022727966Training Epoch: 8 | iteration: 235/262 | Loss: 0.6245457530021667Training Epoch: 8 | iteration: 236/262 | Loss: 0.5902551412582397Training Epoch: 8 | iteration: 237/262 | Loss: 0.653116762638092Training Epoch: 8 | iteration: 238/262 | Loss: 0.6390848755836487Training Epoch: 8 | iteration: 239/262 | Loss: 0.6581448316574097Training Epoch: 8 | iteration: 240/262 | Loss: 0.6654131412506104Training Epoch: 8 | iteration: 241/262 | Loss: 0.6665433645248413Training Epoch: 8 | iteration: 242/262 | Loss: 0.6647810935974121Training Epoch: 8 | iteration: 243/262 | Loss: 0.6490076184272766Training Epoch: 8 | iteration: 244/262 | Loss: 0.6055048704147339Training Epoch: 8 | iteration: 245/262 | Loss: 0.6483609676361084Training Epoch: 8 | iteration: 246/262 | Loss: 0.6354838013648987Training Epoch: 8 | iteration: 247/262 | Loss: 0.5367668867111206Training Epoch: 8 | iteration: 248/262 | Loss: 0.6287515759468079Training Epoch: 8 | iteration: 249/262 | Loss: 0.6325616240501404Training Epoch: 8 | iteration: 250/262 | Loss: 0.6454746127128601Training Epoch: 8 | iteration: 251/262 | Loss: 0.6563769578933716Training Epoch: 8 | iteration: 252/262 | Loss: 0.6300229430198669Training Epoch: 8 | iteration: 253/262 | Loss: 0.5974711179733276Training Epoch: 8 | iteration: 254/262 | Loss: 0.6387536525726318Training Epoch: 8 | iteration: 255/262 | Loss: 0.6644093990325928Training Epoch: 8 | iteration: 256/262 | Loss: 0.5871296525001526Training Epoch: 8 | iteration: 257/262 | Loss: 0.6745576858520508Training Epoch: 8 | iteration: 258/262 | Loss: 0.6539573669433594Training Epoch: 8 | iteration: 259/262 | Loss: 0.6623872518539429Training Epoch: 8 | iteration: 260/262 | Loss: 0.6740388870239258Training Epoch: 8 | iteration: 261/262 | Loss: 0.6593245267868042Validating Epoch: 8 | iteration: 0/66 | Loss: 0.6567509174346924Validating Epoch: 8 | iteration: 1/66 | Loss: 0.679195761680603Validating Epoch: 8 | iteration: 2/66 | Loss: 0.7226407527923584Validating Epoch: 8 | iteration: 3/66 | Loss: 0.7160862684249878Validating Epoch: 8 | iteration: 4/66 | Loss: 0.6809225082397461Validating Epoch: 8 | iteration: 5/66 | Loss: 0.6338118314743042Validating Epoch: 8 | iteration: 6/66 | Loss: 0.7126545906066895Validating Epoch: 8 | iteration: 7/66 | Loss: 0.626160740852356Validating Epoch: 8 | iteration: 8/66 | Loss: 0.7299425005912781Validating Epoch: 8 | iteration: 9/66 | Loss: 0.5959219932556152Validating Epoch: 8 | iteration: 10/66 | Loss: 0.6820656061172485Validating Epoch: 8 | iteration: 11/66 | Loss: 0.5928422212600708Validating Epoch: 8 | iteration: 12/66 | Loss: 0.681015133857727Validating Epoch: 8 | iteration: 13/66 | Loss: 0.5951313972473145Validating Epoch: 8 | iteration: 14/66 | Loss: 0.7597589492797852Validating Epoch: 8 | iteration: 15/66 | Loss: 0.7864491939544678Validating Epoch: 8 | iteration: 16/66 | Loss: 0.6482641696929932Validating Epoch: 8 | iteration: 17/66 | Loss: 0.7478915452957153Validating Epoch: 8 | iteration: 18/66 | Loss: 0.6303960084915161Validating Epoch: 8 | iteration: 19/66 | Loss: 0.7732290029525757Validating Epoch: 8 | iteration: 20/66 | Loss: 0.6959789395332336Validating Epoch: 8 | iteration: 21/66 | Loss: 0.5936970710754395Validating Epoch: 8 | iteration: 22/66 | Loss: 0.7297723293304443Validating Epoch: 8 | iteration: 23/66 | Loss: 0.6726803779602051Validating Epoch: 8 | iteration: 24/66 | Loss: 0.6145810484886169Validating Epoch: 8 | iteration: 25/66 | Loss: 0.670853853225708Validating Epoch: 8 | iteration: 26/66 | Loss: 0.639854907989502Validating Epoch: 8 | iteration: 27/66 | Loss: 0.7891504764556885Validating Epoch: 8 | iteration: 28/66 | Loss: 0.741996169090271Validating Epoch: 8 | iteration: 29/66 | Loss: 0.7833235263824463Validating Epoch: 8 | iteration: 30/66 | Loss: 0.6845154762268066Validating Epoch: 8 | iteration: 31/66 | Loss: 0.6818554401397705Validating Epoch: 8 | iteration: 32/66 | Loss: 0.6725218296051025Validating Epoch: 8 | iteration: 33/66 | Loss: 0.6752673387527466Validating Epoch: 8 | iteration: 34/66 | Loss: 0.7614104151725769Validating Epoch: 8 | iteration: 35/66 | Loss: 0.6703062057495117Validating Epoch: 8 | iteration: 36/66 | Loss: 0.7034457325935364Validating Epoch: 8 | iteration: 37/66 | Loss: 0.6310617327690125Validating Epoch: 8 | iteration: 38/66 | Loss: 0.7067209482192993Validating Epoch: 8 | iteration: 39/66 | Loss: 0.6577926874160767Validating Epoch: 8 | iteration: 40/66 | Loss: 0.747754693031311Validating Epoch: 8 | iteration: 41/66 | Loss: 0.7285217642784119Validating Epoch: 8 | iteration: 42/66 | Loss: 0.6837674975395203Validating Epoch: 8 | iteration: 43/66 | Loss: 0.7163053750991821Validating Epoch: 8 | iteration: 44/66 | Loss: 0.7346456050872803Validating Epoch: 8 | iteration: 45/66 | Loss: 0.7652446031570435Validating Epoch: 8 | iteration: 46/66 | Loss: 0.799575924873352Validating Epoch: 8 | iteration: 47/66 | Loss: 0.6561359763145447Validating Epoch: 8 | iteration: 48/66 | Loss: 0.7152865529060364Validating Epoch: 8 | iteration: 49/66 | Loss: 0.6207075119018555Validating Epoch: 8 | iteration: 50/66 | Loss: 0.6507259607315063Validating Epoch: 8 | iteration: 51/66 | Loss: 0.729840874671936Validating Epoch: 8 | iteration: 52/66 | Loss: 0.6690363883972168Validating Epoch: 8 | iteration: 53/66 | Loss: 0.7329521179199219Validating Epoch: 8 | iteration: 54/66 | Loss: 0.6831445097923279Validating Epoch: 8 | iteration: 55/66 | Loss: 0.6739568710327148Validating Epoch: 8 | iteration: 56/66 | Loss: 0.6730742454528809Validating Epoch: 8 | iteration: 57/66 | Loss: 0.6853721737861633Validating Epoch: 8 | iteration: 58/66 | Loss: 0.7836978435516357Validating Epoch: 8 | iteration: 59/66 | Loss: 0.7046024203300476Validating Epoch: 8 | iteration: 60/66 | Loss: 0.6980752348899841Validating Epoch: 8 | iteration: 61/66 | Loss: 0.7826600074768066Validating Epoch: 8 | iteration: 62/66 | Loss: 0.7156658172607422Validating Epoch: 8 | iteration: 63/66 | Loss: 0.7382782101631165Validating Epoch: 8 | iteration: 64/66 | Loss: 0.7088184952735901Validating Epoch: 8 | iteration: 65/66 | Loss: 0.7180297374725342Temperature:  0.5
Sampling: iteration: 0/51{'Validity': 0.986328125, 'Novelty': 1.0, 'Uniqueness': 0.998019801980198}
Training Epoch: 9 | iteration: 0/262 | Loss: 0.601158082485199Training Epoch: 9 | iteration: 1/262 | Loss: 0.6058400869369507Training Epoch: 9 | iteration: 2/262 | Loss: 0.6144680380821228Training Epoch: 9 | iteration: 3/262 | Loss: 0.5704532861709595Training Epoch: 9 | iteration: 4/262 | Loss: 0.5800282955169678Training Epoch: 9 | iteration: 5/262 | Loss: 0.6137205958366394Training Epoch: 9 | iteration: 6/262 | Loss: 0.5718777179718018Training Epoch: 9 | iteration: 7/262 | Loss: 0.6278226971626282Training Epoch: 9 | iteration: 8/262 | Loss: 0.6057156324386597Training Epoch: 9 | iteration: 9/262 | Loss: 0.5606728792190552Training Epoch: 9 | iteration: 10/262 | Loss: 0.603653073310852Training Epoch: 9 | iteration: 11/262 | Loss: 0.5833386778831482Training Epoch: 9 | iteration: 12/262 | Loss: 0.6272931098937988Training Epoch: 9 | iteration: 13/262 | Loss: 0.6566996574401855Training Epoch: 9 | iteration: 14/262 | Loss: 0.5674991011619568Training Epoch: 9 | iteration: 15/262 | Loss: 0.6444426774978638Training Epoch: 9 | iteration: 16/262 | Loss: 0.6550724506378174Training Epoch: 9 | iteration: 17/262 | Loss: 0.6243926286697388Training Epoch: 9 | iteration: 18/262 | Loss: 0.5795915126800537Training Epoch: 9 | iteration: 19/262 | Loss: 0.5706864595413208Training Epoch: 9 | iteration: 20/262 | Loss: 0.6008539199829102Training Epoch: 9 | iteration: 21/262 | Loss: 0.6500911712646484Training Epoch: 9 | iteration: 22/262 | Loss: 0.6260280013084412Training Epoch: 9 | iteration: 23/262 | Loss: 0.5872180461883545Training Epoch: 9 | iteration: 24/262 | Loss: 0.6641651391983032Training Epoch: 9 | iteration: 25/262 | Loss: 0.6157708764076233Training Epoch: 9 | iteration: 26/262 | Loss: 0.5694350600242615Training Epoch: 9 | iteration: 27/262 | Loss: 0.5465051531791687Training Epoch: 9 | iteration: 28/262 | Loss: 0.5941921472549438Training Epoch: 9 | iteration: 29/262 | Loss: 0.6129174828529358Training Epoch: 9 | iteration: 30/262 | Loss: 0.5882537364959717Training Epoch: 9 | iteration: 31/262 | Loss: 0.6122845411300659Training Epoch: 9 | iteration: 32/262 | Loss: 0.5885666608810425Training Epoch: 9 | iteration: 33/262 | Loss: 0.5992527604103088Training Epoch: 9 | iteration: 34/262 | Loss: 0.5995339751243591Training Epoch: 9 | iteration: 35/262 | Loss: 0.6174902319908142Training Epoch: 9 | iteration: 36/262 | Loss: 0.6550332903862Training Epoch: 9 | iteration: 37/262 | Loss: 0.6122331619262695Training Epoch: 9 | iteration: 38/262 | Loss: 0.587716817855835Training Epoch: 9 | iteration: 39/262 | Loss: 0.61726975440979Training Epoch: 9 | iteration: 40/262 | Loss: 0.6417638063430786Training Epoch: 9 | iteration: 41/262 | Loss: 0.5822174549102783Training Epoch: 9 | iteration: 42/262 | Loss: 0.5976463556289673Training Epoch: 9 | iteration: 43/262 | Loss: 0.5197510719299316Training Epoch: 9 | iteration: 44/262 | Loss: 0.6413172483444214Training Epoch: 9 | iteration: 45/262 | Loss: 0.5943032503128052Training Epoch: 9 | iteration: 46/262 | Loss: 0.5970777273178101Training Epoch: 9 | iteration: 47/262 | Loss: 0.5929874777793884Training Epoch: 9 | iteration: 48/262 | Loss: 0.5934182405471802Training Epoch: 9 | iteration: 49/262 | Loss: 0.59876549243927Training Epoch: 9 | iteration: 50/262 | Loss: 0.5929018259048462Training Epoch: 9 | iteration: 51/262 | Loss: 0.615478515625Training Epoch: 9 | iteration: 52/262 | Loss: 0.6063905954360962Training Epoch: 9 | iteration: 53/262 | Loss: 0.6181774139404297Training Epoch: 9 | iteration: 54/262 | Loss: 0.5829923152923584Training Epoch: 9 | iteration: 55/262 | Loss: 0.5989922285079956Training Epoch: 9 | iteration: 56/262 | Loss: 0.5632618069648743Training Epoch: 9 | iteration: 57/262 | Loss: 0.5999560356140137Training Epoch: 9 | iteration: 58/262 | Loss: 0.615188479423523Training Epoch: 9 | iteration: 59/262 | Loss: 0.6035342216491699Training Epoch: 9 | iteration: 60/262 | Loss: 0.616585373878479Training Epoch: 9 | iteration: 61/262 | Loss: 0.6470122337341309Training Epoch: 9 | iteration: 62/262 | Loss: 0.5933166146278381Training Epoch: 9 | iteration: 63/262 | Loss: 0.6213735342025757Training Epoch: 9 | iteration: 64/262 | Loss: 0.590621829032898Training Epoch: 9 | iteration: 65/262 | Loss: 0.636200487613678Training Epoch: 9 | iteration: 66/262 | Loss: 0.6644769310951233Training Epoch: 9 | iteration: 67/262 | Loss: 0.607951283454895Training Epoch: 9 | iteration: 68/262 | Loss: 0.6051135063171387Training Epoch: 9 | iteration: 69/262 | Loss: 0.6020100116729736Training Epoch: 9 | iteration: 70/262 | Loss: 0.592470645904541Training Epoch: 9 | iteration: 71/262 | Loss: 0.6513880491256714Training Epoch: 9 | iteration: 72/262 | Loss: 0.6824745535850525Training Epoch: 9 | iteration: 73/262 | Loss: 0.6155083775520325Training Epoch: 9 | iteration: 74/262 | Loss: 0.5509504675865173Training Epoch: 9 | iteration: 75/262 | Loss: 0.6019967198371887Training Epoch: 9 | iteration: 76/262 | Loss: 0.5749971866607666Training Epoch: 9 | iteration: 77/262 | Loss: 0.5609831213951111Training Epoch: 9 | iteration: 78/262 | Loss: 0.5806316137313843Training Epoch: 9 | iteration: 79/262 | Loss: 0.6062214970588684Training Epoch: 9 | iteration: 80/262 | Loss: 0.6422128081321716Training Epoch: 9 | iteration: 81/262 | Loss: 0.6563385725021362Training Epoch: 9 | iteration: 82/262 | Loss: 0.552834153175354Training Epoch: 9 | iteration: 83/262 | Loss: 0.5734500885009766Training Epoch: 9 | iteration: 84/262 | Loss: 0.6318128108978271Training Epoch: 9 | iteration: 85/262 | Loss: 0.6148958206176758Training Epoch: 9 | iteration: 86/262 | Loss: 0.6171298623085022Training Epoch: 9 | iteration: 87/262 | Loss: 0.6403948664665222Training Epoch: 9 | iteration: 88/262 | Loss: 0.6185939908027649Training Epoch: 9 | iteration: 89/262 | Loss: 0.6897093653678894Training Epoch: 9 | iteration: 90/262 | Loss: 0.6214919090270996Training Epoch: 9 | iteration: 91/262 | Loss: 0.668685793876648Training Epoch: 9 | iteration: 92/262 | Loss: 0.6393481492996216Training Epoch: 9 | iteration: 93/262 | Loss: 0.5892729163169861Training Epoch: 9 | iteration: 94/262 | Loss: 0.6103413105010986Training Epoch: 9 | iteration: 95/262 | Loss: 0.560929000377655Training Epoch: 9 | iteration: 96/262 | Loss: 0.5573099851608276Training Epoch: 9 | iteration: 97/262 | Loss: 0.6217879056930542Training Epoch: 9 | iteration: 98/262 | Loss: 0.5273774862289429Training Epoch: 9 | iteration: 99/262 | Loss: 0.6100562214851379Training Epoch: 9 | iteration: 100/262 | Loss: 0.5778579711914062Training Epoch: 9 | iteration: 101/262 | Loss: 0.6144229173660278Training Epoch: 9 | iteration: 102/262 | Loss: 0.6569324135780334Training Epoch: 9 | iteration: 103/262 | Loss: 0.5745790004730225Training Epoch: 9 | iteration: 104/262 | Loss: 0.6301422119140625Training Epoch: 9 | iteration: 105/262 | Loss: 0.5783333778381348Training Epoch: 9 | iteration: 106/262 | Loss: 0.6664624810218811Training Epoch: 9 | iteration: 107/262 | Loss: 0.5858579874038696Training Epoch: 9 | iteration: 108/262 | Loss: 0.59828782081604Training Epoch: 9 | iteration: 109/262 | Loss: 0.6211292743682861Training Epoch: 9 | iteration: 110/262 | Loss: 0.6549563407897949Training Epoch: 9 | iteration: 111/262 | Loss: 0.6547322273254395Training Epoch: 9 | iteration: 112/262 | Loss: 0.657868504524231Training Epoch: 9 | iteration: 113/262 | Loss: 0.6350675821304321Training Epoch: 9 | iteration: 114/262 | Loss: 0.692602276802063Training Epoch: 9 | iteration: 115/262 | Loss: 0.6526695489883423Training Epoch: 9 | iteration: 116/262 | Loss: 0.6314095258712769Training Epoch: 9 | iteration: 117/262 | Loss: 0.578885555267334Training Epoch: 9 | iteration: 118/262 | Loss: 0.6249022483825684Training Epoch: 9 | iteration: 119/262 | Loss: 0.5494896173477173Training Epoch: 9 | iteration: 120/262 | Loss: 0.6176618337631226Training Epoch: 9 | iteration: 121/262 | Loss: 0.5976943969726562Training Epoch: 9 | iteration: 122/262 | Loss: 0.6051185131072998Training Epoch: 9 | iteration: 123/262 | Loss: 0.5919176340103149Training Epoch: 9 | iteration: 124/262 | Loss: 0.5505674481391907Training Epoch: 9 | iteration: 125/262 | Loss: 0.6286402940750122Training Epoch: 9 | iteration: 126/262 | Loss: 0.5799633860588074Training Epoch: 9 | iteration: 127/262 | Loss: 0.6855134963989258Training Epoch: 9 | iteration: 128/262 | Loss: 0.5822633504867554Training Epoch: 9 | iteration: 129/262 | Loss: 0.6100131273269653Training Epoch: 9 | iteration: 130/262 | Loss: 0.6134804487228394Training Epoch: 9 | iteration: 131/262 | Loss: 0.593407392501831Training Epoch: 9 | iteration: 132/262 | Loss: 0.629315197467804Training Epoch: 9 | iteration: 133/262 | Loss: 0.5599074959754944Training Epoch: 9 | iteration: 134/262 | Loss: 0.6541708707809448Training Epoch: 9 | iteration: 135/262 | Loss: 0.6515052318572998Training Epoch: 9 | iteration: 136/262 | Loss: 0.6679985523223877Training Epoch: 9 | iteration: 137/262 | Loss: 0.5892241597175598Training Epoch: 9 | iteration: 138/262 | Loss: 0.6168742179870605Training Epoch: 9 | iteration: 139/262 | Loss: 0.6164084076881409Training Epoch: 9 | iteration: 140/262 | Loss: 0.6355483531951904Training Epoch: 9 | iteration: 141/262 | Loss: 0.6152579188346863Training Epoch: 9 | iteration: 142/262 | Loss: 0.6243289709091187Training Epoch: 9 | iteration: 143/262 | Loss: 0.6012164950370789Training Epoch: 9 | iteration: 144/262 | Loss: 0.6655991077423096Training Epoch: 9 | iteration: 145/262 | Loss: 0.5685396194458008Training Epoch: 9 | iteration: 146/262 | Loss: 0.6069376468658447Training Epoch: 9 | iteration: 147/262 | Loss: 0.636990487575531Training Epoch: 9 | iteration: 148/262 | Loss: 0.5840936899185181Training Epoch: 9 | iteration: 149/262 | Loss: 0.6148417592048645Training Epoch: 9 | iteration: 150/262 | Loss: 0.5562180280685425Training Epoch: 9 | iteration: 151/262 | Loss: 0.6858015656471252Training Epoch: 9 | iteration: 152/262 | Loss: 0.5439725518226624Training Epoch: 9 | iteration: 153/262 | Loss: 0.6210023164749146Training Epoch: 9 | iteration: 154/262 | Loss: 0.6061244606971741Training Epoch: 9 | iteration: 155/262 | Loss: 0.5748410224914551Training Epoch: 9 | iteration: 156/262 | Loss: 0.5836362242698669Training Epoch: 9 | iteration: 157/262 | Loss: 0.6221010684967041Training Epoch: 9 | iteration: 158/262 | Loss: 0.5815713405609131Training Epoch: 9 | iteration: 159/262 | Loss: 0.5574721097946167Training Epoch: 9 | iteration: 160/262 | Loss: 0.5642282962799072Training Epoch: 9 | iteration: 161/262 | Loss: 0.6377450227737427Training Epoch: 9 | iteration: 162/262 | Loss: 0.6868416666984558Training Epoch: 9 | iteration: 163/262 | Loss: 0.5691935420036316Training Epoch: 9 | iteration: 164/262 | Loss: 0.571406364440918Training Epoch: 9 | iteration: 165/262 | Loss: 0.6928030252456665Training Epoch: 9 | iteration: 166/262 | Loss: 0.6557904481887817Training Epoch: 9 | iteration: 167/262 | Loss: 0.6749321222305298Training Epoch: 9 | iteration: 168/262 | Loss: 0.5623416900634766Training Epoch: 9 | iteration: 169/262 | Loss: 0.5987602472305298Training Epoch: 9 | iteration: 170/262 | Loss: 0.5815236568450928Training Epoch: 9 | iteration: 171/262 | Loss: 0.6748651266098022Training Epoch: 9 | iteration: 172/262 | Loss: 0.6455342769622803Training Epoch: 9 | iteration: 173/262 | Loss: 0.6084808111190796Training Epoch: 9 | iteration: 174/262 | Loss: 0.6716066002845764Training Epoch: 9 | iteration: 175/262 | Loss: 0.6076799631118774Training Epoch: 9 | iteration: 176/262 | Loss: 0.6097704172134399Training Epoch: 9 | iteration: 177/262 | Loss: 0.6109686493873596Training Epoch: 9 | iteration: 178/262 | Loss: 0.6505968570709229Training Epoch: 9 | iteration: 179/262 | Loss: 0.6204647421836853Training Epoch: 9 | iteration: 180/262 | Loss: 0.5946029424667358Training Epoch: 9 | iteration: 181/262 | Loss: 0.7023794054985046Training Epoch: 9 | iteration: 182/262 | Loss: 0.6569962501525879Training Epoch: 9 | iteration: 183/262 | Loss: 0.6397544145584106Training Epoch: 9 | iteration: 184/262 | Loss: 0.6723747253417969Training Epoch: 9 | iteration: 185/262 | Loss: 0.6204381585121155Training Epoch: 9 | iteration: 186/262 | Loss: 0.6016016006469727Training Epoch: 9 | iteration: 187/262 | Loss: 0.5793477296829224Training Epoch: 9 | iteration: 188/262 | Loss: 0.5869002938270569Training Epoch: 9 | iteration: 189/262 | Loss: 0.550333559513092Training Epoch: 9 | iteration: 190/262 | Loss: 0.6086626052856445Training Epoch: 9 | iteration: 191/262 | Loss: 0.6185873746871948Training Epoch: 9 | iteration: 192/262 | Loss: 0.6133216619491577Training Epoch: 9 | iteration: 193/262 | Loss: 0.5784155130386353Training Epoch: 9 | iteration: 194/262 | Loss: 0.6090335845947266Training Epoch: 9 | iteration: 195/262 | Loss: 0.6441212296485901Training Epoch: 9 | iteration: 196/262 | Loss: 0.6266141533851624Training Epoch: 9 | iteration: 197/262 | Loss: 0.5867526531219482Training Epoch: 9 | iteration: 198/262 | Loss: 0.6012466549873352Training Epoch: 9 | iteration: 199/262 | Loss: 0.5937647819519043Training Epoch: 9 | iteration: 200/262 | Loss: 0.6643701791763306Training Epoch: 9 | iteration: 201/262 | Loss: 0.6064901351928711Training Epoch: 9 | iteration: 202/262 | Loss: 0.6476331949234009Training Epoch: 9 | iteration: 203/262 | Loss: 0.5776745080947876Training Epoch: 9 | iteration: 204/262 | Loss: 0.7031397819519043Training Epoch: 9 | iteration: 205/262 | Loss: 0.5572260022163391Training Epoch: 9 | iteration: 206/262 | Loss: 0.6029454469680786Training Epoch: 9 | iteration: 207/262 | Loss: 0.5580539703369141Training Epoch: 9 | iteration: 208/262 | Loss: 0.6751888990402222Training Epoch: 9 | iteration: 209/262 | Loss: 0.663347601890564Training Epoch: 9 | iteration: 210/262 | Loss: 0.5918718576431274Training Epoch: 9 | iteration: 211/262 | Loss: 0.6718990802764893Training Epoch: 9 | iteration: 212/262 | Loss: 0.581421971321106Training Epoch: 9 | iteration: 213/262 | Loss: 0.5880147218704224Training Epoch: 9 | iteration: 214/262 | Loss: 0.5844804644584656Training Epoch: 9 | iteration: 215/262 | Loss: 0.6439708471298218Training Epoch: 9 | iteration: 216/262 | Loss: 0.6012158989906311Training Epoch: 9 | iteration: 217/262 | Loss: 0.6386613249778748Training Epoch: 9 | iteration: 218/262 | Loss: 0.6642022132873535Training Epoch: 9 | iteration: 219/262 | Loss: 0.5337696075439453Training Epoch: 9 | iteration: 220/262 | Loss: 0.5593286752700806Training Epoch: 9 | iteration: 221/262 | Loss: 0.6445678472518921Training Epoch: 9 | iteration: 222/262 | Loss: 0.6085153818130493Training Epoch: 9 | iteration: 223/262 | Loss: 0.6171611547470093Training Epoch: 9 | iteration: 224/262 | Loss: 0.5721979737281799Training Epoch: 9 | iteration: 225/262 | Loss: 0.5891575813293457Training Epoch: 9 | iteration: 226/262 | Loss: 0.5794548988342285Training Epoch: 9 | iteration: 227/262 | Loss: 0.5922120809555054Training Epoch: 9 | iteration: 228/262 | Loss: 0.6505694389343262Training Epoch: 9 | iteration: 229/262 | Loss: 0.5878807902336121Training Epoch: 9 | iteration: 230/262 | Loss: 0.6377373933792114Training Epoch: 9 | iteration: 231/262 | Loss: 0.6294936537742615Training Epoch: 9 | iteration: 232/262 | Loss: 0.6237800121307373Training Epoch: 9 | iteration: 233/262 | Loss: 0.6243696212768555Training Epoch: 9 | iteration: 234/262 | Loss: 0.6473810076713562Training Epoch: 9 | iteration: 235/262 | Loss: 0.6835933327674866Training Epoch: 9 | iteration: 236/262 | Loss: 0.5840420722961426Training Epoch: 9 | iteration: 237/262 | Loss: 0.640798807144165Training Epoch: 9 | iteration: 238/262 | Loss: 0.5869424939155579Training Epoch: 9 | iteration: 239/262 | Loss: 0.6101709604263306Training Epoch: 9 | iteration: 240/262 | Loss: 0.624066948890686Training Epoch: 9 | iteration: 241/262 | Loss: 0.6680337190628052Training Epoch: 9 | iteration: 242/262 | Loss: 0.6329457759857178Training Epoch: 9 | iteration: 243/262 | Loss: 0.5833537578582764Training Epoch: 9 | iteration: 244/262 | Loss: 0.6293726563453674Training Epoch: 9 | iteration: 245/262 | Loss: 0.580089807510376Training Epoch: 9 | iteration: 246/262 | Loss: 0.6054142117500305Training Epoch: 9 | iteration: 247/262 | Loss: 0.5944871306419373Training Epoch: 9 | iteration: 248/262 | Loss: 0.6215667724609375Training Epoch: 9 | iteration: 249/262 | Loss: 0.5907418727874756Training Epoch: 9 | iteration: 250/262 | Loss: 0.5907293558120728Training Epoch: 9 | iteration: 251/262 | Loss: 0.6109017133712769Training Epoch: 9 | iteration: 252/262 | Loss: 0.588278591632843Training Epoch: 9 | iteration: 253/262 | Loss: 0.553425133228302Training Epoch: 9 | iteration: 254/262 | Loss: 0.6145603656768799Training Epoch: 9 | iteration: 255/262 | Loss: 0.5958828330039978Training Epoch: 9 | iteration: 256/262 | Loss: 0.6223496198654175Training Epoch: 9 | iteration: 257/262 | Loss: 0.5914619565010071Training Epoch: 9 | iteration: 258/262 | Loss: 0.6538432836532593Training Epoch: 9 | iteration: 259/262 | Loss: 0.5958026051521301Training Epoch: 9 | iteration: 260/262 | Loss: 0.628675639629364Training Epoch: 9 | iteration: 261/262 | Loss: 0.7367557883262634Validating Epoch: 9 | iteration: 0/66 | Loss: 0.7746343016624451Validating Epoch: 9 | iteration: 1/66 | Loss: 0.6601583361625671Validating Epoch: 9 | iteration: 2/66 | Loss: 0.7653477191925049Validating Epoch: 9 | iteration: 3/66 | Loss: 0.7609201073646545Validating Epoch: 9 | iteration: 4/66 | Loss: 0.740837812423706Validating Epoch: 9 | iteration: 5/66 | Loss: 0.7438614964485168Validating Epoch: 9 | iteration: 6/66 | Loss: 0.7878404855728149Validating Epoch: 9 | iteration: 7/66 | Loss: 0.705204427242279Validating Epoch: 9 | iteration: 8/66 | Loss: 0.702531635761261Validating Epoch: 9 | iteration: 9/66 | Loss: 0.793727695941925Validating Epoch: 9 | iteration: 10/66 | Loss: 0.7117834091186523Validating Epoch: 9 | iteration: 11/66 | Loss: 0.695440411567688Validating Epoch: 9 | iteration: 12/66 | Loss: 0.6402895450592041Validating Epoch: 9 | iteration: 13/66 | Loss: 0.6815230846405029Validating Epoch: 9 | iteration: 14/66 | Loss: 0.7220485210418701Validating Epoch: 9 | iteration: 15/66 | Loss: 0.6262917518615723Validating Epoch: 9 | iteration: 16/66 | Loss: 0.6571061611175537Validating Epoch: 9 | iteration: 17/66 | Loss: 0.7244304418563843Validating Epoch: 9 | iteration: 18/66 | Loss: 0.6558716893196106Validating Epoch: 9 | iteration: 19/66 | Loss: 0.7321127653121948Validating Epoch: 9 | iteration: 20/66 | Loss: 0.6635738611221313Validating Epoch: 9 | iteration: 21/66 | Loss: 0.7103621959686279Validating Epoch: 9 | iteration: 22/66 | Loss: 0.6786667704582214Validating Epoch: 9 | iteration: 23/66 | Loss: 0.6735270619392395Validating Epoch: 9 | iteration: 24/66 | Loss: 0.6037623882293701Validating Epoch: 9 | iteration: 25/66 | Loss: 0.7032214999198914Validating Epoch: 9 | iteration: 26/66 | Loss: 0.8361437320709229Validating Epoch: 9 | iteration: 27/66 | Loss: 0.6629090309143066Validating Epoch: 9 | iteration: 28/66 | Loss: 0.6727150082588196Validating Epoch: 9 | iteration: 29/66 | Loss: 0.7092760801315308Validating Epoch: 9 | iteration: 30/66 | Loss: 0.6372160911560059Validating Epoch: 9 | iteration: 31/66 | Loss: 0.6659119129180908Validating Epoch: 9 | iteration: 32/66 | Loss: 0.7821685671806335Validating Epoch: 9 | iteration: 33/66 | Loss: 0.7060505747795105Validating Epoch: 9 | iteration: 34/66 | Loss: 0.6738141179084778Validating Epoch: 9 | iteration: 35/66 | Loss: 0.7110735177993774Validating Epoch: 9 | iteration: 36/66 | Loss: 0.7170134782791138Validating Epoch: 9 | iteration: 37/66 | Loss: 0.7539081573486328Validating Epoch: 9 | iteration: 38/66 | Loss: 0.7371180653572083Validating Epoch: 9 | iteration: 39/66 | Loss: 0.754362940788269Validating Epoch: 9 | iteration: 40/66 | Loss: 0.6307627558708191Validating Epoch: 9 | iteration: 41/66 | Loss: 0.6555598974227905Validating Epoch: 9 | iteration: 42/66 | Loss: 0.623103678226471Validating Epoch: 9 | iteration: 43/66 | Loss: 0.6738011837005615Validating Epoch: 9 | iteration: 44/66 | Loss: 0.6463623046875Validating Epoch: 9 | iteration: 45/66 | Loss: 0.7152167558670044Validating Epoch: 9 | iteration: 46/66 | Loss: 0.6865419149398804Validating Epoch: 9 | iteration: 47/66 | Loss: 0.5989111661911011Validating Epoch: 9 | iteration: 48/66 | Loss: 0.7787960767745972Validating Epoch: 9 | iteration: 49/66 | Loss: 0.6806548237800598Validating Epoch: 9 | iteration: 50/66 | Loss: 0.7090626955032349Validating Epoch: 9 | iteration: 51/66 | Loss: 0.6363376379013062Validating Epoch: 9 | iteration: 52/66 | Loss: 0.7996213436126709Validating Epoch: 9 | iteration: 53/66 | Loss: 0.7249816060066223Validating Epoch: 9 | iteration: 54/66 | Loss: 0.622958779335022Validating Epoch: 9 | iteration: 55/66 | Loss: 0.7709130644798279Validating Epoch: 9 | iteration: 56/66 | Loss: 0.6955063939094543Validating Epoch: 9 | iteration: 57/66 | Loss: 0.6555914282798767Validating Epoch: 9 | iteration: 58/66 | Loss: 0.6473473310470581Validating Epoch: 9 | iteration: 59/66 | Loss: 0.7110288739204407Validating Epoch: 9 | iteration: 60/66 | Loss: 0.7104437947273254Validating Epoch: 9 | iteration: 61/66 | Loss: 0.660180389881134Validating Epoch: 9 | iteration: 62/66 | Loss: 0.6857704520225525Validating Epoch: 9 | iteration: 63/66 | Loss: 0.6980072855949402Validating Epoch: 9 | iteration: 64/66 | Loss: 0.6445280909538269Validating Epoch: 9 | iteration: 65/66 | Loss: 0.7185261249542236No of GPUs available 4

==================================================
Generating molecules with target properties...
==================================================

Generating for -10_30...
Generating for -10_80...
Generating for -9_30...
Generating for -9_80...
Generating for -8_30...
Generating for -8_80...
Generating for -7_30...
Generating for -7_80...
Generating for -6_30...
Generating for -6_80...

==================================================
Generated molecules saved to: ../checkpoints/DPO_Finetuning_BETA_0.11_epochs_10_LCK_DOCKSTRING_FAST_ACTUAL_affinity_tpsas_DPO_pref_affinity/generated_molecules.pkl
To analyze and plot results, run:
python analyze_generated_molecules.py --checkpoint_dir ../checkpoints/DPO_Finetuning_BETA_0.11_epochs_10_LCK_DOCKSTRING_FAST_ACTUAL_affinity_tpsas_DPO_pref_affinity --properties affinity tpsas
==================================================

[1;34mwandb[0m: 
[1;34mwandb[0m:  View run [33mDPO_Finetuning_BETA_0.11_epochs_10_LCK_DOCKSTRING_FAST_ACTUAL_affinity_tpsas_DPO_pref_affinity[0m at: [34mhttps://wandb.ai/bhuvan-kapur1-iiith/molgpt2.0%20FINAL/runs/nm54ea5d[0m
[1;34mwandb[0m: Find logs at: [1;35mwandb/run-20260201_194215-nm54ea5d/logs[0m
