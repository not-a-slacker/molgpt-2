Starting ...
cuda
Properties to use:  ['affinity', 'sas']
Building Vocab
{'batch_size': 128, 'd_model': 512, 'n_heads': 8, 'n_layers': 6, 'hidden_units': 512, 'lr': 1e-05, 'epochs': 10, 'properties': ['affinity', 'sas'], 'run_name': 'LCK_DOCKSTRING_FAST_ACTUAL_affinity_sas'}
length of preference data : 41795
Sample preference_data[0]: ['O1C(C(=O)N2CCN(CC2)C3=CC=C(N(=O)=O)C=C3)=C(C=4C1=CC=CC4)C', array([0.47959185, 0.53255087, 0.54797947, 0.15855041, 0.16055913]), [np.str_('O1C(=NC2=C(C1=O)C=CC=C2)C3=C(C(=O)NCCC4=CC=CC=C4)C=CC(OC)=C3'), np.float64(0.9873459430403937), array([0.52040816, 0.54211667, 0.55725834, 0.15815913, 0.16377715])], [np.str_('O1C(=NC2=C(C1=O)C=CC=C2)C3=CC(OC(=O)C=4C=CC=CC4)=C(OC)C=C3'), np.float64(0.9574398041990725), array([0.43877551, 0.55263869, 0.41284505, 0.14420153, 0.15814562])]]
LCK_DOCKSTRING_FAST_ACTUAL_affinity_sas
len(target_smiles): 41800
len(data): 41800
LCK_DOCKSTRING_FAST_ACTUAL_affinity_sas
dataset built
cuda
True
2.7.1+cu118
No of GPUs available 4
No of GPUs available 4
{'batch_size': 128, 'd_model': 512, 'n_heads': 8, 'n_layers': 6, 'hidden_units': 512, 'lr': 1e-05, 'epochs': 10, 'properties': ['affinity', 'sas'], 'ipo': False, 'run_name': 'DPO_Finetuning_BETA_0.11_epochs_10_LCK_DOCKSTRING_FAST_ACTUAL_affinity_sas', 'beta': 0.11}
Temperature:  0.5
Sampling: iteration: 0/51{'Validity': 0.9970703125, 'Novelty': 1.0, 'Uniqueness': 0.9941234084231146}
Training Epoch: 0 | iteration: 0/262 | Loss: 0.9786747097969055Training Epoch: 0 | iteration: 1/262 | Loss: 0.9563084244728088Training Epoch: 0 | iteration: 2/262 | Loss: 0.9274454116821289Training Epoch: 0 | iteration: 3/262 | Loss: 0.9385970830917358Training Epoch: 0 | iteration: 4/262 | Loss: 0.9304494857788086Training Epoch: 0 | iteration: 5/262 | Loss: 0.9079973697662354Training Epoch: 0 | iteration: 6/262 | Loss: 0.899752140045166Training Epoch: 0 | iteration: 7/262 | Loss: 1.0285124778747559Training Epoch: 0 | iteration: 8/262 | Loss: 0.9723005890846252Training Epoch: 0 | iteration: 9/262 | Loss: 0.8565446138381958Training Epoch: 0 | iteration: 10/262 | Loss: 0.9646827578544617Training Epoch: 0 | iteration: 11/262 | Loss: 0.9552127122879028Training Epoch: 0 | iteration: 12/262 | Loss: 0.9248489141464233Training Epoch: 0 | iteration: 13/262 | Loss: 0.8823729753494263Training Epoch: 0 | iteration: 14/262 | Loss: 0.9464054107666016Training Epoch: 0 | iteration: 15/262 | Loss: 0.9159576892852783Training Epoch: 0 | iteration: 16/262 | Loss: 0.9039188623428345Training Epoch: 0 | iteration: 17/262 | Loss: 0.8406450152397156Training Epoch: 0 | iteration: 18/262 | Loss: 0.8919342160224915Training Epoch: 0 | iteration: 19/262 | Loss: 0.9213703870773315Training Epoch: 0 | iteration: 20/262 | Loss: 0.8889422416687012Training Epoch: 0 | iteration: 21/262 | Loss: 0.9151554107666016Training Epoch: 0 | iteration: 22/262 | Loss: 0.914146900177002Training Epoch: 0 | iteration: 23/262 | Loss: 0.8938076496124268Training Epoch: 0 | iteration: 24/262 | Loss: 0.8569836616516113Training Epoch: 0 | iteration: 25/262 | Loss: 0.9294520616531372Training Epoch: 0 | iteration: 26/262 | Loss: 0.8957844376564026Training Epoch: 0 | iteration: 27/262 | Loss: 0.8104981184005737Training Epoch: 0 | iteration: 28/262 | Loss: 0.8821648955345154Training Epoch: 0 | iteration: 29/262 | Loss: 0.8708441257476807Training Epoch: 0 | iteration: 30/262 | Loss: 0.9162256717681885Training Epoch: 0 | iteration: 31/262 | Loss: 0.848386287689209Training Epoch: 0 | iteration: 32/262 | Loss: 0.8567014932632446Training Epoch: 0 | iteration: 33/262 | Loss: 0.7669097185134888Training Epoch: 0 | iteration: 34/262 | Loss: 0.8655593395233154Training Epoch: 0 | iteration: 35/262 | Loss: 0.8948677778244019Training Epoch: 0 | iteration: 36/262 | Loss: 0.7728863954544067Training Epoch: 0 | iteration: 37/262 | Loss: 0.9588488340377808Training Epoch: 0 | iteration: 38/262 | Loss: 0.9554843902587891Training Epoch: 0 | iteration: 39/262 | Loss: 0.8884325623512268Training Epoch: 0 | iteration: 40/262 | Loss: 0.8444047570228577Training Epoch: 0 | iteration: 41/262 | Loss: 0.8464637398719788Training Epoch: 0 | iteration: 42/262 | Loss: 0.8238281607627869Training Epoch: 0 | iteration: 43/262 | Loss: 0.8332761526107788Training Epoch: 0 | iteration: 44/262 | Loss: 0.831034243106842Training Epoch: 0 | iteration: 45/262 | Loss: 0.8272229433059692Training Epoch: 0 | iteration: 46/262 | Loss: 0.9048608541488647Training Epoch: 0 | iteration: 47/262 | Loss: 0.8568277955055237Training Epoch: 0 | iteration: 48/262 | Loss: 0.8454750776290894Training Epoch: 0 | iteration: 49/262 | Loss: 0.8323973417282104Training Epoch: 0 | iteration: 50/262 | Loss: 0.903680145740509Training Epoch: 0 | iteration: 51/262 | Loss: 0.7790011167526245Training Epoch: 0 | iteration: 52/262 | Loss: 0.925065279006958Training Epoch: 0 | iteration: 53/262 | Loss: 0.8655558824539185Training Epoch: 0 | iteration: 54/262 | Loss: 0.9053860902786255Training Epoch: 0 | iteration: 55/262 | Loss: 0.8148282766342163Training Epoch: 0 | iteration: 56/262 | Loss: 0.8869657516479492Training Epoch: 0 | iteration: 57/262 | Loss: 0.898784339427948Training Epoch: 0 | iteration: 58/262 | Loss: 0.8229358196258545Training Epoch: 0 | iteration: 59/262 | Loss: 0.9450865983963013Training Epoch: 0 | iteration: 60/262 | Loss: 0.9095872640609741Training Epoch: 0 | iteration: 61/262 | Loss: 0.906682550907135Training Epoch: 0 | iteration: 62/262 | Loss: 0.8890001773834229Training Epoch: 0 | iteration: 63/262 | Loss: 0.8446038961410522Training Epoch: 0 | iteration: 64/262 | Loss: 0.8039771318435669Training Epoch: 0 | iteration: 65/262 | Loss: 0.8548595905303955Training Epoch: 0 | iteration: 66/262 | Loss: 0.9298689365386963Training Epoch: 0 | iteration: 67/262 | Loss: 0.8769443035125732Training Epoch: 0 | iteration: 68/262 | Loss: 0.7891057133674622Training Epoch: 0 | iteration: 69/262 | Loss: 0.9131109118461609Training Epoch: 0 | iteration: 70/262 | Loss: 0.8754564523696899Training Epoch: 0 | iteration: 71/262 | Loss: 0.8658868074417114Training Epoch: 0 | iteration: 72/262 | Loss: 0.9296804666519165Training Epoch: 0 | iteration: 73/262 | Loss: 0.9452694058418274Training Epoch: 0 | iteration: 74/262 | Loss: 0.9391437768936157Training Epoch: 0 | iteration: 75/262 | Loss: 0.7685834169387817Training Epoch: 0 | iteration: 76/262 | Loss: 0.8140919804573059Training Epoch: 0 | iteration: 77/262 | Loss: 0.8003380298614502Training Epoch: 0 | iteration: 78/262 | Loss: 0.8401923179626465Training Epoch: 0 | iteration: 79/262 | Loss: 0.8205980658531189Training Epoch: 0 | iteration: 80/262 | Loss: 0.8032190203666687Training Epoch: 0 | iteration: 81/262 | Loss: 0.8357948064804077Training Epoch: 0 | iteration: 82/262 | Loss: 0.8152589797973633Training Epoch: 0 | iteration: 83/262 | Loss: 0.9104479551315308Training Epoch: 0 | iteration: 84/262 | Loss: 0.8946917057037354Training Epoch: 0 | iteration: 85/262 | Loss: 0.8162367343902588Training Epoch: 0 | iteration: 86/262 | Loss: 0.8032135963439941Training Epoch: 0 | iteration: 87/262 | Loss: 0.8645862936973572Training Epoch: 0 | iteration: 88/262 | Loss: 0.827555775642395Training Epoch: 0 | iteration: 89/262 | Loss: 0.8300584554672241Training Epoch: 0 | iteration: 90/262 | Loss: 0.741897463798523Training Epoch: 0 | iteration: 91/262 | Loss: 0.8693796992301941Training Epoch: 0 | iteration: 92/262 | Loss: 0.9027742147445679Training Epoch: 0 | iteration: 93/262 | Loss: 0.8698439002037048Training Epoch: 0 | iteration: 94/262 | Loss: 0.8631230592727661Training Epoch: 0 | iteration: 95/262 | Loss: 0.7866865396499634Training Epoch: 0 | iteration: 96/262 | Loss: 0.868148922920227Training Epoch: 0 | iteration: 97/262 | Loss: 0.8106444478034973Training Epoch: 0 | iteration: 98/262 | Loss: 0.8632672429084778Training Epoch: 0 | iteration: 99/262 | Loss: 0.8322803378105164Training Epoch: 0 | iteration: 100/262 | Loss: 0.7770305871963501Training Epoch: 0 | iteration: 101/262 | Loss: 0.7508370876312256Training Epoch: 0 | iteration: 102/262 | Loss: 0.8900675773620605Training Epoch: 0 | iteration: 103/262 | Loss: 0.7971026301383972Training Epoch: 0 | iteration: 104/262 | Loss: 0.7817013263702393Training Epoch: 0 | iteration: 105/262 | Loss: 0.8679898977279663Training Epoch: 0 | iteration: 106/262 | Loss: 0.7935663461685181Training Epoch: 0 | iteration: 107/262 | Loss: 0.8343862295150757Training Epoch: 0 | iteration: 108/262 | Loss: 0.9209573268890381Training Epoch: 0 | iteration: 109/262 | Loss: 0.810749351978302Training Epoch: 0 | iteration: 110/262 | Loss: 0.847758412361145Training Epoch: 0 | iteration: 111/262 | Loss: 0.8494200706481934Training Epoch: 0 | iteration: 112/262 | Loss: 0.8784298896789551Training Epoch: 0 | iteration: 113/262 | Loss: 0.8565077781677246Training Epoch: 0 | iteration: 114/262 | Loss: 0.8172884583473206Training Epoch: 0 | iteration: 115/262 | Loss: 0.8288580775260925Training Epoch: 0 | iteration: 116/262 | Loss: 0.7887954711914062Training Epoch: 0 | iteration: 117/262 | Loss: 0.8918513655662537Training Epoch: 0 | iteration: 118/262 | Loss: 0.788603663444519Training Epoch: 0 | iteration: 119/262 | Loss: 0.8134914040565491Training Epoch: 0 | iteration: 120/262 | Loss: 0.776415228843689Training Epoch: 0 | iteration: 121/262 | Loss: 0.7905839681625366Training Epoch: 0 | iteration: 122/262 | Loss: 0.8418699502944946Training Epoch: 0 | iteration: 123/262 | Loss: 0.7954250574111938Training Epoch: 0 | iteration: 124/262 | Loss: 0.89589524269104Training Epoch: 0 | iteration: 125/262 | Loss: 0.8285157680511475Training Epoch: 0 | iteration: 126/262 | Loss: 0.7301287055015564Training Epoch: 0 | iteration: 127/262 | Loss: 0.8497077226638794Training Epoch: 0 | iteration: 128/262 | Loss: 0.7584083080291748Training Epoch: 0 | iteration: 129/262 | Loss: 0.8809982538223267Training Epoch: 0 | iteration: 130/262 | Loss: 0.8073270320892334Training Epoch: 0 | iteration: 131/262 | Loss: 0.8345481753349304Training Epoch: 0 | iteration: 132/262 | Loss: 0.8205634355545044Training Epoch: 0 | iteration: 133/262 | Loss: 0.7709094882011414Training Epoch: 0 | iteration: 134/262 | Loss: 0.8366144895553589Training Epoch: 0 | iteration: 135/262 | Loss: 0.693089485168457Training Epoch: 0 | iteration: 136/262 | Loss: 0.7468167543411255Training Epoch: 0 | iteration: 137/262 | Loss: 0.860893189907074Training Epoch: 0 | iteration: 138/262 | Loss: 0.8322321176528931Training Epoch: 0 | iteration: 139/262 | Loss: 0.844271183013916Training Epoch: 0 | iteration: 140/262 | Loss: 0.8367494940757751Training Epoch: 0 | iteration: 141/262 | Loss: 0.7503952383995056Training Epoch: 0 | iteration: 142/262 | Loss: 0.8333603143692017Training Epoch: 0 | iteration: 143/262 | Loss: 0.7604364156723022Training Epoch: 0 | iteration: 144/262 | Loss: 0.8116492033004761Training Epoch: 0 | iteration: 145/262 | Loss: 0.8354322910308838Training Epoch: 0 | iteration: 146/262 | Loss: 0.8526032567024231Training Epoch: 0 | iteration: 147/262 | Loss: 0.7723406553268433Training Epoch: 0 | iteration: 148/262 | Loss: 0.8650992512702942Training Epoch: 0 | iteration: 149/262 | Loss: 0.7193045020103455Training Epoch: 0 | iteration: 150/262 | Loss: 0.8170372247695923Training Epoch: 0 | iteration: 151/262 | Loss: 0.8488085865974426Training Epoch: 0 | iteration: 152/262 | Loss: 0.8046239614486694Training Epoch: 0 | iteration: 153/262 | Loss: 0.8103252649307251Training Epoch: 0 | iteration: 154/262 | Loss: 0.7901185750961304Training Epoch: 0 | iteration: 155/262 | Loss: 0.8434709310531616Training Epoch: 0 | iteration: 156/262 | Loss: 0.8779557347297668Training Epoch: 0 | iteration: 157/262 | Loss: 0.8397928476333618Training Epoch: 0 | iteration: 158/262 | Loss: 0.8495359420776367Training Epoch: 0 | iteration: 159/262 | Loss: 0.9183377623558044Training Epoch: 0 | iteration: 160/262 | Loss: 0.8011980056762695Training Epoch: 0 | iteration: 161/262 | Loss: 0.7859660983085632Training Epoch: 0 | iteration: 162/262 | Loss: 0.8152689933776855Training Epoch: 0 | iteration: 163/262 | Loss: 0.7743017673492432Training Epoch: 0 | iteration: 164/262 | Loss: 0.7616629004478455Training Epoch: 0 | iteration: 165/262 | Loss: 0.8155351877212524Training Epoch: 0 | iteration: 166/262 | Loss: 0.9362296462059021Training Epoch: 0 | iteration: 167/262 | Loss: 0.7652033567428589Training Epoch: 0 | iteration: 168/262 | Loss: 0.8186990022659302Training Epoch: 0 | iteration: 169/262 | Loss: 0.7827777862548828Training Epoch: 0 | iteration: 170/262 | Loss: 0.8239231109619141Training Epoch: 0 | iteration: 171/262 | Loss: 0.8651597499847412Training Epoch: 0 | iteration: 172/262 | Loss: 0.8487181067466736Training Epoch: 0 | iteration: 173/262 | Loss: 0.8876982927322388Training Epoch: 0 | iteration: 174/262 | Loss: 0.8539480566978455Training Epoch: 0 | iteration: 175/262 | Loss: 0.7063567638397217Training Epoch: 0 | iteration: 176/262 | Loss: 0.7375166416168213Training Epoch: 0 | iteration: 177/262 | Loss: 0.796233057975769Training Epoch: 0 | iteration: 178/262 | Loss: 0.9546415209770203Training Epoch: 0 | iteration: 179/262 | Loss: 0.7686251997947693Training Epoch: 0 | iteration: 180/262 | Loss: 0.8738108277320862Training Epoch: 0 | iteration: 181/262 | Loss: 0.801945686340332Training Epoch: 0 | iteration: 182/262 | Loss: 0.8514086604118347Training Epoch: 0 | iteration: 183/262 | Loss: 0.8201631307601929Training Epoch: 0 | iteration: 184/262 | Loss: 0.7835550308227539Training Epoch: 0 | iteration: 185/262 | Loss: 0.8586546778678894Training Epoch: 0 | iteration: 186/262 | Loss: 0.7994775176048279Training Epoch: 0 | iteration: 187/262 | Loss: 0.8190351724624634Training Epoch: 0 | iteration: 188/262 | Loss: 0.7893255949020386Training Epoch: 0 | iteration: 189/262 | Loss: 0.8807227611541748Training Epoch: 0 | iteration: 190/262 | Loss: 0.8050771951675415Training Epoch: 0 | iteration: 191/262 | Loss: 0.8350847959518433Training Epoch: 0 | iteration: 192/262 | Loss: 0.7635393142700195Training Epoch: 0 | iteration: 193/262 | Loss: 0.8004076480865479Training Epoch: 0 | iteration: 194/262 | Loss: 0.8745949268341064Training Epoch: 0 | iteration: 195/262 | Loss: 0.7313888072967529Training Epoch: 0 | iteration: 196/262 | Loss: 0.8275108337402344Training Epoch: 0 | iteration: 197/262 | Loss: 0.7992383241653442Training Epoch: 0 | iteration: 198/262 | Loss: 0.7985932230949402Training Epoch: 0 | iteration: 199/262 | Loss: 0.7253439426422119Training Epoch: 0 | iteration: 200/262 | Loss: 0.8740721940994263Training Epoch: 0 | iteration: 201/262 | Loss: 0.7017174959182739Training Epoch: 0 | iteration: 202/262 | Loss: 0.880297064781189Training Epoch: 0 | iteration: 203/262 | Loss: 0.9131584167480469Training Epoch: 0 | iteration: 204/262 | Loss: 0.8981058597564697Training Epoch: 0 | iteration: 205/262 | Loss: 0.724770188331604Training Epoch: 0 | iteration: 206/262 | Loss: 0.7550507187843323Training Epoch: 0 | iteration: 207/262 | Loss: 0.7726126909255981Training Epoch: 0 | iteration: 208/262 | Loss: 0.8346896171569824Training Epoch: 0 | iteration: 209/262 | Loss: 0.8222990036010742Training Epoch: 0 | iteration: 210/262 | Loss: 0.9315805435180664Training Epoch: 0 | iteration: 211/262 | Loss: 0.8052141666412354Training Epoch: 0 | iteration: 212/262 | Loss: 0.8010140657424927Training Epoch: 0 | iteration: 213/262 | Loss: 0.7659655213356018Training Epoch: 0 | iteration: 214/262 | Loss: 0.7891809940338135Training Epoch: 0 | iteration: 215/262 | Loss: 0.72507643699646Training Epoch: 0 | iteration: 216/262 | Loss: 0.7913830280303955Training Epoch: 0 | iteration: 217/262 | Loss: 0.8014304041862488Training Epoch: 0 | iteration: 218/262 | Loss: 0.8236919045448303Training Epoch: 0 | iteration: 219/262 | Loss: 0.7827638387680054Training Epoch: 0 | iteration: 220/262 | Loss: 0.8086368441581726Training Epoch: 0 | iteration: 221/262 | Loss: 0.7472476959228516Training Epoch: 0 | iteration: 222/262 | Loss: 0.8395688533782959Training Epoch: 0 | iteration: 223/262 | Loss: 0.8036760091781616Training Epoch: 0 | iteration: 224/262 | Loss: 0.741254985332489Training Epoch: 0 | iteration: 225/262 | Loss: 0.7348408102989197Training Epoch: 0 | iteration: 226/262 | Loss: 0.7375145554542542Training Epoch: 0 | iteration: 227/262 | Loss: 0.7494246959686279Training Epoch: 0 | iteration: 228/262 | Loss: 0.832329511642456Training Epoch: 0 | iteration: 229/262 | Loss: 0.8285380601882935Training Epoch: 0 | iteration: 230/262 | Loss: 0.8434727191925049Training Epoch: 0 | iteration: 231/262 | Loss: 0.8741931915283203Training Epoch: 0 | iteration: 232/262 | Loss: 0.8338455557823181Training Epoch: 0 | iteration: 233/262 | Loss: 0.7183984518051147Training Epoch: 0 | iteration: 234/262 | Loss: 0.7831307649612427Training Epoch: 0 | iteration: 235/262 | Loss: 0.7946378588676453Training Epoch: 0 | iteration: 236/262 | Loss: 0.7402415871620178Training Epoch: 0 | iteration: 237/262 | Loss: 0.8457417488098145Training Epoch: 0 | iteration: 238/262 | Loss: 0.7436232566833496Training Epoch: 0 | iteration: 239/262 | Loss: 0.8229741454124451Training Epoch: 0 | iteration: 240/262 | Loss: 0.7570546865463257Training Epoch: 0 | iteration: 241/262 | Loss: 0.8224632740020752Training Epoch: 0 | iteration: 242/262 | Loss: 0.7524405717849731Training Epoch: 0 | iteration: 243/262 | Loss: 0.828836977481842Training Epoch: 0 | iteration: 244/262 | Loss: 0.7531152963638306Training Epoch: 0 | iteration: 245/262 | Loss: 0.8807941675186157Training Epoch: 0 | iteration: 246/262 | Loss: 0.7980971336364746Training Epoch: 0 | iteration: 247/262 | Loss: 0.7548489570617676Training Epoch: 0 | iteration: 248/262 | Loss: 0.8449745774269104Training Epoch: 0 | iteration: 249/262 | Loss: 0.8107953071594238Training Epoch: 0 | iteration: 250/262 | Loss: 0.8321308493614197Training Epoch: 0 | iteration: 251/262 | Loss: 0.7639934420585632Training Epoch: 0 | iteration: 252/262 | Loss: 0.8313542604446411Training Epoch: 0 | iteration: 253/262 | Loss: 0.794525146484375Training Epoch: 0 | iteration: 254/262 | Loss: 0.788955807685852Training Epoch: 0 | iteration: 255/262 | Loss: 0.7711743116378784Training Epoch: 0 | iteration: 256/262 | Loss: 0.7977855205535889Training Epoch: 0 | iteration: 257/262 | Loss: 0.8215148448944092Training Epoch: 0 | iteration: 258/262 | Loss: 0.8099472522735596Training Epoch: 0 | iteration: 259/262 | Loss: 0.6593037247657776Training Epoch: 0 | iteration: 260/262 | Loss: 0.7666932940483093Training Epoch: 0 | iteration: 261/262 | Loss: 0.6734616756439209Validating Epoch: 0 | iteration: 0/66 | Loss: 0.6197144389152527Validating Epoch: 0 | iteration: 1/66 | Loss: 0.6101435422897339Validating Epoch: 0 | iteration: 2/66 | Loss: 0.6032116413116455Validating Epoch: 0 | iteration: 3/66 | Loss: 0.6779641509056091Validating Epoch: 0 | iteration: 4/66 | Loss: 0.5900325775146484Validating Epoch: 0 | iteration: 5/66 | Loss: 0.642877459526062Validating Epoch: 0 | iteration: 6/66 | Loss: 0.6354385614395142Validating Epoch: 0 | iteration: 7/66 | Loss: 0.6239610910415649Validating Epoch: 0 | iteration: 8/66 | Loss: 0.6234893798828125Validating Epoch: 0 | iteration: 9/66 | Loss: 0.7401752471923828Validating Epoch: 0 | iteration: 10/66 | Loss: 0.6404948234558105Validating Epoch: 0 | iteration: 11/66 | Loss: 0.6767240166664124Validating Epoch: 0 | iteration: 12/66 | Loss: 0.6825650930404663Validating Epoch: 0 | iteration: 13/66 | Loss: 0.6334648132324219Validating Epoch: 0 | iteration: 14/66 | Loss: 0.6304693222045898Validating Epoch: 0 | iteration: 15/66 | Loss: 0.653020977973938Validating Epoch: 0 | iteration: 16/66 | Loss: 0.6228528022766113Validating Epoch: 0 | iteration: 17/66 | Loss: 0.6853533983230591Validating Epoch: 0 | iteration: 18/66 | Loss: 0.6074366569519043Validating Epoch: 0 | iteration: 19/66 | Loss: 0.6302345991134644Validating Epoch: 0 | iteration: 20/66 | Loss: 0.6432026624679565Validating Epoch: 0 | iteration: 21/66 | Loss: 0.6543354392051697Validating Epoch: 0 | iteration: 22/66 | Loss: 0.6146179437637329Validating Epoch: 0 | iteration: 23/66 | Loss: 0.6191521286964417Validating Epoch: 0 | iteration: 24/66 | Loss: 0.5820603370666504Validating Epoch: 0 | iteration: 25/66 | Loss: 0.7082887887954712Validating Epoch: 0 | iteration: 26/66 | Loss: 0.5568709969520569Validating Epoch: 0 | iteration: 27/66 | Loss: 0.6002070903778076Validating Epoch: 0 | iteration: 28/66 | Loss: 0.6391021013259888Validating Epoch: 0 | iteration: 29/66 | Loss: 0.6237877011299133Validating Epoch: 0 | iteration: 30/66 | Loss: 0.6271318197250366Validating Epoch: 0 | iteration: 31/66 | Loss: 0.6250777840614319Validating Epoch: 0 | iteration: 32/66 | Loss: 0.6278740167617798Validating Epoch: 0 | iteration: 33/66 | Loss: 0.7222540378570557Validating Epoch: 0 | iteration: 34/66 | Loss: 0.6930030584335327Validating Epoch: 0 | iteration: 35/66 | Loss: 0.6422483921051025Validating Epoch: 0 | iteration: 36/66 | Loss: 0.6047931909561157Validating Epoch: 0 | iteration: 37/66 | Loss: 0.6220815777778625Validating Epoch: 0 | iteration: 38/66 | Loss: 0.6213418245315552Validating Epoch: 0 | iteration: 39/66 | Loss: 0.654300332069397Validating Epoch: 0 | iteration: 40/66 | Loss: 0.6593803763389587Validating Epoch: 0 | iteration: 41/66 | Loss: 0.5938850045204163Validating Epoch: 0 | iteration: 42/66 | Loss: 0.6637387871742249Validating Epoch: 0 | iteration: 43/66 | Loss: 0.6326757073402405Validating Epoch: 0 | iteration: 44/66 | Loss: 0.6614068746566772Validating Epoch: 0 | iteration: 45/66 | Loss: 0.6046138405799866Validating Epoch: 0 | iteration: 46/66 | Loss: 0.6394178867340088Validating Epoch: 0 | iteration: 47/66 | Loss: 0.6174740195274353Validating Epoch: 0 | iteration: 48/66 | Loss: 0.6820921301841736Validating Epoch: 0 | iteration: 49/66 | Loss: 0.6514778733253479Validating Epoch: 0 | iteration: 50/66 | Loss: 0.6305348873138428Validating Epoch: 0 | iteration: 51/66 | Loss: 0.5669217109680176Validating Epoch: 0 | iteration: 52/66 | Loss: 0.6368375420570374Validating Epoch: 0 | iteration: 53/66 | Loss: 0.5997430086135864Validating Epoch: 0 | iteration: 54/66 | Loss: 0.615998387336731Validating Epoch: 0 | iteration: 55/66 | Loss: 0.5880160331726074Validating Epoch: 0 | iteration: 56/66 | Loss: 0.6022642850875854Validating Epoch: 0 | iteration: 57/66 | Loss: 0.6151235103607178Validating Epoch: 0 | iteration: 58/66 | Loss: 0.6258370876312256Validating Epoch: 0 | iteration: 59/66 | Loss: 0.6465106010437012Validating Epoch: 0 | iteration: 60/66 | Loss: 0.6415109634399414Validating Epoch: 0 | iteration: 61/66 | Loss: 0.6121836304664612Validating Epoch: 0 | iteration: 62/66 | Loss: 0.6272331476211548Validating Epoch: 0 | iteration: 63/66 | Loss: 0.6431155800819397Validating Epoch: 0 | iteration: 64/66 | Loss: 0.6589928865432739Validating Epoch: 0 | iteration: 65/66 | Loss: 0.5934637188911438Temperature:  0.5
Sampling: iteration: 0/51{'Validity': 0.994140625, 'Novelty': 1.0, 'Uniqueness': 0.9921414538310412}
Training Epoch: 1 | iteration: 0/262 | Loss: 0.7353014945983887Training Epoch: 1 | iteration: 1/262 | Loss: 0.7439627647399902Training Epoch: 1 | iteration: 2/262 | Loss: 0.680232048034668Training Epoch: 1 | iteration: 3/262 | Loss: 0.7068285942077637Training Epoch: 1 | iteration: 4/262 | Loss: 0.7273842692375183Training Epoch: 1 | iteration: 5/262 | Loss: 0.7886905670166016Training Epoch: 1 | iteration: 6/262 | Loss: 0.6834468841552734Training Epoch: 1 | iteration: 7/262 | Loss: 0.6585211753845215Training Epoch: 1 | iteration: 8/262 | Loss: 0.7695956230163574Training Epoch: 1 | iteration: 9/262 | Loss: 0.7130295038223267Training Epoch: 1 | iteration: 10/262 | Loss: 0.6997309923171997Training Epoch: 1 | iteration: 11/262 | Loss: 0.7870702147483826Training Epoch: 1 | iteration: 12/262 | Loss: 0.7189024686813354Training Epoch: 1 | iteration: 13/262 | Loss: 0.6383530497550964Training Epoch: 1 | iteration: 14/262 | Loss: 0.7803595662117004Training Epoch: 1 | iteration: 15/262 | Loss: 0.7232707738876343Training Epoch: 1 | iteration: 16/262 | Loss: 0.7112828493118286Training Epoch: 1 | iteration: 17/262 | Loss: 0.7299278974533081Training Epoch: 1 | iteration: 18/262 | Loss: 0.6843421459197998Training Epoch: 1 | iteration: 19/262 | Loss: 0.786523699760437Training Epoch: 1 | iteration: 20/262 | Loss: 0.7144986987113953Training Epoch: 1 | iteration: 21/262 | Loss: 0.7412473559379578Training Epoch: 1 | iteration: 22/262 | Loss: 0.7169315814971924Training Epoch: 1 | iteration: 23/262 | Loss: 0.7749098539352417Training Epoch: 1 | iteration: 24/262 | Loss: 0.7297805547714233Training Epoch: 1 | iteration: 25/262 | Loss: 0.7580651044845581Training Epoch: 1 | iteration: 26/262 | Loss: 0.7229239344596863Training Epoch: 1 | iteration: 27/262 | Loss: 0.6809998750686646Training Epoch: 1 | iteration: 28/262 | Loss: 0.802682638168335Training Epoch: 1 | iteration: 29/262 | Loss: 0.7714085578918457Training Epoch: 1 | iteration: 30/262 | Loss: 0.7366423010826111Training Epoch: 1 | iteration: 31/262 | Loss: 0.7208178043365479Training Epoch: 1 | iteration: 32/262 | Loss: 0.6686182022094727Training Epoch: 1 | iteration: 33/262 | Loss: 0.7169482707977295Training Epoch: 1 | iteration: 34/262 | Loss: 0.8226528167724609Training Epoch: 1 | iteration: 35/262 | Loss: 0.8000847697257996Training Epoch: 1 | iteration: 36/262 | Loss: 0.8165150880813599Training Epoch: 1 | iteration: 37/262 | Loss: 0.7905815243721008Training Epoch: 1 | iteration: 38/262 | Loss: 0.7413763999938965Training Epoch: 1 | iteration: 39/262 | Loss: 0.7183341979980469Training Epoch: 1 | iteration: 40/262 | Loss: 0.8352906107902527Training Epoch: 1 | iteration: 41/262 | Loss: 0.7339675426483154Training Epoch: 1 | iteration: 42/262 | Loss: 0.8166632652282715Training Epoch: 1 | iteration: 43/262 | Loss: 0.7464038133621216Training Epoch: 1 | iteration: 44/262 | Loss: 0.6910605430603027Training Epoch: 1 | iteration: 45/262 | Loss: 0.742743968963623Training Epoch: 1 | iteration: 46/262 | Loss: 0.75217604637146Training Epoch: 1 | iteration: 47/262 | Loss: 0.771523118019104Training Epoch: 1 | iteration: 48/262 | Loss: 0.7066706418991089Training Epoch: 1 | iteration: 49/262 | Loss: 0.7349843978881836Training Epoch: 1 | iteration: 50/262 | Loss: 0.6742976903915405Training Epoch: 1 | iteration: 51/262 | Loss: 0.7997687458992004Training Epoch: 1 | iteration: 52/262 | Loss: 0.770516574382782Training Epoch: 1 | iteration: 53/262 | Loss: 0.7136068344116211Training Epoch: 1 | iteration: 54/262 | Loss: 0.8017696142196655Training Epoch: 1 | iteration: 55/262 | Loss: 0.7506066560745239Training Epoch: 1 | iteration: 56/262 | Loss: 0.7814974188804626Training Epoch: 1 | iteration: 57/262 | Loss: 0.7103667855262756Training Epoch: 1 | iteration: 58/262 | Loss: 0.7136675119400024Training Epoch: 1 | iteration: 59/262 | Loss: 0.7796874642372131Training Epoch: 1 | iteration: 60/262 | Loss: 0.7702376246452332Training Epoch: 1 | iteration: 61/262 | Loss: 0.7497751712799072Training Epoch: 1 | iteration: 62/262 | Loss: 0.7763127088546753Training Epoch: 1 | iteration: 63/262 | Loss: 0.7141847014427185Training Epoch: 1 | iteration: 64/262 | Loss: 0.7884284257888794Training Epoch: 1 | iteration: 65/262 | Loss: 0.7473857402801514Training Epoch: 1 | iteration: 66/262 | Loss: 0.8532085418701172Training Epoch: 1 | iteration: 67/262 | Loss: 0.7522926330566406Training Epoch: 1 | iteration: 68/262 | Loss: 0.7819318771362305Training Epoch: 1 | iteration: 69/262 | Loss: 0.7777565121650696Training Epoch: 1 | iteration: 70/262 | Loss: 0.7174769639968872Training Epoch: 1 | iteration: 71/262 | Loss: 0.6570369601249695Training Epoch: 1 | iteration: 72/262 | Loss: 0.7641795873641968Training Epoch: 1 | iteration: 73/262 | Loss: 0.7635805606842041Training Epoch: 1 | iteration: 74/262 | Loss: 0.6898773312568665Training Epoch: 1 | iteration: 75/262 | Loss: 0.7439261674880981Training Epoch: 1 | iteration: 76/262 | Loss: 0.7727519869804382Training Epoch: 1 | iteration: 77/262 | Loss: 0.6333867907524109Training Epoch: 1 | iteration: 78/262 | Loss: 0.650357723236084Training Epoch: 1 | iteration: 79/262 | Loss: 0.796103835105896Training Epoch: 1 | iteration: 80/262 | Loss: 0.7275243997573853Training Epoch: 1 | iteration: 81/262 | Loss: 0.7405630350112915Training Epoch: 1 | iteration: 82/262 | Loss: 0.7207019329071045Training Epoch: 1 | iteration: 83/262 | Loss: 0.7108399868011475Training Epoch: 1 | iteration: 84/262 | Loss: 0.7200813889503479Training Epoch: 1 | iteration: 85/262 | Loss: 0.7469419240951538Training Epoch: 1 | iteration: 86/262 | Loss: 0.746486246585846Training Epoch: 1 | iteration: 87/262 | Loss: 0.7548378705978394Training Epoch: 1 | iteration: 88/262 | Loss: 0.7400721907615662Training Epoch: 1 | iteration: 89/262 | Loss: 0.6677595973014832Training Epoch: 1 | iteration: 90/262 | Loss: 0.7389893531799316Training Epoch: 1 | iteration: 91/262 | Loss: 0.7336302399635315Training Epoch: 1 | iteration: 92/262 | Loss: 0.7094662189483643Training Epoch: 1 | iteration: 93/262 | Loss: 0.773543119430542Training Epoch: 1 | iteration: 94/262 | Loss: 0.7256520986557007Training Epoch: 1 | iteration: 95/262 | Loss: 0.7278409004211426Training Epoch: 1 | iteration: 96/262 | Loss: 0.7333636283874512Training Epoch: 1 | iteration: 97/262 | Loss: 0.8461999297142029Training Epoch: 1 | iteration: 98/262 | Loss: 0.6842635273933411Training Epoch: 1 | iteration: 99/262 | Loss: 0.7242094278335571Training Epoch: 1 | iteration: 100/262 | Loss: 0.7982091903686523Training Epoch: 1 | iteration: 101/262 | Loss: 0.6780387759208679Training Epoch: 1 | iteration: 102/262 | Loss: 0.6937728524208069Training Epoch: 1 | iteration: 103/262 | Loss: 0.6707710027694702Training Epoch: 1 | iteration: 104/262 | Loss: 0.7176799178123474Training Epoch: 1 | iteration: 105/262 | Loss: 0.7295931577682495Training Epoch: 1 | iteration: 106/262 | Loss: 0.6849883794784546Training Epoch: 1 | iteration: 107/262 | Loss: 0.745010256767273Training Epoch: 1 | iteration: 108/262 | Loss: 0.6296793222427368Training Epoch: 1 | iteration: 109/262 | Loss: 0.7134305834770203Training Epoch: 1 | iteration: 110/262 | Loss: 0.7747885584831238Training Epoch: 1 | iteration: 111/262 | Loss: 0.656825840473175Training Epoch: 1 | iteration: 112/262 | Loss: 0.7523743510246277Training Epoch: 1 | iteration: 113/262 | Loss: 0.6806452870368958Training Epoch: 1 | iteration: 114/262 | Loss: 0.7958352565765381Training Epoch: 1 | iteration: 115/262 | Loss: 0.6953331232070923Training Epoch: 1 | iteration: 116/262 | Loss: 0.7834975719451904Training Epoch: 1 | iteration: 117/262 | Loss: 0.7075029611587524Training Epoch: 1 | iteration: 118/262 | Loss: 0.803146481513977Training Epoch: 1 | iteration: 119/262 | Loss: 0.6804516315460205Training Epoch: 1 | iteration: 120/262 | Loss: 0.7003992795944214Training Epoch: 1 | iteration: 121/262 | Loss: 0.6807518005371094Training Epoch: 1 | iteration: 122/262 | Loss: 0.7832458019256592Training Epoch: 1 | iteration: 123/262 | Loss: 0.7225548028945923Training Epoch: 1 | iteration: 124/262 | Loss: 0.7812495231628418Training Epoch: 1 | iteration: 125/262 | Loss: 0.7643293738365173Training Epoch: 1 | iteration: 126/262 | Loss: 0.685570478439331Training Epoch: 1 | iteration: 127/262 | Loss: 0.7594541311264038Training Epoch: 1 | iteration: 128/262 | Loss: 0.7722830772399902Training Epoch: 1 | iteration: 129/262 | Loss: 0.7266058921813965Training Epoch: 1 | iteration: 130/262 | Loss: 0.7513774633407593Training Epoch: 1 | iteration: 131/262 | Loss: 0.6760495901107788Training Epoch: 1 | iteration: 132/262 | Loss: 0.728476881980896Training Epoch: 1 | iteration: 133/262 | Loss: 0.7252646088600159Training Epoch: 1 | iteration: 134/262 | Loss: 0.7083160877227783Training Epoch: 1 | iteration: 135/262 | Loss: 0.7842117547988892Training Epoch: 1 | iteration: 136/262 | Loss: 0.7548554539680481Training Epoch: 1 | iteration: 137/262 | Loss: 0.7165431976318359Training Epoch: 1 | iteration: 138/262 | Loss: 0.7100737690925598Training Epoch: 1 | iteration: 139/262 | Loss: 0.7986416816711426Training Epoch: 1 | iteration: 140/262 | Loss: 0.7844994068145752Training Epoch: 1 | iteration: 141/262 | Loss: 0.7647749781608582Training Epoch: 1 | iteration: 142/262 | Loss: 0.7511962652206421Training Epoch: 1 | iteration: 143/262 | Loss: 0.6913914680480957Training Epoch: 1 | iteration: 144/262 | Loss: 0.699296236038208Training Epoch: 1 | iteration: 145/262 | Loss: 0.7608441710472107Training Epoch: 1 | iteration: 146/262 | Loss: 0.7677159309387207Training Epoch: 1 | iteration: 147/262 | Loss: 0.720937967300415Training Epoch: 1 | iteration: 148/262 | Loss: 0.7503460645675659Training Epoch: 1 | iteration: 149/262 | Loss: 0.7323693037033081Training Epoch: 1 | iteration: 150/262 | Loss: 0.7499855756759644Training Epoch: 1 | iteration: 151/262 | Loss: 0.6617104411125183Training Epoch: 1 | iteration: 152/262 | Loss: 0.6332181692123413Training Epoch: 1 | iteration: 153/262 | Loss: 0.7219821214675903Training Epoch: 1 | iteration: 154/262 | Loss: 0.7470747232437134Training Epoch: 1 | iteration: 155/262 | Loss: 0.7213764190673828Training Epoch: 1 | iteration: 156/262 | Loss: 0.7928736209869385Training Epoch: 1 | iteration: 157/262 | Loss: 0.7503694295883179Training Epoch: 1 | iteration: 158/262 | Loss: 0.8204719424247742Training Epoch: 1 | iteration: 159/262 | Loss: 0.78056800365448Training Epoch: 1 | iteration: 160/262 | Loss: 0.7082966566085815Training Epoch: 1 | iteration: 161/262 | Loss: 0.7472812533378601Training Epoch: 1 | iteration: 162/262 | Loss: 0.7265939712524414Training Epoch: 1 | iteration: 163/262 | Loss: 0.7127029895782471Training Epoch: 1 | iteration: 164/262 | Loss: 0.8110640048980713Training Epoch: 1 | iteration: 165/262 | Loss: 0.7944170832633972Training Epoch: 1 | iteration: 166/262 | Loss: 0.7438129186630249Training Epoch: 1 | iteration: 167/262 | Loss: 0.7550896406173706Training Epoch: 1 | iteration: 168/262 | Loss: 0.677664577960968Training Epoch: 1 | iteration: 169/262 | Loss: 0.7431318759918213Training Epoch: 1 | iteration: 170/262 | Loss: 0.7071276903152466Training Epoch: 1 | iteration: 171/262 | Loss: 0.7157977819442749Training Epoch: 1 | iteration: 172/262 | Loss: 0.7523666024208069Training Epoch: 1 | iteration: 173/262 | Loss: 0.7538226842880249Training Epoch: 1 | iteration: 174/262 | Loss: 0.80558842420578Training Epoch: 1 | iteration: 175/262 | Loss: 0.7945103645324707Training Epoch: 1 | iteration: 176/262 | Loss: 0.7262293100357056Training Epoch: 1 | iteration: 177/262 | Loss: 0.7977455854415894Training Epoch: 1 | iteration: 178/262 | Loss: 0.8469033241271973Training Epoch: 1 | iteration: 179/262 | Loss: 0.7178529500961304Training Epoch: 1 | iteration: 180/262 | Loss: 0.7193009257316589Training Epoch: 1 | iteration: 181/262 | Loss: 0.7817742824554443Training Epoch: 1 | iteration: 182/262 | Loss: 0.832451581954956Training Epoch: 1 | iteration: 183/262 | Loss: 0.7363737225532532Training Epoch: 1 | iteration: 184/262 | Loss: 0.7653034329414368Training Epoch: 1 | iteration: 185/262 | Loss: 0.7751731872558594Training Epoch: 1 | iteration: 186/262 | Loss: 0.7137305736541748Training Epoch: 1 | iteration: 187/262 | Loss: 0.6733631491661072Training Epoch: 1 | iteration: 188/262 | Loss: 0.6747052073478699Training Epoch: 1 | iteration: 189/262 | Loss: 0.8011661767959595Training Epoch: 1 | iteration: 190/262 | Loss: 0.8230864405632019Training Epoch: 1 | iteration: 191/262 | Loss: 0.747654378414154Training Epoch: 1 | iteration: 192/262 | Loss: 0.7422468662261963Training Epoch: 1 | iteration: 193/262 | Loss: 0.6970352530479431Training Epoch: 1 | iteration: 194/262 | Loss: 0.7796285152435303Training Epoch: 1 | iteration: 195/262 | Loss: 0.7742831110954285Training Epoch: 1 | iteration: 196/262 | Loss: 0.7264347672462463Training Epoch: 1 | iteration: 197/262 | Loss: 0.6917152404785156Training Epoch: 1 | iteration: 198/262 | Loss: 0.8154372572898865Training Epoch: 1 | iteration: 199/262 | Loss: 0.7429344654083252Training Epoch: 1 | iteration: 200/262 | Loss: 0.6957799792289734Training Epoch: 1 | iteration: 201/262 | Loss: 0.7529286742210388Training Epoch: 1 | iteration: 202/262 | Loss: 0.8397660255432129Training Epoch: 1 | iteration: 203/262 | Loss: 0.7196707129478455Training Epoch: 1 | iteration: 204/262 | Loss: 0.7632942199707031Training Epoch: 1 | iteration: 205/262 | Loss: 0.7692599296569824Training Epoch: 1 | iteration: 206/262 | Loss: 0.7708239555358887Training Epoch: 1 | iteration: 207/262 | Loss: 0.7741148471832275Training Epoch: 1 | iteration: 208/262 | Loss: 0.7249581217765808Training Epoch: 1 | iteration: 209/262 | Loss: 0.7039414644241333Training Epoch: 1 | iteration: 210/262 | Loss: 0.7585055232048035Training Epoch: 1 | iteration: 211/262 | Loss: 0.8108008503913879Training Epoch: 1 | iteration: 212/262 | Loss: 0.7263882160186768Training Epoch: 1 | iteration: 213/262 | Loss: 0.7136276364326477Training Epoch: 1 | iteration: 214/262 | Loss: 0.6760987043380737Training Epoch: 1 | iteration: 215/262 | Loss: 0.7791544198989868Training Epoch: 1 | iteration: 216/262 | Loss: 0.7470526695251465Training Epoch: 1 | iteration: 217/262 | Loss: 0.6711125373840332Training Epoch: 1 | iteration: 218/262 | Loss: 0.7360504865646362Training Epoch: 1 | iteration: 219/262 | Loss: 0.7830124497413635Training Epoch: 1 | iteration: 220/262 | Loss: 0.7128382325172424Training Epoch: 1 | iteration: 221/262 | Loss: 0.6855878829956055Training Epoch: 1 | iteration: 222/262 | Loss: 0.6689044237136841Training Epoch: 1 | iteration: 223/262 | Loss: 0.6682834625244141Training Epoch: 1 | iteration: 224/262 | Loss: 0.6920653581619263Training Epoch: 1 | iteration: 225/262 | Loss: 0.6937388181686401Training Epoch: 1 | iteration: 226/262 | Loss: 0.6913120150566101Training Epoch: 1 | iteration: 227/262 | Loss: 0.6812330484390259Training Epoch: 1 | iteration: 228/262 | Loss: 0.7421977519989014Training Epoch: 1 | iteration: 229/262 | Loss: 0.7750215530395508Training Epoch: 1 | iteration: 230/262 | Loss: 0.6670684814453125Training Epoch: 1 | iteration: 231/262 | Loss: 0.7179111838340759Training Epoch: 1 | iteration: 232/262 | Loss: 0.6779117584228516Training Epoch: 1 | iteration: 233/262 | Loss: 0.7089656591415405Training Epoch: 1 | iteration: 234/262 | Loss: 0.6932445764541626Training Epoch: 1 | iteration: 235/262 | Loss: 0.7506198883056641Training Epoch: 1 | iteration: 236/262 | Loss: 0.647384524345398Training Epoch: 1 | iteration: 237/262 | Loss: 0.7008084058761597Training Epoch: 1 | iteration: 238/262 | Loss: 0.6854534149169922Training Epoch: 1 | iteration: 239/262 | Loss: 0.7747899889945984Training Epoch: 1 | iteration: 240/262 | Loss: 0.7751748561859131Training Epoch: 1 | iteration: 241/262 | Loss: 0.7812620401382446Training Epoch: 1 | iteration: 242/262 | Loss: 0.7133840322494507Training Epoch: 1 | iteration: 243/262 | Loss: 0.7593706250190735Training Epoch: 1 | iteration: 244/262 | Loss: 0.6823306083679199Training Epoch: 1 | iteration: 245/262 | Loss: 0.7007962465286255Training Epoch: 1 | iteration: 246/262 | Loss: 0.7653536200523376Training Epoch: 1 | iteration: 247/262 | Loss: 0.683010458946228Training Epoch: 1 | iteration: 248/262 | Loss: 0.7126720547676086Training Epoch: 1 | iteration: 249/262 | Loss: 0.6925584077835083Training Epoch: 1 | iteration: 250/262 | Loss: 0.7619801759719849Training Epoch: 1 | iteration: 251/262 | Loss: 0.7133306860923767Training Epoch: 1 | iteration: 252/262 | Loss: 0.7177941799163818Training Epoch: 1 | iteration: 253/262 | Loss: 0.7637766599655151Training Epoch: 1 | iteration: 254/262 | Loss: 0.7888638973236084Training Epoch: 1 | iteration: 255/262 | Loss: 0.6374155282974243Training Epoch: 1 | iteration: 256/262 | Loss: 0.731947660446167Training Epoch: 1 | iteration: 257/262 | Loss: 0.7342535853385925Training Epoch: 1 | iteration: 258/262 | Loss: 0.7075790166854858Training Epoch: 1 | iteration: 259/262 | Loss: 0.6894618272781372Training Epoch: 1 | iteration: 260/262 | Loss: 0.8164729475975037Training Epoch: 1 | iteration: 261/262 | Loss: 0.6452487707138062Validating Epoch: 1 | iteration: 0/66 | Loss: 0.6102240085601807Validating Epoch: 1 | iteration: 1/66 | Loss: 0.6416552662849426Validating Epoch: 1 | iteration: 2/66 | Loss: 0.5672104358673096Validating Epoch: 1 | iteration: 3/66 | Loss: 0.5802507400512695Validating Epoch: 1 | iteration: 4/66 | Loss: 0.548264741897583Validating Epoch: 1 | iteration: 5/66 | Loss: 0.6731514930725098Validating Epoch: 1 | iteration: 6/66 | Loss: 0.5784282684326172Validating Epoch: 1 | iteration: 7/66 | Loss: 0.6108327507972717Validating Epoch: 1 | iteration: 8/66 | Loss: 0.6150365471839905Validating Epoch: 1 | iteration: 9/66 | Loss: 0.6128773093223572Validating Epoch: 1 | iteration: 10/66 | Loss: 0.6296333074569702Validating Epoch: 1 | iteration: 11/66 | Loss: 0.6217643618583679Validating Epoch: 1 | iteration: 12/66 | Loss: 0.6362238526344299Validating Epoch: 1 | iteration: 13/66 | Loss: 0.6383304595947266Validating Epoch: 1 | iteration: 14/66 | Loss: 0.5826863646507263Validating Epoch: 1 | iteration: 15/66 | Loss: 0.6379520893096924Validating Epoch: 1 | iteration: 16/66 | Loss: 0.5847341418266296Validating Epoch: 1 | iteration: 17/66 | Loss: 0.6036761999130249Validating Epoch: 1 | iteration: 18/66 | Loss: 0.649870753288269Validating Epoch: 1 | iteration: 19/66 | Loss: 0.6108851432800293Validating Epoch: 1 | iteration: 20/66 | Loss: 0.6357666850090027Validating Epoch: 1 | iteration: 21/66 | Loss: 0.6530324220657349Validating Epoch: 1 | iteration: 22/66 | Loss: 0.6456058025360107Validating Epoch: 1 | iteration: 23/66 | Loss: 0.5987736582756042Validating Epoch: 1 | iteration: 24/66 | Loss: 0.5382636785507202Validating Epoch: 1 | iteration: 25/66 | Loss: 0.6288913488388062Validating Epoch: 1 | iteration: 26/66 | Loss: 0.609417200088501Validating Epoch: 1 | iteration: 27/66 | Loss: 0.5952771902084351Validating Epoch: 1 | iteration: 28/66 | Loss: 0.5833815336227417Validating Epoch: 1 | iteration: 29/66 | Loss: 0.6139081716537476Validating Epoch: 1 | iteration: 30/66 | Loss: 0.6552435159683228Validating Epoch: 1 | iteration: 31/66 | Loss: 0.6474834680557251Validating Epoch: 1 | iteration: 32/66 | Loss: 0.694614827632904Validating Epoch: 1 | iteration: 33/66 | Loss: 0.6132174730300903Validating Epoch: 1 | iteration: 34/66 | Loss: 0.5983924865722656Validating Epoch: 1 | iteration: 35/66 | Loss: 0.6122894287109375Validating Epoch: 1 | iteration: 36/66 | Loss: 0.6109750270843506Validating Epoch: 1 | iteration: 37/66 | Loss: 0.594417929649353Validating Epoch: 1 | iteration: 38/66 | Loss: 0.6021897792816162Validating Epoch: 1 | iteration: 39/66 | Loss: 0.6559559106826782Validating Epoch: 1 | iteration: 40/66 | Loss: 0.637970507144928Validating Epoch: 1 | iteration: 41/66 | Loss: 0.6599957942962646Validating Epoch: 1 | iteration: 42/66 | Loss: 0.5886934399604797Validating Epoch: 1 | iteration: 43/66 | Loss: 0.6344649195671082Validating Epoch: 1 | iteration: 44/66 | Loss: 0.5666359066963196Validating Epoch: 1 | iteration: 45/66 | Loss: 0.6282452344894409Validating Epoch: 1 | iteration: 46/66 | Loss: 0.6321150660514832Validating Epoch: 1 | iteration: 47/66 | Loss: 0.6655694842338562Validating Epoch: 1 | iteration: 48/66 | Loss: 0.6544255018234253Validating Epoch: 1 | iteration: 49/66 | Loss: 0.6300193667411804Validating Epoch: 1 | iteration: 50/66 | Loss: 0.6606186032295227Validating Epoch: 1 | iteration: 51/66 | Loss: 0.5788726210594177Validating Epoch: 1 | iteration: 52/66 | Loss: 0.5828588008880615Validating Epoch: 1 | iteration: 53/66 | Loss: 0.5638562440872192Validating Epoch: 1 | iteration: 54/66 | Loss: 0.5765020847320557Validating Epoch: 1 | iteration: 55/66 | Loss: 0.6213791370391846Validating Epoch: 1 | iteration: 56/66 | Loss: 0.6905301809310913Validating Epoch: 1 | iteration: 57/66 | Loss: 0.679692268371582Validating Epoch: 1 | iteration: 58/66 | Loss: 0.6160265207290649Validating Epoch: 1 | iteration: 59/66 | Loss: 0.6283312439918518Validating Epoch: 1 | iteration: 60/66 | Loss: 0.5980709791183472Validating Epoch: 1 | iteration: 61/66 | Loss: 0.596857488155365Validating Epoch: 1 | iteration: 62/66 | Loss: 0.6357505321502686Validating Epoch: 1 | iteration: 63/66 | Loss: 0.665372371673584Validating Epoch: 1 | iteration: 64/66 | Loss: 0.6321479082107544Validating Epoch: 1 | iteration: 65/66 | Loss: 0.672396183013916Temperature:  0.5
Sampling: iteration: 0/51{'Validity': 0.9990234375, 'Novelty': 1.0, 'Uniqueness': 0.9941348973607038}
Training Epoch: 2 | iteration: 0/262 | Loss: 0.6699661612510681Training Epoch: 2 | iteration: 1/262 | Loss: 0.6595191955566406Training Epoch: 2 | iteration: 2/262 | Loss: 0.7041763663291931Training Epoch: 2 | iteration: 3/262 | Loss: 0.7176505327224731Training Epoch: 2 | iteration: 4/262 | Loss: 0.7204734086990356Training Epoch: 2 | iteration: 5/262 | Loss: 0.7011585235595703Training Epoch: 2 | iteration: 6/262 | Loss: 0.6966413855552673Training Epoch: 2 | iteration: 7/262 | Loss: 0.6884257793426514Training Epoch: 2 | iteration: 8/262 | Loss: 0.663957953453064Training Epoch: 2 | iteration: 9/262 | Loss: 0.7473999261856079Training Epoch: 2 | iteration: 10/262 | Loss: 0.7120708227157593Training Epoch: 2 | iteration: 11/262 | Loss: 0.747191309928894Training Epoch: 2 | iteration: 12/262 | Loss: 0.7405305504798889Training Epoch: 2 | iteration: 13/262 | Loss: 0.8186041116714478Training Epoch: 2 | iteration: 14/262 | Loss: 0.7164602875709534Training Epoch: 2 | iteration: 15/262 | Loss: 0.6754543781280518Training Epoch: 2 | iteration: 16/262 | Loss: 0.7052921056747437Training Epoch: 2 | iteration: 17/262 | Loss: 0.6126757860183716Training Epoch: 2 | iteration: 18/262 | Loss: 0.6667842864990234Training Epoch: 2 | iteration: 19/262 | Loss: 0.6444226503372192Training Epoch: 2 | iteration: 20/262 | Loss: 0.6592290997505188Training Epoch: 2 | iteration: 21/262 | Loss: 0.70327228307724Training Epoch: 2 | iteration: 22/262 | Loss: 0.7239651679992676Training Epoch: 2 | iteration: 23/262 | Loss: 0.7173962593078613Training Epoch: 2 | iteration: 24/262 | Loss: 0.7258182764053345Training Epoch: 2 | iteration: 25/262 | Loss: 0.7212594747543335Training Epoch: 2 | iteration: 26/262 | Loss: 0.6908663511276245Training Epoch: 2 | iteration: 27/262 | Loss: 0.7308177351951599Training Epoch: 2 | iteration: 28/262 | Loss: 0.7438176870346069Training Epoch: 2 | iteration: 29/262 | Loss: 0.7318891286849976Training Epoch: 2 | iteration: 30/262 | Loss: 0.6447149515151978Training Epoch: 2 | iteration: 31/262 | Loss: 0.6951028108596802Training Epoch: 2 | iteration: 32/262 | Loss: 0.7233659029006958Training Epoch: 2 | iteration: 33/262 | Loss: 0.6998485326766968Training Epoch: 2 | iteration: 34/262 | Loss: 0.7071285843849182Training Epoch: 2 | iteration: 35/262 | Loss: 0.6835411787033081Training Epoch: 2 | iteration: 36/262 | Loss: 0.7321186065673828Training Epoch: 2 | iteration: 37/262 | Loss: 0.7146346569061279Training Epoch: 2 | iteration: 38/262 | Loss: 0.6585979461669922Training Epoch: 2 | iteration: 39/262 | Loss: 0.7225039005279541Training Epoch: 2 | iteration: 40/262 | Loss: 0.7072304487228394Training Epoch: 2 | iteration: 41/262 | Loss: 0.6287621259689331Training Epoch: 2 | iteration: 42/262 | Loss: 0.6790326237678528Training Epoch: 2 | iteration: 43/262 | Loss: 0.6211721897125244Training Epoch: 2 | iteration: 44/262 | Loss: 0.5813428163528442Training Epoch: 2 | iteration: 45/262 | Loss: 0.6861275434494019Training Epoch: 2 | iteration: 46/262 | Loss: 0.6872126460075378Training Epoch: 2 | iteration: 47/262 | Loss: 0.6827753782272339Training Epoch: 2 | iteration: 48/262 | Loss: 0.7271604537963867Training Epoch: 2 | iteration: 49/262 | Loss: 0.6664129495620728Training Epoch: 2 | iteration: 50/262 | Loss: 0.6658321619033813Training Epoch: 2 | iteration: 51/262 | Loss: 0.7441779971122742Training Epoch: 2 | iteration: 52/262 | Loss: 0.6647031307220459Training Epoch: 2 | iteration: 53/262 | Loss: 0.6507080793380737Training Epoch: 2 | iteration: 54/262 | Loss: 0.6627341508865356Training Epoch: 2 | iteration: 55/262 | Loss: 0.6687237620353699Training Epoch: 2 | iteration: 56/262 | Loss: 0.7365862131118774Training Epoch: 2 | iteration: 57/262 | Loss: 0.7013229727745056Training Epoch: 2 | iteration: 58/262 | Loss: 0.67901211977005Training Epoch: 2 | iteration: 59/262 | Loss: 0.6636552810668945Training Epoch: 2 | iteration: 60/262 | Loss: 0.753623366355896Training Epoch: 2 | iteration: 61/262 | Loss: 0.585718035697937Training Epoch: 2 | iteration: 62/262 | Loss: 0.696300208568573Training Epoch: 2 | iteration: 63/262 | Loss: 0.6845686435699463Training Epoch: 2 | iteration: 64/262 | Loss: 0.655349850654602Training Epoch: 2 | iteration: 65/262 | Loss: 0.6498838663101196Training Epoch: 2 | iteration: 66/262 | Loss: 0.6926933526992798Training Epoch: 2 | iteration: 67/262 | Loss: 0.6936566829681396Training Epoch: 2 | iteration: 68/262 | Loss: 0.6935073137283325Training Epoch: 2 | iteration: 69/262 | Loss: 0.660040020942688Training Epoch: 2 | iteration: 70/262 | Loss: 0.6527477502822876Training Epoch: 2 | iteration: 71/262 | Loss: 0.646727442741394Training Epoch: 2 | iteration: 72/262 | Loss: 0.6456407308578491Training Epoch: 2 | iteration: 73/262 | Loss: 0.7207979559898376Training Epoch: 2 | iteration: 74/262 | Loss: 0.6856579184532166Training Epoch: 2 | iteration: 75/262 | Loss: 0.7070659399032593Training Epoch: 2 | iteration: 76/262 | Loss: 0.629607617855072Training Epoch: 2 | iteration: 77/262 | Loss: 0.7111255526542664Training Epoch: 2 | iteration: 78/262 | Loss: 0.6747632026672363Training Epoch: 2 | iteration: 79/262 | Loss: 0.7720689177513123Training Epoch: 2 | iteration: 80/262 | Loss: 0.6585091352462769Training Epoch: 2 | iteration: 81/262 | Loss: 0.6684104204177856Training Epoch: 2 | iteration: 82/262 | Loss: 0.6445075273513794Training Epoch: 2 | iteration: 83/262 | Loss: 0.7515959739685059Training Epoch: 2 | iteration: 84/262 | Loss: 0.671487033367157Training Epoch: 2 | iteration: 85/262 | Loss: 0.6420111656188965Training Epoch: 2 | iteration: 86/262 | Loss: 0.6455321311950684Training Epoch: 2 | iteration: 87/262 | Loss: 0.692084550857544Training Epoch: 2 | iteration: 88/262 | Loss: 0.7504791021347046Training Epoch: 2 | iteration: 89/262 | Loss: 0.7058392763137817Training Epoch: 2 | iteration: 90/262 | Loss: 0.7474302053451538Training Epoch: 2 | iteration: 91/262 | Loss: 0.6890934705734253Training Epoch: 2 | iteration: 92/262 | Loss: 0.6811497807502747Training Epoch: 2 | iteration: 93/262 | Loss: 0.6601799130439758Training Epoch: 2 | iteration: 94/262 | Loss: 0.6994523406028748Training Epoch: 2 | iteration: 95/262 | Loss: 0.7530415058135986Training Epoch: 2 | iteration: 96/262 | Loss: 0.6406978964805603Training Epoch: 2 | iteration: 97/262 | Loss: 0.7627524733543396Training Epoch: 2 | iteration: 98/262 | Loss: 0.6760491132736206Training Epoch: 2 | iteration: 99/262 | Loss: 0.7298697233200073Training Epoch: 2 | iteration: 100/262 | Loss: 0.7770922780036926Training Epoch: 2 | iteration: 101/262 | Loss: 0.6734337210655212Training Epoch: 2 | iteration: 102/262 | Loss: 0.6812443137168884Training Epoch: 2 | iteration: 103/262 | Loss: 0.7235816121101379Training Epoch: 2 | iteration: 104/262 | Loss: 0.7230566740036011Training Epoch: 2 | iteration: 105/262 | Loss: 0.6840710639953613Training Epoch: 2 | iteration: 106/262 | Loss: 0.717170774936676Training Epoch: 2 | iteration: 107/262 | Loss: 0.6766953468322754Training Epoch: 2 | iteration: 108/262 | Loss: 0.646075963973999Training Epoch: 2 | iteration: 109/262 | Loss: 0.625605583190918Training Epoch: 2 | iteration: 110/262 | Loss: 0.697689414024353Training Epoch: 2 | iteration: 111/262 | Loss: 0.7772667407989502Training Epoch: 2 | iteration: 112/262 | Loss: 0.7346116304397583Training Epoch: 2 | iteration: 113/262 | Loss: 0.7093726396560669Training Epoch: 2 | iteration: 114/262 | Loss: 0.8005303740501404Training Epoch: 2 | iteration: 115/262 | Loss: 0.706497311592102Training Epoch: 2 | iteration: 116/262 | Loss: 0.644297182559967Training Epoch: 2 | iteration: 117/262 | Loss: 0.670629620552063Training Epoch: 2 | iteration: 118/262 | Loss: 0.7046209573745728Training Epoch: 2 | iteration: 119/262 | Loss: 0.6975663900375366Training Epoch: 2 | iteration: 120/262 | Loss: 0.6732585430145264Training Epoch: 2 | iteration: 121/262 | Loss: 0.7244085073471069Training Epoch: 2 | iteration: 122/262 | Loss: 0.6981977224349976Training Epoch: 2 | iteration: 123/262 | Loss: 0.7737840414047241Training Epoch: 2 | iteration: 124/262 | Loss: 0.6386831402778625Training Epoch: 2 | iteration: 125/262 | Loss: 0.7083516120910645Training Epoch: 2 | iteration: 126/262 | Loss: 0.6276952028274536Training Epoch: 2 | iteration: 127/262 | Loss: 0.6465746760368347Training Epoch: 2 | iteration: 128/262 | Loss: 0.6238310933113098Training Epoch: 2 | iteration: 129/262 | Loss: 0.7161633968353271Training Epoch: 2 | iteration: 130/262 | Loss: 0.6870341300964355Training Epoch: 2 | iteration: 131/262 | Loss: 0.6827730536460876Training Epoch: 2 | iteration: 132/262 | Loss: 0.6692401766777039Training Epoch: 2 | iteration: 133/262 | Loss: 0.6919185519218445Training Epoch: 2 | iteration: 134/262 | Loss: 0.7168552875518799Training Epoch: 2 | iteration: 135/262 | Loss: 0.7165806889533997Training Epoch: 2 | iteration: 136/262 | Loss: 0.6425036787986755Training Epoch: 2 | iteration: 137/262 | Loss: 0.6373905539512634Training Epoch: 2 | iteration: 138/262 | Loss: 0.6609784364700317Training Epoch: 2 | iteration: 139/262 | Loss: 0.6708869934082031Training Epoch: 2 | iteration: 140/262 | Loss: 0.6533322930335999Training Epoch: 2 | iteration: 141/262 | Loss: 0.67780601978302Training Epoch: 2 | iteration: 142/262 | Loss: 0.7531002759933472Training Epoch: 2 | iteration: 143/262 | Loss: 0.7282130718231201Training Epoch: 2 | iteration: 144/262 | Loss: 0.7616158127784729Training Epoch: 2 | iteration: 145/262 | Loss: 0.6902170777320862Training Epoch: 2 | iteration: 146/262 | Loss: 0.6224273443222046Training Epoch: 2 | iteration: 147/262 | Loss: 0.6779546141624451Training Epoch: 2 | iteration: 148/262 | Loss: 0.6641162037849426Training Epoch: 2 | iteration: 149/262 | Loss: 0.6816011071205139Training Epoch: 2 | iteration: 150/262 | Loss: 0.6738091707229614Training Epoch: 2 | iteration: 151/262 | Loss: 0.6941776871681213Training Epoch: 2 | iteration: 152/262 | Loss: 0.6625115871429443Training Epoch: 2 | iteration: 153/262 | Loss: 0.6801662445068359Training Epoch: 2 | iteration: 154/262 | Loss: 0.6932845711708069Training Epoch: 2 | iteration: 155/262 | Loss: 0.6902685165405273Training Epoch: 2 | iteration: 156/262 | Loss: 0.671621561050415Training Epoch: 2 | iteration: 157/262 | Loss: 0.682296633720398Training Epoch: 2 | iteration: 158/262 | Loss: 0.7088487148284912Training Epoch: 2 | iteration: 159/262 | Loss: 0.7172387838363647Training Epoch: 2 | iteration: 160/262 | Loss: 0.6528325080871582Training Epoch: 2 | iteration: 161/262 | Loss: 0.7890846729278564Training Epoch: 2 | iteration: 162/262 | Loss: 0.6757861971855164Training Epoch: 2 | iteration: 163/262 | Loss: 0.6799129247665405Training Epoch: 2 | iteration: 164/262 | Loss: 0.652518093585968Training Epoch: 2 | iteration: 165/262 | Loss: 0.6625493764877319Training Epoch: 2 | iteration: 166/262 | Loss: 0.7352116107940674Training Epoch: 2 | iteration: 167/262 | Loss: 0.616510808467865Training Epoch: 2 | iteration: 168/262 | Loss: 0.673228919506073Training Epoch: 2 | iteration: 169/262 | Loss: 0.6907103061676025Training Epoch: 2 | iteration: 170/262 | Loss: 0.7032389640808105Training Epoch: 2 | iteration: 171/262 | Loss: 0.6934771537780762Training Epoch: 2 | iteration: 172/262 | Loss: 0.6994226574897766Training Epoch: 2 | iteration: 173/262 | Loss: 0.668621838092804Training Epoch: 2 | iteration: 174/262 | Loss: 0.7080447673797607Training Epoch: 2 | iteration: 175/262 | Loss: 0.72635418176651Training Epoch: 2 | iteration: 176/262 | Loss: 0.7146553993225098Training Epoch: 2 | iteration: 177/262 | Loss: 0.6950149536132812Training Epoch: 2 | iteration: 178/262 | Loss: 0.6319301128387451Training Epoch: 2 | iteration: 179/262 | Loss: 0.6881893277168274Training Epoch: 2 | iteration: 180/262 | Loss: 0.6685527563095093Training Epoch: 2 | iteration: 181/262 | Loss: 0.7697597742080688Training Epoch: 2 | iteration: 182/262 | Loss: 0.6772105097770691Training Epoch: 2 | iteration: 183/262 | Loss: 0.6727118492126465Training Epoch: 2 | iteration: 184/262 | Loss: 0.6553177833557129Training Epoch: 2 | iteration: 185/262 | Loss: 0.6920089721679688Training Epoch: 2 | iteration: 186/262 | Loss: 0.629442572593689Training Epoch: 2 | iteration: 187/262 | Loss: 0.7246417999267578Training Epoch: 2 | iteration: 188/262 | Loss: 0.6975871324539185Training Epoch: 2 | iteration: 189/262 | Loss: 0.7343709468841553Training Epoch: 2 | iteration: 190/262 | Loss: 0.705479621887207Training Epoch: 2 | iteration: 191/262 | Loss: 0.6965996026992798Training Epoch: 2 | iteration: 192/262 | Loss: 0.6849302053451538Training Epoch: 2 | iteration: 193/262 | Loss: 0.6423676609992981Training Epoch: 2 | iteration: 194/262 | Loss: 0.6976929903030396Training Epoch: 2 | iteration: 195/262 | Loss: 0.6589521169662476Training Epoch: 2 | iteration: 196/262 | Loss: 0.7465869784355164Training Epoch: 2 | iteration: 197/262 | Loss: 0.6365466117858887Training Epoch: 2 | iteration: 198/262 | Loss: 0.6911505460739136Training Epoch: 2 | iteration: 199/262 | Loss: 0.6308379769325256Training Epoch: 2 | iteration: 200/262 | Loss: 0.767977237701416Training Epoch: 2 | iteration: 201/262 | Loss: 0.7263530492782593Training Epoch: 2 | iteration: 202/262 | Loss: 0.6287214756011963Training Epoch: 2 | iteration: 203/262 | Loss: 0.6624982357025146Training Epoch: 2 | iteration: 204/262 | Loss: 0.6791340112686157Training Epoch: 2 | iteration: 205/262 | Loss: 0.7076131105422974Training Epoch: 2 | iteration: 206/262 | Loss: 0.7116284370422363Training Epoch: 2 | iteration: 207/262 | Loss: 0.6846617460250854Training Epoch: 2 | iteration: 208/262 | Loss: 0.6494629383087158Training Epoch: 2 | iteration: 209/262 | Loss: 0.7522120475769043Training Epoch: 2 | iteration: 210/262 | Loss: 0.7381361722946167Training Epoch: 2 | iteration: 211/262 | Loss: 0.627954363822937Training Epoch: 2 | iteration: 212/262 | Loss: 0.7408121228218079Training Epoch: 2 | iteration: 213/262 | Loss: 0.6426896452903748Training Epoch: 2 | iteration: 214/262 | Loss: 0.6407836675643921Training Epoch: 2 | iteration: 215/262 | Loss: 0.6848410964012146Training Epoch: 2 | iteration: 216/262 | Loss: 0.6853312253952026Training Epoch: 2 | iteration: 217/262 | Loss: 0.6835607290267944Training Epoch: 2 | iteration: 218/262 | Loss: 0.7224146127700806Training Epoch: 2 | iteration: 219/262 | Loss: 0.7146546840667725Training Epoch: 2 | iteration: 220/262 | Loss: 0.6909189820289612Training Epoch: 2 | iteration: 221/262 | Loss: 0.7166754007339478Training Epoch: 2 | iteration: 222/262 | Loss: 0.7117948532104492Training Epoch: 2 | iteration: 223/262 | Loss: 0.6517834663391113Training Epoch: 2 | iteration: 224/262 | Loss: 0.683251142501831Training Epoch: 2 | iteration: 225/262 | Loss: 0.799776017665863Training Epoch: 2 | iteration: 226/262 | Loss: 0.6740691661834717Training Epoch: 2 | iteration: 227/262 | Loss: 0.6660743951797485Training Epoch: 2 | iteration: 228/262 | Loss: 0.6423994302749634Training Epoch: 2 | iteration: 229/262 | Loss: 0.6508182287216187Training Epoch: 2 | iteration: 230/262 | Loss: 0.7703101634979248Training Epoch: 2 | iteration: 231/262 | Loss: 0.7156486511230469Training Epoch: 2 | iteration: 232/262 | Loss: 0.6345279216766357Training Epoch: 2 | iteration: 233/262 | Loss: 0.6788913011550903Training Epoch: 2 | iteration: 234/262 | Loss: 0.679215669631958Training Epoch: 2 | iteration: 235/262 | Loss: 0.7038589715957642Training Epoch: 2 | iteration: 236/262 | Loss: 0.6226097941398621Training Epoch: 2 | iteration: 237/262 | Loss: 0.7630762457847595Training Epoch: 2 | iteration: 238/262 | Loss: 0.7190330028533936Training Epoch: 2 | iteration: 239/262 | Loss: 0.6895840764045715Training Epoch: 2 | iteration: 240/262 | Loss: 0.7467249631881714Training Epoch: 2 | iteration: 241/262 | Loss: 0.6959811449050903Training Epoch: 2 | iteration: 242/262 | Loss: 0.7012456059455872Training Epoch: 2 | iteration: 243/262 | Loss: 0.6795741319656372Training Epoch: 2 | iteration: 244/262 | Loss: 0.7089108228683472Training Epoch: 2 | iteration: 245/262 | Loss: 0.6831834316253662Training Epoch: 2 | iteration: 246/262 | Loss: 0.6774846315383911Training Epoch: 2 | iteration: 247/262 | Loss: 0.7632296085357666Training Epoch: 2 | iteration: 248/262 | Loss: 0.7141447067260742Training Epoch: 2 | iteration: 249/262 | Loss: 0.6672921180725098Training Epoch: 2 | iteration: 250/262 | Loss: 0.7049335241317749Training Epoch: 2 | iteration: 251/262 | Loss: 0.6996490955352783Training Epoch: 2 | iteration: 252/262 | Loss: 0.6367847919464111Training Epoch: 2 | iteration: 253/262 | Loss: 0.715399980545044Training Epoch: 2 | iteration: 254/262 | Loss: 0.7086758613586426Training Epoch: 2 | iteration: 255/262 | Loss: 0.7105787992477417Training Epoch: 2 | iteration: 256/262 | Loss: 0.6937986612319946Training Epoch: 2 | iteration: 257/262 | Loss: 0.6757556200027466Training Epoch: 2 | iteration: 258/262 | Loss: 0.6405982375144958Training Epoch: 2 | iteration: 259/262 | Loss: 0.6821804642677307Training Epoch: 2 | iteration: 260/262 | Loss: 0.681002140045166Training Epoch: 2 | iteration: 261/262 | Loss: 0.6567472219467163Validating Epoch: 2 | iteration: 0/66 | Loss: 0.6397117376327515Validating Epoch: 2 | iteration: 1/66 | Loss: 0.6250225305557251Validating Epoch: 2 | iteration: 2/66 | Loss: 0.614192545413971Validating Epoch: 2 | iteration: 3/66 | Loss: 0.6689023375511169Validating Epoch: 2 | iteration: 4/66 | Loss: 0.5610677003860474Validating Epoch: 2 | iteration: 5/66 | Loss: 0.6164454221725464Validating Epoch: 2 | iteration: 6/66 | Loss: 0.6052296161651611Validating Epoch: 2 | iteration: 7/66 | Loss: 0.6401195526123047Validating Epoch: 2 | iteration: 8/66 | Loss: 0.6289697885513306Validating Epoch: 2 | iteration: 9/66 | Loss: 0.5876510739326477Validating Epoch: 2 | iteration: 10/66 | Loss: 0.6132699251174927Validating Epoch: 2 | iteration: 11/66 | Loss: 0.6082872152328491Validating Epoch: 2 | iteration: 12/66 | Loss: 0.6489964723587036Validating Epoch: 2 | iteration: 13/66 | Loss: 0.6085168123245239Validating Epoch: 2 | iteration: 14/66 | Loss: 0.6319215297698975Validating Epoch: 2 | iteration: 15/66 | Loss: 0.6306802034378052Validating Epoch: 2 | iteration: 16/66 | Loss: 0.6705589294433594Validating Epoch: 2 | iteration: 17/66 | Loss: 0.6108062267303467Validating Epoch: 2 | iteration: 18/66 | Loss: 0.5970016717910767Validating Epoch: 2 | iteration: 19/66 | Loss: 0.6350032091140747Validating Epoch: 2 | iteration: 20/66 | Loss: 0.5961786508560181Validating Epoch: 2 | iteration: 21/66 | Loss: 0.5986940264701843Validating Epoch: 2 | iteration: 22/66 | Loss: 0.6093312501907349Validating Epoch: 2 | iteration: 23/66 | Loss: 0.6945565938949585Validating Epoch: 2 | iteration: 24/66 | Loss: 0.6096110343933105Validating Epoch: 2 | iteration: 25/66 | Loss: 0.6492698192596436Validating Epoch: 2 | iteration: 26/66 | Loss: 0.5965957641601562Validating Epoch: 2 | iteration: 27/66 | Loss: 0.6593379974365234Validating Epoch: 2 | iteration: 28/66 | Loss: 0.6165869235992432Validating Epoch: 2 | iteration: 29/66 | Loss: 0.5588260889053345Validating Epoch: 2 | iteration: 30/66 | Loss: 0.6230727434158325Validating Epoch: 2 | iteration: 31/66 | Loss: 0.6465975046157837Validating Epoch: 2 | iteration: 32/66 | Loss: 0.6278340816497803Validating Epoch: 2 | iteration: 33/66 | Loss: 0.6217002272605896Validating Epoch: 2 | iteration: 34/66 | Loss: 0.5770277976989746Validating Epoch: 2 | iteration: 35/66 | Loss: 0.6512209177017212Validating Epoch: 2 | iteration: 36/66 | Loss: 0.5985322594642639Validating Epoch: 2 | iteration: 37/66 | Loss: 0.6503428220748901Validating Epoch: 2 | iteration: 38/66 | Loss: 0.6078863143920898Validating Epoch: 2 | iteration: 39/66 | Loss: 0.5898263454437256Validating Epoch: 2 | iteration: 40/66 | Loss: 0.5605810880661011Validating Epoch: 2 | iteration: 41/66 | Loss: 0.6498712301254272Validating Epoch: 2 | iteration: 42/66 | Loss: 0.6869716048240662Validating Epoch: 2 | iteration: 43/66 | Loss: 0.5982399582862854Validating Epoch: 2 | iteration: 44/66 | Loss: 0.6155192852020264Validating Epoch: 2 | iteration: 45/66 | Loss: 0.583182692527771Validating Epoch: 2 | iteration: 46/66 | Loss: 0.6149839162826538Validating Epoch: 2 | iteration: 47/66 | Loss: 0.634270191192627Validating Epoch: 2 | iteration: 48/66 | Loss: 0.6565157175064087Validating Epoch: 2 | iteration: 49/66 | Loss: 0.6470618844032288Validating Epoch: 2 | iteration: 50/66 | Loss: 0.6221114993095398Validating Epoch: 2 | iteration: 51/66 | Loss: 0.5799886584281921Validating Epoch: 2 | iteration: 52/66 | Loss: 0.6042099595069885Validating Epoch: 2 | iteration: 53/66 | Loss: 0.6193945407867432Validating Epoch: 2 | iteration: 54/66 | Loss: 0.5819868445396423Validating Epoch: 2 | iteration: 55/66 | Loss: 0.6637868285179138Validating Epoch: 2 | iteration: 56/66 | Loss: 0.5633296370506287Validating Epoch: 2 | iteration: 57/66 | Loss: 0.6503075957298279Validating Epoch: 2 | iteration: 58/66 | Loss: 0.6236952543258667Validating Epoch: 2 | iteration: 59/66 | Loss: 0.6129111051559448Validating Epoch: 2 | iteration: 60/66 | Loss: 0.5856620073318481Validating Epoch: 2 | iteration: 61/66 | Loss: 0.5664730072021484Validating Epoch: 2 | iteration: 62/66 | Loss: 0.5805869102478027Validating Epoch: 2 | iteration: 63/66 | Loss: 0.599000096321106Validating Epoch: 2 | iteration: 64/66 | Loss: 0.6890594959259033Validating Epoch: 2 | iteration: 65/66 | Loss: 0.5948034524917603Temperature:  0.5
Sampling: iteration: 0/51{'Validity': 0.9873046875, 'Novelty': 1.0, 'Uniqueness': 0.9950544015825915}
Training Epoch: 3 | iteration: 0/262 | Loss: 0.621187686920166Training Epoch: 3 | iteration: 1/262 | Loss: 0.7127958536148071Training Epoch: 3 | iteration: 2/262 | Loss: 0.5918970108032227Training Epoch: 3 | iteration: 3/262 | Loss: 0.6686742305755615Training Epoch: 3 | iteration: 4/262 | Loss: 0.612409234046936Training Epoch: 3 | iteration: 5/262 | Loss: 0.7097589373588562Training Epoch: 3 | iteration: 6/262 | Loss: 0.6847147345542908Training Epoch: 3 | iteration: 7/262 | Loss: 0.5919910669326782Training Epoch: 3 | iteration: 8/262 | Loss: 0.7153927087783813Training Epoch: 3 | iteration: 9/262 | Loss: 0.6594209671020508Training Epoch: 3 | iteration: 10/262 | Loss: 0.6512809991836548Training Epoch: 3 | iteration: 11/262 | Loss: 0.742408037185669Training Epoch: 3 | iteration: 12/262 | Loss: 0.587785005569458Training Epoch: 3 | iteration: 13/262 | Loss: 0.6715813875198364Training Epoch: 3 | iteration: 14/262 | Loss: 0.6263110041618347Training Epoch: 3 | iteration: 15/262 | Loss: 0.6306827068328857Training Epoch: 3 | iteration: 16/262 | Loss: 0.6642835140228271Training Epoch: 3 | iteration: 17/262 | Loss: 0.6393018960952759Training Epoch: 3 | iteration: 18/262 | Loss: 0.6308530569076538Training Epoch: 3 | iteration: 19/262 | Loss: 0.6344544887542725Training Epoch: 3 | iteration: 20/262 | Loss: 0.6582245826721191Training Epoch: 3 | iteration: 21/262 | Loss: 0.7129197120666504Training Epoch: 3 | iteration: 22/262 | Loss: 0.6529312133789062Training Epoch: 3 | iteration: 23/262 | Loss: 0.679368793964386Training Epoch: 3 | iteration: 24/262 | Loss: 0.6899678111076355Training Epoch: 3 | iteration: 25/262 | Loss: 0.7319286465644836Training Epoch: 3 | iteration: 26/262 | Loss: 0.6645475625991821Training Epoch: 3 | iteration: 27/262 | Loss: 0.6698216199874878Training Epoch: 3 | iteration: 28/262 | Loss: 0.6439117193222046Training Epoch: 3 | iteration: 29/262 | Loss: 0.6379820108413696Training Epoch: 3 | iteration: 30/262 | Loss: 0.628440797328949Training Epoch: 3 | iteration: 31/262 | Loss: 0.661016583442688Training Epoch: 3 | iteration: 32/262 | Loss: 0.6512154340744019Training Epoch: 3 | iteration: 33/262 | Loss: 0.6534494757652283Training Epoch: 3 | iteration: 34/262 | Loss: 0.7353854179382324Training Epoch: 3 | iteration: 35/262 | Loss: 0.6908072233200073Training Epoch: 3 | iteration: 36/262 | Loss: 0.6616939306259155Training Epoch: 3 | iteration: 37/262 | Loss: 0.615689754486084Training Epoch: 3 | iteration: 38/262 | Loss: 0.6619112491607666Training Epoch: 3 | iteration: 39/262 | Loss: 0.6624561548233032Training Epoch: 3 | iteration: 40/262 | Loss: 0.6802363991737366Training Epoch: 3 | iteration: 41/262 | Loss: 0.7094390392303467Training Epoch: 3 | iteration: 42/262 | Loss: 0.5968008041381836Training Epoch: 3 | iteration: 43/262 | Loss: 0.6585720777511597Training Epoch: 3 | iteration: 44/262 | Loss: 0.647727906703949Training Epoch: 3 | iteration: 45/262 | Loss: 0.6432669758796692Training Epoch: 3 | iteration: 46/262 | Loss: 0.6835458278656006Training Epoch: 3 | iteration: 47/262 | Loss: 0.7360169887542725Training Epoch: 3 | iteration: 48/262 | Loss: 0.6536914706230164Training Epoch: 3 | iteration: 49/262 | Loss: 0.6637576818466187Training Epoch: 3 | iteration: 50/262 | Loss: 0.7913078665733337Training Epoch: 3 | iteration: 51/262 | Loss: 0.6459476947784424Training Epoch: 3 | iteration: 52/262 | Loss: 0.6889643669128418Training Epoch: 3 | iteration: 53/262 | Loss: 0.6967394948005676Training Epoch: 3 | iteration: 54/262 | Loss: 0.6465495228767395Training Epoch: 3 | iteration: 55/262 | Loss: 0.6514788269996643Training Epoch: 3 | iteration: 56/262 | Loss: 0.6255381107330322Training Epoch: 3 | iteration: 57/262 | Loss: 0.7008564472198486Training Epoch: 3 | iteration: 58/262 | Loss: 0.6400130391120911Training Epoch: 3 | iteration: 59/262 | Loss: 0.623501181602478Training Epoch: 3 | iteration: 60/262 | Loss: 0.7165979146957397Training Epoch: 3 | iteration: 61/262 | Loss: 0.6316274404525757Training Epoch: 3 | iteration: 62/262 | Loss: 0.6718514561653137Training Epoch: 3 | iteration: 63/262 | Loss: 0.7353916168212891Training Epoch: 3 | iteration: 64/262 | Loss: 0.6704337000846863Training Epoch: 3 | iteration: 65/262 | Loss: 0.6617483496665955Training Epoch: 3 | iteration: 66/262 | Loss: 0.6639894247055054Training Epoch: 3 | iteration: 67/262 | Loss: 0.645660400390625Training Epoch: 3 | iteration: 68/262 | Loss: 0.7237236499786377Training Epoch: 3 | iteration: 69/262 | Loss: 0.6676552891731262Training Epoch: 3 | iteration: 70/262 | Loss: 0.6321684122085571Training Epoch: 3 | iteration: 71/262 | Loss: 0.6643033027648926Training Epoch: 3 | iteration: 72/262 | Loss: 0.6554614901542664Training Epoch: 3 | iteration: 73/262 | Loss: 0.6877003908157349Training Epoch: 3 | iteration: 74/262 | Loss: 0.6316930055618286Training Epoch: 3 | iteration: 75/262 | Loss: 0.6265047192573547Training Epoch: 3 | iteration: 76/262 | Loss: 0.6997839212417603Training Epoch: 3 | iteration: 77/262 | Loss: 0.5854002237319946Training Epoch: 3 | iteration: 78/262 | Loss: 0.6383213996887207Training Epoch: 3 | iteration: 79/262 | Loss: 0.6376432776451111Training Epoch: 3 | iteration: 80/262 | Loss: 0.6554878950119019Training Epoch: 3 | iteration: 81/262 | Loss: 0.7139750123023987Training Epoch: 3 | iteration: 82/262 | Loss: 0.5955648422241211Training Epoch: 3 | iteration: 83/262 | Loss: 0.716174840927124Training Epoch: 3 | iteration: 84/262 | Loss: 0.5913212299346924Training Epoch: 3 | iteration: 85/262 | Loss: 0.7101020812988281Training Epoch: 3 | iteration: 86/262 | Loss: 0.6341298222541809Training Epoch: 3 | iteration: 87/262 | Loss: 0.6178465485572815Training Epoch: 3 | iteration: 88/262 | Loss: 0.7292163968086243Training Epoch: 3 | iteration: 89/262 | Loss: 0.6171067357063293Training Epoch: 3 | iteration: 90/262 | Loss: 0.6640114188194275Training Epoch: 3 | iteration: 91/262 | Loss: 0.7503007054328918Training Epoch: 3 | iteration: 92/262 | Loss: 0.6340439319610596Training Epoch: 3 | iteration: 93/262 | Loss: 0.6819347143173218Training Epoch: 3 | iteration: 94/262 | Loss: 0.6781719923019409Training Epoch: 3 | iteration: 95/262 | Loss: 0.6246362924575806Training Epoch: 3 | iteration: 96/262 | Loss: 0.6235504746437073Training Epoch: 3 | iteration: 97/262 | Loss: 0.7432965040206909Training Epoch: 3 | iteration: 98/262 | Loss: 0.7557957768440247Training Epoch: 3 | iteration: 99/262 | Loss: 0.6281047463417053Training Epoch: 3 | iteration: 100/262 | Loss: 0.6131157875061035Training Epoch: 3 | iteration: 101/262 | Loss: 0.691804051399231Training Epoch: 3 | iteration: 102/262 | Loss: 0.6734707355499268Training Epoch: 3 | iteration: 103/262 | Loss: 0.625330924987793Training Epoch: 3 | iteration: 104/262 | Loss: 0.6381410360336304Training Epoch: 3 | iteration: 105/262 | Loss: 0.6463494300842285Training Epoch: 3 | iteration: 106/262 | Loss: 0.5965176820755005Training Epoch: 3 | iteration: 107/262 | Loss: 0.6689911484718323Training Epoch: 3 | iteration: 108/262 | Loss: 0.6218533515930176Training Epoch: 3 | iteration: 109/262 | Loss: 0.7134568691253662Training Epoch: 3 | iteration: 110/262 | Loss: 0.6447415351867676Training Epoch: 3 | iteration: 111/262 | Loss: 0.7470184564590454Training Epoch: 3 | iteration: 112/262 | Loss: 0.6874254941940308Training Epoch: 3 | iteration: 113/262 | Loss: 0.6682271361351013Training Epoch: 3 | iteration: 114/262 | Loss: 0.7184152603149414Training Epoch: 3 | iteration: 115/262 | Loss: 0.6762076020240784Training Epoch: 3 | iteration: 116/262 | Loss: 0.6886597871780396Training Epoch: 3 | iteration: 117/262 | Loss: 0.7081894278526306Training Epoch: 3 | iteration: 118/262 | Loss: 0.6817492246627808Training Epoch: 3 | iteration: 119/262 | Loss: 0.6132193207740784Training Epoch: 3 | iteration: 120/262 | Loss: 0.6555551290512085Training Epoch: 3 | iteration: 121/262 | Loss: 0.773574948310852Training Epoch: 3 | iteration: 122/262 | Loss: 0.5735740661621094Training Epoch: 3 | iteration: 123/262 | Loss: 0.5897917747497559Training Epoch: 3 | iteration: 124/262 | Loss: 0.593509316444397Training Epoch: 3 | iteration: 125/262 | Loss: 0.606113076210022Training Epoch: 3 | iteration: 126/262 | Loss: 0.7228826880455017Training Epoch: 3 | iteration: 127/262 | Loss: 0.6350871920585632Training Epoch: 3 | iteration: 128/262 | Loss: 0.6682085990905762Training Epoch: 3 | iteration: 129/262 | Loss: 0.6618685722351074Training Epoch: 3 | iteration: 130/262 | Loss: 0.6852511167526245Training Epoch: 3 | iteration: 131/262 | Loss: 0.6418976783752441Training Epoch: 3 | iteration: 132/262 | Loss: 0.6618896722793579Training Epoch: 3 | iteration: 133/262 | Loss: 0.663334846496582Training Epoch: 3 | iteration: 134/262 | Loss: 0.6543986797332764Training Epoch: 3 | iteration: 135/262 | Loss: 0.6938854455947876Training Epoch: 3 | iteration: 136/262 | Loss: 0.6902865767478943Training Epoch: 3 | iteration: 137/262 | Loss: 0.6654831171035767Training Epoch: 3 | iteration: 138/262 | Loss: 0.7123336791992188Training Epoch: 3 | iteration: 139/262 | Loss: 0.6454704999923706Training Epoch: 3 | iteration: 140/262 | Loss: 0.6583442091941833Training Epoch: 3 | iteration: 141/262 | Loss: 0.6740921139717102Training Epoch: 3 | iteration: 142/262 | Loss: 0.7260041832923889Training Epoch: 3 | iteration: 143/262 | Loss: 0.7768569588661194Training Epoch: 3 | iteration: 144/262 | Loss: 0.6229720115661621Training Epoch: 3 | iteration: 145/262 | Loss: 0.630208432674408Training Epoch: 3 | iteration: 146/262 | Loss: 0.6224511861801147Training Epoch: 3 | iteration: 147/262 | Loss: 0.6403601169586182Training Epoch: 3 | iteration: 148/262 | Loss: 0.7003857493400574Training Epoch: 3 | iteration: 149/262 | Loss: 0.6953836679458618Training Epoch: 3 | iteration: 150/262 | Loss: 0.684685230255127Training Epoch: 3 | iteration: 151/262 | Loss: 0.6557016372680664Training Epoch: 3 | iteration: 152/262 | Loss: 0.6149747371673584Training Epoch: 3 | iteration: 153/262 | Loss: 0.6833031177520752Training Epoch: 3 | iteration: 154/262 | Loss: 0.6003067493438721Training Epoch: 3 | iteration: 155/262 | Loss: 0.7457842230796814Training Epoch: 3 | iteration: 156/262 | Loss: 0.6267430186271667Training Epoch: 3 | iteration: 157/262 | Loss: 0.6440754532814026Training Epoch: 3 | iteration: 158/262 | Loss: 0.6645947098731995Training Epoch: 3 | iteration: 159/262 | Loss: 0.5779314041137695Training Epoch: 3 | iteration: 160/262 | Loss: 0.6400519609451294Training Epoch: 3 | iteration: 161/262 | Loss: 0.6164498329162598Training Epoch: 3 | iteration: 162/262 | Loss: 0.7059789896011353Training Epoch: 3 | iteration: 163/262 | Loss: 0.6480503678321838Training Epoch: 3 | iteration: 164/262 | Loss: 0.7047606706619263Training Epoch: 3 | iteration: 165/262 | Loss: 0.693824052810669Training Epoch: 3 | iteration: 166/262 | Loss: 0.6259387731552124Training Epoch: 3 | iteration: 167/262 | Loss: 0.719570517539978Training Epoch: 3 | iteration: 168/262 | Loss: 0.6606394052505493Training Epoch: 3 | iteration: 169/262 | Loss: 0.7049298286437988Training Epoch: 3 | iteration: 170/262 | Loss: 0.6842381954193115Training Epoch: 3 | iteration: 171/262 | Loss: 0.7066642045974731Training Epoch: 3 | iteration: 172/262 | Loss: 0.7262657880783081Training Epoch: 3 | iteration: 173/262 | Loss: 0.6056299209594727Training Epoch: 3 | iteration: 174/262 | Loss: 0.6603529453277588Training Epoch: 3 | iteration: 175/262 | Loss: 0.6243845224380493Training Epoch: 3 | iteration: 176/262 | Loss: 0.6483385562896729Training Epoch: 3 | iteration: 177/262 | Loss: 0.6301815509796143Training Epoch: 3 | iteration: 178/262 | Loss: 0.6576369404792786Training Epoch: 3 | iteration: 179/262 | Loss: 0.5858814120292664Training Epoch: 3 | iteration: 180/262 | Loss: 0.6699620485305786Training Epoch: 3 | iteration: 181/262 | Loss: 0.6443489789962769Training Epoch: 3 | iteration: 182/262 | Loss: 0.6785843372344971Training Epoch: 3 | iteration: 183/262 | Loss: 0.6529741287231445Training Epoch: 3 | iteration: 184/262 | Loss: 0.6279034614562988Training Epoch: 3 | iteration: 185/262 | Loss: 0.6300884485244751Training Epoch: 3 | iteration: 186/262 | Loss: 0.6978451609611511Training Epoch: 3 | iteration: 187/262 | Loss: 0.6961986422538757Training Epoch: 3 | iteration: 188/262 | Loss: 0.6406460404396057Training Epoch: 3 | iteration: 189/262 | Loss: 0.67473304271698Training Epoch: 3 | iteration: 190/262 | Loss: 0.6748284697532654Training Epoch: 3 | iteration: 191/262 | Loss: 0.6512962579727173Training Epoch: 3 | iteration: 192/262 | Loss: 0.7058435082435608Training Epoch: 3 | iteration: 193/262 | Loss: 0.6838196516036987Training Epoch: 3 | iteration: 194/262 | Loss: 0.6809545755386353Training Epoch: 3 | iteration: 195/262 | Loss: 0.6984508037567139Training Epoch: 3 | iteration: 196/262 | Loss: 0.6629840135574341Training Epoch: 3 | iteration: 197/262 | Loss: 0.6576244235038757Training Epoch: 3 | iteration: 198/262 | Loss: 0.6228742599487305Training Epoch: 3 | iteration: 199/262 | Loss: 0.7179133892059326Training Epoch: 3 | iteration: 200/262 | Loss: 0.6028361916542053Training Epoch: 3 | iteration: 201/262 | Loss: 0.5777862071990967Training Epoch: 3 | iteration: 202/262 | Loss: 0.614303469657898Training Epoch: 3 | iteration: 203/262 | Loss: 0.6746339797973633Training Epoch: 3 | iteration: 204/262 | Loss: 0.6431775093078613Training Epoch: 3 | iteration: 205/262 | Loss: 0.6641913056373596Training Epoch: 3 | iteration: 206/262 | Loss: 0.6096243262290955Training Epoch: 3 | iteration: 207/262 | Loss: 0.6715391874313354Training Epoch: 3 | iteration: 208/262 | Loss: 0.6369143128395081Training Epoch: 3 | iteration: 209/262 | Loss: 0.6776853799819946Training Epoch: 3 | iteration: 210/262 | Loss: 0.6300192475318909Training Epoch: 3 | iteration: 211/262 | Loss: 0.6316134929656982Training Epoch: 3 | iteration: 212/262 | Loss: 0.6821264624595642Training Epoch: 3 | iteration: 213/262 | Loss: 0.6953608989715576Training Epoch: 3 | iteration: 214/262 | Loss: 0.6859121322631836Training Epoch: 3 | iteration: 215/262 | Loss: 0.6977352499961853Training Epoch: 3 | iteration: 216/262 | Loss: 0.6344848275184631Training Epoch: 3 | iteration: 217/262 | Loss: 0.6999136805534363Training Epoch: 3 | iteration: 218/262 | Loss: 0.6250745058059692Training Epoch: 3 | iteration: 219/262 | Loss: 0.6538774967193604Training Epoch: 3 | iteration: 220/262 | Loss: 0.7005126476287842Training Epoch: 3 | iteration: 221/262 | Loss: 0.633379340171814Training Epoch: 3 | iteration: 222/262 | Loss: 0.688373327255249Training Epoch: 3 | iteration: 223/262 | Loss: 0.6512051820755005Training Epoch: 3 | iteration: 224/262 | Loss: 0.6324265003204346Training Epoch: 3 | iteration: 225/262 | Loss: 0.6469074487686157Training Epoch: 3 | iteration: 226/262 | Loss: 0.5704253315925598Training Epoch: 3 | iteration: 227/262 | Loss: 0.6782569289207458Training Epoch: 3 | iteration: 228/262 | Loss: 0.6529078483581543Training Epoch: 3 | iteration: 229/262 | Loss: 0.6294233798980713Training Epoch: 3 | iteration: 230/262 | Loss: 0.6390914916992188Training Epoch: 3 | iteration: 231/262 | Loss: 0.7232847809791565Training Epoch: 3 | iteration: 232/262 | Loss: 0.6009742021560669Training Epoch: 3 | iteration: 233/262 | Loss: 0.6606031656265259Training Epoch: 3 | iteration: 234/262 | Loss: 0.6590238809585571Training Epoch: 3 | iteration: 235/262 | Loss: 0.639216959476471Training Epoch: 3 | iteration: 236/262 | Loss: 0.5879580974578857Training Epoch: 3 | iteration: 237/262 | Loss: 0.6821857690811157Training Epoch: 3 | iteration: 238/262 | Loss: 0.6985165476799011Training Epoch: 3 | iteration: 239/262 | Loss: 0.6818088889122009Training Epoch: 3 | iteration: 240/262 | Loss: 0.6098450422286987Training Epoch: 3 | iteration: 241/262 | Loss: 0.717801570892334Training Epoch: 3 | iteration: 242/262 | Loss: 0.6604466438293457Training Epoch: 3 | iteration: 243/262 | Loss: 0.7213730812072754Training Epoch: 3 | iteration: 244/262 | Loss: 0.6648494005203247Training Epoch: 3 | iteration: 245/262 | Loss: 0.7470768690109253Training Epoch: 3 | iteration: 246/262 | Loss: 0.6789981126785278Training Epoch: 3 | iteration: 247/262 | Loss: 0.6635739803314209Training Epoch: 3 | iteration: 248/262 | Loss: 0.7270998954772949Training Epoch: 3 | iteration: 249/262 | Loss: 0.6617093086242676Training Epoch: 3 | iteration: 250/262 | Loss: 0.6705266237258911Training Epoch: 3 | iteration: 251/262 | Loss: 0.6450815200805664Training Epoch: 3 | iteration: 252/262 | Loss: 0.6850188970565796Training Epoch: 3 | iteration: 253/262 | Loss: 0.6537114381790161Training Epoch: 3 | iteration: 254/262 | Loss: 0.6374228000640869Training Epoch: 3 | iteration: 255/262 | Loss: 0.6290692090988159Training Epoch: 3 | iteration: 256/262 | Loss: 0.6835122108459473Training Epoch: 3 | iteration: 257/262 | Loss: 0.637576699256897Training Epoch: 3 | iteration: 258/262 | Loss: 0.6671973466873169Training Epoch: 3 | iteration: 259/262 | Loss: 0.7360341548919678Training Epoch: 3 | iteration: 260/262 | Loss: 0.6732192039489746Training Epoch: 3 | iteration: 261/262 | Loss: 0.7091543674468994Validating Epoch: 3 | iteration: 0/66 | Loss: 0.6086834669113159Validating Epoch: 3 | iteration: 1/66 | Loss: 0.64170241355896Validating Epoch: 3 | iteration: 2/66 | Loss: 0.5707915425300598Validating Epoch: 3 | iteration: 3/66 | Loss: 0.6164213418960571Validating Epoch: 3 | iteration: 4/66 | Loss: 0.6042147874832153Validating Epoch: 3 | iteration: 5/66 | Loss: 0.5937390327453613Validating Epoch: 3 | iteration: 6/66 | Loss: 0.6165633201599121Validating Epoch: 3 | iteration: 7/66 | Loss: 0.5415533781051636Validating Epoch: 3 | iteration: 8/66 | Loss: 0.6516368985176086Validating Epoch: 3 | iteration: 9/66 | Loss: 0.6453737616539001Validating Epoch: 3 | iteration: 10/66 | Loss: 0.6360751390457153Validating Epoch: 3 | iteration: 11/66 | Loss: 0.5686983466148376Validating Epoch: 3 | iteration: 12/66 | Loss: 0.6203036904335022Validating Epoch: 3 | iteration: 13/66 | Loss: 0.6562718749046326Validating Epoch: 3 | iteration: 14/66 | Loss: 0.618677020072937Validating Epoch: 3 | iteration: 15/66 | Loss: 0.6232963800430298Validating Epoch: 3 | iteration: 16/66 | Loss: 0.6290144920349121Validating Epoch: 3 | iteration: 17/66 | Loss: 0.6237834692001343Validating Epoch: 3 | iteration: 18/66 | Loss: 0.6179686188697815Validating Epoch: 3 | iteration: 19/66 | Loss: 0.6044660210609436Validating Epoch: 3 | iteration: 20/66 | Loss: 0.6108934879302979Validating Epoch: 3 | iteration: 21/66 | Loss: 0.5790154337882996Validating Epoch: 3 | iteration: 22/66 | Loss: 0.6998443007469177Validating Epoch: 3 | iteration: 23/66 | Loss: 0.609738826751709Validating Epoch: 3 | iteration: 24/66 | Loss: 0.7012225389480591Validating Epoch: 3 | iteration: 25/66 | Loss: 0.5983850955963135Validating Epoch: 3 | iteration: 26/66 | Loss: 0.588466465473175Validating Epoch: 3 | iteration: 27/66 | Loss: 0.5691922903060913Validating Epoch: 3 | iteration: 28/66 | Loss: 0.6389761567115784Validating Epoch: 3 | iteration: 29/66 | Loss: 0.5548685193061829Validating Epoch: 3 | iteration: 30/66 | Loss: 0.6356369853019714Validating Epoch: 3 | iteration: 31/66 | Loss: 0.6400212645530701Validating Epoch: 3 | iteration: 32/66 | Loss: 0.5925093293190002Validating Epoch: 3 | iteration: 33/66 | Loss: 0.6024690866470337Validating Epoch: 3 | iteration: 34/66 | Loss: 0.5531144142150879Validating Epoch: 3 | iteration: 35/66 | Loss: 0.6036536693572998Validating Epoch: 3 | iteration: 36/66 | Loss: 0.641965389251709Validating Epoch: 3 | iteration: 37/66 | Loss: 0.5577377676963806Validating Epoch: 3 | iteration: 38/66 | Loss: 0.595963180065155Validating Epoch: 3 | iteration: 39/66 | Loss: 0.6001491546630859Validating Epoch: 3 | iteration: 40/66 | Loss: 0.565205454826355Validating Epoch: 3 | iteration: 41/66 | Loss: 0.5641431212425232Validating Epoch: 3 | iteration: 42/66 | Loss: 0.6487345695495605Validating Epoch: 3 | iteration: 43/66 | Loss: 0.5804572105407715Validating Epoch: 3 | iteration: 44/66 | Loss: 0.5889808535575867Validating Epoch: 3 | iteration: 45/66 | Loss: 0.645293116569519Validating Epoch: 3 | iteration: 46/66 | Loss: 0.583962619304657Validating Epoch: 3 | iteration: 47/66 | Loss: 0.5959967970848083Validating Epoch: 3 | iteration: 48/66 | Loss: 0.5843714475631714Validating Epoch: 3 | iteration: 49/66 | Loss: 0.5841111540794373Validating Epoch: 3 | iteration: 50/66 | Loss: 0.6462841033935547Validating Epoch: 3 | iteration: 51/66 | Loss: 0.6160194873809814Validating Epoch: 3 | iteration: 52/66 | Loss: 0.6461207866668701Validating Epoch: 3 | iteration: 53/66 | Loss: 0.5875487327575684Validating Epoch: 3 | iteration: 54/66 | Loss: 0.6146015524864197Validating Epoch: 3 | iteration: 55/66 | Loss: 0.6101651191711426Validating Epoch: 3 | iteration: 56/66 | Loss: 0.6335740089416504Validating Epoch: 3 | iteration: 57/66 | Loss: 0.6360103487968445Validating Epoch: 3 | iteration: 58/66 | Loss: 0.5888168811798096Validating Epoch: 3 | iteration: 59/66 | Loss: 0.5960551500320435Validating Epoch: 3 | iteration: 60/66 | Loss: 0.6502029895782471Validating Epoch: 3 | iteration: 61/66 | Loss: 0.5736407041549683Validating Epoch: 3 | iteration: 62/66 | Loss: 0.6117942333221436Validating Epoch: 3 | iteration: 63/66 | Loss: 0.5721884965896606Validating Epoch: 3 | iteration: 64/66 | Loss: 0.5736185312271118Validating Epoch: 3 | iteration: 65/66 | Loss: 0.5953547358512878Temperature:  0.5
Sampling: iteration: 0/51{'Validity': 0.9921875, 'Novelty': 1.0, 'Uniqueness': 0.9901574803149606}
Training Epoch: 4 | iteration: 0/262 | Loss: 0.6120737791061401Training Epoch: 4 | iteration: 1/262 | Loss: 0.6540729999542236Training Epoch: 4 | iteration: 2/262 | Loss: 0.612347424030304Training Epoch: 4 | iteration: 3/262 | Loss: 0.759558916091919Training Epoch: 4 | iteration: 4/262 | Loss: 0.6268643736839294Training Epoch: 4 | iteration: 5/262 | Loss: 0.6368845701217651Training Epoch: 4 | iteration: 6/262 | Loss: 0.6949354410171509Training Epoch: 4 | iteration: 7/262 | Loss: 0.6040832996368408Training Epoch: 4 | iteration: 8/262 | Loss: 0.6002002358436584Training Epoch: 4 | iteration: 9/262 | Loss: 0.7093546986579895Training Epoch: 4 | iteration: 10/262 | Loss: 0.612963080406189Training Epoch: 4 | iteration: 11/262 | Loss: 0.6531628966331482Training Epoch: 4 | iteration: 12/262 | Loss: 0.612766444683075Training Epoch: 4 | iteration: 13/262 | Loss: 0.7202037572860718Training Epoch: 4 | iteration: 14/262 | Loss: 0.5998947620391846Training Epoch: 4 | iteration: 15/262 | Loss: 0.6353994607925415Training Epoch: 4 | iteration: 16/262 | Loss: 0.6131725311279297Training Epoch: 4 | iteration: 17/262 | Loss: 0.5729638338088989Training Epoch: 4 | iteration: 18/262 | Loss: 0.6454966068267822Training Epoch: 4 | iteration: 19/262 | Loss: 0.6150786280632019Training Epoch: 4 | iteration: 20/262 | Loss: 0.6280363202095032Training Epoch: 4 | iteration: 21/262 | Loss: 0.6251568794250488Training Epoch: 4 | iteration: 22/262 | Loss: 0.654956579208374Training Epoch: 4 | iteration: 23/262 | Loss: 0.5981354117393494Training Epoch: 4 | iteration: 24/262 | Loss: 0.6860523819923401Training Epoch: 4 | iteration: 25/262 | Loss: 0.648926317691803Training Epoch: 4 | iteration: 26/262 | Loss: 0.6261752843856812Training Epoch: 4 | iteration: 27/262 | Loss: 0.6254903078079224Training Epoch: 4 | iteration: 28/262 | Loss: 0.6264278292655945Training Epoch: 4 | iteration: 29/262 | Loss: 0.655272901058197Training Epoch: 4 | iteration: 30/262 | Loss: 0.5980405807495117Training Epoch: 4 | iteration: 31/262 | Loss: 0.6031239032745361Training Epoch: 4 | iteration: 32/262 | Loss: 0.6482999324798584Training Epoch: 4 | iteration: 33/262 | Loss: 0.6252864599227905Training Epoch: 4 | iteration: 34/262 | Loss: 0.6374836564064026Training Epoch: 4 | iteration: 35/262 | Loss: 0.6833987236022949Training Epoch: 4 | iteration: 36/262 | Loss: 0.6502640247344971Training Epoch: 4 | iteration: 37/262 | Loss: 0.6720083355903625Training Epoch: 4 | iteration: 38/262 | Loss: 0.6003546714782715Training Epoch: 4 | iteration: 39/262 | Loss: 0.6388370990753174Training Epoch: 4 | iteration: 40/262 | Loss: 0.675198495388031Training Epoch: 4 | iteration: 41/262 | Loss: 0.6745191812515259Training Epoch: 4 | iteration: 42/262 | Loss: 0.5944328904151917Training Epoch: 4 | iteration: 43/262 | Loss: 0.630583643913269Training Epoch: 4 | iteration: 44/262 | Loss: 0.5758507251739502Training Epoch: 4 | iteration: 45/262 | Loss: 0.5764971971511841Training Epoch: 4 | iteration: 46/262 | Loss: 0.604674220085144Training Epoch: 4 | iteration: 47/262 | Loss: 0.5970770716667175Training Epoch: 4 | iteration: 48/262 | Loss: 0.641059398651123Training Epoch: 4 | iteration: 49/262 | Loss: 0.6850008964538574Training Epoch: 4 | iteration: 50/262 | Loss: 0.6338679790496826Training Epoch: 4 | iteration: 51/262 | Loss: 0.6407155394554138Training Epoch: 4 | iteration: 52/262 | Loss: 0.6786749362945557Training Epoch: 4 | iteration: 53/262 | Loss: 0.6283776760101318Training Epoch: 4 | iteration: 54/262 | Loss: 0.6093369722366333Training Epoch: 4 | iteration: 55/262 | Loss: 0.6258258819580078Training Epoch: 4 | iteration: 56/262 | Loss: 0.6440669298171997Training Epoch: 4 | iteration: 57/262 | Loss: 0.6623733043670654Training Epoch: 4 | iteration: 58/262 | Loss: 0.601389467716217Training Epoch: 4 | iteration: 59/262 | Loss: 0.6013281345367432Training Epoch: 4 | iteration: 60/262 | Loss: 0.6750732064247131Training Epoch: 4 | iteration: 61/262 | Loss: 0.6271979212760925Training Epoch: 4 | iteration: 62/262 | Loss: 0.6608365774154663Training Epoch: 4 | iteration: 63/262 | Loss: 0.6391379833221436Training Epoch: 4 | iteration: 64/262 | Loss: 0.5950090885162354Training Epoch: 4 | iteration: 65/262 | Loss: 0.6806586980819702Training Epoch: 4 | iteration: 66/262 | Loss: 0.6667834520339966Training Epoch: 4 | iteration: 67/262 | Loss: 0.6381792426109314Training Epoch: 4 | iteration: 68/262 | Loss: 0.7143149375915527Training Epoch: 4 | iteration: 69/262 | Loss: 0.6631410717964172Training Epoch: 4 | iteration: 70/262 | Loss: 0.5872431993484497Training Epoch: 4 | iteration: 71/262 | Loss: 0.5818415284156799Training Epoch: 4 | iteration: 72/262 | Loss: 0.6900703310966492Training Epoch: 4 | iteration: 73/262 | Loss: 0.6340471506118774Training Epoch: 4 | iteration: 74/262 | Loss: 0.6217448711395264Training Epoch: 4 | iteration: 75/262 | Loss: 0.6172014474868774Training Epoch: 4 | iteration: 76/262 | Loss: 0.6624864339828491Training Epoch: 4 | iteration: 77/262 | Loss: 0.6890673637390137Training Epoch: 4 | iteration: 78/262 | Loss: 0.6644362211227417Training Epoch: 4 | iteration: 79/262 | Loss: 0.6791766881942749Training Epoch: 4 | iteration: 80/262 | Loss: 0.6564890146255493Training Epoch: 4 | iteration: 81/262 | Loss: 0.6372215747833252Training Epoch: 4 | iteration: 82/262 | Loss: 0.6580702066421509Training Epoch: 4 | iteration: 83/262 | Loss: 0.7494492530822754Training Epoch: 4 | iteration: 84/262 | Loss: 0.6185833215713501Training Epoch: 4 | iteration: 85/262 | Loss: 0.6157554388046265Training Epoch: 4 | iteration: 86/262 | Loss: 0.632845938205719Training Epoch: 4 | iteration: 87/262 | Loss: 0.5936784744262695Training Epoch: 4 | iteration: 88/262 | Loss: 0.6104763746261597Training Epoch: 4 | iteration: 89/262 | Loss: 0.6403963565826416Training Epoch: 4 | iteration: 90/262 | Loss: 0.6662742495536804Training Epoch: 4 | iteration: 91/262 | Loss: 0.7625056505203247Training Epoch: 4 | iteration: 92/262 | Loss: 0.6803901195526123Training Epoch: 4 | iteration: 93/262 | Loss: 0.6815707683563232Training Epoch: 4 | iteration: 94/262 | Loss: 0.6022319793701172Training Epoch: 4 | iteration: 95/262 | Loss: 0.6653094291687012Training Epoch: 4 | iteration: 96/262 | Loss: 0.5580277442932129Training Epoch: 4 | iteration: 97/262 | Loss: 0.6017756462097168Training Epoch: 4 | iteration: 98/262 | Loss: 0.601496160030365Training Epoch: 4 | iteration: 99/262 | Loss: 0.5746545195579529Training Epoch: 4 | iteration: 100/262 | Loss: 0.6433517932891846Training Epoch: 4 | iteration: 101/262 | Loss: 0.6045485734939575Training Epoch: 4 | iteration: 102/262 | Loss: 0.6518237590789795Training Epoch: 4 | iteration: 103/262 | Loss: 0.6154471635818481Training Epoch: 4 | iteration: 104/262 | Loss: 0.6041441559791565Training Epoch: 4 | iteration: 105/262 | Loss: 0.6775689125061035Training Epoch: 4 | iteration: 106/262 | Loss: 0.6518040895462036Training Epoch: 4 | iteration: 107/262 | Loss: 0.6563621759414673Training Epoch: 4 | iteration: 108/262 | Loss: 0.6081801652908325Training Epoch: 4 | iteration: 109/262 | Loss: 0.6190794706344604Training Epoch: 4 | iteration: 110/262 | Loss: 0.7367961406707764Training Epoch: 4 | iteration: 111/262 | Loss: 0.6239006519317627Training Epoch: 4 | iteration: 112/262 | Loss: 0.6405354142189026Training Epoch: 4 | iteration: 113/262 | Loss: 0.646844208240509Training Epoch: 4 | iteration: 114/262 | Loss: 0.6626479625701904Training Epoch: 4 | iteration: 115/262 | Loss: 0.6596461534500122Training Epoch: 4 | iteration: 116/262 | Loss: 0.6072102189064026Training Epoch: 4 | iteration: 117/262 | Loss: 0.6563138961791992Training Epoch: 4 | iteration: 118/262 | Loss: 0.6807661056518555Training Epoch: 4 | iteration: 119/262 | Loss: 0.5932066440582275Training Epoch: 4 | iteration: 120/262 | Loss: 0.6363506317138672Training Epoch: 4 | iteration: 121/262 | Loss: 0.6131096482276917Training Epoch: 4 | iteration: 122/262 | Loss: 0.6222423315048218Training Epoch: 4 | iteration: 123/262 | Loss: 0.5901613235473633Training Epoch: 4 | iteration: 124/262 | Loss: 0.5988213419914246Training Epoch: 4 | iteration: 125/262 | Loss: 0.6646202802658081Training Epoch: 4 | iteration: 126/262 | Loss: 0.6026785969734192Training Epoch: 4 | iteration: 127/262 | Loss: 0.631937563419342Training Epoch: 4 | iteration: 128/262 | Loss: 0.6605700850486755Training Epoch: 4 | iteration: 129/262 | Loss: 0.5991400480270386Training Epoch: 4 | iteration: 130/262 | Loss: 0.6579228639602661Training Epoch: 4 | iteration: 131/262 | Loss: 0.6879738569259644Training Epoch: 4 | iteration: 132/262 | Loss: 0.6512024402618408Training Epoch: 4 | iteration: 133/262 | Loss: 0.628495454788208Training Epoch: 4 | iteration: 134/262 | Loss: 0.5372825860977173Training Epoch: 4 | iteration: 135/262 | Loss: 0.6083779335021973Training Epoch: 4 | iteration: 136/262 | Loss: 0.5736612677574158Training Epoch: 4 | iteration: 137/262 | Loss: 0.6959364414215088Training Epoch: 4 | iteration: 138/262 | Loss: 0.6440643668174744Training Epoch: 4 | iteration: 139/262 | Loss: 0.6033102869987488Training Epoch: 4 | iteration: 140/262 | Loss: 0.6452714204788208Training Epoch: 4 | iteration: 141/262 | Loss: 0.6691629886627197Training Epoch: 4 | iteration: 142/262 | Loss: 0.6783478260040283Training Epoch: 4 | iteration: 143/262 | Loss: 0.5687469244003296Training Epoch: 4 | iteration: 144/262 | Loss: 0.653774619102478Training Epoch: 4 | iteration: 145/262 | Loss: 0.6687524318695068Training Epoch: 4 | iteration: 146/262 | Loss: 0.6713414192199707Training Epoch: 4 | iteration: 147/262 | Loss: 0.6356143355369568Training Epoch: 4 | iteration: 148/262 | Loss: 0.6492277383804321Training Epoch: 4 | iteration: 149/262 | Loss: 0.6284843683242798Training Epoch: 4 | iteration: 150/262 | Loss: 0.5824830532073975Training Epoch: 4 | iteration: 151/262 | Loss: 0.6643557548522949Training Epoch: 4 | iteration: 152/262 | Loss: 0.5803001523017883Training Epoch: 4 | iteration: 153/262 | Loss: 0.6840105056762695Training Epoch: 4 | iteration: 154/262 | Loss: 0.6911575794219971Training Epoch: 4 | iteration: 155/262 | Loss: 0.6324767470359802Training Epoch: 4 | iteration: 156/262 | Loss: 0.5926820039749146Training Epoch: 4 | iteration: 157/262 | Loss: 0.6584234237670898Training Epoch: 4 | iteration: 158/262 | Loss: 0.6407696008682251Training Epoch: 4 | iteration: 159/262 | Loss: 0.659386157989502Training Epoch: 4 | iteration: 160/262 | Loss: 0.5847826600074768Training Epoch: 4 | iteration: 161/262 | Loss: 0.6733895540237427Training Epoch: 4 | iteration: 162/262 | Loss: 0.6629952192306519Training Epoch: 4 | iteration: 163/262 | Loss: 0.555694043636322Training Epoch: 4 | iteration: 164/262 | Loss: 0.6494673490524292Training Epoch: 4 | iteration: 165/262 | Loss: 0.6230306625366211Training Epoch: 4 | iteration: 166/262 | Loss: 0.5958842039108276Training Epoch: 4 | iteration: 167/262 | Loss: 0.6362705230712891Training Epoch: 4 | iteration: 168/262 | Loss: 0.6152288913726807Training Epoch: 4 | iteration: 169/262 | Loss: 0.5945758819580078Training Epoch: 4 | iteration: 170/262 | Loss: 0.6565954685211182Training Epoch: 4 | iteration: 171/262 | Loss: 0.6383587121963501Training Epoch: 4 | iteration: 172/262 | Loss: 0.6318416595458984Training Epoch: 4 | iteration: 173/262 | Loss: 0.5701751112937927Training Epoch: 4 | iteration: 174/262 | Loss: 0.679356038570404Training Epoch: 4 | iteration: 175/262 | Loss: 0.6555242538452148Training Epoch: 4 | iteration: 176/262 | Loss: 0.6423606872558594Training Epoch: 4 | iteration: 177/262 | Loss: 0.5609570741653442Training Epoch: 4 | iteration: 178/262 | Loss: 0.6154757738113403Training Epoch: 4 | iteration: 179/262 | Loss: 0.653373122215271Training Epoch: 4 | iteration: 180/262 | Loss: 0.7577849626541138Training Epoch: 4 | iteration: 181/262 | Loss: 0.6239323616027832Training Epoch: 4 | iteration: 182/262 | Loss: 0.679288387298584Training Epoch: 4 | iteration: 183/262 | Loss: 0.6336151361465454Training Epoch: 4 | iteration: 184/262 | Loss: 0.6058955788612366Training Epoch: 4 | iteration: 185/262 | Loss: 0.6273369193077087Training Epoch: 4 | iteration: 186/262 | Loss: 0.637417733669281Training Epoch: 4 | iteration: 187/262 | Loss: 0.6814266443252563Training Epoch: 4 | iteration: 188/262 | Loss: 0.5875827074050903Training Epoch: 4 | iteration: 189/262 | Loss: 0.6428636908531189Training Epoch: 4 | iteration: 190/262 | Loss: 0.5962313413619995Training Epoch: 4 | iteration: 191/262 | Loss: 0.7055702209472656Training Epoch: 4 | iteration: 192/262 | Loss: 0.6209395527839661Training Epoch: 4 | iteration: 193/262 | Loss: 0.7350701689720154Training Epoch: 4 | iteration: 194/262 | Loss: 0.6907244920730591Training Epoch: 4 | iteration: 195/262 | Loss: 0.6158705949783325Training Epoch: 4 | iteration: 196/262 | Loss: 0.625256359577179Training Epoch: 4 | iteration: 197/262 | Loss: 0.547716498374939Training Epoch: 4 | iteration: 198/262 | Loss: 0.5714300870895386Training Epoch: 4 | iteration: 199/262 | Loss: 0.6424769163131714Training Epoch: 4 | iteration: 200/262 | Loss: 0.6694419384002686Training Epoch: 4 | iteration: 201/262 | Loss: 0.583917498588562Training Epoch: 4 | iteration: 202/262 | Loss: 0.6577039957046509Training Epoch: 4 | iteration: 203/262 | Loss: 0.6201807260513306Training Epoch: 4 | iteration: 204/262 | Loss: 0.6630069017410278Training Epoch: 4 | iteration: 205/262 | Loss: 0.64420485496521Training Epoch: 4 | iteration: 206/262 | Loss: 0.660449743270874Training Epoch: 4 | iteration: 207/262 | Loss: 0.6824125647544861Training Epoch: 4 | iteration: 208/262 | Loss: 0.6229962110519409Training Epoch: 4 | iteration: 209/262 | Loss: 0.759951114654541Training Epoch: 4 | iteration: 210/262 | Loss: 0.6194124221801758Training Epoch: 4 | iteration: 211/262 | Loss: 0.5817573666572571Training Epoch: 4 | iteration: 212/262 | Loss: 0.5845551490783691Training Epoch: 4 | iteration: 213/262 | Loss: 0.5983524918556213Training Epoch: 4 | iteration: 214/262 | Loss: 0.6496151089668274Training Epoch: 4 | iteration: 215/262 | Loss: 0.6307043433189392Training Epoch: 4 | iteration: 216/262 | Loss: 0.6026732325553894Training Epoch: 4 | iteration: 217/262 | Loss: 0.6555153131484985Training Epoch: 4 | iteration: 218/262 | Loss: 0.5664068460464478Training Epoch: 4 | iteration: 219/262 | Loss: 0.6459134817123413Training Epoch: 4 | iteration: 220/262 | Loss: 0.6370176076889038Training Epoch: 4 | iteration: 221/262 | Loss: 0.6530320644378662Training Epoch: 4 | iteration: 222/262 | Loss: 0.6353740096092224Training Epoch: 4 | iteration: 223/262 | Loss: 0.6026002764701843Training Epoch: 4 | iteration: 224/262 | Loss: 0.6546450257301331Training Epoch: 4 | iteration: 225/262 | Loss: 0.6683351993560791Training Epoch: 4 | iteration: 226/262 | Loss: 0.6814467906951904Training Epoch: 4 | iteration: 227/262 | Loss: 0.6761952638626099Training Epoch: 4 | iteration: 228/262 | Loss: 0.6065762042999268Training Epoch: 4 | iteration: 229/262 | Loss: 0.6450318098068237Training Epoch: 4 | iteration: 230/262 | Loss: 0.6704616546630859Training Epoch: 4 | iteration: 231/262 | Loss: 0.6289196014404297Training Epoch: 4 | iteration: 232/262 | Loss: 0.6585740447044373Training Epoch: 4 | iteration: 233/262 | Loss: 0.5953893065452576Training Epoch: 4 | iteration: 234/262 | Loss: 0.7210014462471008Training Epoch: 4 | iteration: 235/262 | Loss: 0.5759949684143066Training Epoch: 4 | iteration: 236/262 | Loss: 0.6406259536743164Training Epoch: 4 | iteration: 237/262 | Loss: 0.5674874782562256Training Epoch: 4 | iteration: 238/262 | Loss: 0.609027624130249Training Epoch: 4 | iteration: 239/262 | Loss: 0.6923129558563232Training Epoch: 4 | iteration: 240/262 | Loss: 0.6396676898002625Training Epoch: 4 | iteration: 241/262 | Loss: 0.6338571310043335Training Epoch: 4 | iteration: 242/262 | Loss: 0.6981188058853149Training Epoch: 4 | iteration: 243/262 | Loss: 0.6695057153701782Training Epoch: 4 | iteration: 244/262 | Loss: 0.6179937720298767Training Epoch: 4 | iteration: 245/262 | Loss: 0.6052091717720032Training Epoch: 4 | iteration: 246/262 | Loss: 0.6175624132156372Training Epoch: 4 | iteration: 247/262 | Loss: 0.6784564852714539Training Epoch: 4 | iteration: 248/262 | Loss: 0.6753774285316467Training Epoch: 4 | iteration: 249/262 | Loss: 0.6624200344085693Training Epoch: 4 | iteration: 250/262 | Loss: 0.6890736222267151Training Epoch: 4 | iteration: 251/262 | Loss: 0.606133222579956Training Epoch: 4 | iteration: 252/262 | Loss: 0.663835883140564Training Epoch: 4 | iteration: 253/262 | Loss: 0.6893543004989624Training Epoch: 4 | iteration: 254/262 | Loss: 0.6221181154251099Training Epoch: 4 | iteration: 255/262 | Loss: 0.6303590536117554Training Epoch: 4 | iteration: 256/262 | Loss: 0.6458829641342163Training Epoch: 4 | iteration: 257/262 | Loss: 0.5882171392440796Training Epoch: 4 | iteration: 258/262 | Loss: 0.6546450853347778Training Epoch: 4 | iteration: 259/262 | Loss: 0.6212425231933594Training Epoch: 4 | iteration: 260/262 | Loss: 0.6583614349365234Training Epoch: 4 | iteration: 261/262 | Loss: 0.7789672017097473Validating Epoch: 4 | iteration: 0/66 | Loss: 0.6105992794036865Validating Epoch: 4 | iteration: 1/66 | Loss: 0.6519679427146912Validating Epoch: 4 | iteration: 2/66 | Loss: 0.6135833263397217Validating Epoch: 4 | iteration: 3/66 | Loss: 0.6079598665237427Validating Epoch: 4 | iteration: 4/66 | Loss: 0.6284029483795166Validating Epoch: 4 | iteration: 5/66 | Loss: 0.5893887281417847Validating Epoch: 4 | iteration: 6/66 | Loss: 0.5720878839492798Validating Epoch: 4 | iteration: 7/66 | Loss: 0.6178972721099854Validating Epoch: 4 | iteration: 8/66 | Loss: 0.6331626772880554Validating Epoch: 4 | iteration: 9/66 | Loss: 0.6171720027923584Validating Epoch: 4 | iteration: 10/66 | Loss: 0.6131865978240967Validating Epoch: 4 | iteration: 11/66 | Loss: 0.6487433910369873Validating Epoch: 4 | iteration: 12/66 | Loss: 0.673217236995697Validating Epoch: 4 | iteration: 13/66 | Loss: 0.6302067041397095Validating Epoch: 4 | iteration: 14/66 | Loss: 0.6245197057723999Validating Epoch: 4 | iteration: 15/66 | Loss: 0.5878857374191284Validating Epoch: 4 | iteration: 16/66 | Loss: 0.5391053557395935Validating Epoch: 4 | iteration: 17/66 | Loss: 0.630839467048645Validating Epoch: 4 | iteration: 18/66 | Loss: 0.6304129958152771Validating Epoch: 4 | iteration: 19/66 | Loss: 0.6665276288986206Validating Epoch: 4 | iteration: 20/66 | Loss: 0.6277042627334595Validating Epoch: 4 | iteration: 21/66 | Loss: 0.5903063416481018Validating Epoch: 4 | iteration: 22/66 | Loss: 0.6102980971336365Validating Epoch: 4 | iteration: 23/66 | Loss: 0.5707107782363892Validating Epoch: 4 | iteration: 24/66 | Loss: 0.5755947828292847Validating Epoch: 4 | iteration: 25/66 | Loss: 0.609181821346283Validating Epoch: 4 | iteration: 26/66 | Loss: 0.5985164642333984Validating Epoch: 4 | iteration: 27/66 | Loss: 0.5632171630859375Validating Epoch: 4 | iteration: 28/66 | Loss: 0.602358341217041Validating Epoch: 4 | iteration: 29/66 | Loss: 0.576195478439331Validating Epoch: 4 | iteration: 30/66 | Loss: 0.5649521350860596Validating Epoch: 4 | iteration: 31/66 | Loss: 0.6433829069137573Validating Epoch: 4 | iteration: 32/66 | Loss: 0.6544082164764404Validating Epoch: 4 | iteration: 33/66 | Loss: 0.62199866771698Validating Epoch: 4 | iteration: 34/66 | Loss: 0.6261674761772156Validating Epoch: 4 | iteration: 35/66 | Loss: 0.5702505111694336Validating Epoch: 4 | iteration: 36/66 | Loss: 0.6556421518325806Validating Epoch: 4 | iteration: 37/66 | Loss: 0.6321558952331543Validating Epoch: 4 | iteration: 38/66 | Loss: 0.6287707686424255Validating Epoch: 4 | iteration: 39/66 | Loss: 0.5838069915771484Validating Epoch: 4 | iteration: 40/66 | Loss: 0.5910377502441406Validating Epoch: 4 | iteration: 41/66 | Loss: 0.6279171109199524Validating Epoch: 4 | iteration: 42/66 | Loss: 0.599370002746582Validating Epoch: 4 | iteration: 43/66 | Loss: 0.6418429613113403Validating Epoch: 4 | iteration: 44/66 | Loss: 0.7036706209182739Validating Epoch: 4 | iteration: 45/66 | Loss: 0.6046503186225891Validating Epoch: 4 | iteration: 46/66 | Loss: 0.5727087259292603Validating Epoch: 4 | iteration: 47/66 | Loss: 0.6373258829116821Validating Epoch: 4 | iteration: 48/66 | Loss: 0.6132709980010986Validating Epoch: 4 | iteration: 49/66 | Loss: 0.6453337669372559Validating Epoch: 4 | iteration: 50/66 | Loss: 0.6066807508468628Validating Epoch: 4 | iteration: 51/66 | Loss: 0.597693920135498Validating Epoch: 4 | iteration: 52/66 | Loss: 0.6249082684516907Validating Epoch: 4 | iteration: 53/66 | Loss: 0.5752696990966797Validating Epoch: 4 | iteration: 54/66 | Loss: 0.6455446481704712Validating Epoch: 4 | iteration: 55/66 | Loss: 0.6713879108428955Validating Epoch: 4 | iteration: 56/66 | Loss: 0.6075844764709473Validating Epoch: 4 | iteration: 57/66 | Loss: 0.638126790523529Validating Epoch: 4 | iteration: 58/66 | Loss: 0.6049128770828247Validating Epoch: 4 | iteration: 59/66 | Loss: 0.5975813865661621Validating Epoch: 4 | iteration: 60/66 | Loss: 0.6251444816589355Validating Epoch: 4 | iteration: 61/66 | Loss: 0.6362122297286987Validating Epoch: 4 | iteration: 62/66 | Loss: 0.5726915597915649Validating Epoch: 4 | iteration: 63/66 | Loss: 0.584246039390564Validating Epoch: 4 | iteration: 64/66 | Loss: 0.5825169086456299Validating Epoch: 4 | iteration: 65/66 | Loss: 0.580146312713623Temperature:  0.5
Sampling: iteration: 0/51{'Validity': 0.990234375, 'Novelty': 1.0, 'Uniqueness': 0.9921104536489151}
Training Epoch: 5 | iteration: 0/262 | Loss: 0.7212642431259155Training Epoch: 5 | iteration: 1/262 | Loss: 0.6513437032699585Training Epoch: 5 | iteration: 2/262 | Loss: 0.6291499733924866Training Epoch: 5 | iteration: 3/262 | Loss: 0.5937671661376953Training Epoch: 5 | iteration: 4/262 | Loss: 0.5936065912246704Training Epoch: 5 | iteration: 5/262 | Loss: 0.6357622742652893Training Epoch: 5 | iteration: 6/262 | Loss: 0.553690493106842Training Epoch: 5 | iteration: 7/262 | Loss: 0.6525327563285828Training Epoch: 5 | iteration: 8/262 | Loss: 0.5857325792312622Training Epoch: 5 | iteration: 9/262 | Loss: 0.692603349685669Training Epoch: 5 | iteration: 10/262 | Loss: 0.6644564867019653Training Epoch: 5 | iteration: 11/262 | Loss: 0.7034759521484375Training Epoch: 5 | iteration: 12/262 | Loss: 0.616321861743927Training Epoch: 5 | iteration: 13/262 | Loss: 0.6356422305107117Training Epoch: 5 | iteration: 14/262 | Loss: 0.6203734874725342Training Epoch: 5 | iteration: 15/262 | Loss: 0.6227530241012573Training Epoch: 5 | iteration: 16/262 | Loss: 0.6200050115585327Training Epoch: 5 | iteration: 17/262 | Loss: 0.630447268486023Training Epoch: 5 | iteration: 18/262 | Loss: 0.5517153143882751Training Epoch: 5 | iteration: 19/262 | Loss: 0.6270310878753662Training Epoch: 5 | iteration: 20/262 | Loss: 0.636777937412262Training Epoch: 5 | iteration: 21/262 | Loss: 0.6435494422912598Training Epoch: 5 | iteration: 22/262 | Loss: 0.6648115515708923Training Epoch: 5 | iteration: 23/262 | Loss: 0.6333807110786438Training Epoch: 5 | iteration: 24/262 | Loss: 0.6276818513870239Training Epoch: 5 | iteration: 25/262 | Loss: 0.6329266428947449Training Epoch: 5 | iteration: 26/262 | Loss: 0.5776327848434448Training Epoch: 5 | iteration: 27/262 | Loss: 0.6757127046585083Training Epoch: 5 | iteration: 28/262 | Loss: 0.6124104857444763Training Epoch: 5 | iteration: 29/262 | Loss: 0.6408633589744568Training Epoch: 5 | iteration: 30/262 | Loss: 0.621153712272644Training Epoch: 5 | iteration: 31/262 | Loss: 0.5753501653671265Training Epoch: 5 | iteration: 32/262 | Loss: 0.5886930823326111Training Epoch: 5 | iteration: 33/262 | Loss: 0.6075525283813477Training Epoch: 5 | iteration: 34/262 | Loss: 0.6481176614761353Training Epoch: 5 | iteration: 35/262 | Loss: 0.5854418873786926Training Epoch: 5 | iteration: 36/262 | Loss: 0.6140767931938171Training Epoch: 5 | iteration: 37/262 | Loss: 0.6133842468261719Training Epoch: 5 | iteration: 38/262 | Loss: 0.6123634576797485Training Epoch: 5 | iteration: 39/262 | Loss: 0.5887439250946045Training Epoch: 5 | iteration: 40/262 | Loss: 0.6263667941093445Training Epoch: 5 | iteration: 41/262 | Loss: 0.6411884427070618Training Epoch: 5 | iteration: 42/262 | Loss: 0.5966600775718689Training Epoch: 5 | iteration: 43/262 | Loss: 0.6724545359611511Training Epoch: 5 | iteration: 44/262 | Loss: 0.557246208190918Training Epoch: 5 | iteration: 45/262 | Loss: 0.6269153356552124Training Epoch: 5 | iteration: 46/262 | Loss: 0.6043234467506409Training Epoch: 5 | iteration: 47/262 | Loss: 0.6199110150337219Training Epoch: 5 | iteration: 48/262 | Loss: 0.586521565914154Training Epoch: 5 | iteration: 49/262 | Loss: 0.6538858413696289Training Epoch: 5 | iteration: 50/262 | Loss: 0.5742901563644409Training Epoch: 5 | iteration: 51/262 | Loss: 0.6037008762359619Training Epoch: 5 | iteration: 52/262 | Loss: 0.5962406992912292Training Epoch: 5 | iteration: 53/262 | Loss: 0.5903144478797913Training Epoch: 5 | iteration: 54/262 | Loss: 0.5952911376953125Training Epoch: 5 | iteration: 55/262 | Loss: 0.6678564548492432Training Epoch: 5 | iteration: 56/262 | Loss: 0.6415529847145081Training Epoch: 5 | iteration: 57/262 | Loss: 0.5932374000549316Training Epoch: 5 | iteration: 58/262 | Loss: 0.6152128577232361Training Epoch: 5 | iteration: 59/262 | Loss: 0.6236733794212341Training Epoch: 5 | iteration: 60/262 | Loss: 0.5978631973266602Training Epoch: 5 | iteration: 61/262 | Loss: 0.5365679264068604Training Epoch: 5 | iteration: 62/262 | Loss: 0.5725375413894653Training Epoch: 5 | iteration: 63/262 | Loss: 0.6701822280883789Training Epoch: 5 | iteration: 64/262 | Loss: 0.6301267743110657Training Epoch: 5 | iteration: 65/262 | Loss: 0.6120240688323975Training Epoch: 5 | iteration: 66/262 | Loss: 0.5848748683929443Training Epoch: 5 | iteration: 67/262 | Loss: 0.6318249106407166Training Epoch: 5 | iteration: 68/262 | Loss: 0.6632705330848694Training Epoch: 5 | iteration: 69/262 | Loss: 0.6348822116851807Training Epoch: 5 | iteration: 70/262 | Loss: 0.6431875824928284Training Epoch: 5 | iteration: 71/262 | Loss: 0.6673710346221924Training Epoch: 5 | iteration: 72/262 | Loss: 0.6491090059280396Training Epoch: 5 | iteration: 73/262 | Loss: 0.5524435043334961Training Epoch: 5 | iteration: 74/262 | Loss: 0.616497278213501Training Epoch: 5 | iteration: 75/262 | Loss: 0.6155816912651062Training Epoch: 5 | iteration: 76/262 | Loss: 0.612326443195343Training Epoch: 5 | iteration: 77/262 | Loss: 0.6057385206222534Training Epoch: 5 | iteration: 78/262 | Loss: 0.6307079792022705Training Epoch: 5 | iteration: 79/262 | Loss: 0.6770491600036621Training Epoch: 5 | iteration: 80/262 | Loss: 0.5434761643409729Training Epoch: 5 | iteration: 81/262 | Loss: 0.5564230680465698Training Epoch: 5 | iteration: 82/262 | Loss: 0.6176197528839111Training Epoch: 5 | iteration: 83/262 | Loss: 0.5979985594749451Training Epoch: 5 | iteration: 84/262 | Loss: 0.5367323160171509Training Epoch: 5 | iteration: 85/262 | Loss: 0.5785871744155884Training Epoch: 5 | iteration: 86/262 | Loss: 0.5866222381591797Training Epoch: 5 | iteration: 87/262 | Loss: 0.5917507410049438Training Epoch: 5 | iteration: 88/262 | Loss: 0.6179733872413635Training Epoch: 5 | iteration: 89/262 | Loss: 0.7133913040161133Training Epoch: 5 | iteration: 90/262 | Loss: 0.6140924692153931Training Epoch: 5 | iteration: 91/262 | Loss: 0.6133080124855042Training Epoch: 5 | iteration: 92/262 | Loss: 0.6065384745597839Training Epoch: 5 | iteration: 93/262 | Loss: 0.6005058288574219Training Epoch: 5 | iteration: 94/262 | Loss: 0.6039934158325195Training Epoch: 5 | iteration: 95/262 | Loss: 0.5964269042015076Training Epoch: 5 | iteration: 96/262 | Loss: 0.5766690373420715Training Epoch: 5 | iteration: 97/262 | Loss: 0.5821460485458374Training Epoch: 5 | iteration: 98/262 | Loss: 0.534077525138855Training Epoch: 5 | iteration: 99/262 | Loss: 0.5954067707061768Training Epoch: 5 | iteration: 100/262 | Loss: 0.6667077541351318Training Epoch: 5 | iteration: 101/262 | Loss: 0.6106404662132263Training Epoch: 5 | iteration: 102/262 | Loss: 0.6585737466812134Training Epoch: 5 | iteration: 103/262 | Loss: 0.5769824981689453Training Epoch: 5 | iteration: 104/262 | Loss: 0.6598376631736755Training Epoch: 5 | iteration: 105/262 | Loss: 0.5828269720077515Training Epoch: 5 | iteration: 106/262 | Loss: 0.51604163646698Training Epoch: 5 | iteration: 107/262 | Loss: 0.6376643180847168Training Epoch: 5 | iteration: 108/262 | Loss: 0.5968971252441406Training Epoch: 5 | iteration: 109/262 | Loss: 0.6191647052764893Training Epoch: 5 | iteration: 110/262 | Loss: 0.6857540011405945Training Epoch: 5 | iteration: 111/262 | Loss: 0.6472440361976624Training Epoch: 5 | iteration: 112/262 | Loss: 0.6840798854827881Training Epoch: 5 | iteration: 113/262 | Loss: 0.5888727307319641Training Epoch: 5 | iteration: 114/262 | Loss: 0.6758662462234497Training Epoch: 5 | iteration: 115/262 | Loss: 0.646780252456665Training Epoch: 5 | iteration: 116/262 | Loss: 0.6586264967918396Training Epoch: 5 | iteration: 117/262 | Loss: 0.6523277759552002Training Epoch: 5 | iteration: 118/262 | Loss: 0.6556329727172852Training Epoch: 5 | iteration: 119/262 | Loss: 0.6223956346511841Training Epoch: 5 | iteration: 120/262 | Loss: 0.6178988814353943Training Epoch: 5 | iteration: 121/262 | Loss: 0.6259260773658752Training Epoch: 5 | iteration: 122/262 | Loss: 0.5511731505393982Training Epoch: 5 | iteration: 123/262 | Loss: 0.5943316221237183Training Epoch: 5 | iteration: 124/262 | Loss: 0.5768904685974121Training Epoch: 5 | iteration: 125/262 | Loss: 0.5983145833015442Training Epoch: 5 | iteration: 126/262 | Loss: 0.6390743255615234Training Epoch: 5 | iteration: 127/262 | Loss: 0.6009669899940491Training Epoch: 5 | iteration: 128/262 | Loss: 0.5822160840034485Training Epoch: 5 | iteration: 129/262 | Loss: 0.5647927522659302Training Epoch: 5 | iteration: 130/262 | Loss: 0.6306458711624146Training Epoch: 5 | iteration: 131/262 | Loss: 0.6582491397857666Training Epoch: 5 | iteration: 132/262 | Loss: 0.6297789812088013Training Epoch: 5 | iteration: 133/262 | Loss: 0.6131160855293274Training Epoch: 5 | iteration: 134/262 | Loss: 0.6829119920730591Training Epoch: 5 | iteration: 135/262 | Loss: 0.6307219862937927Training Epoch: 5 | iteration: 136/262 | Loss: 0.6681331992149353Training Epoch: 5 | iteration: 137/262 | Loss: 0.6397221684455872Training Epoch: 5 | iteration: 138/262 | Loss: 0.5830075740814209Training Epoch: 5 | iteration: 139/262 | Loss: 0.6678110361099243Training Epoch: 5 | iteration: 140/262 | Loss: 0.6568202376365662Training Epoch: 5 | iteration: 141/262 | Loss: 0.6251382827758789Training Epoch: 5 | iteration: 142/262 | Loss: 0.62895667552948Training Epoch: 5 | iteration: 143/262 | Loss: 0.5668396949768066Training Epoch: 5 | iteration: 144/262 | Loss: 0.5985383987426758Training Epoch: 5 | iteration: 145/262 | Loss: 0.5507334470748901Training Epoch: 5 | iteration: 146/262 | Loss: 0.6120501160621643Training Epoch: 5 | iteration: 147/262 | Loss: 0.6333197355270386Training Epoch: 5 | iteration: 148/262 | Loss: 0.5965187549591064Training Epoch: 5 | iteration: 149/262 | Loss: 0.6666895747184753Training Epoch: 5 | iteration: 150/262 | Loss: 0.6022754311561584Training Epoch: 5 | iteration: 151/262 | Loss: 0.6401008367538452Training Epoch: 5 | iteration: 152/262 | Loss: 0.5905181169509888Training Epoch: 5 | iteration: 153/262 | Loss: 0.6216326355934143Training Epoch: 5 | iteration: 154/262 | Loss: 0.5783351063728333Training Epoch: 5 | iteration: 155/262 | Loss: 0.6163456439971924Training Epoch: 5 | iteration: 156/262 | Loss: 0.5676292181015015Training Epoch: 5 | iteration: 157/262 | Loss: 0.6408475637435913Training Epoch: 5 | iteration: 158/262 | Loss: 0.598649263381958Training Epoch: 5 | iteration: 159/262 | Loss: 0.5748299956321716Training Epoch: 5 | iteration: 160/262 | Loss: 0.6624876260757446Training Epoch: 5 | iteration: 161/262 | Loss: 0.6066147089004517Training Epoch: 5 | iteration: 162/262 | Loss: 0.6314646005630493Training Epoch: 5 | iteration: 163/262 | Loss: 0.6502251029014587Training Epoch: 5 | iteration: 164/262 | Loss: 0.62082839012146Training Epoch: 5 | iteration: 165/262 | Loss: 0.6677161455154419Training Epoch: 5 | iteration: 166/262 | Loss: 0.6415391564369202Training Epoch: 5 | iteration: 167/262 | Loss: 0.6015918850898743Training Epoch: 5 | iteration: 168/262 | Loss: 0.59648597240448Training Epoch: 5 | iteration: 169/262 | Loss: 0.5867462158203125Training Epoch: 5 | iteration: 170/262 | Loss: 0.5726776719093323Training Epoch: 5 | iteration: 171/262 | Loss: 0.6698712110519409Training Epoch: 5 | iteration: 172/262 | Loss: 0.6428086757659912Training Epoch: 5 | iteration: 173/262 | Loss: 0.6018117666244507Training Epoch: 5 | iteration: 174/262 | Loss: 0.6564074754714966Training Epoch: 5 | iteration: 175/262 | Loss: 0.6706700325012207Training Epoch: 5 | iteration: 176/262 | Loss: 0.6138287782669067Training Epoch: 5 | iteration: 177/262 | Loss: 0.6218481063842773Training Epoch: 5 | iteration: 178/262 | Loss: 0.66728675365448Training Epoch: 5 | iteration: 179/262 | Loss: 0.5941914319992065Training Epoch: 5 | iteration: 180/262 | Loss: 0.638924241065979Training Epoch: 5 | iteration: 181/262 | Loss: 0.5467208623886108Training Epoch: 5 | iteration: 182/262 | Loss: 0.6029565334320068Training Epoch: 5 | iteration: 183/262 | Loss: 0.6124390363693237Training Epoch: 5 | iteration: 184/262 | Loss: 0.673270583152771Training Epoch: 5 | iteration: 185/262 | Loss: 0.6914786100387573Training Epoch: 5 | iteration: 186/262 | Loss: 0.6589348316192627Training Epoch: 5 | iteration: 187/262 | Loss: 0.6628053188323975Training Epoch: 5 | iteration: 188/262 | Loss: 0.6437534689903259Training Epoch: 5 | iteration: 189/262 | Loss: 0.5935696959495544Training Epoch: 5 | iteration: 190/262 | Loss: 0.6311231851577759Training Epoch: 5 | iteration: 191/262 | Loss: 0.6626951098442078Training Epoch: 5 | iteration: 192/262 | Loss: 0.5772315263748169Training Epoch: 5 | iteration: 193/262 | Loss: 0.5098930597305298Training Epoch: 5 | iteration: 194/262 | Loss: 0.6910611987113953Training Epoch: 5 | iteration: 195/262 | Loss: 0.6312975883483887Training Epoch: 5 | iteration: 196/262 | Loss: 0.5948188304901123Training Epoch: 5 | iteration: 197/262 | Loss: 0.5894991755485535Training Epoch: 5 | iteration: 198/262 | Loss: 0.6749125123023987Training Epoch: 5 | iteration: 199/262 | Loss: 0.6129238605499268Training Epoch: 5 | iteration: 200/262 | Loss: 0.6622297167778015Training Epoch: 5 | iteration: 201/262 | Loss: 0.6411818265914917Training Epoch: 5 | iteration: 202/262 | Loss: 0.6297335624694824Training Epoch: 5 | iteration: 203/262 | Loss: 0.6784254312515259Training Epoch: 5 | iteration: 204/262 | Loss: 0.6601302027702332Training Epoch: 5 | iteration: 205/262 | Loss: 0.6475656032562256Training Epoch: 5 | iteration: 206/262 | Loss: 0.7192386388778687Training Epoch: 5 | iteration: 207/262 | Loss: 0.5986785888671875Training Epoch: 5 | iteration: 208/262 | Loss: 0.5884984135627747Training Epoch: 5 | iteration: 209/262 | Loss: 0.6294536590576172Training Epoch: 5 | iteration: 210/262 | Loss: 0.6105602383613586Training Epoch: 5 | iteration: 211/262 | Loss: 0.6076074838638306Training Epoch: 5 | iteration: 212/262 | Loss: 0.6430088877677917Training Epoch: 5 | iteration: 213/262 | Loss: 0.6032066345214844Training Epoch: 5 | iteration: 214/262 | Loss: 0.5714329481124878Training Epoch: 5 | iteration: 215/262 | Loss: 0.6035093069076538Training Epoch: 5 | iteration: 216/262 | Loss: 0.5669959783554077Training Epoch: 5 | iteration: 217/262 | Loss: 0.6031631827354431Training Epoch: 5 | iteration: 218/262 | Loss: 0.6101608872413635Training Epoch: 5 | iteration: 219/262 | Loss: 0.604265034198761Training Epoch: 5 | iteration: 220/262 | Loss: 0.6066116094589233Training Epoch: 5 | iteration: 221/262 | Loss: 0.5910141468048096Training Epoch: 5 | iteration: 222/262 | Loss: 0.525199830532074Training Epoch: 5 | iteration: 223/262 | Loss: 0.6217465400695801Training Epoch: 5 | iteration: 224/262 | Loss: 0.6033487319946289Training Epoch: 5 | iteration: 225/262 | Loss: 0.6102114915847778Training Epoch: 5 | iteration: 226/262 | Loss: 0.61219322681427Training Epoch: 5 | iteration: 227/262 | Loss: 0.6653748154640198Training Epoch: 5 | iteration: 228/262 | Loss: 0.6413896083831787Training Epoch: 5 | iteration: 229/262 | Loss: 0.5708595514297485Training Epoch: 5 | iteration: 230/262 | Loss: 0.6634987592697144Training Epoch: 5 | iteration: 231/262 | Loss: 0.6285144090652466Training Epoch: 5 | iteration: 232/262 | Loss: 0.6209990978240967Training Epoch: 5 | iteration: 233/262 | Loss: 0.6496978998184204Training Epoch: 5 | iteration: 234/262 | Loss: 0.6231557130813599Training Epoch: 5 | iteration: 235/262 | Loss: 0.6153042316436768Training Epoch: 5 | iteration: 236/262 | Loss: 0.6163477897644043Training Epoch: 5 | iteration: 237/262 | Loss: 0.6420273780822754Training Epoch: 5 | iteration: 238/262 | Loss: 0.6177018284797668Training Epoch: 5 | iteration: 239/262 | Loss: 0.6392677426338196Training Epoch: 5 | iteration: 240/262 | Loss: 0.6264598369598389Training Epoch: 5 | iteration: 241/262 | Loss: 0.5941615700721741Training Epoch: 5 | iteration: 242/262 | Loss: 0.6224740743637085Training Epoch: 5 | iteration: 243/262 | Loss: 0.65077805519104Training Epoch: 5 | iteration: 244/262 | Loss: 0.6274127960205078Training Epoch: 5 | iteration: 245/262 | Loss: 0.6177368760108948Training Epoch: 5 | iteration: 246/262 | Loss: 0.6190982460975647Training Epoch: 5 | iteration: 247/262 | Loss: 0.6354106664657593Training Epoch: 5 | iteration: 248/262 | Loss: 0.6007426977157593Training Epoch: 5 | iteration: 249/262 | Loss: 0.6274271011352539Training Epoch: 5 | iteration: 250/262 | Loss: 0.5637524127960205Training Epoch: 5 | iteration: 251/262 | Loss: 0.5908482670783997Training Epoch: 5 | iteration: 252/262 | Loss: 0.6119939684867859Training Epoch: 5 | iteration: 253/262 | Loss: 0.5628740787506104Training Epoch: 5 | iteration: 254/262 | Loss: 0.6079294681549072Training Epoch: 5 | iteration: 255/262 | Loss: 0.5887490510940552Training Epoch: 5 | iteration: 256/262 | Loss: 0.6349712610244751Training Epoch: 5 | iteration: 257/262 | Loss: 0.6364892721176147Training Epoch: 5 | iteration: 258/262 | Loss: 0.5965908169746399Training Epoch: 5 | iteration: 259/262 | Loss: 0.603515088558197Training Epoch: 5 | iteration: 260/262 | Loss: 0.5357992053031921Training Epoch: 5 | iteration: 261/262 | Loss: 0.6734559535980225Validating Epoch: 5 | iteration: 0/66 | Loss: 0.6440808773040771Validating Epoch: 5 | iteration: 1/66 | Loss: 0.6203250885009766Validating Epoch: 5 | iteration: 2/66 | Loss: 0.5646048784255981Validating Epoch: 5 | iteration: 3/66 | Loss: 0.6051169037818909Validating Epoch: 5 | iteration: 4/66 | Loss: 0.5985972285270691Validating Epoch: 5 | iteration: 5/66 | Loss: 0.601589024066925Validating Epoch: 5 | iteration: 6/66 | Loss: 0.5941165089607239Validating Epoch: 5 | iteration: 7/66 | Loss: 0.5894079208374023Validating Epoch: 5 | iteration: 8/66 | Loss: 0.5685544610023499Validating Epoch: 5 | iteration: 9/66 | Loss: 0.5853942632675171Validating Epoch: 5 | iteration: 10/66 | Loss: 0.6441161632537842Validating Epoch: 5 | iteration: 11/66 | Loss: 0.6654235124588013Validating Epoch: 5 | iteration: 12/66 | Loss: 0.5982605218887329Validating Epoch: 5 | iteration: 13/66 | Loss: 0.6242870092391968Validating Epoch: 5 | iteration: 14/66 | Loss: 0.5737454295158386Validating Epoch: 5 | iteration: 15/66 | Loss: 0.7005772590637207Validating Epoch: 5 | iteration: 16/66 | Loss: 0.5555447936058044Validating Epoch: 5 | iteration: 17/66 | Loss: 0.6411753296852112Validating Epoch: 5 | iteration: 18/66 | Loss: 0.6329972743988037Validating Epoch: 5 | iteration: 19/66 | Loss: 0.6106188297271729Validating Epoch: 5 | iteration: 20/66 | Loss: 0.619819164276123Validating Epoch: 5 | iteration: 21/66 | Loss: 0.6222366094589233Validating Epoch: 5 | iteration: 22/66 | Loss: 0.6130260229110718Validating Epoch: 5 | iteration: 23/66 | Loss: 0.5315450429916382Validating Epoch: 5 | iteration: 24/66 | Loss: 0.5955073833465576Validating Epoch: 5 | iteration: 25/66 | Loss: 0.5995611548423767Validating Epoch: 5 | iteration: 26/66 | Loss: 0.589741587638855Validating Epoch: 5 | iteration: 27/66 | Loss: 0.6021552085876465Validating Epoch: 5 | iteration: 28/66 | Loss: 0.5608377456665039Validating Epoch: 5 | iteration: 29/66 | Loss: 0.5922644734382629Validating Epoch: 5 | iteration: 30/66 | Loss: 0.6017526388168335Validating Epoch: 5 | iteration: 31/66 | Loss: 0.6007393002510071Validating Epoch: 5 | iteration: 32/66 | Loss: 0.5917840003967285Validating Epoch: 5 | iteration: 33/66 | Loss: 0.5288954973220825Validating Epoch: 5 | iteration: 34/66 | Loss: 0.6043698787689209Validating Epoch: 5 | iteration: 35/66 | Loss: 0.5621211528778076Validating Epoch: 5 | iteration: 36/66 | Loss: 0.6789761781692505Validating Epoch: 5 | iteration: 37/66 | Loss: 0.5754782557487488Validating Epoch: 5 | iteration: 38/66 | Loss: 0.6277755498886108Validating Epoch: 5 | iteration: 39/66 | Loss: 0.5735146999359131Validating Epoch: 5 | iteration: 40/66 | Loss: 0.5738555192947388Validating Epoch: 5 | iteration: 41/66 | Loss: 0.5899393558502197Validating Epoch: 5 | iteration: 42/66 | Loss: 0.5782960057258606Validating Epoch: 5 | iteration: 43/66 | Loss: 0.5760254263877869Validating Epoch: 5 | iteration: 44/66 | Loss: 0.6042691469192505Validating Epoch: 5 | iteration: 45/66 | Loss: 0.6340731382369995Validating Epoch: 5 | iteration: 46/66 | Loss: 0.6684286594390869Validating Epoch: 5 | iteration: 47/66 | Loss: 0.6268451809883118Validating Epoch: 5 | iteration: 48/66 | Loss: 0.609874963760376Validating Epoch: 5 | iteration: 49/66 | Loss: 0.619518518447876Validating Epoch: 5 | iteration: 50/66 | Loss: 0.57395339012146Validating Epoch: 5 | iteration: 51/66 | Loss: 0.6058483719825745Validating Epoch: 5 | iteration: 52/66 | Loss: 0.6320216059684753Validating Epoch: 5 | iteration: 53/66 | Loss: 0.6167079210281372Validating Epoch: 5 | iteration: 54/66 | Loss: 0.6016209721565247Validating Epoch: 5 | iteration: 55/66 | Loss: 0.6302741765975952Validating Epoch: 5 | iteration: 56/66 | Loss: 0.6555744409561157Validating Epoch: 5 | iteration: 57/66 | Loss: 0.6756304502487183Validating Epoch: 5 | iteration: 58/66 | Loss: 0.628642201423645Validating Epoch: 5 | iteration: 59/66 | Loss: 0.6169254779815674Validating Epoch: 5 | iteration: 60/66 | Loss: 0.6072434186935425Validating Epoch: 5 | iteration: 61/66 | Loss: 0.5722948312759399Validating Epoch: 5 | iteration: 62/66 | Loss: 0.6525126695632935Validating Epoch: 5 | iteration: 63/66 | Loss: 0.5770837664604187Validating Epoch: 5 | iteration: 64/66 | Loss: 0.5760632157325745Validating Epoch: 5 | iteration: 65/66 | Loss: 0.6888485550880432Temperature:  0.5
Sampling: iteration: 0/51{'Validity': 0.9765625, 'Novelty': 1.0, 'Uniqueness': 0.995}
Training Epoch: 6 | iteration: 0/262 | Loss: 0.6664764881134033Training Epoch: 6 | iteration: 1/262 | Loss: 0.6138284206390381Training Epoch: 6 | iteration: 2/262 | Loss: 0.642907440662384Training Epoch: 6 | iteration: 3/262 | Loss: 0.563448429107666Training Epoch: 6 | iteration: 4/262 | Loss: 0.5603758692741394Training Epoch: 6 | iteration: 5/262 | Loss: 0.5537930727005005Training Epoch: 6 | iteration: 6/262 | Loss: 0.6068955659866333Training Epoch: 6 | iteration: 7/262 | Loss: 0.6214871406555176Training Epoch: 6 | iteration: 8/262 | Loss: 0.5343101024627686Training Epoch: 6 | iteration: 9/262 | Loss: 0.5518913269042969Training Epoch: 6 | iteration: 10/262 | Loss: 0.6030017733573914Training Epoch: 6 | iteration: 11/262 | Loss: 0.570668637752533Training Epoch: 6 | iteration: 12/262 | Loss: 0.6187952756881714Training Epoch: 6 | iteration: 13/262 | Loss: 0.6682233810424805Training Epoch: 6 | iteration: 14/262 | Loss: 0.5745911598205566Training Epoch: 6 | iteration: 15/262 | Loss: 0.6097503900527954Training Epoch: 6 | iteration: 16/262 | Loss: 0.5803235173225403Training Epoch: 6 | iteration: 17/262 | Loss: 0.573573648929596Training Epoch: 6 | iteration: 18/262 | Loss: 0.579465389251709Training Epoch: 6 | iteration: 19/262 | Loss: 0.5794877409934998Training Epoch: 6 | iteration: 20/262 | Loss: 0.5667265057563782Training Epoch: 6 | iteration: 21/262 | Loss: 0.6063976883888245Training Epoch: 6 | iteration: 22/262 | Loss: 0.6251223087310791Training Epoch: 6 | iteration: 23/262 | Loss: 0.5929352045059204Training Epoch: 6 | iteration: 24/262 | Loss: 0.6003885269165039Training Epoch: 6 | iteration: 25/262 | Loss: 0.6431506276130676Training Epoch: 6 | iteration: 26/262 | Loss: 0.5749649405479431Training Epoch: 6 | iteration: 27/262 | Loss: 0.6244263648986816Training Epoch: 6 | iteration: 28/262 | Loss: 0.6232138872146606Training Epoch: 6 | iteration: 29/262 | Loss: 0.642636775970459Training Epoch: 6 | iteration: 30/262 | Loss: 0.6208705902099609Training Epoch: 6 | iteration: 31/262 | Loss: 0.6898419260978699Training Epoch: 6 | iteration: 32/262 | Loss: 0.5580484867095947Training Epoch: 6 | iteration: 33/262 | Loss: 0.6076452732086182Training Epoch: 6 | iteration: 34/262 | Loss: 0.6357545256614685Training Epoch: 6 | iteration: 35/262 | Loss: 0.593336820602417Training Epoch: 6 | iteration: 36/262 | Loss: 0.5813226103782654Training Epoch: 6 | iteration: 37/262 | Loss: 0.607052206993103Training Epoch: 6 | iteration: 38/262 | Loss: 0.6445985436439514Training Epoch: 6 | iteration: 39/262 | Loss: 0.5546777248382568Training Epoch: 6 | iteration: 40/262 | Loss: 0.6987231969833374Training Epoch: 6 | iteration: 41/262 | Loss: 0.6423702239990234Training Epoch: 6 | iteration: 42/262 | Loss: 0.5816423892974854Training Epoch: 6 | iteration: 43/262 | Loss: 0.5870201587677002Training Epoch: 6 | iteration: 44/262 | Loss: 0.5438400506973267Training Epoch: 6 | iteration: 45/262 | Loss: 0.614859402179718Training Epoch: 6 | iteration: 46/262 | Loss: 0.6236279010772705Training Epoch: 6 | iteration: 47/262 | Loss: 0.5372116565704346Training Epoch: 6 | iteration: 48/262 | Loss: 0.5930267572402954Training Epoch: 6 | iteration: 49/262 | Loss: 0.6036144495010376Training Epoch: 6 | iteration: 50/262 | Loss: 0.6142880320549011Training Epoch: 6 | iteration: 51/262 | Loss: 0.5714915990829468Training Epoch: 6 | iteration: 52/262 | Loss: 0.6098291873931885Training Epoch: 6 | iteration: 53/262 | Loss: 0.5623281002044678Training Epoch: 6 | iteration: 54/262 | Loss: 0.5771916508674622Training Epoch: 6 | iteration: 55/262 | Loss: 0.5948339104652405Training Epoch: 6 | iteration: 56/262 | Loss: 0.5910372734069824Training Epoch: 6 | iteration: 57/262 | Loss: 0.5647327303886414Training Epoch: 6 | iteration: 58/262 | Loss: 0.5633899569511414Training Epoch: 6 | iteration: 59/262 | Loss: 0.5874754786491394Training Epoch: 6 | iteration: 60/262 | Loss: 0.5841749906539917Training Epoch: 6 | iteration: 61/262 | Loss: 0.6063999533653259Training Epoch: 6 | iteration: 62/262 | Loss: 0.5405625104904175Training Epoch: 6 | iteration: 63/262 | Loss: 0.5890167951583862Training Epoch: 6 | iteration: 64/262 | Loss: 0.6071295142173767Training Epoch: 6 | iteration: 65/262 | Loss: 0.5722802877426147Training Epoch: 6 | iteration: 66/262 | Loss: 0.5179308652877808Training Epoch: 6 | iteration: 67/262 | Loss: 0.5988790392875671Training Epoch: 6 | iteration: 68/262 | Loss: 0.6390732526779175Training Epoch: 6 | iteration: 69/262 | Loss: 0.5587207078933716Training Epoch: 6 | iteration: 70/262 | Loss: 0.6351979970932007Training Epoch: 6 | iteration: 71/262 | Loss: 0.596193790435791Training Epoch: 6 | iteration: 72/262 | Loss: 0.5695620775222778Training Epoch: 6 | iteration: 73/262 | Loss: 0.6277874112129211Training Epoch: 6 | iteration: 74/262 | Loss: 0.6369813680648804Training Epoch: 6 | iteration: 75/262 | Loss: 0.5662076473236084Training Epoch: 6 | iteration: 76/262 | Loss: 0.6303825378417969Training Epoch: 6 | iteration: 77/262 | Loss: 0.6150035858154297Training Epoch: 6 | iteration: 78/262 | Loss: 0.5881770253181458Training Epoch: 6 | iteration: 79/262 | Loss: 0.5720049142837524Training Epoch: 6 | iteration: 80/262 | Loss: 0.589558482170105Training Epoch: 6 | iteration: 81/262 | Loss: 0.6231468915939331Training Epoch: 6 | iteration: 82/262 | Loss: 0.5982574224472046Training Epoch: 6 | iteration: 83/262 | Loss: 0.5814410448074341Training Epoch: 6 | iteration: 84/262 | Loss: 0.5871515274047852Training Epoch: 6 | iteration: 85/262 | Loss: 0.6035443544387817Training Epoch: 6 | iteration: 86/262 | Loss: 0.6391238570213318Training Epoch: 6 | iteration: 87/262 | Loss: 0.6090679168701172Training Epoch: 6 | iteration: 88/262 | Loss: 0.6297801733016968Training Epoch: 6 | iteration: 89/262 | Loss: 0.6309448480606079Training Epoch: 6 | iteration: 90/262 | Loss: 0.6222436428070068Training Epoch: 6 | iteration: 91/262 | Loss: 0.590909481048584Training Epoch: 6 | iteration: 92/262 | Loss: 0.5706803202629089Training Epoch: 6 | iteration: 93/262 | Loss: 0.6427740454673767Training Epoch: 6 | iteration: 94/262 | Loss: 0.6138943433761597Training Epoch: 6 | iteration: 95/262 | Loss: 0.5671830773353577Training Epoch: 6 | iteration: 96/262 | Loss: 0.5793799757957458Training Epoch: 6 | iteration: 97/262 | Loss: 0.6029611825942993Training Epoch: 6 | iteration: 98/262 | Loss: 0.5966111421585083Training Epoch: 6 | iteration: 99/262 | Loss: 0.5410454869270325Training Epoch: 6 | iteration: 100/262 | Loss: 0.6543941497802734Training Epoch: 6 | iteration: 101/262 | Loss: 0.64576655626297Training Epoch: 6 | iteration: 102/262 | Loss: 0.6189030408859253Training Epoch: 6 | iteration: 103/262 | Loss: 0.6253961324691772Training Epoch: 6 | iteration: 104/262 | Loss: 0.6027895212173462Training Epoch: 6 | iteration: 105/262 | Loss: 0.5925930738449097Training Epoch: 6 | iteration: 106/262 | Loss: 0.6114654541015625Training Epoch: 6 | iteration: 107/262 | Loss: 0.6207692623138428Training Epoch: 6 | iteration: 108/262 | Loss: 0.6000182628631592Training Epoch: 6 | iteration: 109/262 | Loss: 0.6037719249725342Training Epoch: 6 | iteration: 110/262 | Loss: 0.5840635299682617Training Epoch: 6 | iteration: 111/262 | Loss: 0.5958871245384216Training Epoch: 6 | iteration: 112/262 | Loss: 0.6616306900978088Training Epoch: 6 | iteration: 113/262 | Loss: 0.587661862373352Training Epoch: 6 | iteration: 114/262 | Loss: 0.5787531137466431Training Epoch: 6 | iteration: 115/262 | Loss: 0.6321978569030762Training Epoch: 6 | iteration: 116/262 | Loss: 0.6049837470054626Training Epoch: 6 | iteration: 117/262 | Loss: 0.6477741599082947Training Epoch: 6 | iteration: 118/262 | Loss: 0.5846000909805298Training Epoch: 6 | iteration: 119/262 | Loss: 0.6053023338317871Training Epoch: 6 | iteration: 120/262 | Loss: 0.6170516014099121Training Epoch: 6 | iteration: 121/262 | Loss: 0.6060184240341187Training Epoch: 6 | iteration: 122/262 | Loss: 0.5990152359008789Training Epoch: 6 | iteration: 123/262 | Loss: 0.560954213142395Training Epoch: 6 | iteration: 124/262 | Loss: 0.5960338115692139Training Epoch: 6 | iteration: 125/262 | Loss: 0.6067811250686646Training Epoch: 6 | iteration: 126/262 | Loss: 0.584752082824707Training Epoch: 6 | iteration: 127/262 | Loss: 0.6378723382949829Training Epoch: 6 | iteration: 128/262 | Loss: 0.5833448767662048Training Epoch: 6 | iteration: 129/262 | Loss: 0.5680978894233704Training Epoch: 6 | iteration: 130/262 | Loss: 0.6526014804840088Training Epoch: 6 | iteration: 131/262 | Loss: 0.5906683802604675Training Epoch: 6 | iteration: 132/262 | Loss: 0.5576486587524414Training Epoch: 6 | iteration: 133/262 | Loss: 0.5616828799247742Training Epoch: 6 | iteration: 134/262 | Loss: 0.5751419067382812Training Epoch: 6 | iteration: 135/262 | Loss: 0.5421435832977295Training Epoch: 6 | iteration: 136/262 | Loss: 0.5944068431854248Training Epoch: 6 | iteration: 137/262 | Loss: 0.5587060451507568Training Epoch: 6 | iteration: 138/262 | Loss: 0.6049507856369019Training Epoch: 6 | iteration: 139/262 | Loss: 0.5682464838027954Training Epoch: 6 | iteration: 140/262 | Loss: 0.6868947744369507Training Epoch: 6 | iteration: 141/262 | Loss: 0.5305521488189697Training Epoch: 6 | iteration: 142/262 | Loss: 0.6199771165847778Training Epoch: 6 | iteration: 143/262 | Loss: 0.6661595106124878Training Epoch: 6 | iteration: 144/262 | Loss: 0.5569008588790894Training Epoch: 6 | iteration: 145/262 | Loss: 0.6268882155418396Training Epoch: 6 | iteration: 146/262 | Loss: 0.5886108875274658Training Epoch: 6 | iteration: 147/262 | Loss: 0.6179672479629517Training Epoch: 6 | iteration: 148/262 | Loss: 0.595051109790802Training Epoch: 6 | iteration: 149/262 | Loss: 0.6272875070571899Training Epoch: 6 | iteration: 150/262 | Loss: 0.6127256155014038Training Epoch: 6 | iteration: 151/262 | Loss: 0.5845383405685425Training Epoch: 6 | iteration: 152/262 | Loss: 0.5439940690994263Training Epoch: 6 | iteration: 153/262 | Loss: 0.6129498481750488Training Epoch: 6 | iteration: 154/262 | Loss: 0.591064989566803Training Epoch: 6 | iteration: 155/262 | Loss: 0.6237226128578186Training Epoch: 6 | iteration: 156/262 | Loss: 0.5113077163696289Training Epoch: 6 | iteration: 157/262 | Loss: 0.6186395883560181Training Epoch: 6 | iteration: 158/262 | Loss: 0.565360426902771Training Epoch: 6 | iteration: 159/262 | Loss: 0.6529660820960999Training Epoch: 6 | iteration: 160/262 | Loss: 0.5573260188102722Training Epoch: 6 | iteration: 161/262 | Loss: 0.6031802892684937Training Epoch: 6 | iteration: 162/262 | Loss: 0.5760560631752014Training Epoch: 6 | iteration: 163/262 | Loss: 0.60725998878479Training Epoch: 6 | iteration: 164/262 | Loss: 0.6186454892158508Training Epoch: 6 | iteration: 165/262 | Loss: 0.5832107067108154Training Epoch: 6 | iteration: 166/262 | Loss: 0.6428213119506836Training Epoch: 6 | iteration: 167/262 | Loss: 0.6288506984710693Training Epoch: 6 | iteration: 168/262 | Loss: 0.5809109210968018Training Epoch: 6 | iteration: 169/262 | Loss: 0.5757532119750977Training Epoch: 6 | iteration: 170/262 | Loss: 0.5473124980926514Training Epoch: 6 | iteration: 171/262 | Loss: 0.5477896928787231Training Epoch: 6 | iteration: 172/262 | Loss: 0.6432832479476929Training Epoch: 6 | iteration: 173/262 | Loss: 0.6286826133728027Training Epoch: 6 | iteration: 174/262 | Loss: 0.6187375783920288Training Epoch: 6 | iteration: 175/262 | Loss: 0.6037763953208923Training Epoch: 6 | iteration: 176/262 | Loss: 0.6129580736160278Training Epoch: 6 | iteration: 177/262 | Loss: 0.59748375415802Training Epoch: 6 | iteration: 178/262 | Loss: 0.6097782254219055Training Epoch: 6 | iteration: 179/262 | Loss: 0.5894414186477661Training Epoch: 6 | iteration: 180/262 | Loss: 0.5347777605056763Training Epoch: 6 | iteration: 181/262 | Loss: 0.6572471857070923Training Epoch: 6 | iteration: 182/262 | Loss: 0.5771210193634033Training Epoch: 6 | iteration: 183/262 | Loss: 0.633393406867981Training Epoch: 6 | iteration: 184/262 | Loss: 0.6171627640724182Training Epoch: 6 | iteration: 185/262 | Loss: 0.6429501175880432Training Epoch: 6 | iteration: 186/262 | Loss: 0.582210898399353Training Epoch: 6 | iteration: 187/262 | Loss: 0.622014582157135Training Epoch: 6 | iteration: 188/262 | Loss: 0.626785159111023Training Epoch: 6 | iteration: 189/262 | Loss: 0.5443746447563171Training Epoch: 6 | iteration: 190/262 | Loss: 0.6071533560752869Training Epoch: 6 | iteration: 191/262 | Loss: 0.6021979451179504Training Epoch: 6 | iteration: 192/262 | Loss: 0.6095272302627563Training Epoch: 6 | iteration: 193/262 | Loss: 0.5629535913467407Training Epoch: 6 | iteration: 194/262 | Loss: 0.6157664060592651Training Epoch: 6 | iteration: 195/262 | Loss: 0.6300989389419556Training Epoch: 6 | iteration: 196/262 | Loss: 0.6005135774612427Training Epoch: 6 | iteration: 197/262 | Loss: 0.5717648267745972Training Epoch: 6 | iteration: 198/262 | Loss: 0.5993596315383911Training Epoch: 6 | iteration: 199/262 | Loss: 0.533851683139801Training Epoch: 6 | iteration: 200/262 | Loss: 0.631527841091156Training Epoch: 6 | iteration: 201/262 | Loss: 0.5531954765319824Training Epoch: 6 | iteration: 202/262 | Loss: 0.647369384765625Training Epoch: 6 | iteration: 203/262 | Loss: 0.585475742816925Training Epoch: 6 | iteration: 204/262 | Loss: 0.5921928882598877Training Epoch: 6 | iteration: 205/262 | Loss: 0.6173830628395081Training Epoch: 6 | iteration: 206/262 | Loss: 0.6200141310691833Training Epoch: 6 | iteration: 207/262 | Loss: 0.6298474669456482Training Epoch: 6 | iteration: 208/262 | Loss: 0.6529853343963623Training Epoch: 6 | iteration: 209/262 | Loss: 0.644317626953125Training Epoch: 6 | iteration: 210/262 | Loss: 0.6356741189956665Training Epoch: 6 | iteration: 211/262 | Loss: 0.6539077758789062Training Epoch: 6 | iteration: 212/262 | Loss: 0.5515676736831665Training Epoch: 6 | iteration: 213/262 | Loss: 0.5852535367012024Training Epoch: 6 | iteration: 214/262 | Loss: 0.6174478530883789Training Epoch: 6 | iteration: 215/262 | Loss: 0.584343671798706Training Epoch: 6 | iteration: 216/262 | Loss: 0.6995002627372742Training Epoch: 6 | iteration: 217/262 | Loss: 0.5848073959350586Training Epoch: 6 | iteration: 218/262 | Loss: 0.594303548336029Training Epoch: 6 | iteration: 219/262 | Loss: 0.6412490606307983Training Epoch: 6 | iteration: 220/262 | Loss: 0.5875229835510254Training Epoch: 6 | iteration: 221/262 | Loss: 0.5564466714859009Training Epoch: 6 | iteration: 222/262 | Loss: 0.6486327648162842Training Epoch: 6 | iteration: 223/262 | Loss: 0.5925255417823792Training Epoch: 6 | iteration: 224/262 | Loss: 0.6030030250549316Training Epoch: 6 | iteration: 225/262 | Loss: 0.5720065236091614Training Epoch: 6 | iteration: 226/262 | Loss: 0.566929042339325Training Epoch: 6 | iteration: 227/262 | Loss: 0.595081090927124Training Epoch: 6 | iteration: 228/262 | Loss: 0.5909324288368225Training Epoch: 6 | iteration: 229/262 | Loss: 0.5348844528198242Training Epoch: 6 | iteration: 230/262 | Loss: 0.5742505788803101Training Epoch: 6 | iteration: 231/262 | Loss: 0.5548442602157593Training Epoch: 6 | iteration: 232/262 | Loss: 0.5727463960647583Training Epoch: 6 | iteration: 233/262 | Loss: 0.5719306468963623Training Epoch: 6 | iteration: 234/262 | Loss: 0.6630755662918091Training Epoch: 6 | iteration: 235/262 | Loss: 0.5257683992385864Training Epoch: 6 | iteration: 236/262 | Loss: 0.5855756998062134Training Epoch: 6 | iteration: 237/262 | Loss: 0.5702109336853027Training Epoch: 6 | iteration: 238/262 | Loss: 0.5894753932952881Training Epoch: 6 | iteration: 239/262 | Loss: 0.6719136238098145Training Epoch: 6 | iteration: 240/262 | Loss: 0.5710334181785583Training Epoch: 6 | iteration: 241/262 | Loss: 0.5561397075653076Training Epoch: 6 | iteration: 242/262 | Loss: 0.6043925285339355Training Epoch: 6 | iteration: 243/262 | Loss: 0.6479116678237915Training Epoch: 6 | iteration: 244/262 | Loss: 0.5526950359344482Training Epoch: 6 | iteration: 245/262 | Loss: 0.6283462047576904Training Epoch: 6 | iteration: 246/262 | Loss: 0.5662978887557983Training Epoch: 6 | iteration: 247/262 | Loss: 0.6395988464355469Training Epoch: 6 | iteration: 248/262 | Loss: 0.5566728711128235Training Epoch: 6 | iteration: 249/262 | Loss: 0.6365978717803955Training Epoch: 6 | iteration: 250/262 | Loss: 0.5625187158584595Training Epoch: 6 | iteration: 251/262 | Loss: 0.5534192323684692Training Epoch: 6 | iteration: 252/262 | Loss: 0.6362165808677673Training Epoch: 6 | iteration: 253/262 | Loss: 0.5671012997627258Training Epoch: 6 | iteration: 254/262 | Loss: 0.6700026392936707Training Epoch: 6 | iteration: 255/262 | Loss: 0.5525553822517395Training Epoch: 6 | iteration: 256/262 | Loss: 0.6396794319152832Training Epoch: 6 | iteration: 257/262 | Loss: 0.5408467054367065Training Epoch: 6 | iteration: 258/262 | Loss: 0.6452308893203735Training Epoch: 6 | iteration: 259/262 | Loss: 0.5911931991577148Training Epoch: 6 | iteration: 260/262 | Loss: 0.5641627311706543Training Epoch: 6 | iteration: 261/262 | Loss: 0.6378315687179565Validating Epoch: 6 | iteration: 0/66 | Loss: 0.5900497436523438Validating Epoch: 6 | iteration: 1/66 | Loss: 0.6348376870155334Validating Epoch: 6 | iteration: 2/66 | Loss: 0.5734614729881287Validating Epoch: 6 | iteration: 3/66 | Loss: 0.6649835109710693Validating Epoch: 6 | iteration: 4/66 | Loss: 0.567092776298523Validating Epoch: 6 | iteration: 5/66 | Loss: 0.5812153816223145Validating Epoch: 6 | iteration: 6/66 | Loss: 0.5909841060638428Validating Epoch: 6 | iteration: 7/66 | Loss: 0.5989116430282593Validating Epoch: 6 | iteration: 8/66 | Loss: 0.6029024720191956Validating Epoch: 6 | iteration: 9/66 | Loss: 0.6538587808609009Validating Epoch: 6 | iteration: 10/66 | Loss: 0.6146442890167236Validating Epoch: 6 | iteration: 11/66 | Loss: 0.651923418045044Validating Epoch: 6 | iteration: 12/66 | Loss: 0.6416493654251099Validating Epoch: 6 | iteration: 13/66 | Loss: 0.6867247819900513Validating Epoch: 6 | iteration: 14/66 | Loss: 0.6706661581993103Validating Epoch: 6 | iteration: 15/66 | Loss: 0.6287419199943542Validating Epoch: 6 | iteration: 16/66 | Loss: 0.6388468742370605Validating Epoch: 6 | iteration: 17/66 | Loss: 0.5875743627548218Validating Epoch: 6 | iteration: 18/66 | Loss: 0.5588035583496094Validating Epoch: 6 | iteration: 19/66 | Loss: 0.5464401245117188Validating Epoch: 6 | iteration: 20/66 | Loss: 0.5843777060508728Validating Epoch: 6 | iteration: 21/66 | Loss: 0.6303554773330688Validating Epoch: 6 | iteration: 22/66 | Loss: 0.6334459185600281Validating Epoch: 6 | iteration: 23/66 | Loss: 0.5750101804733276Validating Epoch: 6 | iteration: 24/66 | Loss: 0.5675501823425293Validating Epoch: 6 | iteration: 25/66 | Loss: 0.6132344007492065Validating Epoch: 6 | iteration: 26/66 | Loss: 0.624011218547821Validating Epoch: 6 | iteration: 27/66 | Loss: 0.593783974647522Validating Epoch: 6 | iteration: 28/66 | Loss: 0.6881066560745239Validating Epoch: 6 | iteration: 29/66 | Loss: 0.6340013742446899Validating Epoch: 6 | iteration: 30/66 | Loss: 0.645854651927948Validating Epoch: 6 | iteration: 31/66 | Loss: 0.6428359746932983Validating Epoch: 6 | iteration: 32/66 | Loss: 0.5975450277328491Validating Epoch: 6 | iteration: 33/66 | Loss: 0.5955238342285156Validating Epoch: 6 | iteration: 34/66 | Loss: 0.6170494556427002Validating Epoch: 6 | iteration: 35/66 | Loss: 0.5992465019226074Validating Epoch: 6 | iteration: 36/66 | Loss: 0.5958960652351379Validating Epoch: 6 | iteration: 37/66 | Loss: 0.5566334128379822Validating Epoch: 6 | iteration: 38/66 | Loss: 0.6351120471954346Validating Epoch: 6 | iteration: 39/66 | Loss: 0.5622668266296387Validating Epoch: 6 | iteration: 40/66 | Loss: 0.6040761470794678Validating Epoch: 6 | iteration: 41/66 | Loss: 0.6423576474189758Validating Epoch: 6 | iteration: 42/66 | Loss: 0.5822025537490845Validating Epoch: 6 | iteration: 43/66 | Loss: 0.6380776166915894Validating Epoch: 6 | iteration: 44/66 | Loss: 0.5893429517745972Validating Epoch: 6 | iteration: 45/66 | Loss: 0.6070619225502014Validating Epoch: 6 | iteration: 46/66 | Loss: 0.6205134391784668Validating Epoch: 6 | iteration: 47/66 | Loss: 0.5689836740493774Validating Epoch: 6 | iteration: 48/66 | Loss: 0.5367259979248047Validating Epoch: 6 | iteration: 49/66 | Loss: 0.5297425389289856Validating Epoch: 6 | iteration: 50/66 | Loss: 0.6105483770370483Validating Epoch: 6 | iteration: 51/66 | Loss: 0.625002384185791Validating Epoch: 6 | iteration: 52/66 | Loss: 0.6430380940437317Validating Epoch: 6 | iteration: 53/66 | Loss: 0.5349230766296387Validating Epoch: 6 | iteration: 54/66 | Loss: 0.5939327478408813Validating Epoch: 6 | iteration: 55/66 | Loss: 0.6327224373817444Validating Epoch: 6 | iteration: 56/66 | Loss: 0.6296762228012085Validating Epoch: 6 | iteration: 57/66 | Loss: 0.6147459745407104Validating Epoch: 6 | iteration: 58/66 | Loss: 0.5621919631958008Validating Epoch: 6 | iteration: 59/66 | Loss: 0.5668206214904785Validating Epoch: 6 | iteration: 60/66 | Loss: 0.5808964967727661Validating Epoch: 6 | iteration: 61/66 | Loss: 0.6049704551696777Validating Epoch: 6 | iteration: 62/66 | Loss: 0.6120729446411133Validating Epoch: 6 | iteration: 63/66 | Loss: 0.5710004568099976Validating Epoch: 6 | iteration: 64/66 | Loss: 0.6676249504089355Validating Epoch: 6 | iteration: 65/66 | Loss: 0.6718381643295288Temperature:  0.5
Sampling: iteration: 0/51{'Validity': 0.9853515625, 'Novelty': 1.0, 'Uniqueness': 0.9900891972249752}
Training Epoch: 7 | iteration: 0/262 | Loss: 0.5855588912963867Training Epoch: 7 | iteration: 1/262 | Loss: 0.6036522388458252Training Epoch: 7 | iteration: 2/262 | Loss: 0.6016144752502441Training Epoch: 7 | iteration: 3/262 | Loss: 0.5878560543060303Training Epoch: 7 | iteration: 4/262 | Loss: 0.6182375550270081Training Epoch: 7 | iteration: 5/262 | Loss: 0.6273059844970703Training Epoch: 7 | iteration: 6/262 | Loss: 0.5533690452575684Training Epoch: 7 | iteration: 7/262 | Loss: 0.5348096489906311Training Epoch: 7 | iteration: 8/262 | Loss: 0.6168811917304993Training Epoch: 7 | iteration: 9/262 | Loss: 0.6312634944915771Training Epoch: 7 | iteration: 10/262 | Loss: 0.5907217264175415Training Epoch: 7 | iteration: 11/262 | Loss: 0.619025468826294Training Epoch: 7 | iteration: 12/262 | Loss: 0.5694135427474976Training Epoch: 7 | iteration: 13/262 | Loss: 0.5866285562515259Training Epoch: 7 | iteration: 14/262 | Loss: 0.6030919551849365Training Epoch: 7 | iteration: 15/262 | Loss: 0.618290901184082Training Epoch: 7 | iteration: 16/262 | Loss: 0.5962249636650085Training Epoch: 7 | iteration: 17/262 | Loss: 0.6210484504699707Training Epoch: 7 | iteration: 18/262 | Loss: 0.5277137160301208Training Epoch: 7 | iteration: 19/262 | Loss: 0.5567233562469482Training Epoch: 7 | iteration: 20/262 | Loss: 0.5465759038925171Training Epoch: 7 | iteration: 21/262 | Loss: 0.6031825542449951Training Epoch: 7 | iteration: 22/262 | Loss: 0.5858058929443359Training Epoch: 7 | iteration: 23/262 | Loss: 0.5548005104064941Training Epoch: 7 | iteration: 24/262 | Loss: 0.5612623691558838Training Epoch: 7 | iteration: 25/262 | Loss: 0.6164047718048096Training Epoch: 7 | iteration: 26/262 | Loss: 0.5541877150535583Training Epoch: 7 | iteration: 27/262 | Loss: 0.5454328060150146Training Epoch: 7 | iteration: 28/262 | Loss: 0.5826191902160645Training Epoch: 7 | iteration: 29/262 | Loss: 0.5644028782844543Training Epoch: 7 | iteration: 30/262 | Loss: 0.6209293007850647Training Epoch: 7 | iteration: 31/262 | Loss: 0.5784628391265869Training Epoch: 7 | iteration: 32/262 | Loss: 0.5616823434829712Training Epoch: 7 | iteration: 33/262 | Loss: 0.5880731344223022Training Epoch: 7 | iteration: 34/262 | Loss: 0.5775084495544434Training Epoch: 7 | iteration: 35/262 | Loss: 0.5360084176063538Training Epoch: 7 | iteration: 36/262 | Loss: 0.6023543477058411Training Epoch: 7 | iteration: 37/262 | Loss: 0.5628522634506226Training Epoch: 7 | iteration: 38/262 | Loss: 0.597835898399353Training Epoch: 7 | iteration: 39/262 | Loss: 0.5949361324310303Training Epoch: 7 | iteration: 40/262 | Loss: 0.6061414480209351Training Epoch: 7 | iteration: 41/262 | Loss: 0.5679578185081482Training Epoch: 7 | iteration: 42/262 | Loss: 0.5446338653564453Training Epoch: 7 | iteration: 43/262 | Loss: 0.6436079740524292Training Epoch: 7 | iteration: 44/262 | Loss: 0.5760077834129333Training Epoch: 7 | iteration: 45/262 | Loss: 0.5475131273269653Training Epoch: 7 | iteration: 46/262 | Loss: 0.5896636843681335Training Epoch: 7 | iteration: 47/262 | Loss: 0.5729930996894836Training Epoch: 7 | iteration: 48/262 | Loss: 0.6033309698104858Training Epoch: 7 | iteration: 49/262 | Loss: 0.5508321523666382Training Epoch: 7 | iteration: 50/262 | Loss: 0.5561213493347168Training Epoch: 7 | iteration: 51/262 | Loss: 0.6194214820861816Training Epoch: 7 | iteration: 52/262 | Loss: 0.5795334577560425Training Epoch: 7 | iteration: 53/262 | Loss: 0.6482119560241699Training Epoch: 7 | iteration: 54/262 | Loss: 0.5552819967269897Training Epoch: 7 | iteration: 55/262 | Loss: 0.5411657094955444Training Epoch: 7 | iteration: 56/262 | Loss: 0.5685667991638184Training Epoch: 7 | iteration: 57/262 | Loss: 0.5272731781005859Training Epoch: 7 | iteration: 58/262 | Loss: 0.5493378639221191Training Epoch: 7 | iteration: 59/262 | Loss: 0.5936295986175537Training Epoch: 7 | iteration: 60/262 | Loss: 0.5459920167922974Training Epoch: 7 | iteration: 61/262 | Loss: 0.6000238656997681Training Epoch: 7 | iteration: 62/262 | Loss: 0.557059645652771Training Epoch: 7 | iteration: 63/262 | Loss: 0.565290093421936Training Epoch: 7 | iteration: 64/262 | Loss: 0.5827254056930542Training Epoch: 7 | iteration: 65/262 | Loss: 0.5273327827453613Training Epoch: 7 | iteration: 66/262 | Loss: 0.6278387308120728Training Epoch: 7 | iteration: 67/262 | Loss: 0.6242396831512451Training Epoch: 7 | iteration: 68/262 | Loss: 0.5848333835601807Training Epoch: 7 | iteration: 69/262 | Loss: 0.612514078617096Training Epoch: 7 | iteration: 70/262 | Loss: 0.5842179656028748Training Epoch: 7 | iteration: 71/262 | Loss: 0.555323600769043Training Epoch: 7 | iteration: 72/262 | Loss: 0.5706932544708252Training Epoch: 7 | iteration: 73/262 | Loss: 0.56758713722229Training Epoch: 7 | iteration: 74/262 | Loss: 0.5625993013381958Training Epoch: 7 | iteration: 75/262 | Loss: 0.6352587938308716Training Epoch: 7 | iteration: 76/262 | Loss: 0.6100195050239563Training Epoch: 7 | iteration: 77/262 | Loss: 0.5542280077934265Training Epoch: 7 | iteration: 78/262 | Loss: 0.5611995458602905Training Epoch: 7 | iteration: 79/262 | Loss: 0.6058287620544434Training Epoch: 7 | iteration: 80/262 | Loss: 0.6392115354537964Training Epoch: 7 | iteration: 81/262 | Loss: 0.6385208964347839Training Epoch: 7 | iteration: 82/262 | Loss: 0.5516344308853149Training Epoch: 7 | iteration: 83/262 | Loss: 0.5942581295967102Training Epoch: 7 | iteration: 84/262 | Loss: 0.613515317440033Training Epoch: 7 | iteration: 85/262 | Loss: 0.590056836605072Training Epoch: 7 | iteration: 86/262 | Loss: 0.5608598589897156Training Epoch: 7 | iteration: 87/262 | Loss: 0.5206001996994019Training Epoch: 7 | iteration: 88/262 | Loss: 0.5524787902832031Training Epoch: 7 | iteration: 89/262 | Loss: 0.5842709541320801Training Epoch: 7 | iteration: 90/262 | Loss: 0.6050167679786682Training Epoch: 7 | iteration: 91/262 | Loss: 0.5823640823364258Training Epoch: 7 | iteration: 92/262 | Loss: 0.5891192555427551Training Epoch: 7 | iteration: 93/262 | Loss: 0.6760219931602478Training Epoch: 7 | iteration: 94/262 | Loss: 0.5814341306686401Training Epoch: 7 | iteration: 95/262 | Loss: 0.578461766242981Training Epoch: 7 | iteration: 96/262 | Loss: 0.548920750617981Training Epoch: 7 | iteration: 97/262 | Loss: 0.5831106901168823Training Epoch: 7 | iteration: 98/262 | Loss: 0.6485819220542908Training Epoch: 7 | iteration: 99/262 | Loss: 0.6265467405319214Training Epoch: 7 | iteration: 100/262 | Loss: 0.6476977467536926Training Epoch: 7 | iteration: 101/262 | Loss: 0.5819576382637024Training Epoch: 7 | iteration: 102/262 | Loss: 0.5506610870361328Training Epoch: 7 | iteration: 103/262 | Loss: 0.6077269315719604Training Epoch: 7 | iteration: 104/262 | Loss: 0.5615051984786987Training Epoch: 7 | iteration: 105/262 | Loss: 0.5648456811904907Training Epoch: 7 | iteration: 106/262 | Loss: 0.5836851000785828Training Epoch: 7 | iteration: 107/262 | Loss: 0.5311235189437866Training Epoch: 7 | iteration: 108/262 | Loss: 0.6258381009101868Training Epoch: 7 | iteration: 109/262 | Loss: 0.535179853439331Training Epoch: 7 | iteration: 110/262 | Loss: 0.5804199576377869Training Epoch: 7 | iteration: 111/262 | Loss: 0.520882785320282Training Epoch: 7 | iteration: 112/262 | Loss: 0.5921016931533813Training Epoch: 7 | iteration: 113/262 | Loss: 0.5892696976661682Training Epoch: 7 | iteration: 114/262 | Loss: 0.5700550079345703Training Epoch: 7 | iteration: 115/262 | Loss: 0.624280571937561Training Epoch: 7 | iteration: 116/262 | Loss: 0.5593950152397156Training Epoch: 7 | iteration: 117/262 | Loss: 0.5937581658363342Training Epoch: 7 | iteration: 118/262 | Loss: 0.5694751739501953Training Epoch: 7 | iteration: 119/262 | Loss: 0.5213037133216858Training Epoch: 7 | iteration: 120/262 | Loss: 0.5388992428779602Training Epoch: 7 | iteration: 121/262 | Loss: 0.5832068920135498Training Epoch: 7 | iteration: 122/262 | Loss: 0.615779459476471Training Epoch: 7 | iteration: 123/262 | Loss: 0.5634211301803589Training Epoch: 7 | iteration: 124/262 | Loss: 0.6230744123458862Training Epoch: 7 | iteration: 125/262 | Loss: 0.5977991819381714Training Epoch: 7 | iteration: 126/262 | Loss: 0.6186175346374512Training Epoch: 7 | iteration: 127/262 | Loss: 0.6464234590530396Training Epoch: 7 | iteration: 128/262 | Loss: 0.5194615125656128Training Epoch: 7 | iteration: 129/262 | Loss: 0.5842087864875793Training Epoch: 7 | iteration: 130/262 | Loss: 0.5875879526138306Training Epoch: 7 | iteration: 131/262 | Loss: 0.6093701720237732Training Epoch: 7 | iteration: 132/262 | Loss: 0.5900915265083313Training Epoch: 7 | iteration: 133/262 | Loss: 0.5380529165267944Training Epoch: 7 | iteration: 134/262 | Loss: 0.6144301891326904Training Epoch: 7 | iteration: 135/262 | Loss: 0.6016924381256104Training Epoch: 7 | iteration: 136/262 | Loss: 0.6017712354660034Training Epoch: 7 | iteration: 137/262 | Loss: 0.5994206070899963Training Epoch: 7 | iteration: 138/262 | Loss: 0.6849603056907654Training Epoch: 7 | iteration: 139/262 | Loss: 0.5548325777053833Training Epoch: 7 | iteration: 140/262 | Loss: 0.6510118246078491Training Epoch: 7 | iteration: 141/262 | Loss: 0.6345424652099609Training Epoch: 7 | iteration: 142/262 | Loss: 0.564314603805542Training Epoch: 7 | iteration: 143/262 | Loss: 0.575093150138855Training Epoch: 7 | iteration: 144/262 | Loss: 0.5898528099060059Training Epoch: 7 | iteration: 145/262 | Loss: 0.628087043762207Training Epoch: 7 | iteration: 146/262 | Loss: 0.6186798810958862Training Epoch: 7 | iteration: 147/262 | Loss: 0.5848153829574585Training Epoch: 7 | iteration: 148/262 | Loss: 0.5978431701660156Training Epoch: 7 | iteration: 149/262 | Loss: 0.6261945366859436Training Epoch: 7 | iteration: 150/262 | Loss: 0.6735397577285767Training Epoch: 7 | iteration: 151/262 | Loss: 0.5605485439300537Training Epoch: 7 | iteration: 152/262 | Loss: 0.612982988357544Training Epoch: 7 | iteration: 153/262 | Loss: 0.6036308407783508Training Epoch: 7 | iteration: 154/262 | Loss: 0.5569451451301575Training Epoch: 7 | iteration: 155/262 | Loss: 0.5765818357467651Training Epoch: 7 | iteration: 156/262 | Loss: 0.5408427715301514Training Epoch: 7 | iteration: 157/262 | Loss: 0.5672456622123718Training Epoch: 7 | iteration: 158/262 | Loss: 0.647662878036499Training Epoch: 7 | iteration: 159/262 | Loss: 0.6282321214675903Training Epoch: 7 | iteration: 160/262 | Loss: 0.5745977163314819Training Epoch: 7 | iteration: 161/262 | Loss: 0.631341814994812Training Epoch: 7 | iteration: 162/262 | Loss: 0.6216158270835876Training Epoch: 7 | iteration: 163/262 | Loss: 0.6337306499481201Training Epoch: 7 | iteration: 164/262 | Loss: 0.6601774096488953Training Epoch: 7 | iteration: 165/262 | Loss: 0.584766685962677Training Epoch: 7 | iteration: 166/262 | Loss: 0.5910063982009888Training Epoch: 7 | iteration: 167/262 | Loss: 0.560171365737915Training Epoch: 7 | iteration: 168/262 | Loss: 0.636029064655304Training Epoch: 7 | iteration: 169/262 | Loss: 0.5923049449920654Training Epoch: 7 | iteration: 170/262 | Loss: 0.5476133823394775Training Epoch: 7 | iteration: 171/262 | Loss: 0.5849570631980896Training Epoch: 7 | iteration: 172/262 | Loss: 0.5456709861755371Training Epoch: 7 | iteration: 173/262 | Loss: 0.6246514320373535Training Epoch: 7 | iteration: 174/262 | Loss: 0.6230076551437378Training Epoch: 7 | iteration: 175/262 | Loss: 0.5733548402786255Training Epoch: 7 | iteration: 176/262 | Loss: 0.6169472932815552Training Epoch: 7 | iteration: 177/262 | Loss: 0.5596564412117004Training Epoch: 7 | iteration: 178/262 | Loss: 0.5826077461242676Training Epoch: 7 | iteration: 179/262 | Loss: 0.6433691382408142Training Epoch: 7 | iteration: 180/262 | Loss: 0.6237106323242188Training Epoch: 7 | iteration: 181/262 | Loss: 0.5861278772354126Training Epoch: 7 | iteration: 182/262 | Loss: 0.6701242923736572Training Epoch: 7 | iteration: 183/262 | Loss: 0.5281571745872498Training Epoch: 7 | iteration: 184/262 | Loss: 0.5822192430496216Training Epoch: 7 | iteration: 185/262 | Loss: 0.6290745735168457Training Epoch: 7 | iteration: 186/262 | Loss: 0.6166235208511353Training Epoch: 7 | iteration: 187/262 | Loss: 0.5785917639732361Training Epoch: 7 | iteration: 188/262 | Loss: 0.6207406520843506Training Epoch: 7 | iteration: 189/262 | Loss: 0.6045252084732056Training Epoch: 7 | iteration: 190/262 | Loss: 0.6585587859153748Training Epoch: 7 | iteration: 191/262 | Loss: 0.5866827368736267Training Epoch: 7 | iteration: 192/262 | Loss: 0.6093107461929321Training Epoch: 7 | iteration: 193/262 | Loss: 0.5798640847206116Training Epoch: 7 | iteration: 194/262 | Loss: 0.5309569835662842Training Epoch: 7 | iteration: 195/262 | Loss: 0.5902692079544067Training Epoch: 7 | iteration: 196/262 | Loss: 0.570831298828125Training Epoch: 7 | iteration: 197/262 | Loss: 0.5912439823150635Training Epoch: 7 | iteration: 198/262 | Loss: 0.5249021649360657Training Epoch: 7 | iteration: 199/262 | Loss: 0.5537657141685486Training Epoch: 7 | iteration: 200/262 | Loss: 0.5960306525230408Training Epoch: 7 | iteration: 201/262 | Loss: 0.6311304569244385Training Epoch: 7 | iteration: 202/262 | Loss: 0.5673531889915466Training Epoch: 7 | iteration: 203/262 | Loss: 0.5523651242256165Training Epoch: 7 | iteration: 204/262 | Loss: 0.5851162672042847Training Epoch: 7 | iteration: 205/262 | Loss: 0.5985685586929321Training Epoch: 7 | iteration: 206/262 | Loss: 0.5356170535087585Training Epoch: 7 | iteration: 207/262 | Loss: 0.5859944820404053Training Epoch: 7 | iteration: 208/262 | Loss: 0.5512821078300476Training Epoch: 7 | iteration: 209/262 | Loss: 0.6022399067878723Training Epoch: 7 | iteration: 210/262 | Loss: 0.5822358131408691Training Epoch: 7 | iteration: 211/262 | Loss: 0.6033548712730408Training Epoch: 7 | iteration: 212/262 | Loss: 0.5621389150619507Training Epoch: 7 | iteration: 213/262 | Loss: 0.5491788983345032Training Epoch: 7 | iteration: 214/262 | Loss: 0.5805709958076477Training Epoch: 7 | iteration: 215/262 | Loss: 0.5962875485420227Training Epoch: 7 | iteration: 216/262 | Loss: 0.575874924659729Training Epoch: 7 | iteration: 217/262 | Loss: 0.6031385660171509Training Epoch: 7 | iteration: 218/262 | Loss: 0.6193218231201172Training Epoch: 7 | iteration: 219/262 | Loss: 0.5410067439079285Training Epoch: 7 | iteration: 220/262 | Loss: 0.5415314435958862Training Epoch: 7 | iteration: 221/262 | Loss: 0.6003915071487427Training Epoch: 7 | iteration: 222/262 | Loss: 0.6154860258102417Training Epoch: 7 | iteration: 223/262 | Loss: 0.5811597108840942Training Epoch: 7 | iteration: 224/262 | Loss: 0.5504610538482666Training Epoch: 7 | iteration: 225/262 | Loss: 0.5694786310195923Training Epoch: 7 | iteration: 226/262 | Loss: 0.5732197761535645Training Epoch: 7 | iteration: 227/262 | Loss: 0.6277814507484436Training Epoch: 7 | iteration: 228/262 | Loss: 0.6027212142944336Training Epoch: 7 | iteration: 229/262 | Loss: 0.6251072883605957Training Epoch: 7 | iteration: 230/262 | Loss: 0.6229863166809082Training Epoch: 7 | iteration: 231/262 | Loss: 0.5811185836791992Training Epoch: 7 | iteration: 232/262 | Loss: 0.5738593935966492Training Epoch: 7 | iteration: 233/262 | Loss: 0.6223975419998169Training Epoch: 7 | iteration: 234/262 | Loss: 0.5707889795303345Training Epoch: 7 | iteration: 235/262 | Loss: 0.5442889332771301Training Epoch: 7 | iteration: 236/262 | Loss: 0.6098278164863586Training Epoch: 7 | iteration: 237/262 | Loss: 0.5649741888046265Training Epoch: 7 | iteration: 238/262 | Loss: 0.517974853515625Training Epoch: 7 | iteration: 239/262 | Loss: 0.5864601135253906Training Epoch: 7 | iteration: 240/262 | Loss: 0.54842209815979Training Epoch: 7 | iteration: 241/262 | Loss: 0.5869396924972534Training Epoch: 7 | iteration: 242/262 | Loss: 0.6313965916633606Training Epoch: 7 | iteration: 243/262 | Loss: 0.5907238721847534Training Epoch: 7 | iteration: 244/262 | Loss: 0.6294811964035034Training Epoch: 7 | iteration: 245/262 | Loss: 0.5910899639129639Training Epoch: 7 | iteration: 246/262 | Loss: 0.5905658602714539Training Epoch: 7 | iteration: 247/262 | Loss: 0.5373985767364502Training Epoch: 7 | iteration: 248/262 | Loss: 0.5593249797821045Training Epoch: 7 | iteration: 249/262 | Loss: 0.5746411085128784Training Epoch: 7 | iteration: 250/262 | Loss: 0.5359021425247192Training Epoch: 7 | iteration: 251/262 | Loss: 0.5544486045837402Training Epoch: 7 | iteration: 252/262 | Loss: 0.5622482895851135Training Epoch: 7 | iteration: 253/262 | Loss: 0.5782252550125122Training Epoch: 7 | iteration: 254/262 | Loss: 0.6183791756629944Training Epoch: 7 | iteration: 255/262 | Loss: 0.5640915632247925Training Epoch: 7 | iteration: 256/262 | Loss: 0.5700162649154663Training Epoch: 7 | iteration: 257/262 | Loss: 0.6477986574172974Training Epoch: 7 | iteration: 258/262 | Loss: 0.5705077648162842Training Epoch: 7 | iteration: 259/262 | Loss: 0.5516362190246582Training Epoch: 7 | iteration: 260/262 | Loss: 0.5807932019233704Training Epoch: 7 | iteration: 261/262 | Loss: 0.5723556280136108Validating Epoch: 7 | iteration: 0/66 | Loss: 0.6273823976516724Validating Epoch: 7 | iteration: 1/66 | Loss: 0.5705294609069824Validating Epoch: 7 | iteration: 2/66 | Loss: 0.6120424270629883Validating Epoch: 7 | iteration: 3/66 | Loss: 0.6326131820678711Validating Epoch: 7 | iteration: 4/66 | Loss: 0.6121070384979248Validating Epoch: 7 | iteration: 5/66 | Loss: 0.5702465176582336Validating Epoch: 7 | iteration: 6/66 | Loss: 0.5663175582885742Validating Epoch: 7 | iteration: 7/66 | Loss: 0.5714683532714844Validating Epoch: 7 | iteration: 8/66 | Loss: 0.5883843898773193Validating Epoch: 7 | iteration: 9/66 | Loss: 0.6159241199493408Validating Epoch: 7 | iteration: 10/66 | Loss: 0.5904333591461182Validating Epoch: 7 | iteration: 11/66 | Loss: 0.6492211818695068Validating Epoch: 7 | iteration: 12/66 | Loss: 0.5900264978408813Validating Epoch: 7 | iteration: 13/66 | Loss: 0.5809659957885742Validating Epoch: 7 | iteration: 14/66 | Loss: 0.6717892289161682Validating Epoch: 7 | iteration: 15/66 | Loss: 0.5889304876327515Validating Epoch: 7 | iteration: 16/66 | Loss: 0.6253493428230286Validating Epoch: 7 | iteration: 17/66 | Loss: 0.5682076811790466Validating Epoch: 7 | iteration: 18/66 | Loss: 0.6358993053436279Validating Epoch: 7 | iteration: 19/66 | Loss: 0.6164243221282959Validating Epoch: 7 | iteration: 20/66 | Loss: 0.5981351137161255Validating Epoch: 7 | iteration: 21/66 | Loss: 0.660213828086853Validating Epoch: 7 | iteration: 22/66 | Loss: 0.593330442905426Validating Epoch: 7 | iteration: 23/66 | Loss: 0.6258750557899475Validating Epoch: 7 | iteration: 24/66 | Loss: 0.6052181124687195Validating Epoch: 7 | iteration: 25/66 | Loss: 0.5667763948440552Validating Epoch: 7 | iteration: 26/66 | Loss: 0.595154881477356Validating Epoch: 7 | iteration: 27/66 | Loss: 0.6472866535186768Validating Epoch: 7 | iteration: 28/66 | Loss: 0.5949252843856812Validating Epoch: 7 | iteration: 29/66 | Loss: 0.6642944812774658Validating Epoch: 7 | iteration: 30/66 | Loss: 0.5559556484222412Validating Epoch: 7 | iteration: 31/66 | Loss: 0.5990461707115173Validating Epoch: 7 | iteration: 32/66 | Loss: 0.5868675708770752Validating Epoch: 7 | iteration: 33/66 | Loss: 0.5950372219085693Validating Epoch: 7 | iteration: 34/66 | Loss: 0.6257011890411377Validating Epoch: 7 | iteration: 35/66 | Loss: 0.5742053985595703Validating Epoch: 7 | iteration: 36/66 | Loss: 0.6131439805030823Validating Epoch: 7 | iteration: 37/66 | Loss: 0.6039541959762573Validating Epoch: 7 | iteration: 38/66 | Loss: 0.6136135458946228Validating Epoch: 7 | iteration: 39/66 | Loss: 0.6214192509651184Validating Epoch: 7 | iteration: 40/66 | Loss: 0.6126599311828613Validating Epoch: 7 | iteration: 41/66 | Loss: 0.5497542023658752Validating Epoch: 7 | iteration: 42/66 | Loss: 0.618983268737793Validating Epoch: 7 | iteration: 43/66 | Loss: 0.6130225658416748Validating Epoch: 7 | iteration: 44/66 | Loss: 0.6081974506378174Validating Epoch: 7 | iteration: 45/66 | Loss: 0.5505682826042175Validating Epoch: 7 | iteration: 46/66 | Loss: 0.6471313238143921Validating Epoch: 7 | iteration: 47/66 | Loss: 0.6031476259231567Validating Epoch: 7 | iteration: 48/66 | Loss: 0.5669779777526855Validating Epoch: 7 | iteration: 49/66 | Loss: 0.603655219078064Validating Epoch: 7 | iteration: 50/66 | Loss: 0.6220746636390686Validating Epoch: 7 | iteration: 51/66 | Loss: 0.661801278591156Validating Epoch: 7 | iteration: 52/66 | Loss: 0.5938999652862549Validating Epoch: 7 | iteration: 53/66 | Loss: 0.6101252436637878Validating Epoch: 7 | iteration: 54/66 | Loss: 0.6723771691322327Validating Epoch: 7 | iteration: 55/66 | Loss: 0.6349276900291443Validating Epoch: 7 | iteration: 56/66 | Loss: 0.5948026776313782Validating Epoch: 7 | iteration: 57/66 | Loss: 0.5830206871032715Validating Epoch: 7 | iteration: 58/66 | Loss: 0.672836184501648Validating Epoch: 7 | iteration: 59/66 | Loss: 0.584450900554657Validating Epoch: 7 | iteration: 60/66 | Loss: 0.6776366233825684Validating Epoch: 7 | iteration: 61/66 | Loss: 0.6197478175163269Validating Epoch: 7 | iteration: 62/66 | Loss: 0.5504507422447205Validating Epoch: 7 | iteration: 63/66 | Loss: 0.6023159027099609Validating Epoch: 7 | iteration: 64/66 | Loss: 0.6309630870819092Validating Epoch: 7 | iteration: 65/66 | Loss: 0.6846906542778015Temperature:  0.5
Sampling: iteration: 0/51{'Validity': 0.98828125, 'Novelty': 1.0, 'Uniqueness': 0.9940711462450593}
Training Epoch: 8 | iteration: 0/262 | Loss: 0.5759490132331848Training Epoch: 8 | iteration: 1/262 | Loss: 0.6130723357200623Training Epoch: 8 | iteration: 2/262 | Loss: 0.6055951118469238Training Epoch: 8 | iteration: 3/262 | Loss: 0.5644047856330872Training Epoch: 8 | iteration: 4/262 | Loss: 0.6329082250595093Training Epoch: 8 | iteration: 5/262 | Loss: 0.6244964003562927Training Epoch: 8 | iteration: 6/262 | Loss: 0.5839961171150208Training Epoch: 8 | iteration: 7/262 | Loss: 0.5645070672035217Training Epoch: 8 | iteration: 8/262 | Loss: 0.5890394449234009Training Epoch: 8 | iteration: 9/262 | Loss: 0.5780857801437378Training Epoch: 8 | iteration: 10/262 | Loss: 0.5434560775756836Training Epoch: 8 | iteration: 11/262 | Loss: 0.5694258213043213Training Epoch: 8 | iteration: 12/262 | Loss: 0.577275812625885Training Epoch: 8 | iteration: 13/262 | Loss: 0.5642758011817932Training Epoch: 8 | iteration: 14/262 | Loss: 0.6034139394760132Training Epoch: 8 | iteration: 15/262 | Loss: 0.5472398400306702Training Epoch: 8 | iteration: 16/262 | Loss: 0.5871754884719849Training Epoch: 8 | iteration: 17/262 | Loss: 0.5652580261230469Training Epoch: 8 | iteration: 18/262 | Loss: 0.5634918212890625Training Epoch: 8 | iteration: 19/262 | Loss: 0.5775918960571289Training Epoch: 8 | iteration: 20/262 | Loss: 0.5279802083969116Training Epoch: 8 | iteration: 21/262 | Loss: 0.5125211477279663Training Epoch: 8 | iteration: 22/262 | Loss: 0.5568903684616089Training Epoch: 8 | iteration: 23/262 | Loss: 0.578543484210968Training Epoch: 8 | iteration: 24/262 | Loss: 0.5545272827148438Training Epoch: 8 | iteration: 25/262 | Loss: 0.5375127196311951Training Epoch: 8 | iteration: 26/262 | Loss: 0.5621324777603149Training Epoch: 8 | iteration: 27/262 | Loss: 0.5637941956520081Training Epoch: 8 | iteration: 28/262 | Loss: 0.5242488384246826Training Epoch: 8 | iteration: 29/262 | Loss: 0.5985090136528015Training Epoch: 8 | iteration: 30/262 | Loss: 0.5214667916297913Training Epoch: 8 | iteration: 31/262 | Loss: 0.56168532371521Training Epoch: 8 | iteration: 32/262 | Loss: 0.5979077816009521Training Epoch: 8 | iteration: 33/262 | Loss: 0.5439814329147339Training Epoch: 8 | iteration: 34/262 | Loss: 0.6273638010025024Training Epoch: 8 | iteration: 35/262 | Loss: 0.5575909614562988Training Epoch: 8 | iteration: 36/262 | Loss: 0.5189208984375Training Epoch: 8 | iteration: 37/262 | Loss: 0.5876336693763733Training Epoch: 8 | iteration: 38/262 | Loss: 0.5490638017654419Training Epoch: 8 | iteration: 39/262 | Loss: 0.5916630625724792Training Epoch: 8 | iteration: 40/262 | Loss: 0.5806865692138672Training Epoch: 8 | iteration: 41/262 | Loss: 0.5716971158981323Training Epoch: 8 | iteration: 42/262 | Loss: 0.5532827377319336Training Epoch: 8 | iteration: 43/262 | Loss: 0.5394881963729858Training Epoch: 8 | iteration: 44/262 | Loss: 0.5981197357177734Training Epoch: 8 | iteration: 45/262 | Loss: 0.5678470134735107Training Epoch: 8 | iteration: 46/262 | Loss: 0.5485519170761108Training Epoch: 8 | iteration: 47/262 | Loss: 0.6048897504806519Training Epoch: 8 | iteration: 48/262 | Loss: 0.5113667845726013Training Epoch: 8 | iteration: 49/262 | Loss: 0.5137133598327637Training Epoch: 8 | iteration: 50/262 | Loss: 0.5924217700958252Training Epoch: 8 | iteration: 51/262 | Loss: 0.5077170729637146Training Epoch: 8 | iteration: 52/262 | Loss: 0.5483461022377014Training Epoch: 8 | iteration: 53/262 | Loss: 0.6022616028785706Training Epoch: 8 | iteration: 54/262 | Loss: 0.5874379873275757Training Epoch: 8 | iteration: 55/262 | Loss: 0.6144300699234009Training Epoch: 8 | iteration: 56/262 | Loss: 0.5978137850761414Training Epoch: 8 | iteration: 57/262 | Loss: 0.5900857448577881Training Epoch: 8 | iteration: 58/262 | Loss: 0.5354397296905518Training Epoch: 8 | iteration: 59/262 | Loss: 0.5750341415405273Training Epoch: 8 | iteration: 60/262 | Loss: 0.5294395685195923Training Epoch: 8 | iteration: 61/262 | Loss: 0.5645407438278198Training Epoch: 8 | iteration: 62/262 | Loss: 0.5521359443664551Training Epoch: 8 | iteration: 63/262 | Loss: 0.5654449462890625Training Epoch: 8 | iteration: 64/262 | Loss: 0.5834020376205444Training Epoch: 8 | iteration: 65/262 | Loss: 0.5906887054443359Training Epoch: 8 | iteration: 66/262 | Loss: 0.528350830078125Training Epoch: 8 | iteration: 67/262 | Loss: 0.5502948760986328Training Epoch: 8 | iteration: 68/262 | Loss: 0.5668010711669922Training Epoch: 8 | iteration: 69/262 | Loss: 0.6101372838020325Training Epoch: 8 | iteration: 70/262 | Loss: 0.6101529598236084Training Epoch: 8 | iteration: 71/262 | Loss: 0.5771257281303406Training Epoch: 8 | iteration: 72/262 | Loss: 0.5598140358924866Training Epoch: 8 | iteration: 73/262 | Loss: 0.5898170471191406Training Epoch: 8 | iteration: 74/262 | Loss: 0.5793587565422058Training Epoch: 8 | iteration: 75/262 | Loss: 0.548333466053009Training Epoch: 8 | iteration: 76/262 | Loss: 0.5802190899848938Training Epoch: 8 | iteration: 77/262 | Loss: 0.5997759103775024Training Epoch: 8 | iteration: 78/262 | Loss: 0.5629087090492249Training Epoch: 8 | iteration: 79/262 | Loss: 0.5794556140899658Training Epoch: 8 | iteration: 80/262 | Loss: 0.5395969152450562Training Epoch: 8 | iteration: 81/262 | Loss: 0.6065512895584106Training Epoch: 8 | iteration: 82/262 | Loss: 0.6016493439674377Training Epoch: 8 | iteration: 83/262 | Loss: 0.5530120134353638Training Epoch: 8 | iteration: 84/262 | Loss: 0.5293464064598083Training Epoch: 8 | iteration: 85/262 | Loss: 0.5970228314399719Training Epoch: 8 | iteration: 86/262 | Loss: 0.5686428546905518Training Epoch: 8 | iteration: 87/262 | Loss: 0.5455625057220459Training Epoch: 8 | iteration: 88/262 | Loss: 0.5545263290405273Training Epoch: 8 | iteration: 89/262 | Loss: 0.5922268629074097Training Epoch: 8 | iteration: 90/262 | Loss: 0.5941481590270996Training Epoch: 8 | iteration: 91/262 | Loss: 0.5833826065063477Training Epoch: 8 | iteration: 92/262 | Loss: 0.5187992453575134Training Epoch: 8 | iteration: 93/262 | Loss: 0.555794894695282Training Epoch: 8 | iteration: 94/262 | Loss: 0.6035146713256836Training Epoch: 8 | iteration: 95/262 | Loss: 0.5860936641693115Training Epoch: 8 | iteration: 96/262 | Loss: 0.6208848357200623Training Epoch: 8 | iteration: 97/262 | Loss: 0.6037641763687134Training Epoch: 8 | iteration: 98/262 | Loss: 0.6243857741355896Training Epoch: 8 | iteration: 99/262 | Loss: 0.5834702253341675Training Epoch: 8 | iteration: 100/262 | Loss: 0.6175163984298706Training Epoch: 8 | iteration: 101/262 | Loss: 0.5633318424224854Training Epoch: 8 | iteration: 102/262 | Loss: 0.6116662621498108Training Epoch: 8 | iteration: 103/262 | Loss: 0.6172194480895996Training Epoch: 8 | iteration: 104/262 | Loss: 0.570280909538269Training Epoch: 8 | iteration: 105/262 | Loss: 0.6164665222167969Training Epoch: 8 | iteration: 106/262 | Loss: 0.5971740484237671Training Epoch: 8 | iteration: 107/262 | Loss: 0.5701600313186646Training Epoch: 8 | iteration: 108/262 | Loss: 0.552924394607544Training Epoch: 8 | iteration: 109/262 | Loss: 0.5548104047775269Training Epoch: 8 | iteration: 110/262 | Loss: 0.598834753036499Training Epoch: 8 | iteration: 111/262 | Loss: 0.521795392036438Training Epoch: 8 | iteration: 112/262 | Loss: 0.5859965682029724Training Epoch: 8 | iteration: 113/262 | Loss: 0.5133885145187378Training Epoch: 8 | iteration: 114/262 | Loss: 0.5346750617027283Training Epoch: 8 | iteration: 115/262 | Loss: 0.5408389568328857Training Epoch: 8 | iteration: 116/262 | Loss: 0.5313996076583862Training Epoch: 8 | iteration: 117/262 | Loss: 0.6532737612724304Training Epoch: 8 | iteration: 118/262 | Loss: 0.6089977025985718Training Epoch: 8 | iteration: 119/262 | Loss: 0.5777932405471802Training Epoch: 8 | iteration: 120/262 | Loss: 0.5812814235687256Training Epoch: 8 | iteration: 121/262 | Loss: 0.5947767496109009Training Epoch: 8 | iteration: 122/262 | Loss: 0.5394499897956848Training Epoch: 8 | iteration: 123/262 | Loss: 0.49605345726013184Training Epoch: 8 | iteration: 124/262 | Loss: 0.5534155964851379Training Epoch: 8 | iteration: 125/262 | Loss: 0.5604080557823181Training Epoch: 8 | iteration: 126/262 | Loss: 0.5869226455688477Training Epoch: 8 | iteration: 127/262 | Loss: 0.5422712564468384Training Epoch: 8 | iteration: 128/262 | Loss: 0.5609562397003174Training Epoch: 8 | iteration: 129/262 | Loss: 0.6758639812469482Training Epoch: 8 | iteration: 130/262 | Loss: 0.5715232491493225Training Epoch: 8 | iteration: 131/262 | Loss: 0.5481299161911011Training Epoch: 8 | iteration: 132/262 | Loss: 0.5830390453338623Training Epoch: 8 | iteration: 133/262 | Loss: 0.5904994010925293Training Epoch: 8 | iteration: 134/262 | Loss: 0.5824984312057495Training Epoch: 8 | iteration: 135/262 | Loss: 0.5917371511459351Training Epoch: 8 | iteration: 136/262 | Loss: 0.5359821319580078Training Epoch: 8 | iteration: 137/262 | Loss: 0.5776737928390503Training Epoch: 8 | iteration: 138/262 | Loss: 0.6088510751724243Training Epoch: 8 | iteration: 139/262 | Loss: 0.5427067279815674Training Epoch: 8 | iteration: 140/262 | Loss: 0.5448707342147827Training Epoch: 8 | iteration: 141/262 | Loss: 0.5666274428367615Training Epoch: 8 | iteration: 142/262 | Loss: 0.638722836971283Training Epoch: 8 | iteration: 143/262 | Loss: 0.6044777631759644Training Epoch: 8 | iteration: 144/262 | Loss: 0.5915495157241821Training Epoch: 8 | iteration: 145/262 | Loss: 0.5607595443725586Training Epoch: 8 | iteration: 146/262 | Loss: 0.5609445571899414Training Epoch: 8 | iteration: 147/262 | Loss: 0.5656317472457886Training Epoch: 8 | iteration: 148/262 | Loss: 0.4670379161834717Training Epoch: 8 | iteration: 149/262 | Loss: 0.5743200182914734Training Epoch: 8 | iteration: 150/262 | Loss: 0.5747689604759216Training Epoch: 8 | iteration: 151/262 | Loss: 0.6086099147796631Training Epoch: 8 | iteration: 152/262 | Loss: 0.5735113024711609Training Epoch: 8 | iteration: 153/262 | Loss: 0.580093264579773Training Epoch: 8 | iteration: 154/262 | Loss: 0.5512229204177856Training Epoch: 8 | iteration: 155/262 | Loss: 0.6009222865104675Training Epoch: 8 | iteration: 156/262 | Loss: 0.5622578263282776Training Epoch: 8 | iteration: 157/262 | Loss: 0.5579899549484253Training Epoch: 8 | iteration: 158/262 | Loss: 0.6105813384056091Training Epoch: 8 | iteration: 159/262 | Loss: 0.5605177879333496Training Epoch: 8 | iteration: 160/262 | Loss: 0.5422000885009766Training Epoch: 8 | iteration: 161/262 | Loss: 0.5329872965812683Training Epoch: 8 | iteration: 162/262 | Loss: 0.583612322807312Training Epoch: 8 | iteration: 163/262 | Loss: 0.5323454737663269Training Epoch: 8 | iteration: 164/262 | Loss: 0.5556071996688843Training Epoch: 8 | iteration: 165/262 | Loss: 0.5674535036087036Training Epoch: 8 | iteration: 166/262 | Loss: 0.5634388327598572Training Epoch: 8 | iteration: 167/262 | Loss: 0.6068410277366638Training Epoch: 8 | iteration: 168/262 | Loss: 0.5660265684127808Training Epoch: 8 | iteration: 169/262 | Loss: 0.5478607416152954Training Epoch: 8 | iteration: 170/262 | Loss: 0.5688084363937378Training Epoch: 8 | iteration: 171/262 | Loss: 0.5618712306022644Training Epoch: 8 | iteration: 172/262 | Loss: 0.5240784287452698Training Epoch: 8 | iteration: 173/262 | Loss: 0.5550715923309326Training Epoch: 8 | iteration: 174/262 | Loss: 0.5732226371765137Training Epoch: 8 | iteration: 175/262 | Loss: 0.5648484230041504Training Epoch: 8 | iteration: 176/262 | Loss: 0.5715226531028748Training Epoch: 8 | iteration: 177/262 | Loss: 0.6106452941894531Training Epoch: 8 | iteration: 178/262 | Loss: 0.5561899542808533Training Epoch: 8 | iteration: 179/262 | Loss: 0.5237154364585876Training Epoch: 8 | iteration: 180/262 | Loss: 0.541618824005127Training Epoch: 8 | iteration: 181/262 | Loss: 0.6311869025230408Training Epoch: 8 | iteration: 182/262 | Loss: 0.5935608744621277Training Epoch: 8 | iteration: 183/262 | Loss: 0.5768581628799438Training Epoch: 8 | iteration: 184/262 | Loss: 0.540442943572998Training Epoch: 8 | iteration: 185/262 | Loss: 0.5508989691734314Training Epoch: 8 | iteration: 186/262 | Loss: 0.49477043747901917Training Epoch: 8 | iteration: 187/262 | Loss: 0.5437219738960266Training Epoch: 8 | iteration: 188/262 | Loss: 0.5745706558227539Training Epoch: 8 | iteration: 189/262 | Loss: 0.5576380491256714Training Epoch: 8 | iteration: 190/262 | Loss: 0.5812855362892151Training Epoch: 8 | iteration: 191/262 | Loss: 0.5530344843864441Training Epoch: 8 | iteration: 192/262 | Loss: 0.6368231773376465Training Epoch: 8 | iteration: 193/262 | Loss: 0.5566087961196899Training Epoch: 8 | iteration: 194/262 | Loss: 0.5755267143249512Training Epoch: 8 | iteration: 195/262 | Loss: 0.5927038192749023Training Epoch: 8 | iteration: 196/262 | Loss: 0.6055608987808228Training Epoch: 8 | iteration: 197/262 | Loss: 0.5251719951629639Training Epoch: 8 | iteration: 198/262 | Loss: 0.60256028175354Training Epoch: 8 | iteration: 199/262 | Loss: 0.5927667617797852Training Epoch: 8 | iteration: 200/262 | Loss: 0.613945722579956Training Epoch: 8 | iteration: 201/262 | Loss: 0.5398974418640137Training Epoch: 8 | iteration: 202/262 | Loss: 0.5872200727462769Training Epoch: 8 | iteration: 203/262 | Loss: 0.5363562107086182Training Epoch: 8 | iteration: 204/262 | Loss: 0.606450080871582Training Epoch: 8 | iteration: 205/262 | Loss: 0.5821782350540161Training Epoch: 8 | iteration: 206/262 | Loss: 0.535929262638092Training Epoch: 8 | iteration: 207/262 | Loss: 0.5719093084335327Training Epoch: 8 | iteration: 208/262 | Loss: 0.5069667100906372Training Epoch: 8 | iteration: 209/262 | Loss: 0.6263576745986938Training Epoch: 8 | iteration: 210/262 | Loss: 0.5599037408828735Training Epoch: 8 | iteration: 211/262 | Loss: 0.5679309368133545Training Epoch: 8 | iteration: 212/262 | Loss: 0.5330851078033447Training Epoch: 8 | iteration: 213/262 | Loss: 0.6151150465011597Training Epoch: 8 | iteration: 214/262 | Loss: 0.5795763731002808Training Epoch: 8 | iteration: 215/262 | Loss: 0.5194085836410522Training Epoch: 8 | iteration: 216/262 | Loss: 0.6921555995941162Training Epoch: 8 | iteration: 217/262 | Loss: 0.5607852935791016Training Epoch: 8 | iteration: 218/262 | Loss: 0.6094363927841187Training Epoch: 8 | iteration: 219/262 | Loss: 0.6843998432159424Training Epoch: 8 | iteration: 220/262 | Loss: 0.5587524175643921Training Epoch: 8 | iteration: 221/262 | Loss: 0.5943539142608643Training Epoch: 8 | iteration: 222/262 | Loss: 0.5757201910018921Training Epoch: 8 | iteration: 223/262 | Loss: 0.48870134353637695Training Epoch: 8 | iteration: 224/262 | Loss: 0.5644619464874268Training Epoch: 8 | iteration: 225/262 | Loss: 0.5834987163543701Training Epoch: 8 | iteration: 226/262 | Loss: 0.6143221259117126Training Epoch: 8 | iteration: 227/262 | Loss: 0.5765295028686523Training Epoch: 8 | iteration: 228/262 | Loss: 0.5848993062973022Training Epoch: 8 | iteration: 229/262 | Loss: 0.5606077313423157Training Epoch: 8 | iteration: 230/262 | Loss: 0.5546256303787231Training Epoch: 8 | iteration: 231/262 | Loss: 0.5373814105987549Training Epoch: 8 | iteration: 232/262 | Loss: 0.5511652827262878Training Epoch: 8 | iteration: 233/262 | Loss: 0.5866971015930176Training Epoch: 8 | iteration: 234/262 | Loss: 0.5626642107963562Training Epoch: 8 | iteration: 235/262 | Loss: 0.5840437412261963Training Epoch: 8 | iteration: 236/262 | Loss: 0.6103881597518921Training Epoch: 8 | iteration: 237/262 | Loss: 0.5710074305534363Training Epoch: 8 | iteration: 238/262 | Loss: 0.5906854867935181Training Epoch: 8 | iteration: 239/262 | Loss: 0.5686928033828735Training Epoch: 8 | iteration: 240/262 | Loss: 0.6102849841117859Training Epoch: 8 | iteration: 241/262 | Loss: 0.5493234395980835Training Epoch: 8 | iteration: 242/262 | Loss: 0.577164888381958Training Epoch: 8 | iteration: 243/262 | Loss: 0.5746248960494995Training Epoch: 8 | iteration: 244/262 | Loss: 0.5998343229293823Training Epoch: 8 | iteration: 245/262 | Loss: 0.5998996496200562Training Epoch: 8 | iteration: 246/262 | Loss: 0.6287899017333984Training Epoch: 8 | iteration: 247/262 | Loss: 0.5449906587600708Training Epoch: 8 | iteration: 248/262 | Loss: 0.567766547203064Training Epoch: 8 | iteration: 249/262 | Loss: 0.5472358465194702Training Epoch: 8 | iteration: 250/262 | Loss: 0.5834468603134155Training Epoch: 8 | iteration: 251/262 | Loss: 0.5894420146942139Training Epoch: 8 | iteration: 252/262 | Loss: 0.5905558466911316Training Epoch: 8 | iteration: 253/262 | Loss: 0.5316542387008667Training Epoch: 8 | iteration: 254/262 | Loss: 0.575061023235321Training Epoch: 8 | iteration: 255/262 | Loss: 0.6321781873703003Training Epoch: 8 | iteration: 256/262 | Loss: 0.5717521905899048Training Epoch: 8 | iteration: 257/262 | Loss: 0.5672760009765625Training Epoch: 8 | iteration: 258/262 | Loss: 0.5986212491989136Training Epoch: 8 | iteration: 259/262 | Loss: 0.5638664960861206Training Epoch: 8 | iteration: 260/262 | Loss: 0.5150723457336426Training Epoch: 8 | iteration: 261/262 | Loss: 0.5620856881141663Validating Epoch: 8 | iteration: 0/66 | Loss: 0.5728452801704407Validating Epoch: 8 | iteration: 1/66 | Loss: 0.6389217376708984Validating Epoch: 8 | iteration: 2/66 | Loss: 0.6634213924407959Validating Epoch: 8 | iteration: 3/66 | Loss: 0.6123151183128357Validating Epoch: 8 | iteration: 4/66 | Loss: 0.6815074682235718Validating Epoch: 8 | iteration: 5/66 | Loss: 0.6337205171585083Validating Epoch: 8 | iteration: 6/66 | Loss: 0.6457615494728088Validating Epoch: 8 | iteration: 7/66 | Loss: 0.6350193619728088Validating Epoch: 8 | iteration: 8/66 | Loss: 0.6272679567337036Validating Epoch: 8 | iteration: 9/66 | Loss: 0.6351506114006042Validating Epoch: 8 | iteration: 10/66 | Loss: 0.5664476156234741Validating Epoch: 8 | iteration: 11/66 | Loss: 0.5599598288536072Validating Epoch: 8 | iteration: 12/66 | Loss: 0.6170676946640015Validating Epoch: 8 | iteration: 13/66 | Loss: 0.6265421509742737Validating Epoch: 8 | iteration: 14/66 | Loss: 0.6074782609939575Validating Epoch: 8 | iteration: 15/66 | Loss: 0.5640099048614502Validating Epoch: 8 | iteration: 16/66 | Loss: 0.6692796945571899Validating Epoch: 8 | iteration: 17/66 | Loss: 0.6020263433456421Validating Epoch: 8 | iteration: 18/66 | Loss: 0.5886701345443726Validating Epoch: 8 | iteration: 19/66 | Loss: 0.56258225440979Validating Epoch: 8 | iteration: 20/66 | Loss: 0.5955246686935425Validating Epoch: 8 | iteration: 21/66 | Loss: 0.6332542896270752Validating Epoch: 8 | iteration: 22/66 | Loss: 0.5848612785339355Validating Epoch: 8 | iteration: 23/66 | Loss: 0.5524402856826782Validating Epoch: 8 | iteration: 24/66 | Loss: 0.6719226241111755Validating Epoch: 8 | iteration: 25/66 | Loss: 0.6232502460479736Validating Epoch: 8 | iteration: 26/66 | Loss: 0.5866438150405884Validating Epoch: 8 | iteration: 27/66 | Loss: 0.6603808999061584Validating Epoch: 8 | iteration: 28/66 | Loss: 0.6171950101852417Validating Epoch: 8 | iteration: 29/66 | Loss: 0.6403743624687195Validating Epoch: 8 | iteration: 30/66 | Loss: 0.556365966796875Validating Epoch: 8 | iteration: 31/66 | Loss: 0.614315390586853Validating Epoch: 8 | iteration: 32/66 | Loss: 0.6236599683761597Validating Epoch: 8 | iteration: 33/66 | Loss: 0.6082271337509155Validating Epoch: 8 | iteration: 34/66 | Loss: 0.6198813915252686Validating Epoch: 8 | iteration: 35/66 | Loss: 0.5983142852783203Validating Epoch: 8 | iteration: 36/66 | Loss: 0.6239763498306274Validating Epoch: 8 | iteration: 37/66 | Loss: 0.544937014579773Validating Epoch: 8 | iteration: 38/66 | Loss: 0.6086655855178833Validating Epoch: 8 | iteration: 39/66 | Loss: 0.5566462278366089Validating Epoch: 8 | iteration: 40/66 | Loss: 0.5505291223526001Validating Epoch: 8 | iteration: 41/66 | Loss: 0.6086673140525818Validating Epoch: 8 | iteration: 42/66 | Loss: 0.6363925337791443Validating Epoch: 8 | iteration: 43/66 | Loss: 0.5865588188171387Validating Epoch: 8 | iteration: 44/66 | Loss: 0.5279539227485657Validating Epoch: 8 | iteration: 45/66 | Loss: 0.5673327445983887Validating Epoch: 8 | iteration: 46/66 | Loss: 0.6218639612197876Validating Epoch: 8 | iteration: 47/66 | Loss: 0.6004625558853149Validating Epoch: 8 | iteration: 48/66 | Loss: 0.5776705741882324Validating Epoch: 8 | iteration: 49/66 | Loss: 0.6492778062820435Validating Epoch: 8 | iteration: 50/66 | Loss: 0.525161623954773Validating Epoch: 8 | iteration: 51/66 | Loss: 0.6075541973114014Validating Epoch: 8 | iteration: 52/66 | Loss: 0.5947792530059814Validating Epoch: 8 | iteration: 53/66 | Loss: 0.6349105834960938Validating Epoch: 8 | iteration: 54/66 | Loss: 0.5783902406692505Validating Epoch: 8 | iteration: 55/66 | Loss: 0.7242134809494019Validating Epoch: 8 | iteration: 56/66 | Loss: 0.5663689374923706Validating Epoch: 8 | iteration: 57/66 | Loss: 0.5861653089523315Validating Epoch: 8 | iteration: 58/66 | Loss: 0.6325763463973999Validating Epoch: 8 | iteration: 59/66 | Loss: 0.695734977722168Validating Epoch: 8 | iteration: 60/66 | Loss: 0.6175488233566284Validating Epoch: 8 | iteration: 61/66 | Loss: 0.5893047451972961Validating Epoch: 8 | iteration: 62/66 | Loss: 0.6294008493423462Validating Epoch: 8 | iteration: 63/66 | Loss: 0.5949569344520569Validating Epoch: 8 | iteration: 64/66 | Loss: 0.5471264123916626Validating Epoch: 8 | iteration: 65/66 | Loss: 0.569138765335083Temperature:  0.5
Sampling: iteration: 0/51{'Validity': 0.98046875, 'Novelty': 1.0, 'Uniqueness': 0.9900398406374502}
Training Epoch: 9 | iteration: 0/262 | Loss: 0.5685032606124878Training Epoch: 9 | iteration: 1/262 | Loss: 0.5743433237075806Training Epoch: 9 | iteration: 2/262 | Loss: 0.5142641067504883Training Epoch: 9 | iteration: 3/262 | Loss: 0.5862078666687012Training Epoch: 9 | iteration: 4/262 | Loss: 0.5269964337348938Training Epoch: 9 | iteration: 5/262 | Loss: 0.6050716638565063Training Epoch: 9 | iteration: 6/262 | Loss: 0.5721758604049683Training Epoch: 9 | iteration: 7/262 | Loss: 0.5004405975341797Training Epoch: 9 | iteration: 8/262 | Loss: 0.552333652973175Training Epoch: 9 | iteration: 9/262 | Loss: 0.5377590656280518Training Epoch: 9 | iteration: 10/262 | Loss: 0.5529569983482361Training Epoch: 9 | iteration: 11/262 | Loss: 0.5615575313568115Training Epoch: 9 | iteration: 12/262 | Loss: 0.5407894849777222Training Epoch: 9 | iteration: 13/262 | Loss: 0.5755313038825989Training Epoch: 9 | iteration: 14/262 | Loss: 0.49715155363082886Training Epoch: 9 | iteration: 15/262 | Loss: 0.5398768186569214Training Epoch: 9 | iteration: 16/262 | Loss: 0.5856940746307373Training Epoch: 9 | iteration: 17/262 | Loss: 0.5203646421432495Training Epoch: 9 | iteration: 18/262 | Loss: 0.561599850654602Training Epoch: 9 | iteration: 19/262 | Loss: 0.5755627155303955Training Epoch: 9 | iteration: 20/262 | Loss: 0.5063392519950867Training Epoch: 9 | iteration: 21/262 | Loss: 0.5613144636154175Training Epoch: 9 | iteration: 22/262 | Loss: 0.5148232579231262Training Epoch: 9 | iteration: 23/262 | Loss: 0.5634196996688843Training Epoch: 9 | iteration: 24/262 | Loss: 0.5364328026771545Training Epoch: 9 | iteration: 25/262 | Loss: 0.6170589923858643Training Epoch: 9 | iteration: 26/262 | Loss: 0.5845216512680054Training Epoch: 9 | iteration: 27/262 | Loss: 0.5806734561920166Training Epoch: 9 | iteration: 28/262 | Loss: 0.5245016813278198Training Epoch: 9 | iteration: 29/262 | Loss: 0.5233795642852783Training Epoch: 9 | iteration: 30/262 | Loss: 0.5116820931434631Training Epoch: 9 | iteration: 31/262 | Loss: 0.5922867655754089Training Epoch: 9 | iteration: 32/262 | Loss: 0.5996510982513428Training Epoch: 9 | iteration: 33/262 | Loss: 0.5147729516029358Training Epoch: 9 | iteration: 34/262 | Loss: 0.5438364148139954Training Epoch: 9 | iteration: 35/262 | Loss: 0.5506981015205383Training Epoch: 9 | iteration: 36/262 | Loss: 0.6086902022361755Training Epoch: 9 | iteration: 37/262 | Loss: 0.5511808395385742Training Epoch: 9 | iteration: 38/262 | Loss: 0.4852893352508545Training Epoch: 9 | iteration: 39/262 | Loss: 0.552306056022644Training Epoch: 9 | iteration: 40/262 | Loss: 0.5557634830474854Training Epoch: 9 | iteration: 41/262 | Loss: 0.5667072534561157Training Epoch: 9 | iteration: 42/262 | Loss: 0.6023789048194885Training Epoch: 9 | iteration: 43/262 | Loss: 0.5563385486602783Training Epoch: 9 | iteration: 44/262 | Loss: 0.5449153780937195Training Epoch: 9 | iteration: 45/262 | Loss: 0.5550892353057861Training Epoch: 9 | iteration: 46/262 | Loss: 0.5204927921295166Training Epoch: 9 | iteration: 47/262 | Loss: 0.500585675239563Training Epoch: 9 | iteration: 48/262 | Loss: 0.5919766426086426Training Epoch: 9 | iteration: 49/262 | Loss: 0.5332517027854919Training Epoch: 9 | iteration: 50/262 | Loss: 0.5900537967681885Training Epoch: 9 | iteration: 51/262 | Loss: 0.6579327583312988Training Epoch: 9 | iteration: 52/262 | Loss: 0.534919023513794Training Epoch: 9 | iteration: 53/262 | Loss: 0.5487439036369324Training Epoch: 9 | iteration: 54/262 | Loss: 0.5464465022087097Training Epoch: 9 | iteration: 55/262 | Loss: 0.4945083260536194Training Epoch: 9 | iteration: 56/262 | Loss: 0.5811864733695984Training Epoch: 9 | iteration: 57/262 | Loss: 0.49045249819755554Training Epoch: 9 | iteration: 58/262 | Loss: 0.560949444770813Training Epoch: 9 | iteration: 59/262 | Loss: 0.5574896931648254Training Epoch: 9 | iteration: 60/262 | Loss: 0.5820111036300659Training Epoch: 9 | iteration: 61/262 | Loss: 0.6226214170455933Training Epoch: 9 | iteration: 62/262 | Loss: 0.5970898866653442Training Epoch: 9 | iteration: 63/262 | Loss: 0.5546631813049316Training Epoch: 9 | iteration: 64/262 | Loss: 0.5729241371154785Training Epoch: 9 | iteration: 65/262 | Loss: 0.5204455256462097Training Epoch: 9 | iteration: 66/262 | Loss: 0.5896422863006592Training Epoch: 9 | iteration: 67/262 | Loss: 0.5084857940673828Training Epoch: 9 | iteration: 68/262 | Loss: 0.5766000747680664Training Epoch: 9 | iteration: 69/262 | Loss: 0.5592302083969116Training Epoch: 9 | iteration: 70/262 | Loss: 0.5452455282211304Training Epoch: 9 | iteration: 71/262 | Loss: 0.569507896900177Training Epoch: 9 | iteration: 72/262 | Loss: 0.5400182008743286Training Epoch: 9 | iteration: 73/262 | Loss: 0.5989683866500854Training Epoch: 9 | iteration: 74/262 | Loss: 0.5358222723007202Training Epoch: 9 | iteration: 75/262 | Loss: 0.6037138104438782Training Epoch: 9 | iteration: 76/262 | Loss: 0.5494903326034546Training Epoch: 9 | iteration: 77/262 | Loss: 0.5884412527084351Training Epoch: 9 | iteration: 78/262 | Loss: 0.617617130279541Training Epoch: 9 | iteration: 79/262 | Loss: 0.5479265451431274Training Epoch: 9 | iteration: 80/262 | Loss: 0.5297189950942993Training Epoch: 9 | iteration: 81/262 | Loss: 0.5839462280273438Training Epoch: 9 | iteration: 82/262 | Loss: 0.6214251518249512Training Epoch: 9 | iteration: 83/262 | Loss: 0.49453023076057434Training Epoch: 9 | iteration: 84/262 | Loss: 0.5591908693313599Training Epoch: 9 | iteration: 85/262 | Loss: 0.5583038330078125Training Epoch: 9 | iteration: 86/262 | Loss: 0.5887566804885864Training Epoch: 9 | iteration: 87/262 | Loss: 0.6115972995758057Training Epoch: 9 | iteration: 88/262 | Loss: 0.5284472703933716Training Epoch: 9 | iteration: 89/262 | Loss: 0.55926513671875Training Epoch: 9 | iteration: 90/262 | Loss: 0.5470733642578125Training Epoch: 9 | iteration: 91/262 | Loss: 0.5415878295898438Training Epoch: 9 | iteration: 92/262 | Loss: 0.5683932304382324Training Epoch: 9 | iteration: 93/262 | Loss: 0.574387788772583Training Epoch: 9 | iteration: 94/262 | Loss: 0.6062371730804443Training Epoch: 9 | iteration: 95/262 | Loss: 0.5503491163253784Training Epoch: 9 | iteration: 96/262 | Loss: 0.5630061626434326Training Epoch: 9 | iteration: 97/262 | Loss: 0.5840818881988525Training Epoch: 9 | iteration: 98/262 | Loss: 0.5244077444076538Training Epoch: 9 | iteration: 99/262 | Loss: 0.570354163646698Training Epoch: 9 | iteration: 100/262 | Loss: 0.5456209182739258Training Epoch: 9 | iteration: 101/262 | Loss: 0.5942056179046631Training Epoch: 9 | iteration: 102/262 | Loss: 0.5682706832885742Training Epoch: 9 | iteration: 103/262 | Loss: 0.5737259984016418Training Epoch: 9 | iteration: 104/262 | Loss: 0.5530964136123657Training Epoch: 9 | iteration: 105/262 | Loss: 0.6268560886383057Training Epoch: 9 | iteration: 106/262 | Loss: 0.5937261581420898Training Epoch: 9 | iteration: 107/262 | Loss: 0.48210451006889343Training Epoch: 9 | iteration: 108/262 | Loss: 0.5505189895629883Training Epoch: 9 | iteration: 109/262 | Loss: 0.5002408623695374Training Epoch: 9 | iteration: 110/262 | Loss: 0.5289770364761353Training Epoch: 9 | iteration: 111/262 | Loss: 0.5562984943389893Training Epoch: 9 | iteration: 112/262 | Loss: 0.5235621929168701Training Epoch: 9 | iteration: 113/262 | Loss: 0.5895295143127441Training Epoch: 9 | iteration: 114/262 | Loss: 0.5770774483680725Training Epoch: 9 | iteration: 115/262 | Loss: 0.5858567953109741Training Epoch: 9 | iteration: 116/262 | Loss: 0.5816863179206848Training Epoch: 9 | iteration: 117/262 | Loss: 0.5577746629714966Training Epoch: 9 | iteration: 118/262 | Loss: 0.6337488889694214Training Epoch: 9 | iteration: 119/262 | Loss: 0.5302879810333252Training Epoch: 9 | iteration: 120/262 | Loss: 0.5071227550506592Training Epoch: 9 | iteration: 121/262 | Loss: 0.6025481224060059Training Epoch: 9 | iteration: 122/262 | Loss: 0.5790573954582214Training Epoch: 9 | iteration: 123/262 | Loss: 0.5438237190246582Training Epoch: 9 | iteration: 124/262 | Loss: 0.5362900495529175Training Epoch: 9 | iteration: 125/262 | Loss: 0.5500164031982422Training Epoch: 9 | iteration: 126/262 | Loss: 0.5936279892921448Training Epoch: 9 | iteration: 127/262 | Loss: 0.5411949753761292Training Epoch: 9 | iteration: 128/262 | Loss: 0.5271812677383423Training Epoch: 9 | iteration: 129/262 | Loss: 0.5697051286697388Training Epoch: 9 | iteration: 130/262 | Loss: 0.6140168905258179Training Epoch: 9 | iteration: 131/262 | Loss: 0.5247088670730591Training Epoch: 9 | iteration: 132/262 | Loss: 0.5479810237884521Training Epoch: 9 | iteration: 133/262 | Loss: 0.5288615226745605Training Epoch: 9 | iteration: 134/262 | Loss: 0.5684031248092651Training Epoch: 9 | iteration: 135/262 | Loss: 0.526976466178894Training Epoch: 9 | iteration: 136/262 | Loss: 0.5652526617050171Training Epoch: 9 | iteration: 137/262 | Loss: 0.504910945892334Training Epoch: 9 | iteration: 138/262 | Loss: 0.522739052772522Training Epoch: 9 | iteration: 139/262 | Loss: 0.5547884106636047Training Epoch: 9 | iteration: 140/262 | Loss: 0.5237417221069336Training Epoch: 9 | iteration: 141/262 | Loss: 0.548649787902832Training Epoch: 9 | iteration: 142/262 | Loss: 0.5765084028244019Training Epoch: 9 | iteration: 143/262 | Loss: 0.5693824291229248Training Epoch: 9 | iteration: 144/262 | Loss: 0.5887397527694702Training Epoch: 9 | iteration: 145/262 | Loss: 0.5456948280334473Training Epoch: 9 | iteration: 146/262 | Loss: 0.5497910976409912Training Epoch: 9 | iteration: 147/262 | Loss: 0.5176882147789001Training Epoch: 9 | iteration: 148/262 | Loss: 0.592255711555481Training Epoch: 9 | iteration: 149/262 | Loss: 0.5484805703163147Training Epoch: 9 | iteration: 150/262 | Loss: 0.5077109336853027Training Epoch: 9 | iteration: 151/262 | Loss: 0.5550036430358887Training Epoch: 9 | iteration: 152/262 | Loss: 0.5159928202629089Training Epoch: 9 | iteration: 153/262 | Loss: 0.5622246265411377Training Epoch: 9 | iteration: 154/262 | Loss: 0.5645909905433655Training Epoch: 9 | iteration: 155/262 | Loss: 0.5114712715148926Training Epoch: 9 | iteration: 156/262 | Loss: 0.5434893369674683Training Epoch: 9 | iteration: 157/262 | Loss: 0.5428068041801453Training Epoch: 9 | iteration: 158/262 | Loss: 0.5658552646636963Training Epoch: 9 | iteration: 159/262 | Loss: 0.5889837741851807Training Epoch: 9 | iteration: 160/262 | Loss: 0.5884047150611877Training Epoch: 9 | iteration: 161/262 | Loss: 0.5310192704200745Training Epoch: 9 | iteration: 162/262 | Loss: 0.5200759172439575Training Epoch: 9 | iteration: 163/262 | Loss: 0.5745401382446289Training Epoch: 9 | iteration: 164/262 | Loss: 0.5438319444656372Training Epoch: 9 | iteration: 165/262 | Loss: 0.5641160011291504Training Epoch: 9 | iteration: 166/262 | Loss: 0.6334018111228943Training Epoch: 9 | iteration: 167/262 | Loss: 0.538231372833252Training Epoch: 9 | iteration: 168/262 | Loss: 0.5731997489929199Training Epoch: 9 | iteration: 169/262 | Loss: 0.55083829164505Training Epoch: 9 | iteration: 170/262 | Loss: 0.6536673307418823Training Epoch: 9 | iteration: 171/262 | Loss: 0.5361518263816833Training Epoch: 9 | iteration: 172/262 | Loss: 0.5327463150024414Training Epoch: 9 | iteration: 173/262 | Loss: 0.5251845717430115Training Epoch: 9 | iteration: 174/262 | Loss: 0.5829957127571106Training Epoch: 9 | iteration: 175/262 | Loss: 0.6065410375595093Training Epoch: 9 | iteration: 176/262 | Loss: 0.5528895854949951Training Epoch: 9 | iteration: 177/262 | Loss: 0.618489682674408Training Epoch: 9 | iteration: 178/262 | Loss: 0.5763382911682129Training Epoch: 9 | iteration: 179/262 | Loss: 0.5102962255477905Training Epoch: 9 | iteration: 180/262 | Loss: 0.6039702296257019Training Epoch: 9 | iteration: 181/262 | Loss: 0.5821057558059692Training Epoch: 9 | iteration: 182/262 | Loss: 0.5134598016738892Training Epoch: 9 | iteration: 183/262 | Loss: 0.599525511264801Training Epoch: 9 | iteration: 184/262 | Loss: 0.4941031038761139Training Epoch: 9 | iteration: 185/262 | Loss: 0.5404043197631836Training Epoch: 9 | iteration: 186/262 | Loss: 0.5575761795043945Training Epoch: 9 | iteration: 187/262 | Loss: 0.572766125202179Training Epoch: 9 | iteration: 188/262 | Loss: 0.5776538252830505Training Epoch: 9 | iteration: 189/262 | Loss: 0.4976198375225067Training Epoch: 9 | iteration: 190/262 | Loss: 0.5813266038894653Training Epoch: 9 | iteration: 191/262 | Loss: 0.5648046731948853Training Epoch: 9 | iteration: 192/262 | Loss: 0.5587099194526672Training Epoch: 9 | iteration: 193/262 | Loss: 0.5386739373207092Training Epoch: 9 | iteration: 194/262 | Loss: 0.5361570715904236Training Epoch: 9 | iteration: 195/262 | Loss: 0.5513602495193481Training Epoch: 9 | iteration: 196/262 | Loss: 0.6003684997558594Training Epoch: 9 | iteration: 197/262 | Loss: 0.564839780330658Training Epoch: 9 | iteration: 198/262 | Loss: 0.5916804075241089Training Epoch: 9 | iteration: 199/262 | Loss: 0.6272597312927246Training Epoch: 9 | iteration: 200/262 | Loss: 0.5634060502052307Training Epoch: 9 | iteration: 201/262 | Loss: 0.5883004665374756Training Epoch: 9 | iteration: 202/262 | Loss: 0.5341107845306396Training Epoch: 9 | iteration: 203/262 | Loss: 0.5936979055404663Training Epoch: 9 | iteration: 204/262 | Loss: 0.5434589982032776Training Epoch: 9 | iteration: 205/262 | Loss: 0.5814775228500366Training Epoch: 9 | iteration: 206/262 | Loss: 0.5396075248718262Training Epoch: 9 | iteration: 207/262 | Loss: 0.5547981262207031Training Epoch: 9 | iteration: 208/262 | Loss: 0.5833775997161865Training Epoch: 9 | iteration: 209/262 | Loss: 0.5254921913146973Training Epoch: 9 | iteration: 210/262 | Loss: 0.5440949201583862Training Epoch: 9 | iteration: 211/262 | Loss: 0.5407532453536987Training Epoch: 9 | iteration: 212/262 | Loss: 0.5529904365539551Training Epoch: 9 | iteration: 213/262 | Loss: 0.5869865417480469Training Epoch: 9 | iteration: 214/262 | Loss: 0.5237321853637695Training Epoch: 9 | iteration: 215/262 | Loss: 0.5489110946655273Training Epoch: 9 | iteration: 216/262 | Loss: 0.5556886196136475Training Epoch: 9 | iteration: 217/262 | Loss: 0.5509798526763916Training Epoch: 9 | iteration: 218/262 | Loss: 0.4921986758708954Training Epoch: 9 | iteration: 219/262 | Loss: 0.6144051551818848Training Epoch: 9 | iteration: 220/262 | Loss: 0.6125795841217041Training Epoch: 9 | iteration: 221/262 | Loss: 0.5562711954116821Training Epoch: 9 | iteration: 222/262 | Loss: 0.5769360661506653Training Epoch: 9 | iteration: 223/262 | Loss: 0.5686838030815125Training Epoch: 9 | iteration: 224/262 | Loss: 0.5679550170898438Training Epoch: 9 | iteration: 225/262 | Loss: 0.6018131971359253Training Epoch: 9 | iteration: 226/262 | Loss: 0.5763819217681885Training Epoch: 9 | iteration: 227/262 | Loss: 0.575303316116333Training Epoch: 9 | iteration: 228/262 | Loss: 0.5852240324020386Training Epoch: 9 | iteration: 229/262 | Loss: 0.5500730276107788Training Epoch: 9 | iteration: 230/262 | Loss: 0.5747997164726257Training Epoch: 9 | iteration: 231/262 | Loss: 0.5432836413383484Training Epoch: 9 | iteration: 232/262 | Loss: 0.5714027881622314Training Epoch: 9 | iteration: 233/262 | Loss: 0.632319986820221Training Epoch: 9 | iteration: 234/262 | Loss: 0.5823605060577393Training Epoch: 9 | iteration: 235/262 | Loss: 0.5672916173934937Training Epoch: 9 | iteration: 236/262 | Loss: 0.5448504090309143Training Epoch: 9 | iteration: 237/262 | Loss: 0.5665649175643921Training Epoch: 9 | iteration: 238/262 | Loss: 0.5787369608879089Training Epoch: 9 | iteration: 239/262 | Loss: 0.5862929821014404Training Epoch: 9 | iteration: 240/262 | Loss: 0.597741961479187Training Epoch: 9 | iteration: 241/262 | Loss: 0.5110845565795898Training Epoch: 9 | iteration: 242/262 | Loss: 0.5321044921875Training Epoch: 9 | iteration: 243/262 | Loss: 0.5513676404953003Training Epoch: 9 | iteration: 244/262 | Loss: 0.5966598987579346Training Epoch: 9 | iteration: 245/262 | Loss: 0.5326566696166992Training Epoch: 9 | iteration: 246/262 | Loss: 0.5639392137527466Training Epoch: 9 | iteration: 247/262 | Loss: 0.5327679514884949Training Epoch: 9 | iteration: 248/262 | Loss: 0.5844841003417969Training Epoch: 9 | iteration: 249/262 | Loss: 0.5730702877044678Training Epoch: 9 | iteration: 250/262 | Loss: 0.5588018894195557Training Epoch: 9 | iteration: 251/262 | Loss: 0.5519798994064331Training Epoch: 9 | iteration: 252/262 | Loss: 0.5241419672966003Training Epoch: 9 | iteration: 253/262 | Loss: 0.5774044394493103Training Epoch: 9 | iteration: 254/262 | Loss: 0.5819036960601807Training Epoch: 9 | iteration: 255/262 | Loss: 0.5695098638534546Training Epoch: 9 | iteration: 256/262 | Loss: 0.59247887134552Training Epoch: 9 | iteration: 257/262 | Loss: 0.5277189612388611Training Epoch: 9 | iteration: 258/262 | Loss: 0.6045053005218506Training Epoch: 9 | iteration: 259/262 | Loss: 0.5630981922149658Training Epoch: 9 | iteration: 260/262 | Loss: 0.5603566765785217Training Epoch: 9 | iteration: 261/262 | Loss: 0.47059035301208496Validating Epoch: 9 | iteration: 0/66 | Loss: 0.6017616987228394Validating Epoch: 9 | iteration: 1/66 | Loss: 0.5776814222335815Validating Epoch: 9 | iteration: 2/66 | Loss: 0.6016292572021484Validating Epoch: 9 | iteration: 3/66 | Loss: 0.6025593876838684Validating Epoch: 9 | iteration: 4/66 | Loss: 0.5615832209587097Validating Epoch: 9 | iteration: 5/66 | Loss: 0.6007429361343384Validating Epoch: 9 | iteration: 6/66 | Loss: 0.606825590133667Validating Epoch: 9 | iteration: 7/66 | Loss: 0.6247438192367554Validating Epoch: 9 | iteration: 8/66 | Loss: 0.6284487247467041Validating Epoch: 9 | iteration: 9/66 | Loss: 0.5033589601516724Validating Epoch: 9 | iteration: 10/66 | Loss: 0.5951645374298096Validating Epoch: 9 | iteration: 11/66 | Loss: 0.613125205039978Validating Epoch: 9 | iteration: 12/66 | Loss: 0.6210534572601318Validating Epoch: 9 | iteration: 13/66 | Loss: 0.5990713834762573Validating Epoch: 9 | iteration: 14/66 | Loss: 0.6037267446517944Validating Epoch: 9 | iteration: 15/66 | Loss: 0.6293877363204956Validating Epoch: 9 | iteration: 16/66 | Loss: 0.6253714561462402Validating Epoch: 9 | iteration: 17/66 | Loss: 0.5918101072311401Validating Epoch: 9 | iteration: 18/66 | Loss: 0.5853593945503235Validating Epoch: 9 | iteration: 19/66 | Loss: 0.6046820282936096Validating Epoch: 9 | iteration: 20/66 | Loss: 0.5908936262130737Validating Epoch: 9 | iteration: 21/66 | Loss: 0.6293789744377136Validating Epoch: 9 | iteration: 22/66 | Loss: 0.6123517751693726Validating Epoch: 9 | iteration: 23/66 | Loss: 0.5267645120620728Validating Epoch: 9 | iteration: 24/66 | Loss: 0.6065406203269958Validating Epoch: 9 | iteration: 25/66 | Loss: 0.6394453048706055Validating Epoch: 9 | iteration: 26/66 | Loss: 0.6520545482635498Validating Epoch: 9 | iteration: 27/66 | Loss: 0.5719199180603027Validating Epoch: 9 | iteration: 28/66 | Loss: 0.5929797887802124Validating Epoch: 9 | iteration: 29/66 | Loss: 0.6539316773414612Validating Epoch: 9 | iteration: 30/66 | Loss: 0.5977339744567871Validating Epoch: 9 | iteration: 31/66 | Loss: 0.6163190603256226Validating Epoch: 9 | iteration: 32/66 | Loss: 0.5934070348739624Validating Epoch: 9 | iteration: 33/66 | Loss: 0.5820395946502686Validating Epoch: 9 | iteration: 34/66 | Loss: 0.5794233083724976Validating Epoch: 9 | iteration: 35/66 | Loss: 0.5976594090461731Validating Epoch: 9 | iteration: 36/66 | Loss: 0.5787774324417114Validating Epoch: 9 | iteration: 37/66 | Loss: 0.6081008315086365Validating Epoch: 9 | iteration: 38/66 | Loss: 0.628657341003418Validating Epoch: 9 | iteration: 39/66 | Loss: 0.5967655181884766Validating Epoch: 9 | iteration: 40/66 | Loss: 0.6499292850494385Validating Epoch: 9 | iteration: 41/66 | Loss: 0.6242548227310181Validating Epoch: 9 | iteration: 42/66 | Loss: 0.5322599411010742Validating Epoch: 9 | iteration: 43/66 | Loss: 0.6193044781684875Validating Epoch: 9 | iteration: 44/66 | Loss: 0.6065027713775635Validating Epoch: 9 | iteration: 45/66 | Loss: 0.6326568126678467Validating Epoch: 9 | iteration: 46/66 | Loss: 0.5839202404022217Validating Epoch: 9 | iteration: 47/66 | Loss: 0.6188657283782959Validating Epoch: 9 | iteration: 48/66 | Loss: 0.5252678394317627Validating Epoch: 9 | iteration: 49/66 | Loss: 0.7122459411621094Validating Epoch: 9 | iteration: 50/66 | Loss: 0.5551757216453552Validating Epoch: 9 | iteration: 51/66 | Loss: 0.5980428457260132Validating Epoch: 9 | iteration: 52/66 | Loss: 0.6376197934150696Validating Epoch: 9 | iteration: 53/66 | Loss: 0.6369661688804626Validating Epoch: 9 | iteration: 54/66 | Loss: 0.6819864511489868Validating Epoch: 9 | iteration: 55/66 | Loss: 0.5838980674743652Validating Epoch: 9 | iteration: 56/66 | Loss: 0.5602661371231079Validating Epoch: 9 | iteration: 57/66 | Loss: 0.6254041790962219Validating Epoch: 9 | iteration: 58/66 | Loss: 0.6547757387161255Validating Epoch: 9 | iteration: 59/66 | Loss: 0.5821244120597839Validating Epoch: 9 | iteration: 60/66 | Loss: 0.6009383201599121Validating Epoch: 9 | iteration: 61/66 | Loss: 0.6289926767349243Validating Epoch: 9 | iteration: 62/66 | Loss: 0.5806918144226074Validating Epoch: 9 | iteration: 63/66 | Loss: 0.6055219173431396Validating Epoch: 9 | iteration: 64/66 | Loss: 0.6299068927764893Validating Epoch: 9 | iteration: 65/66 | Loss: 0.7763493061065674No of GPUs available 4

==================================================
Generating molecules with target properties...
==================================================

Generating for -10_2...
Generating for -9_2...
Generating for -8_2...
Generating for -7_2...
Generating for -6_2...

==================================================
Generated molecules saved to: ../checkpoints/DPO_Finetuning_BETA_0.11_epochs_10_LCK_DOCKSTRING_FAST_ACTUAL_affinity_sas/generated_molecules.pkl
To analyze and plot results, run:
python analyze_generated_molecules.py --checkpoint_dir ../checkpoints/DPO_Finetuning_BETA_0.11_epochs_10_LCK_DOCKSTRING_FAST_ACTUAL_affinity_sas --properties affinity sas
==================================================

No of GPUs available 4
Temperature:  0.5
Sampling: iteration: 0/51{'Validity': 0.9814453125, 'Novelty': 1.0, 'Uniqueness': 0.9880597014925373}
dict_keys(['smiles_rep', 'properties', 'smiles'])
validity rate 0.9814453125
[1;34mwandb[0m: 
[1;34mwandb[0m:  View run [33mDPO_Finetuning_BETA_0.11_epochs_10_LCK_DOCKSTRING_FAST_ACTUAL_affinity_sas[0m at: [34mhttps://wandb.ai/bhuvan-kapur1-iiith/molgpt2.0%20FINAL/runs/te4okyt2[0m
[1;34mwandb[0m: Find logs at: [1;35mwandb/run-20260117_210446-te4okyt2/logs[0m
