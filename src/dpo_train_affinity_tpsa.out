Starting ...
cuda
Properties to use:  ['affinity', 'tpsas']
Building Vocab
{'batch_size': 128, 'd_model': 512, 'n_heads': 8, 'n_layers': 6, 'hidden_units': 512, 'lr': 1e-05, 'epochs': 10, 'properties': ['affinity', 'tpsas'], 'run_name': 'LCK_DOCKSTRING_FAST_ACTUAL_affinity_tpsas'}
length of preference data : 41795
Sample preference_data[0]: ['O1C(C(=O)N2CCN(CC2)C3=CC=C(N(=O)=O)C=C3)=C(C=4C1=CC=CC4)C', array([0.47959185, 0.53255087, 0.54797947, 0.15855041, 0.16055913]), [np.str_('O1C(=NC2=C(C1=O)C=CC=C2)C3=C(C(=O)NCCC4=CC=CC=C4)C=CC(OC)=C3'), np.float64(0.9873459430403937), array([0.52040816, 0.54211667, 0.55725834, 0.15815913, 0.16377715])], [np.str_('O1C(=NC2=C(C1=O)C=CC=C2)C3=CC(OC(=O)C=4C=CC=CC4)=C(OC)C=C3'), np.float64(0.9574398041990725), array([0.43877551, 0.55263869, 0.41284505, 0.14420153, 0.15814562])]]
LCK_DOCKSTRING_FAST_ACTUAL_affinity_tpsas
len(target_smiles): 41800
len(data): 41800
LCK_DOCKSTRING_FAST_ACTUAL_affinity_tpsas
dataset built
cuda
True
2.7.1+cu118
No of GPUs available 4
No of GPUs available 4
{'batch_size': 128, 'd_model': 512, 'n_heads': 8, 'n_layers': 6, 'hidden_units': 512, 'lr': 1e-05, 'epochs': 10, 'properties': ['affinity', 'tpsas'], 'ipo': False, 'run_name': 'DPO_Finetuning_BETA_0.11_epochs_10_LCK_DOCKSTRING_FAST_ACTUAL_affinity_tpsas', 'beta': 0.11}
Temperature:  0.5
Sampling: iteration: 0/51{'Validity': 0.9951171875, 'Novelty': 1.0, 'Uniqueness': 0.9950932286555446}
Training Epoch: 0 | iteration: 0/262 | Loss: 1.0754859447479248Training Epoch: 0 | iteration: 1/262 | Loss: 0.9898402690887451Training Epoch: 0 | iteration: 2/262 | Loss: 0.8995699882507324Training Epoch: 0 | iteration: 3/262 | Loss: 1.1379860639572144Training Epoch: 0 | iteration: 4/262 | Loss: 1.1244465112686157Training Epoch: 0 | iteration: 5/262 | Loss: 0.9397009611129761Training Epoch: 0 | iteration: 6/262 | Loss: 0.9584678411483765Training Epoch: 0 | iteration: 7/262 | Loss: 1.0072988271713257Training Epoch: 0 | iteration: 8/262 | Loss: 0.9722787141799927Training Epoch: 0 | iteration: 9/262 | Loss: 0.9369579553604126Training Epoch: 0 | iteration: 10/262 | Loss: 0.9945954084396362Training Epoch: 0 | iteration: 11/262 | Loss: 1.0441129207611084Training Epoch: 0 | iteration: 12/262 | Loss: 0.9894334077835083Training Epoch: 0 | iteration: 13/262 | Loss: 0.9023327827453613Training Epoch: 0 | iteration: 14/262 | Loss: 0.914806604385376Training Epoch: 0 | iteration: 15/262 | Loss: 0.9602636098861694Training Epoch: 0 | iteration: 16/262 | Loss: 0.9342223405838013Training Epoch: 0 | iteration: 17/262 | Loss: 0.9459731578826904Training Epoch: 0 | iteration: 18/262 | Loss: 0.9465194940567017Training Epoch: 0 | iteration: 19/262 | Loss: 0.9049240350723267Training Epoch: 0 | iteration: 20/262 | Loss: 0.9490554332733154Training Epoch: 0 | iteration: 21/262 | Loss: 1.0203741788864136Training Epoch: 0 | iteration: 22/262 | Loss: 0.9066812992095947Training Epoch: 0 | iteration: 23/262 | Loss: 0.9383702278137207Training Epoch: 0 | iteration: 24/262 | Loss: 0.9577591419219971Training Epoch: 0 | iteration: 25/262 | Loss: 0.911102294921875Training Epoch: 0 | iteration: 26/262 | Loss: 0.9673423767089844Training Epoch: 0 | iteration: 27/262 | Loss: 0.9126611948013306Training Epoch: 0 | iteration: 28/262 | Loss: 0.9343171119689941Training Epoch: 0 | iteration: 29/262 | Loss: 0.989181637763977Training Epoch: 0 | iteration: 30/262 | Loss: 0.8593746423721313Training Epoch: 0 | iteration: 31/262 | Loss: 0.8330782651901245Training Epoch: 0 | iteration: 32/262 | Loss: 0.8631469011306763Training Epoch: 0 | iteration: 33/262 | Loss: 0.8862637877464294Training Epoch: 0 | iteration: 34/262 | Loss: 1.0123870372772217Training Epoch: 0 | iteration: 35/262 | Loss: 0.8695258498191833Training Epoch: 0 | iteration: 36/262 | Loss: 0.9150949120521545Training Epoch: 0 | iteration: 37/262 | Loss: 0.8728095889091492Training Epoch: 0 | iteration: 38/262 | Loss: 0.8051316738128662Training Epoch: 0 | iteration: 39/262 | Loss: 0.8880412578582764Training Epoch: 0 | iteration: 40/262 | Loss: 0.9237805008888245Training Epoch: 0 | iteration: 41/262 | Loss: 0.8408502340316772Training Epoch: 0 | iteration: 42/262 | Loss: 0.8316556215286255Training Epoch: 0 | iteration: 43/262 | Loss: 0.8969514966011047Training Epoch: 0 | iteration: 44/262 | Loss: 0.8140161633491516Training Epoch: 0 | iteration: 45/262 | Loss: 0.8354214429855347Training Epoch: 0 | iteration: 46/262 | Loss: 0.9110324382781982Training Epoch: 0 | iteration: 47/262 | Loss: 0.8279143571853638Training Epoch: 0 | iteration: 48/262 | Loss: 0.8158742785453796Training Epoch: 0 | iteration: 49/262 | Loss: 0.9218162894248962Training Epoch: 0 | iteration: 50/262 | Loss: 0.8349411487579346Training Epoch: 0 | iteration: 51/262 | Loss: 0.830858588218689Training Epoch: 0 | iteration: 52/262 | Loss: 0.8777003288269043Training Epoch: 0 | iteration: 53/262 | Loss: 0.89187091588974Training Epoch: 0 | iteration: 54/262 | Loss: 0.8958767652511597Training Epoch: 0 | iteration: 55/262 | Loss: 0.8811473250389099Training Epoch: 0 | iteration: 56/262 | Loss: 0.944678544998169Training Epoch: 0 | iteration: 57/262 | Loss: 0.9339070320129395Training Epoch: 0 | iteration: 58/262 | Loss: 0.912484884262085Training Epoch: 0 | iteration: 59/262 | Loss: 1.0036702156066895Training Epoch: 0 | iteration: 60/262 | Loss: 0.9610233306884766Training Epoch: 0 | iteration: 61/262 | Loss: 0.8858604431152344Training Epoch: 0 | iteration: 62/262 | Loss: 0.7452315092086792Training Epoch: 0 | iteration: 63/262 | Loss: 0.7963829636573792Training Epoch: 0 | iteration: 64/262 | Loss: 0.9009649753570557Training Epoch: 0 | iteration: 65/262 | Loss: 0.8918525576591492Training Epoch: 0 | iteration: 66/262 | Loss: 0.8457577228546143Training Epoch: 0 | iteration: 67/262 | Loss: 0.9632489085197449Training Epoch: 0 | iteration: 68/262 | Loss: 0.8943849802017212Training Epoch: 0 | iteration: 69/262 | Loss: 0.9525039196014404Training Epoch: 0 | iteration: 70/262 | Loss: 0.8432443141937256Training Epoch: 0 | iteration: 71/262 | Loss: 0.9211109280586243Training Epoch: 0 | iteration: 72/262 | Loss: 0.870624303817749Training Epoch: 0 | iteration: 73/262 | Loss: 0.8460427522659302Training Epoch: 0 | iteration: 74/262 | Loss: 0.9299769997596741Training Epoch: 0 | iteration: 75/262 | Loss: 0.8230348825454712Training Epoch: 0 | iteration: 76/262 | Loss: 0.914980411529541Training Epoch: 0 | iteration: 77/262 | Loss: 0.8763115406036377Training Epoch: 0 | iteration: 78/262 | Loss: 0.8050116896629333Training Epoch: 0 | iteration: 79/262 | Loss: 0.921988308429718Training Epoch: 0 | iteration: 80/262 | Loss: 0.8849197030067444Training Epoch: 0 | iteration: 81/262 | Loss: 0.8866660594940186Training Epoch: 0 | iteration: 82/262 | Loss: 0.9007797241210938Training Epoch: 0 | iteration: 83/262 | Loss: 0.7541386485099792Training Epoch: 0 | iteration: 84/262 | Loss: 0.8467285633087158Training Epoch: 0 | iteration: 85/262 | Loss: 0.8473678231239319Training Epoch: 0 | iteration: 86/262 | Loss: 0.9329877495765686Training Epoch: 0 | iteration: 87/262 | Loss: 0.8800672292709351Training Epoch: 0 | iteration: 88/262 | Loss: 0.9868807792663574Training Epoch: 0 | iteration: 89/262 | Loss: 0.896059513092041Training Epoch: 0 | iteration: 90/262 | Loss: 0.8752104640007019Training Epoch: 0 | iteration: 91/262 | Loss: 0.9840850234031677Training Epoch: 0 | iteration: 92/262 | Loss: 0.9049091339111328Training Epoch: 0 | iteration: 93/262 | Loss: 0.8747645616531372Training Epoch: 0 | iteration: 94/262 | Loss: 0.7902302742004395Training Epoch: 0 | iteration: 95/262 | Loss: 0.959480881690979Training Epoch: 0 | iteration: 96/262 | Loss: 0.815232515335083Training Epoch: 0 | iteration: 97/262 | Loss: 0.8717771768569946Training Epoch: 0 | iteration: 98/262 | Loss: 0.7747150659561157Training Epoch: 0 | iteration: 99/262 | Loss: 0.7999813556671143Training Epoch: 0 | iteration: 100/262 | Loss: 0.9356442093849182Training Epoch: 0 | iteration: 101/262 | Loss: 0.8338496685028076Training Epoch: 0 | iteration: 102/262 | Loss: 0.8388956785202026Training Epoch: 0 | iteration: 103/262 | Loss: 0.8063002824783325Training Epoch: 0 | iteration: 104/262 | Loss: 0.7566527724266052Training Epoch: 0 | iteration: 105/262 | Loss: 0.7778912782669067Training Epoch: 0 | iteration: 106/262 | Loss: 0.8810017704963684Training Epoch: 0 | iteration: 107/262 | Loss: 0.8276209831237793Training Epoch: 0 | iteration: 108/262 | Loss: 0.8642497062683105Training Epoch: 0 | iteration: 109/262 | Loss: 0.7834475040435791Training Epoch: 0 | iteration: 110/262 | Loss: 0.8759342432022095Training Epoch: 0 | iteration: 111/262 | Loss: 0.8347848653793335Training Epoch: 0 | iteration: 112/262 | Loss: 0.8050062656402588Training Epoch: 0 | iteration: 113/262 | Loss: 0.9976526498794556Training Epoch: 0 | iteration: 114/262 | Loss: 0.8301920890808105Training Epoch: 0 | iteration: 115/262 | Loss: 0.8207719326019287Training Epoch: 0 | iteration: 116/262 | Loss: 0.7735263109207153Training Epoch: 0 | iteration: 117/262 | Loss: 0.9038583040237427Training Epoch: 0 | iteration: 118/262 | Loss: 0.955146074295044Training Epoch: 0 | iteration: 119/262 | Loss: 0.9004002213478088Training Epoch: 0 | iteration: 120/262 | Loss: 0.8117040395736694Training Epoch: 0 | iteration: 121/262 | Loss: 0.8100544214248657Training Epoch: 0 | iteration: 122/262 | Loss: 0.8279905319213867Training Epoch: 0 | iteration: 123/262 | Loss: 0.7964013814926147Training Epoch: 0 | iteration: 124/262 | Loss: 0.7953590750694275Training Epoch: 0 | iteration: 125/262 | Loss: 0.8191308975219727Training Epoch: 0 | iteration: 126/262 | Loss: 0.8097625970840454Training Epoch: 0 | iteration: 127/262 | Loss: 0.8984864950180054Training Epoch: 0 | iteration: 128/262 | Loss: 0.8250130414962769Training Epoch: 0 | iteration: 129/262 | Loss: 0.8835391998291016Training Epoch: 0 | iteration: 130/262 | Loss: 0.9426978230476379Training Epoch: 0 | iteration: 131/262 | Loss: 0.7582509517669678Training Epoch: 0 | iteration: 132/262 | Loss: 0.8690320253372192Training Epoch: 0 | iteration: 133/262 | Loss: 0.816304087638855Training Epoch: 0 | iteration: 134/262 | Loss: 0.8716157674789429Training Epoch: 0 | iteration: 135/262 | Loss: 0.8225098848342896Training Epoch: 0 | iteration: 136/262 | Loss: 0.9724935293197632Training Epoch: 0 | iteration: 137/262 | Loss: 0.7883891463279724Training Epoch: 0 | iteration: 138/262 | Loss: 0.7560347318649292Training Epoch: 0 | iteration: 139/262 | Loss: 0.8185939788818359Training Epoch: 0 | iteration: 140/262 | Loss: 0.7788742184638977Training Epoch: 0 | iteration: 141/262 | Loss: 0.8763630390167236Training Epoch: 0 | iteration: 142/262 | Loss: 0.8008939623832703Training Epoch: 0 | iteration: 143/262 | Loss: 0.9906245470046997Training Epoch: 0 | iteration: 144/262 | Loss: 0.7973651885986328Training Epoch: 0 | iteration: 145/262 | Loss: 0.9072986841201782Training Epoch: 0 | iteration: 146/262 | Loss: 0.7914261817932129Training Epoch: 0 | iteration: 147/262 | Loss: 0.84195876121521Training Epoch: 0 | iteration: 148/262 | Loss: 1.0003201961517334Training Epoch: 0 | iteration: 149/262 | Loss: 0.7884116172790527Training Epoch: 0 | iteration: 150/262 | Loss: 0.78985595703125Training Epoch: 0 | iteration: 151/262 | Loss: 0.8645304441452026Training Epoch: 0 | iteration: 152/262 | Loss: 0.812229335308075Training Epoch: 0 | iteration: 153/262 | Loss: 0.8609479665756226Training Epoch: 0 | iteration: 154/262 | Loss: 0.8443981409072876Training Epoch: 0 | iteration: 155/262 | Loss: 0.8708515763282776Training Epoch: 0 | iteration: 156/262 | Loss: 0.8554785251617432Training Epoch: 0 | iteration: 157/262 | Loss: 0.7811651825904846Training Epoch: 0 | iteration: 158/262 | Loss: 0.8324438333511353Training Epoch: 0 | iteration: 159/262 | Loss: 0.861007034778595Training Epoch: 0 | iteration: 160/262 | Loss: 0.8373028039932251Training Epoch: 0 | iteration: 161/262 | Loss: 0.7645570039749146Training Epoch: 0 | iteration: 162/262 | Loss: 0.8313159942626953Training Epoch: 0 | iteration: 163/262 | Loss: 0.759687066078186Training Epoch: 0 | iteration: 164/262 | Loss: 0.9239932894706726Training Epoch: 0 | iteration: 165/262 | Loss: 0.9180267453193665Training Epoch: 0 | iteration: 166/262 | Loss: 0.7876172065734863Training Epoch: 0 | iteration: 167/262 | Loss: 0.7866445779800415Training Epoch: 0 | iteration: 168/262 | Loss: 0.8627767562866211Training Epoch: 0 | iteration: 169/262 | Loss: 0.8391590714454651Training Epoch: 0 | iteration: 170/262 | Loss: 0.8878188729286194Training Epoch: 0 | iteration: 171/262 | Loss: 0.8148612976074219Training Epoch: 0 | iteration: 172/262 | Loss: 0.8474017381668091Training Epoch: 0 | iteration: 173/262 | Loss: 0.9329137802124023Training Epoch: 0 | iteration: 174/262 | Loss: 0.8723311424255371Training Epoch: 0 | iteration: 175/262 | Loss: 0.753798246383667Training Epoch: 0 | iteration: 176/262 | Loss: 0.92118239402771Training Epoch: 0 | iteration: 177/262 | Loss: 0.8170291185379028Training Epoch: 0 | iteration: 178/262 | Loss: 0.8183716535568237Training Epoch: 0 | iteration: 179/262 | Loss: 0.8352804780006409Training Epoch: 0 | iteration: 180/262 | Loss: 0.874841570854187Training Epoch: 0 | iteration: 181/262 | Loss: 0.8626121282577515Training Epoch: 0 | iteration: 182/262 | Loss: 0.8598188161849976Training Epoch: 0 | iteration: 183/262 | Loss: 0.8714718818664551Training Epoch: 0 | iteration: 184/262 | Loss: 0.8840435743331909Training Epoch: 0 | iteration: 185/262 | Loss: 0.7683855891227722Training Epoch: 0 | iteration: 186/262 | Loss: 0.7597243785858154Training Epoch: 0 | iteration: 187/262 | Loss: 0.8427529335021973Training Epoch: 0 | iteration: 188/262 | Loss: 0.842380166053772Training Epoch: 0 | iteration: 189/262 | Loss: 1.0061030387878418Training Epoch: 0 | iteration: 190/262 | Loss: 0.704833984375Training Epoch: 0 | iteration: 191/262 | Loss: 0.7251231074333191Training Epoch: 0 | iteration: 192/262 | Loss: 0.8146752119064331Training Epoch: 0 | iteration: 193/262 | Loss: 0.7675138711929321Training Epoch: 0 | iteration: 194/262 | Loss: 0.7887853384017944Training Epoch: 0 | iteration: 195/262 | Loss: 0.9541987776756287Training Epoch: 0 | iteration: 196/262 | Loss: 0.8462008833885193Training Epoch: 0 | iteration: 197/262 | Loss: 0.7737036347389221Training Epoch: 0 | iteration: 198/262 | Loss: 0.8589744567871094Training Epoch: 0 | iteration: 199/262 | Loss: 0.7734903693199158Training Epoch: 0 | iteration: 200/262 | Loss: 0.843503475189209Training Epoch: 0 | iteration: 201/262 | Loss: 0.9376271367073059Training Epoch: 0 | iteration: 202/262 | Loss: 0.9048076868057251Training Epoch: 0 | iteration: 203/262 | Loss: 0.789505124092102Training Epoch: 0 | iteration: 204/262 | Loss: 0.8564972877502441Training Epoch: 0 | iteration: 205/262 | Loss: 0.8216104507446289Training Epoch: 0 | iteration: 206/262 | Loss: 0.7942781448364258Training Epoch: 0 | iteration: 207/262 | Loss: 0.7348742485046387Training Epoch: 0 | iteration: 208/262 | Loss: 0.8667842149734497Training Epoch: 0 | iteration: 209/262 | Loss: 0.7825469374656677Training Epoch: 0 | iteration: 210/262 | Loss: 0.8045358657836914Training Epoch: 0 | iteration: 211/262 | Loss: 0.7712215185165405Training Epoch: 0 | iteration: 212/262 | Loss: 0.9589093923568726Training Epoch: 0 | iteration: 213/262 | Loss: 0.8879067897796631Training Epoch: 0 | iteration: 214/262 | Loss: 0.8952014446258545Training Epoch: 0 | iteration: 215/262 | Loss: 0.8442499041557312Training Epoch: 0 | iteration: 216/262 | Loss: 0.8508924245834351Training Epoch: 0 | iteration: 217/262 | Loss: 0.7767421007156372Training Epoch: 0 | iteration: 218/262 | Loss: 0.7066537141799927Training Epoch: 0 | iteration: 219/262 | Loss: 0.8503683805465698Training Epoch: 0 | iteration: 220/262 | Loss: 0.8061836957931519Training Epoch: 0 | iteration: 221/262 | Loss: 0.8493136167526245Training Epoch: 0 | iteration: 222/262 | Loss: 0.8551248908042908Training Epoch: 0 | iteration: 223/262 | Loss: 0.815651535987854Training Epoch: 0 | iteration: 224/262 | Loss: 0.7216075658798218Training Epoch: 0 | iteration: 225/262 | Loss: 0.7902896404266357Training Epoch: 0 | iteration: 226/262 | Loss: 0.7747219800949097Training Epoch: 0 | iteration: 227/262 | Loss: 0.8455326557159424Training Epoch: 0 | iteration: 228/262 | Loss: 0.8733552694320679Training Epoch: 0 | iteration: 229/262 | Loss: 0.9606804847717285Training Epoch: 0 | iteration: 230/262 | Loss: 0.793143630027771Training Epoch: 0 | iteration: 231/262 | Loss: 0.7079647779464722Training Epoch: 0 | iteration: 232/262 | Loss: 0.8402639627456665Training Epoch: 0 | iteration: 233/262 | Loss: 0.8335232734680176Training Epoch: 0 | iteration: 234/262 | Loss: 0.7328441143035889Training Epoch: 0 | iteration: 235/262 | Loss: 0.7571439743041992Training Epoch: 0 | iteration: 236/262 | Loss: 0.7955718040466309Training Epoch: 0 | iteration: 237/262 | Loss: 0.870205283164978Training Epoch: 0 | iteration: 238/262 | Loss: 0.8018592596054077Training Epoch: 0 | iteration: 239/262 | Loss: 0.8642638325691223Training Epoch: 0 | iteration: 240/262 | Loss: 0.7647056579589844Training Epoch: 0 | iteration: 241/262 | Loss: 0.8151759505271912Training Epoch: 0 | iteration: 242/262 | Loss: 0.7870116829872131Training Epoch: 0 | iteration: 243/262 | Loss: 0.7424155473709106Training Epoch: 0 | iteration: 244/262 | Loss: 0.814026951789856Training Epoch: 0 | iteration: 245/262 | Loss: 0.7678033113479614Training Epoch: 0 | iteration: 246/262 | Loss: 0.8310317993164062Training Epoch: 0 | iteration: 247/262 | Loss: 0.7173333168029785Training Epoch: 0 | iteration: 248/262 | Loss: 0.758607029914856Training Epoch: 0 | iteration: 249/262 | Loss: 0.9447661638259888Training Epoch: 0 | iteration: 250/262 | Loss: 0.8072454929351807Training Epoch: 0 | iteration: 251/262 | Loss: 0.8783565759658813Training Epoch: 0 | iteration: 252/262 | Loss: 0.7944538593292236Training Epoch: 0 | iteration: 253/262 | Loss: 0.849618673324585Training Epoch: 0 | iteration: 254/262 | Loss: 0.8157651424407959Training Epoch: 0 | iteration: 255/262 | Loss: 0.7786740064620972Training Epoch: 0 | iteration: 256/262 | Loss: 0.8817750215530396Training Epoch: 0 | iteration: 257/262 | Loss: 0.7897887825965881Training Epoch: 0 | iteration: 258/262 | Loss: 0.7141690254211426Training Epoch: 0 | iteration: 259/262 | Loss: 0.8075529336929321Training Epoch: 0 | iteration: 260/262 | Loss: 0.7940701246261597Training Epoch: 0 | iteration: 261/262 | Loss: 0.7114683985710144Validating Epoch: 0 | iteration: 0/66 | Loss: 0.7102974653244019Validating Epoch: 0 | iteration: 1/66 | Loss: 0.5843051075935364Validating Epoch: 0 | iteration: 2/66 | Loss: 0.6332001686096191Validating Epoch: 0 | iteration: 3/66 | Loss: 0.6747892498970032Validating Epoch: 0 | iteration: 4/66 | Loss: 0.6813389658927917Validating Epoch: 0 | iteration: 5/66 | Loss: 0.6372199654579163Validating Epoch: 0 | iteration: 6/66 | Loss: 0.6284980177879333Validating Epoch: 0 | iteration: 7/66 | Loss: 0.7153561115264893Validating Epoch: 0 | iteration: 8/66 | Loss: 0.7170149087905884Validating Epoch: 0 | iteration: 9/66 | Loss: 0.6791493892669678Validating Epoch: 0 | iteration: 10/66 | Loss: 0.6610471606254578Validating Epoch: 0 | iteration: 11/66 | Loss: 0.6215959191322327Validating Epoch: 0 | iteration: 12/66 | Loss: 0.6693627834320068Validating Epoch: 0 | iteration: 13/66 | Loss: 0.6494343876838684Validating Epoch: 0 | iteration: 14/66 | Loss: 0.6520221829414368Validating Epoch: 0 | iteration: 15/66 | Loss: 0.6839940547943115Validating Epoch: 0 | iteration: 16/66 | Loss: 0.6444007158279419Validating Epoch: 0 | iteration: 17/66 | Loss: 0.5629004240036011Validating Epoch: 0 | iteration: 18/66 | Loss: 0.6544644236564636Validating Epoch: 0 | iteration: 19/66 | Loss: 0.6590491533279419Validating Epoch: 0 | iteration: 20/66 | Loss: 0.6410937309265137Validating Epoch: 0 | iteration: 21/66 | Loss: 0.7009097337722778Validating Epoch: 0 | iteration: 22/66 | Loss: 0.7214546203613281Validating Epoch: 0 | iteration: 23/66 | Loss: 0.7105821371078491Validating Epoch: 0 | iteration: 24/66 | Loss: 0.7139051556587219Validating Epoch: 0 | iteration: 25/66 | Loss: 0.666443407535553Validating Epoch: 0 | iteration: 26/66 | Loss: 0.6776851415634155Validating Epoch: 0 | iteration: 27/66 | Loss: 0.6606487035751343Validating Epoch: 0 | iteration: 28/66 | Loss: 0.6150020360946655Validating Epoch: 0 | iteration: 29/66 | Loss: 0.6969455480575562Validating Epoch: 0 | iteration: 30/66 | Loss: 0.710724949836731Validating Epoch: 0 | iteration: 31/66 | Loss: 0.6797126531600952Validating Epoch: 0 | iteration: 32/66 | Loss: 0.7784408330917358Validating Epoch: 0 | iteration: 33/66 | Loss: 0.717904269695282Validating Epoch: 0 | iteration: 34/66 | Loss: 0.7368839979171753Validating Epoch: 0 | iteration: 35/66 | Loss: 0.7101236581802368Validating Epoch: 0 | iteration: 36/66 | Loss: 0.7361773252487183Validating Epoch: 0 | iteration: 37/66 | Loss: 0.6640294790267944Validating Epoch: 0 | iteration: 38/66 | Loss: 0.7406666278839111Validating Epoch: 0 | iteration: 39/66 | Loss: 0.5783134698867798Validating Epoch: 0 | iteration: 40/66 | Loss: 0.6397535800933838Validating Epoch: 0 | iteration: 41/66 | Loss: 0.6010894775390625Validating Epoch: 0 | iteration: 42/66 | Loss: 0.6534755229949951Validating Epoch: 0 | iteration: 43/66 | Loss: 0.6161138415336609Validating Epoch: 0 | iteration: 44/66 | Loss: 0.6455540657043457Validating Epoch: 0 | iteration: 45/66 | Loss: 0.6770602464675903Validating Epoch: 0 | iteration: 46/66 | Loss: 0.6729077100753784Validating Epoch: 0 | iteration: 47/66 | Loss: 0.6376906037330627Validating Epoch: 0 | iteration: 48/66 | Loss: 0.6668909192085266Validating Epoch: 0 | iteration: 49/66 | Loss: 0.6309373378753662Validating Epoch: 0 | iteration: 50/66 | Loss: 0.6658687591552734Validating Epoch: 0 | iteration: 51/66 | Loss: 0.6994584202766418Validating Epoch: 0 | iteration: 52/66 | Loss: 0.671047568321228Validating Epoch: 0 | iteration: 53/66 | Loss: 0.6353834867477417Validating Epoch: 0 | iteration: 54/66 | Loss: 0.7057036757469177Validating Epoch: 0 | iteration: 55/66 | Loss: 0.6681985855102539Validating Epoch: 0 | iteration: 56/66 | Loss: 0.648887038230896Validating Epoch: 0 | iteration: 57/66 | Loss: 0.6620059013366699Validating Epoch: 0 | iteration: 58/66 | Loss: 0.7076495289802551Validating Epoch: 0 | iteration: 59/66 | Loss: 0.5989614725112915Validating Epoch: 0 | iteration: 60/66 | Loss: 0.6588081121444702Validating Epoch: 0 | iteration: 61/66 | Loss: 0.6941362619400024Validating Epoch: 0 | iteration: 62/66 | Loss: 0.7221993207931519Validating Epoch: 0 | iteration: 63/66 | Loss: 0.7163108587265015Validating Epoch: 0 | iteration: 64/66 | Loss: 0.6378649473190308Validating Epoch: 0 | iteration: 65/66 | Loss: 0.6915028691291809Temperature:  0.5
Sampling: iteration: 0/51{'Validity': 0.9951171875, 'Novelty': 1.0, 'Uniqueness': 0.9931305201177625}
Training Epoch: 1 | iteration: 0/262 | Loss: 0.8444758653640747Training Epoch: 1 | iteration: 1/262 | Loss: 0.8051179051399231Training Epoch: 1 | iteration: 2/262 | Loss: 0.7043346166610718Training Epoch: 1 | iteration: 3/262 | Loss: 0.7417715787887573Training Epoch: 1 | iteration: 4/262 | Loss: 0.8091171979904175Training Epoch: 1 | iteration: 5/262 | Loss: 0.7633383870124817Training Epoch: 1 | iteration: 6/262 | Loss: 0.6562137007713318Training Epoch: 1 | iteration: 7/262 | Loss: 0.7981479167938232Training Epoch: 1 | iteration: 8/262 | Loss: 0.9207670092582703Training Epoch: 1 | iteration: 9/262 | Loss: 0.799335241317749Training Epoch: 1 | iteration: 10/262 | Loss: 0.6906977295875549Training Epoch: 1 | iteration: 11/262 | Loss: 0.7585992217063904Training Epoch: 1 | iteration: 12/262 | Loss: 0.6810202598571777Training Epoch: 1 | iteration: 13/262 | Loss: 0.8672914505004883Training Epoch: 1 | iteration: 14/262 | Loss: 0.8060919642448425Training Epoch: 1 | iteration: 15/262 | Loss: 0.7740687131881714Training Epoch: 1 | iteration: 16/262 | Loss: 0.7412763237953186Training Epoch: 1 | iteration: 17/262 | Loss: 0.8294945955276489Training Epoch: 1 | iteration: 18/262 | Loss: 0.7149419784545898Training Epoch: 1 | iteration: 19/262 | Loss: 0.7060670256614685Training Epoch: 1 | iteration: 20/262 | Loss: 0.722578763961792Training Epoch: 1 | iteration: 21/262 | Loss: 0.723697304725647Training Epoch: 1 | iteration: 22/262 | Loss: 0.7464371919631958Training Epoch: 1 | iteration: 23/262 | Loss: 0.7749198079109192Training Epoch: 1 | iteration: 24/262 | Loss: 0.7397483587265015Training Epoch: 1 | iteration: 25/262 | Loss: 0.7423492074012756Training Epoch: 1 | iteration: 26/262 | Loss: 0.7593055963516235Training Epoch: 1 | iteration: 27/262 | Loss: 0.7499035596847534Training Epoch: 1 | iteration: 28/262 | Loss: 0.6671661138534546Training Epoch: 1 | iteration: 29/262 | Loss: 0.6556620597839355Training Epoch: 1 | iteration: 30/262 | Loss: 0.9033938050270081Training Epoch: 1 | iteration: 31/262 | Loss: 0.7406400442123413Training Epoch: 1 | iteration: 32/262 | Loss: 0.7643738985061646Training Epoch: 1 | iteration: 33/262 | Loss: 0.6964962482452393Training Epoch: 1 | iteration: 34/262 | Loss: 0.722376823425293Training Epoch: 1 | iteration: 35/262 | Loss: 0.6811224222183228Training Epoch: 1 | iteration: 36/262 | Loss: 0.7940285801887512Training Epoch: 1 | iteration: 37/262 | Loss: 0.705864667892456Training Epoch: 1 | iteration: 38/262 | Loss: 0.7850081920623779Training Epoch: 1 | iteration: 39/262 | Loss: 0.8376386165618896Training Epoch: 1 | iteration: 40/262 | Loss: 0.7525982856750488Training Epoch: 1 | iteration: 41/262 | Loss: 0.745978593826294Training Epoch: 1 | iteration: 42/262 | Loss: 0.7706995010375977Training Epoch: 1 | iteration: 43/262 | Loss: 0.7526302337646484Training Epoch: 1 | iteration: 44/262 | Loss: 0.7924643754959106Training Epoch: 1 | iteration: 45/262 | Loss: 0.8476138114929199Training Epoch: 1 | iteration: 46/262 | Loss: 0.7407679557800293Training Epoch: 1 | iteration: 47/262 | Loss: 0.7866332530975342Training Epoch: 1 | iteration: 48/262 | Loss: 0.7320026755332947Training Epoch: 1 | iteration: 49/262 | Loss: 0.7729713916778564Training Epoch: 1 | iteration: 50/262 | Loss: 0.7691208720207214Training Epoch: 1 | iteration: 51/262 | Loss: 0.7801641821861267Training Epoch: 1 | iteration: 52/262 | Loss: 0.7158685922622681Training Epoch: 1 | iteration: 53/262 | Loss: 0.848091185092926Training Epoch: 1 | iteration: 54/262 | Loss: 0.7735776901245117Training Epoch: 1 | iteration: 55/262 | Loss: 0.7807106971740723Training Epoch: 1 | iteration: 56/262 | Loss: 0.7580850124359131Training Epoch: 1 | iteration: 57/262 | Loss: 0.8582069873809814Training Epoch: 1 | iteration: 58/262 | Loss: 0.7723603248596191Training Epoch: 1 | iteration: 59/262 | Loss: 0.7771734595298767Training Epoch: 1 | iteration: 60/262 | Loss: 0.7290542125701904Training Epoch: 1 | iteration: 61/262 | Loss: 0.8036626577377319Training Epoch: 1 | iteration: 62/262 | Loss: 0.7929455041885376Training Epoch: 1 | iteration: 63/262 | Loss: 0.809678852558136Training Epoch: 1 | iteration: 64/262 | Loss: 0.7609196901321411Training Epoch: 1 | iteration: 65/262 | Loss: 0.6824523210525513Training Epoch: 1 | iteration: 66/262 | Loss: 0.7443327903747559Training Epoch: 1 | iteration: 67/262 | Loss: 0.8351917266845703Training Epoch: 1 | iteration: 68/262 | Loss: 0.7630873918533325Training Epoch: 1 | iteration: 69/262 | Loss: 0.6963874697685242Training Epoch: 1 | iteration: 70/262 | Loss: 0.8237231373786926Training Epoch: 1 | iteration: 71/262 | Loss: 0.7663259506225586Training Epoch: 1 | iteration: 72/262 | Loss: 0.6804566383361816Training Epoch: 1 | iteration: 73/262 | Loss: 0.790360689163208Training Epoch: 1 | iteration: 74/262 | Loss: 0.837194561958313Training Epoch: 1 | iteration: 75/262 | Loss: 0.7278901934623718Training Epoch: 1 | iteration: 76/262 | Loss: 0.7671346664428711Training Epoch: 1 | iteration: 77/262 | Loss: 0.8141579627990723Training Epoch: 1 | iteration: 78/262 | Loss: 0.80277419090271Training Epoch: 1 | iteration: 79/262 | Loss: 0.872465968132019Training Epoch: 1 | iteration: 80/262 | Loss: 0.8317202925682068Training Epoch: 1 | iteration: 81/262 | Loss: 0.7535335421562195Training Epoch: 1 | iteration: 82/262 | Loss: 0.7956575155258179Training Epoch: 1 | iteration: 83/262 | Loss: 0.7159050703048706Training Epoch: 1 | iteration: 84/262 | Loss: 0.8709893226623535Training Epoch: 1 | iteration: 85/262 | Loss: 0.9424614310264587Training Epoch: 1 | iteration: 86/262 | Loss: 0.7945282459259033Training Epoch: 1 | iteration: 87/262 | Loss: 0.7485102415084839Training Epoch: 1 | iteration: 88/262 | Loss: 0.71328204870224Training Epoch: 1 | iteration: 89/262 | Loss: 0.8007947206497192Training Epoch: 1 | iteration: 90/262 | Loss: 0.8314467668533325Training Epoch: 1 | iteration: 91/262 | Loss: 0.8041245341300964Training Epoch: 1 | iteration: 92/262 | Loss: 0.7243870496749878Training Epoch: 1 | iteration: 93/262 | Loss: 0.8512001633644104Training Epoch: 1 | iteration: 94/262 | Loss: 0.7210077047348022Training Epoch: 1 | iteration: 95/262 | Loss: 0.7929258346557617Training Epoch: 1 | iteration: 96/262 | Loss: 0.810684084892273Training Epoch: 1 | iteration: 97/262 | Loss: 0.7569448351860046Training Epoch: 1 | iteration: 98/262 | Loss: 0.8104375600814819Training Epoch: 1 | iteration: 99/262 | Loss: 0.7566715478897095Training Epoch: 1 | iteration: 100/262 | Loss: 0.737881064414978Training Epoch: 1 | iteration: 101/262 | Loss: 0.816043496131897Training Epoch: 1 | iteration: 102/262 | Loss: 0.7028651833534241Training Epoch: 1 | iteration: 103/262 | Loss: 0.8098624348640442Training Epoch: 1 | iteration: 104/262 | Loss: 0.834003210067749Training Epoch: 1 | iteration: 105/262 | Loss: 0.6910145878791809Training Epoch: 1 | iteration: 106/262 | Loss: 0.841050386428833Training Epoch: 1 | iteration: 107/262 | Loss: 0.6789915561676025Training Epoch: 1 | iteration: 108/262 | Loss: 0.7060417532920837Training Epoch: 1 | iteration: 109/262 | Loss: 0.7783161401748657Training Epoch: 1 | iteration: 110/262 | Loss: 0.7176086902618408Training Epoch: 1 | iteration: 111/262 | Loss: 0.6754754781723022Training Epoch: 1 | iteration: 112/262 | Loss: 0.76981520652771Training Epoch: 1 | iteration: 113/262 | Loss: 0.7815903425216675Training Epoch: 1 | iteration: 114/262 | Loss: 0.8144943714141846Training Epoch: 1 | iteration: 115/262 | Loss: 0.7857871055603027Training Epoch: 1 | iteration: 116/262 | Loss: 0.8010751008987427Training Epoch: 1 | iteration: 117/262 | Loss: 0.8074889183044434Training Epoch: 1 | iteration: 118/262 | Loss: 0.8183783292770386Training Epoch: 1 | iteration: 119/262 | Loss: 0.7473245859146118Training Epoch: 1 | iteration: 120/262 | Loss: 0.7750047445297241Training Epoch: 1 | iteration: 121/262 | Loss: 0.7607693672180176Training Epoch: 1 | iteration: 122/262 | Loss: 0.7553753852844238Training Epoch: 1 | iteration: 123/262 | Loss: 0.7018910646438599Training Epoch: 1 | iteration: 124/262 | Loss: 0.7907368540763855Training Epoch: 1 | iteration: 125/262 | Loss: 0.7934269905090332Training Epoch: 1 | iteration: 126/262 | Loss: 0.815933108329773Training Epoch: 1 | iteration: 127/262 | Loss: 0.7638537883758545Training Epoch: 1 | iteration: 128/262 | Loss: 0.8570380806922913Training Epoch: 1 | iteration: 129/262 | Loss: 0.7183449864387512Training Epoch: 1 | iteration: 130/262 | Loss: 0.6986387968063354Training Epoch: 1 | iteration: 131/262 | Loss: 0.6605497598648071Training Epoch: 1 | iteration: 132/262 | Loss: 0.7502172589302063Training Epoch: 1 | iteration: 133/262 | Loss: 0.792061448097229Training Epoch: 1 | iteration: 134/262 | Loss: 0.724943220615387Training Epoch: 1 | iteration: 135/262 | Loss: 0.7119231224060059Training Epoch: 1 | iteration: 136/262 | Loss: 0.6956429481506348Training Epoch: 1 | iteration: 137/262 | Loss: 0.8744182586669922Training Epoch: 1 | iteration: 138/262 | Loss: 0.7536042928695679Training Epoch: 1 | iteration: 139/262 | Loss: 0.7350515127182007Training Epoch: 1 | iteration: 140/262 | Loss: 0.7877272367477417Training Epoch: 1 | iteration: 141/262 | Loss: 0.7027390003204346Training Epoch: 1 | iteration: 142/262 | Loss: 0.8563097715377808Training Epoch: 1 | iteration: 143/262 | Loss: 0.7233976125717163Training Epoch: 1 | iteration: 144/262 | Loss: 0.710710883140564Training Epoch: 1 | iteration: 145/262 | Loss: 0.7863208055496216Training Epoch: 1 | iteration: 146/262 | Loss: 0.8286612033843994Training Epoch: 1 | iteration: 147/262 | Loss: 0.7283188104629517Training Epoch: 1 | iteration: 148/262 | Loss: 0.7225363850593567Training Epoch: 1 | iteration: 149/262 | Loss: 0.7745336294174194Training Epoch: 1 | iteration: 150/262 | Loss: 0.7789015769958496Training Epoch: 1 | iteration: 151/262 | Loss: 0.8156165480613708Training Epoch: 1 | iteration: 152/262 | Loss: 0.8181936144828796Training Epoch: 1 | iteration: 153/262 | Loss: 0.6832661628723145Training Epoch: 1 | iteration: 154/262 | Loss: 0.7351205348968506Training Epoch: 1 | iteration: 155/262 | Loss: 0.796471118927002Training Epoch: 1 | iteration: 156/262 | Loss: 0.7489113807678223Training Epoch: 1 | iteration: 157/262 | Loss: 0.7700512409210205Training Epoch: 1 | iteration: 158/262 | Loss: 0.7185639142990112Training Epoch: 1 | iteration: 159/262 | Loss: 0.7860783338546753Training Epoch: 1 | iteration: 160/262 | Loss: 0.7246994972229004Training Epoch: 1 | iteration: 161/262 | Loss: 0.7183078527450562Training Epoch: 1 | iteration: 162/262 | Loss: 0.7183185815811157Training Epoch: 1 | iteration: 163/262 | Loss: 0.8268007636070251Training Epoch: 1 | iteration: 164/262 | Loss: 0.7975298762321472Training Epoch: 1 | iteration: 165/262 | Loss: 0.7494364976882935Training Epoch: 1 | iteration: 166/262 | Loss: 0.720736563205719Training Epoch: 1 | iteration: 167/262 | Loss: 0.7400863170623779Training Epoch: 1 | iteration: 168/262 | Loss: 0.8126873970031738Training Epoch: 1 | iteration: 169/262 | Loss: 0.7763658761978149Training Epoch: 1 | iteration: 170/262 | Loss: 0.7112236618995667Training Epoch: 1 | iteration: 171/262 | Loss: 0.7504570484161377Training Epoch: 1 | iteration: 172/262 | Loss: 0.7310499548912048Training Epoch: 1 | iteration: 173/262 | Loss: 0.7014950513839722Training Epoch: 1 | iteration: 174/262 | Loss: 0.8225892782211304Training Epoch: 1 | iteration: 175/262 | Loss: 0.7959123253822327Training Epoch: 1 | iteration: 176/262 | Loss: 0.7296426296234131Training Epoch: 1 | iteration: 177/262 | Loss: 0.7687363028526306Training Epoch: 1 | iteration: 178/262 | Loss: 0.7224636077880859Training Epoch: 1 | iteration: 179/262 | Loss: 0.6953408122062683Training Epoch: 1 | iteration: 180/262 | Loss: 0.6843273639678955Training Epoch: 1 | iteration: 181/262 | Loss: 0.8028168678283691Training Epoch: 1 | iteration: 182/262 | Loss: 0.7201231718063354Training Epoch: 1 | iteration: 183/262 | Loss: 0.7297713756561279Training Epoch: 1 | iteration: 184/262 | Loss: 0.7628076076507568Training Epoch: 1 | iteration: 185/262 | Loss: 0.7285428047180176Training Epoch: 1 | iteration: 186/262 | Loss: 0.7116225957870483Training Epoch: 1 | iteration: 187/262 | Loss: 0.8513203859329224Training Epoch: 1 | iteration: 188/262 | Loss: 0.732341468334198Training Epoch: 1 | iteration: 189/262 | Loss: 0.7785217761993408Training Epoch: 1 | iteration: 190/262 | Loss: 0.7555768489837646Training Epoch: 1 | iteration: 191/262 | Loss: 0.7385292649269104Training Epoch: 1 | iteration: 192/262 | Loss: 0.7832876443862915Training Epoch: 1 | iteration: 193/262 | Loss: 0.7728472948074341Training Epoch: 1 | iteration: 194/262 | Loss: 0.8143746852874756Training Epoch: 1 | iteration: 195/262 | Loss: 0.6002478003501892Training Epoch: 1 | iteration: 196/262 | Loss: 0.8852697610855103Training Epoch: 1 | iteration: 197/262 | Loss: 0.8094430565834045Training Epoch: 1 | iteration: 198/262 | Loss: 0.7431058883666992Training Epoch: 1 | iteration: 199/262 | Loss: 0.77024906873703Training Epoch: 1 | iteration: 200/262 | Loss: 0.7093242406845093Training Epoch: 1 | iteration: 201/262 | Loss: 0.79085773229599Training Epoch: 1 | iteration: 202/262 | Loss: 0.8505730032920837Training Epoch: 1 | iteration: 203/262 | Loss: 0.8185634613037109Training Epoch: 1 | iteration: 204/262 | Loss: 0.7308059930801392Training Epoch: 1 | iteration: 205/262 | Loss: 0.843367874622345Training Epoch: 1 | iteration: 206/262 | Loss: 0.9058572053909302Training Epoch: 1 | iteration: 207/262 | Loss: 0.7108336091041565Training Epoch: 1 | iteration: 208/262 | Loss: 0.7001644372940063Training Epoch: 1 | iteration: 209/262 | Loss: 0.789732813835144Training Epoch: 1 | iteration: 210/262 | Loss: 0.750538170337677Training Epoch: 1 | iteration: 211/262 | Loss: 0.7378159761428833Training Epoch: 1 | iteration: 212/262 | Loss: 0.7525542974472046Training Epoch: 1 | iteration: 213/262 | Loss: 0.8129035830497742Training Epoch: 1 | iteration: 214/262 | Loss: 0.8615431189537048Training Epoch: 1 | iteration: 215/262 | Loss: 0.6731852889060974Training Epoch: 1 | iteration: 216/262 | Loss: 0.7331736087799072Training Epoch: 1 | iteration: 217/262 | Loss: 0.681100606918335Training Epoch: 1 | iteration: 218/262 | Loss: 0.7938193082809448Training Epoch: 1 | iteration: 219/262 | Loss: 0.8078826069831848Training Epoch: 1 | iteration: 220/262 | Loss: 0.818216860294342Training Epoch: 1 | iteration: 221/262 | Loss: 0.7580002546310425Training Epoch: 1 | iteration: 222/262 | Loss: 0.7117811441421509Training Epoch: 1 | iteration: 223/262 | Loss: 0.748735785484314Training Epoch: 1 | iteration: 224/262 | Loss: 0.7864266037940979Training Epoch: 1 | iteration: 225/262 | Loss: 0.6455242037773132Training Epoch: 1 | iteration: 226/262 | Loss: 0.7486518621444702Training Epoch: 1 | iteration: 227/262 | Loss: 0.7905105352401733Training Epoch: 1 | iteration: 228/262 | Loss: 0.7284006476402283Training Epoch: 1 | iteration: 229/262 | Loss: 0.7242952585220337Training Epoch: 1 | iteration: 230/262 | Loss: 0.7596568465232849Training Epoch: 1 | iteration: 231/262 | Loss: 0.7779802083969116Training Epoch: 1 | iteration: 232/262 | Loss: 0.7665603756904602Training Epoch: 1 | iteration: 233/262 | Loss: 0.719265878200531Training Epoch: 1 | iteration: 234/262 | Loss: 0.7377532720565796Training Epoch: 1 | iteration: 235/262 | Loss: 0.7203129529953003Training Epoch: 1 | iteration: 236/262 | Loss: 0.7172079682350159Training Epoch: 1 | iteration: 237/262 | Loss: 0.8011311888694763Training Epoch: 1 | iteration: 238/262 | Loss: 0.7831793427467346Training Epoch: 1 | iteration: 239/262 | Loss: 0.75230872631073Training Epoch: 1 | iteration: 240/262 | Loss: 0.7645482420921326Training Epoch: 1 | iteration: 241/262 | Loss: 0.8492962121963501Training Epoch: 1 | iteration: 242/262 | Loss: 0.7524397969245911Training Epoch: 1 | iteration: 243/262 | Loss: 0.8138285875320435Training Epoch: 1 | iteration: 244/262 | Loss: 0.7483319640159607Training Epoch: 1 | iteration: 245/262 | Loss: 0.8386496305465698Training Epoch: 1 | iteration: 246/262 | Loss: 0.7774968147277832Training Epoch: 1 | iteration: 247/262 | Loss: 0.6560356616973877Training Epoch: 1 | iteration: 248/262 | Loss: 0.6559009552001953Training Epoch: 1 | iteration: 249/262 | Loss: 0.7601510882377625Training Epoch: 1 | iteration: 250/262 | Loss: 0.8407975435256958Training Epoch: 1 | iteration: 251/262 | Loss: 0.7793797850608826Training Epoch: 1 | iteration: 252/262 | Loss: 0.6579782962799072Training Epoch: 1 | iteration: 253/262 | Loss: 0.7514393329620361Training Epoch: 1 | iteration: 254/262 | Loss: 0.7522768378257751Training Epoch: 1 | iteration: 255/262 | Loss: 0.725591778755188Training Epoch: 1 | iteration: 256/262 | Loss: 0.7891698479652405Training Epoch: 1 | iteration: 257/262 | Loss: 0.8132821321487427Training Epoch: 1 | iteration: 258/262 | Loss: 0.7417981624603271Training Epoch: 1 | iteration: 259/262 | Loss: 0.7403542399406433Training Epoch: 1 | iteration: 260/262 | Loss: 0.7875155210494995Training Epoch: 1 | iteration: 261/262 | Loss: 0.7874652743339539Validating Epoch: 1 | iteration: 0/66 | Loss: 0.5796467661857605Validating Epoch: 1 | iteration: 1/66 | Loss: 0.6760222911834717Validating Epoch: 1 | iteration: 2/66 | Loss: 0.6038698554039001Validating Epoch: 1 | iteration: 3/66 | Loss: 0.648178219795227Validating Epoch: 1 | iteration: 4/66 | Loss: 0.5652261972427368Validating Epoch: 1 | iteration: 5/66 | Loss: 0.591812252998352Validating Epoch: 1 | iteration: 6/66 | Loss: 0.6694366931915283Validating Epoch: 1 | iteration: 7/66 | Loss: 0.7622769474983215Validating Epoch: 1 | iteration: 8/66 | Loss: 0.6208357214927673Validating Epoch: 1 | iteration: 9/66 | Loss: 0.5722787380218506Validating Epoch: 1 | iteration: 10/66 | Loss: 0.6365605592727661Validating Epoch: 1 | iteration: 11/66 | Loss: 0.6034557223320007Validating Epoch: 1 | iteration: 12/66 | Loss: 0.5953765511512756Validating Epoch: 1 | iteration: 13/66 | Loss: 0.6487220525741577Validating Epoch: 1 | iteration: 14/66 | Loss: 0.6351971626281738Validating Epoch: 1 | iteration: 15/66 | Loss: 0.6124972105026245Validating Epoch: 1 | iteration: 16/66 | Loss: 0.6553221940994263Validating Epoch: 1 | iteration: 17/66 | Loss: 0.6165304780006409Validating Epoch: 1 | iteration: 18/66 | Loss: 0.5960663557052612Validating Epoch: 1 | iteration: 19/66 | Loss: 0.6810238361358643Validating Epoch: 1 | iteration: 20/66 | Loss: 0.7475887537002563Validating Epoch: 1 | iteration: 21/66 | Loss: 0.6569008827209473Validating Epoch: 1 | iteration: 22/66 | Loss: 0.6572990417480469Validating Epoch: 1 | iteration: 23/66 | Loss: 0.716507613658905Validating Epoch: 1 | iteration: 24/66 | Loss: 0.5648166537284851Validating Epoch: 1 | iteration: 25/66 | Loss: 0.6090691089630127Validating Epoch: 1 | iteration: 26/66 | Loss: 0.6546228528022766Validating Epoch: 1 | iteration: 27/66 | Loss: 0.6900033950805664Validating Epoch: 1 | iteration: 28/66 | Loss: 0.6071733832359314Validating Epoch: 1 | iteration: 29/66 | Loss: 0.6169323921203613Validating Epoch: 1 | iteration: 30/66 | Loss: 0.6768498420715332Validating Epoch: 1 | iteration: 31/66 | Loss: 0.6736081838607788Validating Epoch: 1 | iteration: 32/66 | Loss: 0.7039437890052795Validating Epoch: 1 | iteration: 33/66 | Loss: 0.6463142037391663Validating Epoch: 1 | iteration: 34/66 | Loss: 0.667741060256958Validating Epoch: 1 | iteration: 35/66 | Loss: 0.6569972634315491Validating Epoch: 1 | iteration: 36/66 | Loss: 0.6405899524688721Validating Epoch: 1 | iteration: 37/66 | Loss: 0.6579232215881348Validating Epoch: 1 | iteration: 38/66 | Loss: 0.7354156970977783Validating Epoch: 1 | iteration: 39/66 | Loss: 0.6753362417221069Validating Epoch: 1 | iteration: 40/66 | Loss: 0.6813331842422485Validating Epoch: 1 | iteration: 41/66 | Loss: 0.6713139414787292Validating Epoch: 1 | iteration: 42/66 | Loss: 0.6233968734741211Validating Epoch: 1 | iteration: 43/66 | Loss: 0.6756662130355835Validating Epoch: 1 | iteration: 44/66 | Loss: 0.7164826989173889Validating Epoch: 1 | iteration: 45/66 | Loss: 0.6230670213699341Validating Epoch: 1 | iteration: 46/66 | Loss: 0.6982736587524414Validating Epoch: 1 | iteration: 47/66 | Loss: 0.6363973617553711Validating Epoch: 1 | iteration: 48/66 | Loss: 0.6507656574249268Validating Epoch: 1 | iteration: 49/66 | Loss: 0.6864117383956909Validating Epoch: 1 | iteration: 50/66 | Loss: 0.6081274747848511Validating Epoch: 1 | iteration: 51/66 | Loss: 0.7155246138572693Validating Epoch: 1 | iteration: 52/66 | Loss: 0.6660369038581848Validating Epoch: 1 | iteration: 53/66 | Loss: 0.7015091180801392Validating Epoch: 1 | iteration: 54/66 | Loss: 0.6692821979522705Validating Epoch: 1 | iteration: 55/66 | Loss: 0.6546258926391602Validating Epoch: 1 | iteration: 56/66 | Loss: 0.621715784072876Validating Epoch: 1 | iteration: 57/66 | Loss: 0.6916437149047852Validating Epoch: 1 | iteration: 58/66 | Loss: 0.6632798910140991Validating Epoch: 1 | iteration: 59/66 | Loss: 0.6142526268959045Validating Epoch: 1 | iteration: 60/66 | Loss: 0.6608300805091858Validating Epoch: 1 | iteration: 61/66 | Loss: 0.6614503860473633Validating Epoch: 1 | iteration: 62/66 | Loss: 0.6692030429840088Validating Epoch: 1 | iteration: 63/66 | Loss: 0.6055436134338379Validating Epoch: 1 | iteration: 64/66 | Loss: 0.6473996043205261Validating Epoch: 1 | iteration: 65/66 | Loss: 0.6815617680549622Temperature:  0.5
Sampling: iteration: 0/51{'Validity': 0.994140625, 'Novelty': 1.0, 'Uniqueness': 0.9941060903732809}
Training Epoch: 2 | iteration: 0/262 | Loss: 0.7014511823654175Training Epoch: 2 | iteration: 1/262 | Loss: 0.6991468667984009Training Epoch: 2 | iteration: 2/262 | Loss: 0.7224582433700562Training Epoch: 2 | iteration: 3/262 | Loss: 0.6728800535202026Training Epoch: 2 | iteration: 4/262 | Loss: 0.7753042578697205Training Epoch: 2 | iteration: 5/262 | Loss: 0.7335866689682007Training Epoch: 2 | iteration: 6/262 | Loss: 0.715548038482666Training Epoch: 2 | iteration: 7/262 | Loss: 0.6939584016799927Training Epoch: 2 | iteration: 8/262 | Loss: 0.599319577217102Training Epoch: 2 | iteration: 9/262 | Loss: 0.7439072132110596Training Epoch: 2 | iteration: 10/262 | Loss: 0.7824468016624451Training Epoch: 2 | iteration: 11/262 | Loss: 0.7112671732902527Training Epoch: 2 | iteration: 12/262 | Loss: 0.713940441608429Training Epoch: 2 | iteration: 13/262 | Loss: 0.7215214371681213Training Epoch: 2 | iteration: 14/262 | Loss: 0.7581416368484497Training Epoch: 2 | iteration: 15/262 | Loss: 0.727925181388855Training Epoch: 2 | iteration: 16/262 | Loss: 0.7768617868423462Training Epoch: 2 | iteration: 17/262 | Loss: 0.7146782875061035Training Epoch: 2 | iteration: 18/262 | Loss: 0.7086549997329712Training Epoch: 2 | iteration: 19/262 | Loss: 0.7722667455673218Training Epoch: 2 | iteration: 20/262 | Loss: 0.7223790884017944Training Epoch: 2 | iteration: 21/262 | Loss: 0.679430365562439Training Epoch: 2 | iteration: 22/262 | Loss: 0.7484630346298218Training Epoch: 2 | iteration: 23/262 | Loss: 0.6885710954666138Training Epoch: 2 | iteration: 24/262 | Loss: 0.7030357122421265Training Epoch: 2 | iteration: 25/262 | Loss: 0.7302857637405396Training Epoch: 2 | iteration: 26/262 | Loss: 0.7258463501930237Training Epoch: 2 | iteration: 27/262 | Loss: 0.7059654593467712Training Epoch: 2 | iteration: 28/262 | Loss: 0.7070101499557495Training Epoch: 2 | iteration: 29/262 | Loss: 0.6288696527481079Training Epoch: 2 | iteration: 30/262 | Loss: 0.6982011795043945Training Epoch: 2 | iteration: 31/262 | Loss: 0.7132471799850464Training Epoch: 2 | iteration: 32/262 | Loss: 0.6982935070991516Training Epoch: 2 | iteration: 33/262 | Loss: 0.6961823105812073Training Epoch: 2 | iteration: 34/262 | Loss: 0.7314960360527039Training Epoch: 2 | iteration: 35/262 | Loss: 0.7406225204467773Training Epoch: 2 | iteration: 36/262 | Loss: 0.6108860969543457Training Epoch: 2 | iteration: 37/262 | Loss: 0.7245849370956421Training Epoch: 2 | iteration: 38/262 | Loss: 0.7557718753814697Training Epoch: 2 | iteration: 39/262 | Loss: 0.8001353144645691Training Epoch: 2 | iteration: 40/262 | Loss: 0.7299582958221436Training Epoch: 2 | iteration: 41/262 | Loss: 0.6916660070419312Training Epoch: 2 | iteration: 42/262 | Loss: 0.7801530957221985Training Epoch: 2 | iteration: 43/262 | Loss: 0.7322834730148315Training Epoch: 2 | iteration: 44/262 | Loss: 0.7101082801818848Training Epoch: 2 | iteration: 45/262 | Loss: 0.6925547122955322Training Epoch: 2 | iteration: 46/262 | Loss: 0.7651503086090088Training Epoch: 2 | iteration: 47/262 | Loss: 0.7818572521209717Training Epoch: 2 | iteration: 48/262 | Loss: 0.6645347476005554Training Epoch: 2 | iteration: 49/262 | Loss: 0.7132983207702637Training Epoch: 2 | iteration: 50/262 | Loss: 0.702506422996521Training Epoch: 2 | iteration: 51/262 | Loss: 0.6796287298202515Training Epoch: 2 | iteration: 52/262 | Loss: 0.7357430458068848Training Epoch: 2 | iteration: 53/262 | Loss: 0.7394342422485352Training Epoch: 2 | iteration: 54/262 | Loss: 0.7053221464157104Training Epoch: 2 | iteration: 55/262 | Loss: 0.7852888107299805Training Epoch: 2 | iteration: 56/262 | Loss: 0.7152259349822998Training Epoch: 2 | iteration: 57/262 | Loss: 0.6778258681297302Training Epoch: 2 | iteration: 58/262 | Loss: 0.8098583817481995Training Epoch: 2 | iteration: 59/262 | Loss: 0.7265543937683105Training Epoch: 2 | iteration: 60/262 | Loss: 0.7297829389572144Training Epoch: 2 | iteration: 61/262 | Loss: 0.7044508457183838Training Epoch: 2 | iteration: 62/262 | Loss: 0.7230806946754456Training Epoch: 2 | iteration: 63/262 | Loss: 0.7341198921203613Training Epoch: 2 | iteration: 64/262 | Loss: 0.8056941032409668Training Epoch: 2 | iteration: 65/262 | Loss: 0.8733036518096924Training Epoch: 2 | iteration: 66/262 | Loss: 0.7090122699737549Training Epoch: 2 | iteration: 67/262 | Loss: 0.6392460465431213Training Epoch: 2 | iteration: 68/262 | Loss: 0.6505446434020996Training Epoch: 2 | iteration: 69/262 | Loss: 0.6615301966667175Training Epoch: 2 | iteration: 70/262 | Loss: 0.8029304146766663Training Epoch: 2 | iteration: 71/262 | Loss: 0.7417957782745361Training Epoch: 2 | iteration: 72/262 | Loss: 0.7306460738182068Training Epoch: 2 | iteration: 73/262 | Loss: 0.7755458354949951Training Epoch: 2 | iteration: 74/262 | Loss: 0.8157845735549927Training Epoch: 2 | iteration: 75/262 | Loss: 0.7742613554000854Training Epoch: 2 | iteration: 76/262 | Loss: 0.7519302368164062Training Epoch: 2 | iteration: 77/262 | Loss: 0.6688624024391174Training Epoch: 2 | iteration: 78/262 | Loss: 0.7401446104049683Training Epoch: 2 | iteration: 79/262 | Loss: 0.7475377321243286Training Epoch: 2 | iteration: 80/262 | Loss: 0.7974940538406372Training Epoch: 2 | iteration: 81/262 | Loss: 0.6793693900108337Training Epoch: 2 | iteration: 82/262 | Loss: 0.6914805173873901Training Epoch: 2 | iteration: 83/262 | Loss: 0.6990299224853516Training Epoch: 2 | iteration: 84/262 | Loss: 0.607568621635437Training Epoch: 2 | iteration: 85/262 | Loss: 0.7121970653533936Training Epoch: 2 | iteration: 86/262 | Loss: 0.7166032791137695Training Epoch: 2 | iteration: 87/262 | Loss: 0.7098267674446106Training Epoch: 2 | iteration: 88/262 | Loss: 0.8360303044319153Training Epoch: 2 | iteration: 89/262 | Loss: 0.6338727474212646Training Epoch: 2 | iteration: 90/262 | Loss: 0.688167929649353Training Epoch: 2 | iteration: 91/262 | Loss: 0.7056145668029785Training Epoch: 2 | iteration: 92/262 | Loss: 0.6826172471046448Training Epoch: 2 | iteration: 93/262 | Loss: 0.751227617263794Training Epoch: 2 | iteration: 94/262 | Loss: 0.6935616731643677Training Epoch: 2 | iteration: 95/262 | Loss: 0.7009121179580688Training Epoch: 2 | iteration: 96/262 | Loss: 0.752062201499939Training Epoch: 2 | iteration: 97/262 | Loss: 0.7246017456054688Training Epoch: 2 | iteration: 98/262 | Loss: 0.7657156586647034Training Epoch: 2 | iteration: 99/262 | Loss: 0.755633533000946Training Epoch: 2 | iteration: 100/262 | Loss: 0.7367095947265625Training Epoch: 2 | iteration: 101/262 | Loss: 0.7047914862632751Training Epoch: 2 | iteration: 102/262 | Loss: 0.697322428226471Training Epoch: 2 | iteration: 103/262 | Loss: 0.7737616300582886Training Epoch: 2 | iteration: 104/262 | Loss: 0.7548514604568481Training Epoch: 2 | iteration: 105/262 | Loss: 0.6977897882461548Training Epoch: 2 | iteration: 106/262 | Loss: 0.8231899738311768Training Epoch: 2 | iteration: 107/262 | Loss: 0.8093572854995728Training Epoch: 2 | iteration: 108/262 | Loss: 0.7238739728927612Training Epoch: 2 | iteration: 109/262 | Loss: 0.757473349571228Training Epoch: 2 | iteration: 110/262 | Loss: 0.740066409111023Training Epoch: 2 | iteration: 111/262 | Loss: 0.7525215148925781Training Epoch: 2 | iteration: 112/262 | Loss: 0.7354699969291687Training Epoch: 2 | iteration: 113/262 | Loss: 0.7243561148643494Training Epoch: 2 | iteration: 114/262 | Loss: 0.6808648109436035Training Epoch: 2 | iteration: 115/262 | Loss: 0.7605383992195129Training Epoch: 2 | iteration: 116/262 | Loss: 0.7412515878677368Training Epoch: 2 | iteration: 117/262 | Loss: 0.639763593673706Training Epoch: 2 | iteration: 118/262 | Loss: 0.6056090593338013Training Epoch: 2 | iteration: 119/262 | Loss: 0.8241084218025208Training Epoch: 2 | iteration: 120/262 | Loss: 0.7482662200927734Training Epoch: 2 | iteration: 121/262 | Loss: 0.7167788743972778Training Epoch: 2 | iteration: 122/262 | Loss: 0.7082796692848206Training Epoch: 2 | iteration: 123/262 | Loss: 0.7155468463897705Training Epoch: 2 | iteration: 124/262 | Loss: 0.7266348004341125Training Epoch: 2 | iteration: 125/262 | Loss: 0.6719068288803101Training Epoch: 2 | iteration: 126/262 | Loss: 0.7783970832824707Training Epoch: 2 | iteration: 127/262 | Loss: 0.6781695485115051Training Epoch: 2 | iteration: 128/262 | Loss: 0.761181116104126Training Epoch: 2 | iteration: 129/262 | Loss: 0.6722149848937988Training Epoch: 2 | iteration: 130/262 | Loss: 0.8091795444488525Training Epoch: 2 | iteration: 131/262 | Loss: 0.752761960029602Training Epoch: 2 | iteration: 132/262 | Loss: 0.6747167110443115Training Epoch: 2 | iteration: 133/262 | Loss: 0.7441680431365967Training Epoch: 2 | iteration: 134/262 | Loss: 0.704484224319458Training Epoch: 2 | iteration: 135/262 | Loss: 0.775071382522583Training Epoch: 2 | iteration: 136/262 | Loss: 0.6896017789840698Training Epoch: 2 | iteration: 137/262 | Loss: 0.7337093949317932Training Epoch: 2 | iteration: 138/262 | Loss: 0.7357574701309204Training Epoch: 2 | iteration: 139/262 | Loss: 0.6815443634986877Training Epoch: 2 | iteration: 140/262 | Loss: 0.7775689363479614Training Epoch: 2 | iteration: 141/262 | Loss: 0.6638056039810181Training Epoch: 2 | iteration: 142/262 | Loss: 0.8149021863937378Training Epoch: 2 | iteration: 143/262 | Loss: 0.6852133274078369Training Epoch: 2 | iteration: 144/262 | Loss: 0.7098397016525269Training Epoch: 2 | iteration: 145/262 | Loss: 0.6917940378189087Training Epoch: 2 | iteration: 146/262 | Loss: 0.6726434230804443Training Epoch: 2 | iteration: 147/262 | Loss: 0.6771769523620605Training Epoch: 2 | iteration: 148/262 | Loss: 0.7465626001358032Training Epoch: 2 | iteration: 149/262 | Loss: 0.6902197003364563Training Epoch: 2 | iteration: 150/262 | Loss: 0.7183240652084351Training Epoch: 2 | iteration: 151/262 | Loss: 0.7109363675117493Training Epoch: 2 | iteration: 152/262 | Loss: 0.7852205038070679Training Epoch: 2 | iteration: 153/262 | Loss: 0.7211157083511353Training Epoch: 2 | iteration: 154/262 | Loss: 0.6783202290534973Training Epoch: 2 | iteration: 155/262 | Loss: 0.7369723916053772Training Epoch: 2 | iteration: 156/262 | Loss: 0.7257422804832458Training Epoch: 2 | iteration: 157/262 | Loss: 0.7947427034378052Training Epoch: 2 | iteration: 158/262 | Loss: 0.7005635499954224Training Epoch: 2 | iteration: 159/262 | Loss: 0.6517312526702881Training Epoch: 2 | iteration: 160/262 | Loss: 0.7213152647018433Training Epoch: 2 | iteration: 161/262 | Loss: 0.7046364545822144Training Epoch: 2 | iteration: 162/262 | Loss: 0.7943165302276611Training Epoch: 2 | iteration: 163/262 | Loss: 0.7064772844314575Training Epoch: 2 | iteration: 164/262 | Loss: 0.718986451625824Training Epoch: 2 | iteration: 165/262 | Loss: 0.8098044395446777Training Epoch: 2 | iteration: 166/262 | Loss: 0.6627591848373413Training Epoch: 2 | iteration: 167/262 | Loss: 0.6396822333335876Training Epoch: 2 | iteration: 168/262 | Loss: 0.6996811628341675Training Epoch: 2 | iteration: 169/262 | Loss: 0.6986080408096313Training Epoch: 2 | iteration: 170/262 | Loss: 0.7084985971450806Training Epoch: 2 | iteration: 171/262 | Loss: 0.7135031819343567Training Epoch: 2 | iteration: 172/262 | Loss: 0.8008732795715332Training Epoch: 2 | iteration: 173/262 | Loss: 0.7230250835418701Training Epoch: 2 | iteration: 174/262 | Loss: 0.6764435768127441Training Epoch: 2 | iteration: 175/262 | Loss: 0.7315855026245117Training Epoch: 2 | iteration: 176/262 | Loss: 0.7349640130996704Training Epoch: 2 | iteration: 177/262 | Loss: 0.6276171207427979Training Epoch: 2 | iteration: 178/262 | Loss: 0.7754603624343872Training Epoch: 2 | iteration: 179/262 | Loss: 0.7741298675537109Training Epoch: 2 | iteration: 180/262 | Loss: 0.6910837888717651Training Epoch: 2 | iteration: 181/262 | Loss: 0.6918193101882935Training Epoch: 2 | iteration: 182/262 | Loss: 0.6433570384979248Training Epoch: 2 | iteration: 183/262 | Loss: 0.7444878220558167Training Epoch: 2 | iteration: 184/262 | Loss: 0.6427421569824219Training Epoch: 2 | iteration: 185/262 | Loss: 0.7490605711936951Training Epoch: 2 | iteration: 186/262 | Loss: 0.7564188241958618Training Epoch: 2 | iteration: 187/262 | Loss: 0.753132700920105Training Epoch: 2 | iteration: 188/262 | Loss: 0.5926035642623901Training Epoch: 2 | iteration: 189/262 | Loss: 0.5941399335861206Training Epoch: 2 | iteration: 190/262 | Loss: 0.627284586429596Training Epoch: 2 | iteration: 191/262 | Loss: 0.6884419322013855Training Epoch: 2 | iteration: 192/262 | Loss: 0.7343417406082153Training Epoch: 2 | iteration: 193/262 | Loss: 0.7535836100578308Training Epoch: 2 | iteration: 194/262 | Loss: 0.7097221612930298Training Epoch: 2 | iteration: 195/262 | Loss: 0.7522753477096558Training Epoch: 2 | iteration: 196/262 | Loss: 0.6983712911605835Training Epoch: 2 | iteration: 197/262 | Loss: 0.7891484498977661Training Epoch: 2 | iteration: 198/262 | Loss: 0.7392858266830444Training Epoch: 2 | iteration: 199/262 | Loss: 0.6647834777832031Training Epoch: 2 | iteration: 200/262 | Loss: 0.6680284738540649Training Epoch: 2 | iteration: 201/262 | Loss: 0.7837473750114441Training Epoch: 2 | iteration: 202/262 | Loss: 0.7675336599349976Training Epoch: 2 | iteration: 203/262 | Loss: 0.7330929040908813Training Epoch: 2 | iteration: 204/262 | Loss: 0.6961150169372559Training Epoch: 2 | iteration: 205/262 | Loss: 0.6953719258308411Training Epoch: 2 | iteration: 206/262 | Loss: 0.744081974029541Training Epoch: 2 | iteration: 207/262 | Loss: 0.7161788940429688Training Epoch: 2 | iteration: 208/262 | Loss: 0.7513687610626221Training Epoch: 2 | iteration: 209/262 | Loss: 0.7391144633293152Training Epoch: 2 | iteration: 210/262 | Loss: 0.8063212037086487Training Epoch: 2 | iteration: 211/262 | Loss: 0.8243914842605591Training Epoch: 2 | iteration: 212/262 | Loss: 0.6306270956993103Training Epoch: 2 | iteration: 213/262 | Loss: 0.7228312492370605Training Epoch: 2 | iteration: 214/262 | Loss: 0.6717589497566223Training Epoch: 2 | iteration: 215/262 | Loss: 0.6872759461402893Training Epoch: 2 | iteration: 216/262 | Loss: 0.6494044065475464Training Epoch: 2 | iteration: 217/262 | Loss: 0.7453640699386597Training Epoch: 2 | iteration: 218/262 | Loss: 0.7025662660598755Training Epoch: 2 | iteration: 219/262 | Loss: 0.6914708614349365Training Epoch: 2 | iteration: 220/262 | Loss: 0.8178752660751343Training Epoch: 2 | iteration: 221/262 | Loss: 0.7511513829231262Training Epoch: 2 | iteration: 222/262 | Loss: 0.7058248519897461Training Epoch: 2 | iteration: 223/262 | Loss: 0.6212839484214783Training Epoch: 2 | iteration: 224/262 | Loss: 0.7263398170471191Training Epoch: 2 | iteration: 225/262 | Loss: 0.6889052987098694Training Epoch: 2 | iteration: 226/262 | Loss: 0.7071001529693604Training Epoch: 2 | iteration: 227/262 | Loss: 0.7020205855369568Training Epoch: 2 | iteration: 228/262 | Loss: 0.6507527828216553Training Epoch: 2 | iteration: 229/262 | Loss: 0.6488127112388611Training Epoch: 2 | iteration: 230/262 | Loss: 0.6758674383163452Training Epoch: 2 | iteration: 231/262 | Loss: 0.6965210437774658Training Epoch: 2 | iteration: 232/262 | Loss: 0.7161110639572144Training Epoch: 2 | iteration: 233/262 | Loss: 0.664474368095398Training Epoch: 2 | iteration: 234/262 | Loss: 0.6757960319519043Training Epoch: 2 | iteration: 235/262 | Loss: 0.7178689241409302Training Epoch: 2 | iteration: 236/262 | Loss: 0.7213906645774841Training Epoch: 2 | iteration: 237/262 | Loss: 0.7202922701835632Training Epoch: 2 | iteration: 238/262 | Loss: 0.7646472454071045Training Epoch: 2 | iteration: 239/262 | Loss: 0.7353993654251099Training Epoch: 2 | iteration: 240/262 | Loss: 0.6846038103103638Training Epoch: 2 | iteration: 241/262 | Loss: 0.7223206758499146Training Epoch: 2 | iteration: 242/262 | Loss: 0.7662328481674194Training Epoch: 2 | iteration: 243/262 | Loss: 0.7634481191635132Training Epoch: 2 | iteration: 244/262 | Loss: 0.7347829341888428Training Epoch: 2 | iteration: 245/262 | Loss: 0.7506500482559204Training Epoch: 2 | iteration: 246/262 | Loss: 0.7261015176773071Training Epoch: 2 | iteration: 247/262 | Loss: 0.7647373080253601Training Epoch: 2 | iteration: 248/262 | Loss: 0.8201390504837036Training Epoch: 2 | iteration: 249/262 | Loss: 0.7474927306175232Training Epoch: 2 | iteration: 250/262 | Loss: 0.8058640956878662Training Epoch: 2 | iteration: 251/262 | Loss: 0.7965719699859619Training Epoch: 2 | iteration: 252/262 | Loss: 0.7392065525054932Training Epoch: 2 | iteration: 253/262 | Loss: 0.7123960852622986Training Epoch: 2 | iteration: 254/262 | Loss: 0.6925541758537292Training Epoch: 2 | iteration: 255/262 | Loss: 0.6483522653579712Training Epoch: 2 | iteration: 256/262 | Loss: 0.7810101509094238Training Epoch: 2 | iteration: 257/262 | Loss: 0.6602442860603333Training Epoch: 2 | iteration: 258/262 | Loss: 0.7333314418792725Training Epoch: 2 | iteration: 259/262 | Loss: 0.6937713623046875Training Epoch: 2 | iteration: 260/262 | Loss: 0.7028448581695557Training Epoch: 2 | iteration: 261/262 | Loss: 0.7317126393318176Validating Epoch: 2 | iteration: 0/66 | Loss: 0.7468146085739136Validating Epoch: 2 | iteration: 1/66 | Loss: 0.5836086273193359Validating Epoch: 2 | iteration: 2/66 | Loss: 0.6530461311340332Validating Epoch: 2 | iteration: 3/66 | Loss: 0.6439450979232788Validating Epoch: 2 | iteration: 4/66 | Loss: 0.6679928302764893Validating Epoch: 2 | iteration: 5/66 | Loss: 0.656902015209198Validating Epoch: 2 | iteration: 6/66 | Loss: 0.6951096653938293Validating Epoch: 2 | iteration: 7/66 | Loss: 0.6337504982948303Validating Epoch: 2 | iteration: 8/66 | Loss: 0.6915151476860046Validating Epoch: 2 | iteration: 9/66 | Loss: 0.6094167232513428Validating Epoch: 2 | iteration: 10/66 | Loss: 0.6469358801841736Validating Epoch: 2 | iteration: 11/66 | Loss: 0.6530654430389404Validating Epoch: 2 | iteration: 12/66 | Loss: 0.6420397758483887Validating Epoch: 2 | iteration: 13/66 | Loss: 0.6400232315063477Validating Epoch: 2 | iteration: 14/66 | Loss: 0.5815751552581787Validating Epoch: 2 | iteration: 15/66 | Loss: 0.6132203340530396Validating Epoch: 2 | iteration: 16/66 | Loss: 0.6473453044891357Validating Epoch: 2 | iteration: 17/66 | Loss: 0.6181318163871765Validating Epoch: 2 | iteration: 18/66 | Loss: 0.6597614288330078Validating Epoch: 2 | iteration: 19/66 | Loss: 0.6836663484573364Validating Epoch: 2 | iteration: 20/66 | Loss: 0.6849303245544434Validating Epoch: 2 | iteration: 21/66 | Loss: 0.6082684993743896Validating Epoch: 2 | iteration: 22/66 | Loss: 0.6041908860206604Validating Epoch: 2 | iteration: 23/66 | Loss: 0.6982690691947937Validating Epoch: 2 | iteration: 24/66 | Loss: 0.6505026817321777Validating Epoch: 2 | iteration: 25/66 | Loss: 0.6836823225021362Validating Epoch: 2 | iteration: 26/66 | Loss: 0.6210848689079285Validating Epoch: 2 | iteration: 27/66 | Loss: 0.6787711977958679Validating Epoch: 2 | iteration: 28/66 | Loss: 0.5949138402938843Validating Epoch: 2 | iteration: 29/66 | Loss: 0.8072313666343689Validating Epoch: 2 | iteration: 30/66 | Loss: 0.6537528038024902Validating Epoch: 2 | iteration: 31/66 | Loss: 0.6656696200370789Validating Epoch: 2 | iteration: 32/66 | Loss: 0.6572660207748413Validating Epoch: 2 | iteration: 33/66 | Loss: 0.659055233001709Validating Epoch: 2 | iteration: 34/66 | Loss: 0.647255003452301Validating Epoch: 2 | iteration: 35/66 | Loss: 0.5815905332565308Validating Epoch: 2 | iteration: 36/66 | Loss: 0.5839258432388306Validating Epoch: 2 | iteration: 37/66 | Loss: 0.6355710029602051Validating Epoch: 2 | iteration: 38/66 | Loss: 0.6705372929573059Validating Epoch: 2 | iteration: 39/66 | Loss: 0.589598536491394Validating Epoch: 2 | iteration: 40/66 | Loss: 0.6938485503196716Validating Epoch: 2 | iteration: 41/66 | Loss: 0.6346657872200012Validating Epoch: 2 | iteration: 42/66 | Loss: 0.6298667192459106Validating Epoch: 2 | iteration: 43/66 | Loss: 0.6712526082992554Validating Epoch: 2 | iteration: 44/66 | Loss: 0.6421404480934143Validating Epoch: 2 | iteration: 45/66 | Loss: 0.6010447144508362Validating Epoch: 2 | iteration: 46/66 | Loss: 0.6555960178375244Validating Epoch: 2 | iteration: 47/66 | Loss: 0.6716411113739014Validating Epoch: 2 | iteration: 48/66 | Loss: 0.5880448818206787Validating Epoch: 2 | iteration: 49/66 | Loss: 0.6642986536026001Validating Epoch: 2 | iteration: 50/66 | Loss: 0.6081708669662476Validating Epoch: 2 | iteration: 51/66 | Loss: 0.6972837448120117Validating Epoch: 2 | iteration: 52/66 | Loss: 0.698249101638794Validating Epoch: 2 | iteration: 53/66 | Loss: 0.6352423429489136Validating Epoch: 2 | iteration: 54/66 | Loss: 0.6833163499832153Validating Epoch: 2 | iteration: 55/66 | Loss: 0.6662843823432922Validating Epoch: 2 | iteration: 56/66 | Loss: 0.6009105443954468Validating Epoch: 2 | iteration: 57/66 | Loss: 0.7161259055137634Validating Epoch: 2 | iteration: 58/66 | Loss: 0.6154785752296448Validating Epoch: 2 | iteration: 59/66 | Loss: 0.6638721227645874Validating Epoch: 2 | iteration: 60/66 | Loss: 0.6841082572937012Validating Epoch: 2 | iteration: 61/66 | Loss: 0.6566995978355408Validating Epoch: 2 | iteration: 62/66 | Loss: 0.65559321641922Validating Epoch: 2 | iteration: 63/66 | Loss: 0.5923708081245422Validating Epoch: 2 | iteration: 64/66 | Loss: 0.6899797916412354Validating Epoch: 2 | iteration: 65/66 | Loss: 0.6515376567840576Temperature:  0.5
Sampling: iteration: 0/51{'Validity': 0.98828125, 'Novelty': 1.0, 'Uniqueness': 0.9930830039525692}
Training Epoch: 3 | iteration: 0/262 | Loss: 0.6061609983444214Training Epoch: 3 | iteration: 1/262 | Loss: 0.6884616613388062Training Epoch: 3 | iteration: 2/262 | Loss: 0.5977839231491089Training Epoch: 3 | iteration: 3/262 | Loss: 0.6757563352584839Training Epoch: 3 | iteration: 4/262 | Loss: 0.6721782684326172Training Epoch: 3 | iteration: 5/262 | Loss: 0.7639531493186951Training Epoch: 3 | iteration: 6/262 | Loss: 0.8026942014694214Training Epoch: 3 | iteration: 7/262 | Loss: 0.7116807699203491Training Epoch: 3 | iteration: 8/262 | Loss: 0.7983351945877075Training Epoch: 3 | iteration: 9/262 | Loss: 0.670754075050354Training Epoch: 3 | iteration: 10/262 | Loss: 0.7335319519042969Training Epoch: 3 | iteration: 11/262 | Loss: 0.6867852807044983Training Epoch: 3 | iteration: 12/262 | Loss: 0.7504211068153381Training Epoch: 3 | iteration: 13/262 | Loss: 0.7900538444519043Training Epoch: 3 | iteration: 14/262 | Loss: 0.6692999005317688Training Epoch: 3 | iteration: 15/262 | Loss: 0.7011594772338867Training Epoch: 3 | iteration: 16/262 | Loss: 0.6658569574356079Training Epoch: 3 | iteration: 17/262 | Loss: 0.6959289908409119Training Epoch: 3 | iteration: 18/262 | Loss: 0.6458276510238647Training Epoch: 3 | iteration: 19/262 | Loss: 0.6587526798248291Training Epoch: 3 | iteration: 20/262 | Loss: 0.7245162129402161Training Epoch: 3 | iteration: 21/262 | Loss: 0.6627530455589294Training Epoch: 3 | iteration: 22/262 | Loss: 0.6776148080825806Training Epoch: 3 | iteration: 23/262 | Loss: 0.7599166631698608Training Epoch: 3 | iteration: 24/262 | Loss: 0.6183698177337646Training Epoch: 3 | iteration: 25/262 | Loss: 0.671204149723053Training Epoch: 3 | iteration: 26/262 | Loss: 0.7018673419952393Training Epoch: 3 | iteration: 27/262 | Loss: 0.7275493144989014Training Epoch: 3 | iteration: 28/262 | Loss: 0.6777195334434509Training Epoch: 3 | iteration: 29/262 | Loss: 0.7187566161155701Training Epoch: 3 | iteration: 30/262 | Loss: 0.712232232093811Training Epoch: 3 | iteration: 31/262 | Loss: 0.6882458925247192Training Epoch: 3 | iteration: 32/262 | Loss: 0.6407434344291687Training Epoch: 3 | iteration: 33/262 | Loss: 0.6638088822364807Training Epoch: 3 | iteration: 34/262 | Loss: 0.7230179309844971Training Epoch: 3 | iteration: 35/262 | Loss: 0.6382319331169128Training Epoch: 3 | iteration: 36/262 | Loss: 0.6330198049545288Training Epoch: 3 | iteration: 37/262 | Loss: 0.6885159015655518Training Epoch: 3 | iteration: 38/262 | Loss: 0.646946132183075Training Epoch: 3 | iteration: 39/262 | Loss: 0.7326412796974182Training Epoch: 3 | iteration: 40/262 | Loss: 0.6972355842590332Training Epoch: 3 | iteration: 41/262 | Loss: 0.7567803859710693Training Epoch: 3 | iteration: 42/262 | Loss: 0.6881614923477173Training Epoch: 3 | iteration: 43/262 | Loss: 0.6367820501327515Training Epoch: 3 | iteration: 44/262 | Loss: 0.731489896774292Training Epoch: 3 | iteration: 45/262 | Loss: 0.6216311454772949Training Epoch: 3 | iteration: 46/262 | Loss: 0.7308045029640198Training Epoch: 3 | iteration: 47/262 | Loss: 0.6726242899894714Training Epoch: 3 | iteration: 48/262 | Loss: 0.6945092678070068Training Epoch: 3 | iteration: 49/262 | Loss: 0.6994891166687012Training Epoch: 3 | iteration: 50/262 | Loss: 0.6191589832305908Training Epoch: 3 | iteration: 51/262 | Loss: 0.6643915772438049Training Epoch: 3 | iteration: 52/262 | Loss: 0.6499603986740112Training Epoch: 3 | iteration: 53/262 | Loss: 0.6589674949645996Training Epoch: 3 | iteration: 54/262 | Loss: 0.6691204309463501Training Epoch: 3 | iteration: 55/262 | Loss: 0.6518018841743469Training Epoch: 3 | iteration: 56/262 | Loss: 0.6865070462226868Training Epoch: 3 | iteration: 57/262 | Loss: 0.679413914680481Training Epoch: 3 | iteration: 58/262 | Loss: 0.6637346744537354Training Epoch: 3 | iteration: 59/262 | Loss: 0.6795787811279297Training Epoch: 3 | iteration: 60/262 | Loss: 0.6826191544532776Training Epoch: 3 | iteration: 61/262 | Loss: 0.5781679749488831Training Epoch: 3 | iteration: 62/262 | Loss: 0.7203349471092224Training Epoch: 3 | iteration: 63/262 | Loss: 0.6857496500015259Training Epoch: 3 | iteration: 64/262 | Loss: 0.7193537950515747Training Epoch: 3 | iteration: 65/262 | Loss: 0.7929250597953796Training Epoch: 3 | iteration: 66/262 | Loss: 0.6859986186027527Training Epoch: 3 | iteration: 67/262 | Loss: 0.6145600080490112Training Epoch: 3 | iteration: 68/262 | Loss: 0.8325769305229187Training Epoch: 3 | iteration: 69/262 | Loss: 0.7166556119918823Training Epoch: 3 | iteration: 70/262 | Loss: 0.6875923871994019Training Epoch: 3 | iteration: 71/262 | Loss: 0.7137846946716309Training Epoch: 3 | iteration: 72/262 | Loss: 0.6849305629730225Training Epoch: 3 | iteration: 73/262 | Loss: 0.6827903985977173Training Epoch: 3 | iteration: 74/262 | Loss: 0.6846686005592346Training Epoch: 3 | iteration: 75/262 | Loss: 0.7480596303939819Training Epoch: 3 | iteration: 76/262 | Loss: 0.7308851480484009Training Epoch: 3 | iteration: 77/262 | Loss: 0.6382567882537842Training Epoch: 3 | iteration: 78/262 | Loss: 0.6730827689170837Training Epoch: 3 | iteration: 79/262 | Loss: 0.7231338024139404Training Epoch: 3 | iteration: 80/262 | Loss: 0.6133802533149719Training Epoch: 3 | iteration: 81/262 | Loss: 0.6735879182815552Training Epoch: 3 | iteration: 82/262 | Loss: 0.6443605422973633Training Epoch: 3 | iteration: 83/262 | Loss: 0.6479114294052124Training Epoch: 3 | iteration: 84/262 | Loss: 0.6977461576461792Training Epoch: 3 | iteration: 85/262 | Loss: 0.7023446559906006Training Epoch: 3 | iteration: 86/262 | Loss: 0.7058624625205994Training Epoch: 3 | iteration: 87/262 | Loss: 0.6500743627548218Training Epoch: 3 | iteration: 88/262 | Loss: 0.7176489233970642Training Epoch: 3 | iteration: 89/262 | Loss: 0.7324008941650391Training Epoch: 3 | iteration: 90/262 | Loss: 0.7237597703933716Training Epoch: 3 | iteration: 91/262 | Loss: 0.6560602188110352Training Epoch: 3 | iteration: 92/262 | Loss: 0.6515930891036987Training Epoch: 3 | iteration: 93/262 | Loss: 0.6336727142333984Training Epoch: 3 | iteration: 94/262 | Loss: 0.7098643779754639Training Epoch: 3 | iteration: 95/262 | Loss: 0.63595050573349Training Epoch: 3 | iteration: 96/262 | Loss: 0.6842397451400757Training Epoch: 3 | iteration: 97/262 | Loss: 0.6954087018966675Training Epoch: 3 | iteration: 98/262 | Loss: 0.6541978120803833Training Epoch: 3 | iteration: 99/262 | Loss: 0.693429172039032Training Epoch: 3 | iteration: 100/262 | Loss: 0.6753358244895935Training Epoch: 3 | iteration: 101/262 | Loss: 0.7056868076324463Training Epoch: 3 | iteration: 102/262 | Loss: 0.7991615533828735Training Epoch: 3 | iteration: 103/262 | Loss: 0.7144141793251038Training Epoch: 3 | iteration: 104/262 | Loss: 0.7960836887359619Training Epoch: 3 | iteration: 105/262 | Loss: 0.726287841796875Training Epoch: 3 | iteration: 106/262 | Loss: 0.6725512146949768Training Epoch: 3 | iteration: 107/262 | Loss: 0.624087929725647Training Epoch: 3 | iteration: 108/262 | Loss: 0.771511435508728Training Epoch: 3 | iteration: 109/262 | Loss: 0.7032683491706848Training Epoch: 3 | iteration: 110/262 | Loss: 0.6647723317146301Training Epoch: 3 | iteration: 111/262 | Loss: 0.690015435218811Training Epoch: 3 | iteration: 112/262 | Loss: 0.6438337564468384Training Epoch: 3 | iteration: 113/262 | Loss: 0.6299628615379333Training Epoch: 3 | iteration: 114/262 | Loss: 0.7266430854797363Training Epoch: 3 | iteration: 115/262 | Loss: 0.7558043599128723Training Epoch: 3 | iteration: 116/262 | Loss: 0.6704577207565308Training Epoch: 3 | iteration: 117/262 | Loss: 0.6825326681137085Training Epoch: 3 | iteration: 118/262 | Loss: 0.6437280178070068Training Epoch: 3 | iteration: 119/262 | Loss: 0.766640305519104Training Epoch: 3 | iteration: 120/262 | Loss: 0.6611695289611816Training Epoch: 3 | iteration: 121/262 | Loss: 0.7279497981071472Training Epoch: 3 | iteration: 122/262 | Loss: 0.7549153566360474Training Epoch: 3 | iteration: 123/262 | Loss: 0.6541236639022827Training Epoch: 3 | iteration: 124/262 | Loss: 0.6417956352233887Training Epoch: 3 | iteration: 125/262 | Loss: 0.5886597633361816Training Epoch: 3 | iteration: 126/262 | Loss: 0.6616833806037903Training Epoch: 3 | iteration: 127/262 | Loss: 0.6794508099555969Training Epoch: 3 | iteration: 128/262 | Loss: 0.6741216778755188Training Epoch: 3 | iteration: 129/262 | Loss: 0.6584691405296326Training Epoch: 3 | iteration: 130/262 | Loss: 0.6318103671073914Training Epoch: 3 | iteration: 131/262 | Loss: 0.775757372379303Training Epoch: 3 | iteration: 132/262 | Loss: 0.7295001149177551Training Epoch: 3 | iteration: 133/262 | Loss: 0.6573424339294434Training Epoch: 3 | iteration: 134/262 | Loss: 0.6484239101409912Training Epoch: 3 | iteration: 135/262 | Loss: 0.6564933061599731Training Epoch: 3 | iteration: 136/262 | Loss: 0.6113409996032715Training Epoch: 3 | iteration: 137/262 | Loss: 0.6516152620315552Training Epoch: 3 | iteration: 138/262 | Loss: 0.6862012147903442Training Epoch: 3 | iteration: 139/262 | Loss: 0.6680712699890137Training Epoch: 3 | iteration: 140/262 | Loss: 0.6834933757781982Training Epoch: 3 | iteration: 141/262 | Loss: 0.6749703884124756Training Epoch: 3 | iteration: 142/262 | Loss: 0.6638041734695435Training Epoch: 3 | iteration: 143/262 | Loss: 0.6395077109336853Training Epoch: 3 | iteration: 144/262 | Loss: 0.663432776927948Training Epoch: 3 | iteration: 145/262 | Loss: 0.6458966135978699Training Epoch: 3 | iteration: 146/262 | Loss: 0.7719573974609375Training Epoch: 3 | iteration: 147/262 | Loss: 0.5537443161010742Training Epoch: 3 | iteration: 148/262 | Loss: 0.6432279348373413Training Epoch: 3 | iteration: 149/262 | Loss: 0.6754952073097229Training Epoch: 3 | iteration: 150/262 | Loss: 0.6837623119354248Training Epoch: 3 | iteration: 151/262 | Loss: 0.6045207977294922Training Epoch: 3 | iteration: 152/262 | Loss: 0.7479443550109863Training Epoch: 3 | iteration: 153/262 | Loss: 0.645860493183136Training Epoch: 3 | iteration: 154/262 | Loss: 0.8234113454818726Training Epoch: 3 | iteration: 155/262 | Loss: 0.692063570022583Training Epoch: 3 | iteration: 156/262 | Loss: 0.7056101560592651Training Epoch: 3 | iteration: 157/262 | Loss: 0.6426421403884888Training Epoch: 3 | iteration: 158/262 | Loss: 0.7523626089096069Training Epoch: 3 | iteration: 159/262 | Loss: 0.7541375160217285Training Epoch: 3 | iteration: 160/262 | Loss: 0.6652307510375977Training Epoch: 3 | iteration: 161/262 | Loss: 0.68427574634552Training Epoch: 3 | iteration: 162/262 | Loss: 0.7008579969406128Training Epoch: 3 | iteration: 163/262 | Loss: 0.6778508424758911Training Epoch: 3 | iteration: 164/262 | Loss: 0.6659455299377441Training Epoch: 3 | iteration: 165/262 | Loss: 0.6434190273284912Training Epoch: 3 | iteration: 166/262 | Loss: 0.6821197271347046Training Epoch: 3 | iteration: 167/262 | Loss: 0.6962039470672607Training Epoch: 3 | iteration: 168/262 | Loss: 0.605772852897644Training Epoch: 3 | iteration: 169/262 | Loss: 0.6358283758163452Training Epoch: 3 | iteration: 170/262 | Loss: 0.6111788153648376Training Epoch: 3 | iteration: 171/262 | Loss: 0.750961184501648Training Epoch: 3 | iteration: 172/262 | Loss: 0.6837313175201416Training Epoch: 3 | iteration: 173/262 | Loss: 0.6809911131858826Training Epoch: 3 | iteration: 174/262 | Loss: 0.7287204265594482Training Epoch: 3 | iteration: 175/262 | Loss: 0.7267694473266602Training Epoch: 3 | iteration: 176/262 | Loss: 0.6672214269638062Training Epoch: 3 | iteration: 177/262 | Loss: 0.7084035873413086Training Epoch: 3 | iteration: 178/262 | Loss: 0.6809980869293213Training Epoch: 3 | iteration: 179/262 | Loss: 0.6725998520851135Training Epoch: 3 | iteration: 180/262 | Loss: 0.7022079825401306Training Epoch: 3 | iteration: 181/262 | Loss: 0.6936359405517578Training Epoch: 3 | iteration: 182/262 | Loss: 0.7057558298110962Training Epoch: 3 | iteration: 183/262 | Loss: 0.6742098331451416Training Epoch: 3 | iteration: 184/262 | Loss: 0.753654956817627Training Epoch: 3 | iteration: 185/262 | Loss: 0.7649500370025635Training Epoch: 3 | iteration: 186/262 | Loss: 0.6983569264411926Training Epoch: 3 | iteration: 187/262 | Loss: 0.6527490615844727Training Epoch: 3 | iteration: 188/262 | Loss: 0.7280645966529846Training Epoch: 3 | iteration: 189/262 | Loss: 0.6941088438034058Training Epoch: 3 | iteration: 190/262 | Loss: 0.636817216873169Training Epoch: 3 | iteration: 191/262 | Loss: 0.6739976406097412Training Epoch: 3 | iteration: 192/262 | Loss: 0.691636860370636Training Epoch: 3 | iteration: 193/262 | Loss: 0.7188695073127747Training Epoch: 3 | iteration: 194/262 | Loss: 0.6783738136291504Training Epoch: 3 | iteration: 195/262 | Loss: 0.6893049478530884Training Epoch: 3 | iteration: 196/262 | Loss: 0.7104458212852478Training Epoch: 3 | iteration: 197/262 | Loss: 0.6850084066390991Training Epoch: 3 | iteration: 198/262 | Loss: 0.6644040942192078Training Epoch: 3 | iteration: 199/262 | Loss: 0.7546645402908325Training Epoch: 3 | iteration: 200/262 | Loss: 0.7555334568023682Training Epoch: 3 | iteration: 201/262 | Loss: 0.6860537528991699Training Epoch: 3 | iteration: 202/262 | Loss: 0.6759047508239746Training Epoch: 3 | iteration: 203/262 | Loss: 0.6057488918304443Training Epoch: 3 | iteration: 204/262 | Loss: 0.7424262762069702Training Epoch: 3 | iteration: 205/262 | Loss: 0.6539080142974854Training Epoch: 3 | iteration: 206/262 | Loss: 0.7404831647872925Training Epoch: 3 | iteration: 207/262 | Loss: 0.635676383972168Training Epoch: 3 | iteration: 208/262 | Loss: 0.6305526494979858Training Epoch: 3 | iteration: 209/262 | Loss: 0.6773049831390381Training Epoch: 3 | iteration: 210/262 | Loss: 0.6950024366378784Training Epoch: 3 | iteration: 211/262 | Loss: 0.7018256187438965Training Epoch: 3 | iteration: 212/262 | Loss: 0.7437058687210083Training Epoch: 3 | iteration: 213/262 | Loss: 0.728161633014679Training Epoch: 3 | iteration: 214/262 | Loss: 0.6927963495254517Training Epoch: 3 | iteration: 215/262 | Loss: 0.7312981486320496Training Epoch: 3 | iteration: 216/262 | Loss: 0.6585423946380615Training Epoch: 3 | iteration: 217/262 | Loss: 0.7126376628875732Training Epoch: 3 | iteration: 218/262 | Loss: 0.6532115936279297Training Epoch: 3 | iteration: 219/262 | Loss: 0.6480750441551208Training Epoch: 3 | iteration: 220/262 | Loss: 0.7140070199966431Training Epoch: 3 | iteration: 221/262 | Loss: 0.7291896939277649Training Epoch: 3 | iteration: 222/262 | Loss: 0.6580895185470581Training Epoch: 3 | iteration: 223/262 | Loss: 0.7098462581634521Training Epoch: 3 | iteration: 224/262 | Loss: 0.6353929042816162Training Epoch: 3 | iteration: 225/262 | Loss: 0.6615254282951355Training Epoch: 3 | iteration: 226/262 | Loss: 0.7142737507820129Training Epoch: 3 | iteration: 227/262 | Loss: 0.6536582708358765Training Epoch: 3 | iteration: 228/262 | Loss: 0.7001128792762756Training Epoch: 3 | iteration: 229/262 | Loss: 0.6522811651229858Training Epoch: 3 | iteration: 230/262 | Loss: 0.7719386219978333Training Epoch: 3 | iteration: 231/262 | Loss: 0.6123940944671631Training Epoch: 3 | iteration: 232/262 | Loss: 0.7538816928863525Training Epoch: 3 | iteration: 233/262 | Loss: 0.6717640161514282Training Epoch: 3 | iteration: 234/262 | Loss: 0.6527280807495117Training Epoch: 3 | iteration: 235/262 | Loss: 0.705344557762146Training Epoch: 3 | iteration: 236/262 | Loss: 0.6412497162818909Training Epoch: 3 | iteration: 237/262 | Loss: 0.6483982801437378Training Epoch: 3 | iteration: 238/262 | Loss: 0.6557376384735107Training Epoch: 3 | iteration: 239/262 | Loss: 0.6893633604049683Training Epoch: 3 | iteration: 240/262 | Loss: 0.7431572079658508Training Epoch: 3 | iteration: 241/262 | Loss: 0.7420044541358948Training Epoch: 3 | iteration: 242/262 | Loss: 0.6448948383331299Training Epoch: 3 | iteration: 243/262 | Loss: 0.690238893032074Training Epoch: 3 | iteration: 244/262 | Loss: 0.7145962119102478Training Epoch: 3 | iteration: 245/262 | Loss: 0.6920868158340454Training Epoch: 3 | iteration: 246/262 | Loss: 0.7622426748275757Training Epoch: 3 | iteration: 247/262 | Loss: 0.6934013366699219Training Epoch: 3 | iteration: 248/262 | Loss: 0.6490312814712524Training Epoch: 3 | iteration: 249/262 | Loss: 0.7421350479125977Training Epoch: 3 | iteration: 250/262 | Loss: 0.7573603391647339Training Epoch: 3 | iteration: 251/262 | Loss: 0.7411832213401794Training Epoch: 3 | iteration: 252/262 | Loss: 0.731635570526123Training Epoch: 3 | iteration: 253/262 | Loss: 0.6593314409255981Training Epoch: 3 | iteration: 254/262 | Loss: 0.695778489112854Training Epoch: 3 | iteration: 255/262 | Loss: 0.7261558771133423Training Epoch: 3 | iteration: 256/262 | Loss: 0.8648492693901062Training Epoch: 3 | iteration: 257/262 | Loss: 0.684288740158081Training Epoch: 3 | iteration: 258/262 | Loss: 0.7202787399291992Training Epoch: 3 | iteration: 259/262 | Loss: 0.6672881841659546Training Epoch: 3 | iteration: 260/262 | Loss: 0.6689184904098511Training Epoch: 3 | iteration: 261/262 | Loss: 0.5722578763961792Validating Epoch: 3 | iteration: 0/66 | Loss: 0.5945377349853516Validating Epoch: 3 | iteration: 1/66 | Loss: 0.6299026012420654Validating Epoch: 3 | iteration: 2/66 | Loss: 0.712925910949707Validating Epoch: 3 | iteration: 3/66 | Loss: 0.619869589805603Validating Epoch: 3 | iteration: 4/66 | Loss: 0.6195032596588135Validating Epoch: 3 | iteration: 5/66 | Loss: 0.674656331539154Validating Epoch: 3 | iteration: 6/66 | Loss: 0.6985012888908386Validating Epoch: 3 | iteration: 7/66 | Loss: 0.666030764579773Validating Epoch: 3 | iteration: 8/66 | Loss: 0.6372379660606384Validating Epoch: 3 | iteration: 9/66 | Loss: 0.5944876670837402Validating Epoch: 3 | iteration: 10/66 | Loss: 0.6666671633720398Validating Epoch: 3 | iteration: 11/66 | Loss: 0.6031466126441956Validating Epoch: 3 | iteration: 12/66 | Loss: 0.6507860422134399Validating Epoch: 3 | iteration: 13/66 | Loss: 0.6707021594047546Validating Epoch: 3 | iteration: 14/66 | Loss: 0.6427533626556396Validating Epoch: 3 | iteration: 15/66 | Loss: 0.6594489812850952Validating Epoch: 3 | iteration: 16/66 | Loss: 0.6282213926315308Validating Epoch: 3 | iteration: 17/66 | Loss: 0.5751352310180664Validating Epoch: 3 | iteration: 18/66 | Loss: 0.6546541452407837Validating Epoch: 3 | iteration: 19/66 | Loss: 0.6572721004486084Validating Epoch: 3 | iteration: 20/66 | Loss: 0.6700246334075928Validating Epoch: 3 | iteration: 21/66 | Loss: 0.6232806444168091Validating Epoch: 3 | iteration: 22/66 | Loss: 0.6584764719009399Validating Epoch: 3 | iteration: 23/66 | Loss: 0.6981377601623535Validating Epoch: 3 | iteration: 24/66 | Loss: 0.6493848562240601Validating Epoch: 3 | iteration: 25/66 | Loss: 0.556753933429718Validating Epoch: 3 | iteration: 26/66 | Loss: 0.6654766798019409Validating Epoch: 3 | iteration: 27/66 | Loss: 0.6012860536575317Validating Epoch: 3 | iteration: 28/66 | Loss: 0.6607798337936401Validating Epoch: 3 | iteration: 29/66 | Loss: 0.6631363034248352Validating Epoch: 3 | iteration: 30/66 | Loss: 0.6123351454734802Validating Epoch: 3 | iteration: 31/66 | Loss: 0.6775310039520264Validating Epoch: 3 | iteration: 32/66 | Loss: 0.6546366214752197Validating Epoch: 3 | iteration: 33/66 | Loss: 0.662225604057312Validating Epoch: 3 | iteration: 34/66 | Loss: 0.6269121766090393Validating Epoch: 3 | iteration: 35/66 | Loss: 0.7210798859596252Validating Epoch: 3 | iteration: 36/66 | Loss: 0.6550312638282776Validating Epoch: 3 | iteration: 37/66 | Loss: 0.6182012557983398Validating Epoch: 3 | iteration: 38/66 | Loss: 0.6874179244041443Validating Epoch: 3 | iteration: 39/66 | Loss: 0.7088191509246826Validating Epoch: 3 | iteration: 40/66 | Loss: 0.6150503158569336Validating Epoch: 3 | iteration: 41/66 | Loss: 0.6190974712371826Validating Epoch: 3 | iteration: 42/66 | Loss: 0.5943520069122314Validating Epoch: 3 | iteration: 43/66 | Loss: 0.6296069622039795Validating Epoch: 3 | iteration: 44/66 | Loss: 0.6395108699798584Validating Epoch: 3 | iteration: 45/66 | Loss: 0.6129834651947021Validating Epoch: 3 | iteration: 46/66 | Loss: 0.5921818017959595Validating Epoch: 3 | iteration: 47/66 | Loss: 0.6534794569015503Validating Epoch: 3 | iteration: 48/66 | Loss: 0.6595190167427063Validating Epoch: 3 | iteration: 49/66 | Loss: 0.6220616102218628Validating Epoch: 3 | iteration: 50/66 | Loss: 0.6500262022018433Validating Epoch: 3 | iteration: 51/66 | Loss: 0.6514574289321899Validating Epoch: 3 | iteration: 52/66 | Loss: 0.611504316329956Validating Epoch: 3 | iteration: 53/66 | Loss: 0.6454931497573853Validating Epoch: 3 | iteration: 54/66 | Loss: 0.6542117595672607Validating Epoch: 3 | iteration: 55/66 | Loss: 0.6902241110801697Validating Epoch: 3 | iteration: 56/66 | Loss: 0.6710951328277588Validating Epoch: 3 | iteration: 57/66 | Loss: 0.7099999189376831Validating Epoch: 3 | iteration: 58/66 | Loss: 0.5991151332855225Validating Epoch: 3 | iteration: 59/66 | Loss: 0.6231451034545898Validating Epoch: 3 | iteration: 60/66 | Loss: 0.6390453577041626Validating Epoch: 3 | iteration: 61/66 | Loss: 0.6896792650222778Validating Epoch: 3 | iteration: 62/66 | Loss: 0.6389215588569641Validating Epoch: 3 | iteration: 63/66 | Loss: 0.6863488554954529Validating Epoch: 3 | iteration: 64/66 | Loss: 0.6873199343681335Validating Epoch: 3 | iteration: 65/66 | Loss: 0.6925644874572754Temperature:  0.5
Sampling: iteration: 0/51{'Validity': 0.986328125, 'Novelty': 1.0, 'Uniqueness': 0.997029702970297}
Training Epoch: 4 | iteration: 0/262 | Loss: 0.6816799640655518Training Epoch: 4 | iteration: 1/262 | Loss: 0.6802476644515991Training Epoch: 4 | iteration: 2/262 | Loss: 0.621940016746521Training Epoch: 4 | iteration: 3/262 | Loss: 0.6464253664016724Training Epoch: 4 | iteration: 4/262 | Loss: 0.5665024518966675Training Epoch: 4 | iteration: 5/262 | Loss: 0.6809430718421936Training Epoch: 4 | iteration: 6/262 | Loss: 0.5972603559494019Training Epoch: 4 | iteration: 7/262 | Loss: 0.6136682033538818Training Epoch: 4 | iteration: 8/262 | Loss: 0.6715275645256042Training Epoch: 4 | iteration: 9/262 | Loss: 0.6962300539016724Training Epoch: 4 | iteration: 10/262 | Loss: 0.5764529705047607Training Epoch: 4 | iteration: 11/262 | Loss: 0.6681078672409058Training Epoch: 4 | iteration: 12/262 | Loss: 0.623503565788269Training Epoch: 4 | iteration: 13/262 | Loss: 0.6581234931945801Training Epoch: 4 | iteration: 14/262 | Loss: 0.6403723359107971Training Epoch: 4 | iteration: 15/262 | Loss: 0.5932945013046265Training Epoch: 4 | iteration: 16/262 | Loss: 0.7112768888473511Training Epoch: 4 | iteration: 17/262 | Loss: 0.6638972759246826Training Epoch: 4 | iteration: 18/262 | Loss: 0.6536515355110168Training Epoch: 4 | iteration: 19/262 | Loss: 0.6593625545501709Training Epoch: 4 | iteration: 20/262 | Loss: 0.5904761552810669Training Epoch: 4 | iteration: 21/262 | Loss: 0.6546770334243774Training Epoch: 4 | iteration: 22/262 | Loss: 0.7338645458221436Training Epoch: 4 | iteration: 23/262 | Loss: 0.6475657224655151Training Epoch: 4 | iteration: 24/262 | Loss: 0.6917450428009033Training Epoch: 4 | iteration: 25/262 | Loss: 0.6370397806167603Training Epoch: 4 | iteration: 26/262 | Loss: 0.6567018032073975Training Epoch: 4 | iteration: 27/262 | Loss: 0.6066792011260986Training Epoch: 4 | iteration: 28/262 | Loss: 0.6773279905319214Training Epoch: 4 | iteration: 29/262 | Loss: 0.5831906199455261Training Epoch: 4 | iteration: 30/262 | Loss: 0.6676185727119446Training Epoch: 4 | iteration: 31/262 | Loss: 0.6719812750816345Training Epoch: 4 | iteration: 32/262 | Loss: 0.6548656225204468Training Epoch: 4 | iteration: 33/262 | Loss: 0.7046427726745605Training Epoch: 4 | iteration: 34/262 | Loss: 0.6805223226547241Training Epoch: 4 | iteration: 35/262 | Loss: 0.5484298467636108Training Epoch: 4 | iteration: 36/262 | Loss: 0.6181711554527283Training Epoch: 4 | iteration: 37/262 | Loss: 0.6574317216873169Training Epoch: 4 | iteration: 38/262 | Loss: 0.6244008541107178Training Epoch: 4 | iteration: 39/262 | Loss: 0.6269834041595459Training Epoch: 4 | iteration: 40/262 | Loss: 0.7175314426422119Training Epoch: 4 | iteration: 41/262 | Loss: 0.6414272785186768Training Epoch: 4 | iteration: 42/262 | Loss: 0.6271296739578247Training Epoch: 4 | iteration: 43/262 | Loss: 0.643044650554657Training Epoch: 4 | iteration: 44/262 | Loss: 0.635900616645813Training Epoch: 4 | iteration: 45/262 | Loss: 0.6642744541168213Training Epoch: 4 | iteration: 46/262 | Loss: 0.6978439688682556Training Epoch: 4 | iteration: 47/262 | Loss: 0.6202402114868164Training Epoch: 4 | iteration: 48/262 | Loss: 0.6746190190315247Training Epoch: 4 | iteration: 49/262 | Loss: 0.6973152756690979Training Epoch: 4 | iteration: 50/262 | Loss: 0.6722880601882935Training Epoch: 4 | iteration: 51/262 | Loss: 0.6906033754348755Training Epoch: 4 | iteration: 52/262 | Loss: 0.6245230436325073Training Epoch: 4 | iteration: 53/262 | Loss: 0.5667305588722229Training Epoch: 4 | iteration: 54/262 | Loss: 0.6794399619102478Training Epoch: 4 | iteration: 55/262 | Loss: 0.6288621425628662Training Epoch: 4 | iteration: 56/262 | Loss: 0.6655895709991455Training Epoch: 4 | iteration: 57/262 | Loss: 0.6234017610549927Training Epoch: 4 | iteration: 58/262 | Loss: 0.6773209571838379Training Epoch: 4 | iteration: 59/262 | Loss: 0.6346209645271301Training Epoch: 4 | iteration: 60/262 | Loss: 0.6114040613174438Training Epoch: 4 | iteration: 61/262 | Loss: 0.6382550001144409Training Epoch: 4 | iteration: 62/262 | Loss: 0.5944005250930786Training Epoch: 4 | iteration: 63/262 | Loss: 0.6369234919548035Training Epoch: 4 | iteration: 64/262 | Loss: 0.7539785504341125Training Epoch: 4 | iteration: 65/262 | Loss: 0.6299706697463989Training Epoch: 4 | iteration: 66/262 | Loss: 0.6403869986534119Training Epoch: 4 | iteration: 67/262 | Loss: 0.7260916233062744Training Epoch: 4 | iteration: 68/262 | Loss: 0.7076277732849121Training Epoch: 4 | iteration: 69/262 | Loss: 0.7026572227478027Training Epoch: 4 | iteration: 70/262 | Loss: 0.7084270715713501Training Epoch: 4 | iteration: 71/262 | Loss: 0.6361806392669678Training Epoch: 4 | iteration: 72/262 | Loss: 0.6249756813049316Training Epoch: 4 | iteration: 73/262 | Loss: 0.6624597311019897Training Epoch: 4 | iteration: 74/262 | Loss: 0.6458725333213806Training Epoch: 4 | iteration: 75/262 | Loss: 0.6300883293151855Training Epoch: 4 | iteration: 76/262 | Loss: 0.6795792579650879Training Epoch: 4 | iteration: 77/262 | Loss: 0.688289999961853Training Epoch: 4 | iteration: 78/262 | Loss: 0.7433098554611206Training Epoch: 4 | iteration: 79/262 | Loss: 0.6522514820098877Training Epoch: 4 | iteration: 80/262 | Loss: 0.6654092073440552Training Epoch: 4 | iteration: 81/262 | Loss: 0.6224073171615601Training Epoch: 4 | iteration: 82/262 | Loss: 0.679766058921814Training Epoch: 4 | iteration: 83/262 | Loss: 0.6753931641578674Training Epoch: 4 | iteration: 84/262 | Loss: 0.7024960517883301Training Epoch: 4 | iteration: 85/262 | Loss: 0.6856195330619812Training Epoch: 4 | iteration: 86/262 | Loss: 0.6627340316772461Training Epoch: 4 | iteration: 87/262 | Loss: 0.652990460395813Training Epoch: 4 | iteration: 88/262 | Loss: 0.688878059387207Training Epoch: 4 | iteration: 89/262 | Loss: 0.6800727844238281Training Epoch: 4 | iteration: 90/262 | Loss: 0.6803889274597168Training Epoch: 4 | iteration: 91/262 | Loss: 0.6656601428985596Training Epoch: 4 | iteration: 92/262 | Loss: 0.6601433753967285Training Epoch: 4 | iteration: 93/262 | Loss: 0.7256831526756287Training Epoch: 4 | iteration: 94/262 | Loss: 0.7312009930610657Training Epoch: 4 | iteration: 95/262 | Loss: 0.6004675626754761Training Epoch: 4 | iteration: 96/262 | Loss: 0.7251586318016052Training Epoch: 4 | iteration: 97/262 | Loss: 0.6671240925788879Training Epoch: 4 | iteration: 98/262 | Loss: 0.6242237687110901Training Epoch: 4 | iteration: 99/262 | Loss: 0.6566789150238037Training Epoch: 4 | iteration: 100/262 | Loss: 0.6892496347427368Training Epoch: 4 | iteration: 101/262 | Loss: 0.7344520688056946Training Epoch: 4 | iteration: 102/262 | Loss: 0.720267653465271Training Epoch: 4 | iteration: 103/262 | Loss: 0.7145339846611023Training Epoch: 4 | iteration: 104/262 | Loss: 0.6024433374404907Training Epoch: 4 | iteration: 105/262 | Loss: 0.7072961330413818Training Epoch: 4 | iteration: 106/262 | Loss: 0.7224165201187134Training Epoch: 4 | iteration: 107/262 | Loss: 0.6145744919776917Training Epoch: 4 | iteration: 108/262 | Loss: 0.6271383166313171Training Epoch: 4 | iteration: 109/262 | Loss: 0.6997625827789307Training Epoch: 4 | iteration: 110/262 | Loss: 0.6642167568206787Training Epoch: 4 | iteration: 111/262 | Loss: 0.6498405933380127Training Epoch: 4 | iteration: 112/262 | Loss: 0.6671409606933594Training Epoch: 4 | iteration: 113/262 | Loss: 0.6631428003311157Training Epoch: 4 | iteration: 114/262 | Loss: 0.6602241396903992Training Epoch: 4 | iteration: 115/262 | Loss: 0.678688108921051Training Epoch: 4 | iteration: 116/262 | Loss: 0.6700969934463501Training Epoch: 4 | iteration: 117/262 | Loss: 0.6602462530136108Training Epoch: 4 | iteration: 118/262 | Loss: 0.7459405660629272Training Epoch: 4 | iteration: 119/262 | Loss: 0.6700210571289062Training Epoch: 4 | iteration: 120/262 | Loss: 0.6167446374893188Training Epoch: 4 | iteration: 121/262 | Loss: 0.6469401121139526Training Epoch: 4 | iteration: 122/262 | Loss: 0.6378186941146851Training Epoch: 4 | iteration: 123/262 | Loss: 0.606378436088562Training Epoch: 4 | iteration: 124/262 | Loss: 0.6240897178649902Training Epoch: 4 | iteration: 125/262 | Loss: 0.6677223443984985Training Epoch: 4 | iteration: 126/262 | Loss: 0.6745309233665466Training Epoch: 4 | iteration: 127/262 | Loss: 0.6255402565002441Training Epoch: 4 | iteration: 128/262 | Loss: 0.6524306535720825Training Epoch: 4 | iteration: 129/262 | Loss: 0.7533262968063354Training Epoch: 4 | iteration: 130/262 | Loss: 0.6275092959403992Training Epoch: 4 | iteration: 131/262 | Loss: 0.6179996728897095Training Epoch: 4 | iteration: 132/262 | Loss: 0.6900064945220947Training Epoch: 4 | iteration: 133/262 | Loss: 0.6113690137863159Training Epoch: 4 | iteration: 134/262 | Loss: 0.6045709848403931Training Epoch: 4 | iteration: 135/262 | Loss: 0.6538829803466797Training Epoch: 4 | iteration: 136/262 | Loss: 0.7479304075241089Training Epoch: 4 | iteration: 137/262 | Loss: 0.755763828754425Training Epoch: 4 | iteration: 138/262 | Loss: 0.6397947072982788Training Epoch: 4 | iteration: 139/262 | Loss: 0.6495425701141357Training Epoch: 4 | iteration: 140/262 | Loss: 0.6663390398025513Training Epoch: 4 | iteration: 141/262 | Loss: 0.6453841924667358Training Epoch: 4 | iteration: 142/262 | Loss: 0.6859897375106812Training Epoch: 4 | iteration: 143/262 | Loss: 0.62149977684021Training Epoch: 4 | iteration: 144/262 | Loss: 0.7117059826850891Training Epoch: 4 | iteration: 145/262 | Loss: 0.6859565377235413Training Epoch: 4 | iteration: 146/262 | Loss: 0.6284307837486267Training Epoch: 4 | iteration: 147/262 | Loss: 0.7335203886032104Training Epoch: 4 | iteration: 148/262 | Loss: 0.656268835067749Training Epoch: 4 | iteration: 149/262 | Loss: 0.677322268486023Training Epoch: 4 | iteration: 150/262 | Loss: 0.6959876418113708Training Epoch: 4 | iteration: 151/262 | Loss: 0.654987096786499Training Epoch: 4 | iteration: 152/262 | Loss: 0.6556727886199951Training Epoch: 4 | iteration: 153/262 | Loss: 0.7804265022277832Training Epoch: 4 | iteration: 154/262 | Loss: 0.7223334312438965Training Epoch: 4 | iteration: 155/262 | Loss: 0.660904049873352Training Epoch: 4 | iteration: 156/262 | Loss: 0.7063409090042114Training Epoch: 4 | iteration: 157/262 | Loss: 0.633094847202301Training Epoch: 4 | iteration: 158/262 | Loss: 0.6712281703948975Training Epoch: 4 | iteration: 159/262 | Loss: 0.5939661860466003Training Epoch: 4 | iteration: 160/262 | Loss: 0.7285507917404175Training Epoch: 4 | iteration: 161/262 | Loss: 0.6974591016769409Training Epoch: 4 | iteration: 162/262 | Loss: 0.6540777683258057Training Epoch: 4 | iteration: 163/262 | Loss: 0.6753013134002686Training Epoch: 4 | iteration: 164/262 | Loss: 0.6426622271537781Training Epoch: 4 | iteration: 165/262 | Loss: 0.6424512267112732Training Epoch: 4 | iteration: 166/262 | Loss: 0.631446361541748Training Epoch: 4 | iteration: 167/262 | Loss: 0.5774691104888916Training Epoch: 4 | iteration: 168/262 | Loss: 0.6484677195549011Training Epoch: 4 | iteration: 169/262 | Loss: 0.6323785185813904Training Epoch: 4 | iteration: 170/262 | Loss: 0.7010214328765869Training Epoch: 4 | iteration: 171/262 | Loss: 0.620683491230011Training Epoch: 4 | iteration: 172/262 | Loss: 0.6522153615951538Training Epoch: 4 | iteration: 173/262 | Loss: 0.6415591239929199Training Epoch: 4 | iteration: 174/262 | Loss: 0.6519001126289368Training Epoch: 4 | iteration: 175/262 | Loss: 0.5844471454620361Training Epoch: 4 | iteration: 176/262 | Loss: 0.6753982305526733Training Epoch: 4 | iteration: 177/262 | Loss: 0.6859266757965088Training Epoch: 4 | iteration: 178/262 | Loss: 0.6662149429321289Training Epoch: 4 | iteration: 179/262 | Loss: 0.7007228136062622Training Epoch: 4 | iteration: 180/262 | Loss: 0.6246803998947144Training Epoch: 4 | iteration: 181/262 | Loss: 0.6199221611022949Training Epoch: 4 | iteration: 182/262 | Loss: 0.6664090752601624Training Epoch: 4 | iteration: 183/262 | Loss: 0.7092636823654175Training Epoch: 4 | iteration: 184/262 | Loss: 0.6462187170982361Training Epoch: 4 | iteration: 185/262 | Loss: 0.6400293111801147Training Epoch: 4 | iteration: 186/262 | Loss: 0.663250207901001Training Epoch: 4 | iteration: 187/262 | Loss: 0.6010664701461792Training Epoch: 4 | iteration: 188/262 | Loss: 0.6584943532943726Training Epoch: 4 | iteration: 189/262 | Loss: 0.7111906409263611Training Epoch: 4 | iteration: 190/262 | Loss: 0.6033816337585449Training Epoch: 4 | iteration: 191/262 | Loss: 0.5930043458938599Training Epoch: 4 | iteration: 192/262 | Loss: 0.6693167090415955Training Epoch: 4 | iteration: 193/262 | Loss: 0.6736521124839783Training Epoch: 4 | iteration: 194/262 | Loss: 0.6845004558563232Training Epoch: 4 | iteration: 195/262 | Loss: 0.6498252153396606Training Epoch: 4 | iteration: 196/262 | Loss: 0.6279371976852417Training Epoch: 4 | iteration: 197/262 | Loss: 0.5986905694007874Training Epoch: 4 | iteration: 198/262 | Loss: 0.6848005056381226Training Epoch: 4 | iteration: 199/262 | Loss: 0.6503819227218628Training Epoch: 4 | iteration: 200/262 | Loss: 0.6530723571777344Training Epoch: 4 | iteration: 201/262 | Loss: 0.7187683582305908Training Epoch: 4 | iteration: 202/262 | Loss: 0.628771185874939Training Epoch: 4 | iteration: 203/262 | Loss: 0.6182596683502197Training Epoch: 4 | iteration: 204/262 | Loss: 0.7053933143615723Training Epoch: 4 | iteration: 205/262 | Loss: 0.6198989748954773Training Epoch: 4 | iteration: 206/262 | Loss: 0.7157853841781616Training Epoch: 4 | iteration: 207/262 | Loss: 0.7083369493484497Training Epoch: 4 | iteration: 208/262 | Loss: 0.6915661096572876Training Epoch: 4 | iteration: 209/262 | Loss: 0.7066001296043396Training Epoch: 4 | iteration: 210/262 | Loss: 0.6974509954452515Training Epoch: 4 | iteration: 211/262 | Loss: 0.6434136629104614Training Epoch: 4 | iteration: 212/262 | Loss: 0.6053810119628906Training Epoch: 4 | iteration: 213/262 | Loss: 0.6890903115272522Training Epoch: 4 | iteration: 214/262 | Loss: 0.6064300537109375Training Epoch: 4 | iteration: 215/262 | Loss: 0.6544710397720337Training Epoch: 4 | iteration: 216/262 | Loss: 0.6189342141151428Training Epoch: 4 | iteration: 217/262 | Loss: 0.6270396709442139Training Epoch: 4 | iteration: 218/262 | Loss: 0.6932958364486694Training Epoch: 4 | iteration: 219/262 | Loss: 0.6709579229354858Training Epoch: 4 | iteration: 220/262 | Loss: 0.6288152933120728Training Epoch: 4 | iteration: 221/262 | Loss: 0.6369961500167847Training Epoch: 4 | iteration: 222/262 | Loss: 0.6946734189987183Training Epoch: 4 | iteration: 223/262 | Loss: 0.6319054365158081Training Epoch: 4 | iteration: 224/262 | Loss: 0.6454774737358093Training Epoch: 4 | iteration: 225/262 | Loss: 0.7203880548477173Training Epoch: 4 | iteration: 226/262 | Loss: 0.6853155493736267Training Epoch: 4 | iteration: 227/262 | Loss: 0.6099578142166138Training Epoch: 4 | iteration: 228/262 | Loss: 0.7176885604858398Training Epoch: 4 | iteration: 229/262 | Loss: 0.700792670249939Training Epoch: 4 | iteration: 230/262 | Loss: 0.6052513122558594Training Epoch: 4 | iteration: 231/262 | Loss: 0.7127802968025208Training Epoch: 4 | iteration: 232/262 | Loss: 0.703447699546814Training Epoch: 4 | iteration: 233/262 | Loss: 0.6238881349563599Training Epoch: 4 | iteration: 234/262 | Loss: 0.7024840712547302Training Epoch: 4 | iteration: 235/262 | Loss: 0.6227018237113953Training Epoch: 4 | iteration: 236/262 | Loss: 0.6933016777038574Training Epoch: 4 | iteration: 237/262 | Loss: 0.575610876083374Training Epoch: 4 | iteration: 238/262 | Loss: 0.7038689255714417Training Epoch: 4 | iteration: 239/262 | Loss: 0.6187669038772583Training Epoch: 4 | iteration: 240/262 | Loss: 0.7268401980400085Training Epoch: 4 | iteration: 241/262 | Loss: 0.7391780614852905Training Epoch: 4 | iteration: 242/262 | Loss: 0.6669049263000488Training Epoch: 4 | iteration: 243/262 | Loss: 0.6880326271057129Training Epoch: 4 | iteration: 244/262 | Loss: 0.6295841336250305Training Epoch: 4 | iteration: 245/262 | Loss: 0.6629100441932678Training Epoch: 4 | iteration: 246/262 | Loss: 0.6639946103096008Training Epoch: 4 | iteration: 247/262 | Loss: 0.6388096213340759Training Epoch: 4 | iteration: 248/262 | Loss: 0.6441311836242676Training Epoch: 4 | iteration: 249/262 | Loss: 0.6187787055969238Training Epoch: 4 | iteration: 250/262 | Loss: 0.7735525369644165Training Epoch: 4 | iteration: 251/262 | Loss: 0.6546230316162109Training Epoch: 4 | iteration: 252/262 | Loss: 0.6954787373542786Training Epoch: 4 | iteration: 253/262 | Loss: 0.6966326832771301Training Epoch: 4 | iteration: 254/262 | Loss: 0.6167471408843994Training Epoch: 4 | iteration: 255/262 | Loss: 0.6161553263664246Training Epoch: 4 | iteration: 256/262 | Loss: 0.7494335770606995Training Epoch: 4 | iteration: 257/262 | Loss: 0.7016562223434448Training Epoch: 4 | iteration: 258/262 | Loss: 0.7332981824874878Training Epoch: 4 | iteration: 259/262 | Loss: 0.6326356530189514Training Epoch: 4 | iteration: 260/262 | Loss: 0.6640430688858032Training Epoch: 4 | iteration: 261/262 | Loss: 0.4805784523487091Validating Epoch: 4 | iteration: 0/66 | Loss: 0.5920729637145996Validating Epoch: 4 | iteration: 1/66 | Loss: 0.619586706161499Validating Epoch: 4 | iteration: 2/66 | Loss: 0.5837352871894836Validating Epoch: 4 | iteration: 3/66 | Loss: 0.6887372732162476Validating Epoch: 4 | iteration: 4/66 | Loss: 0.6199671030044556Validating Epoch: 4 | iteration: 5/66 | Loss: 0.6172495484352112Validating Epoch: 4 | iteration: 6/66 | Loss: 0.6543987989425659Validating Epoch: 4 | iteration: 7/66 | Loss: 0.6402644515037537Validating Epoch: 4 | iteration: 8/66 | Loss: 0.5721460580825806Validating Epoch: 4 | iteration: 9/66 | Loss: 0.5622938871383667Validating Epoch: 4 | iteration: 10/66 | Loss: 0.6369293332099915Validating Epoch: 4 | iteration: 11/66 | Loss: 0.7345845699310303Validating Epoch: 4 | iteration: 12/66 | Loss: 0.6645017862319946Validating Epoch: 4 | iteration: 13/66 | Loss: 0.6219956874847412Validating Epoch: 4 | iteration: 14/66 | Loss: 0.5778770446777344Validating Epoch: 4 | iteration: 15/66 | Loss: 0.662737250328064Validating Epoch: 4 | iteration: 16/66 | Loss: 0.5982418656349182Validating Epoch: 4 | iteration: 17/66 | Loss: 0.6027027368545532Validating Epoch: 4 | iteration: 18/66 | Loss: 0.6821820735931396Validating Epoch: 4 | iteration: 19/66 | Loss: 0.6201151609420776Validating Epoch: 4 | iteration: 20/66 | Loss: 0.6292756199836731Validating Epoch: 4 | iteration: 21/66 | Loss: 0.6965572834014893Validating Epoch: 4 | iteration: 22/66 | Loss: 0.6088546514511108Validating Epoch: 4 | iteration: 23/66 | Loss: 0.638545036315918Validating Epoch: 4 | iteration: 24/66 | Loss: 0.6563876867294312Validating Epoch: 4 | iteration: 25/66 | Loss: 0.5836374163627625Validating Epoch: 4 | iteration: 26/66 | Loss: 0.6225931644439697Validating Epoch: 4 | iteration: 27/66 | Loss: 0.7265651226043701Validating Epoch: 4 | iteration: 28/66 | Loss: 0.6151909232139587Validating Epoch: 4 | iteration: 29/66 | Loss: 0.6457979679107666Validating Epoch: 4 | iteration: 30/66 | Loss: 0.6237844824790955Validating Epoch: 4 | iteration: 31/66 | Loss: 0.6046018600463867Validating Epoch: 4 | iteration: 32/66 | Loss: 0.605235755443573Validating Epoch: 4 | iteration: 33/66 | Loss: 0.6542270183563232Validating Epoch: 4 | iteration: 34/66 | Loss: 0.686630129814148Validating Epoch: 4 | iteration: 35/66 | Loss: 0.6836893558502197Validating Epoch: 4 | iteration: 36/66 | Loss: 0.6364684104919434Validating Epoch: 4 | iteration: 37/66 | Loss: 0.6658916473388672Validating Epoch: 4 | iteration: 38/66 | Loss: 0.6653802990913391Validating Epoch: 4 | iteration: 39/66 | Loss: 0.6203951835632324Validating Epoch: 4 | iteration: 40/66 | Loss: 0.6875975131988525Validating Epoch: 4 | iteration: 41/66 | Loss: 0.6281059980392456Validating Epoch: 4 | iteration: 42/66 | Loss: 0.6908916234970093Validating Epoch: 4 | iteration: 43/66 | Loss: 0.6755749583244324Validating Epoch: 4 | iteration: 44/66 | Loss: 0.7112202644348145Validating Epoch: 4 | iteration: 45/66 | Loss: 0.5944808125495911Validating Epoch: 4 | iteration: 46/66 | Loss: 0.6656433343887329Validating Epoch: 4 | iteration: 47/66 | Loss: 0.6210171580314636Validating Epoch: 4 | iteration: 48/66 | Loss: 0.6289823055267334Validating Epoch: 4 | iteration: 49/66 | Loss: 0.6394035816192627Validating Epoch: 4 | iteration: 50/66 | Loss: 0.6187120676040649Validating Epoch: 4 | iteration: 51/66 | Loss: 0.6248612999916077Validating Epoch: 4 | iteration: 52/66 | Loss: 0.6224054098129272Validating Epoch: 4 | iteration: 53/66 | Loss: 0.6047086119651794Validating Epoch: 4 | iteration: 54/66 | Loss: 0.6068583130836487Validating Epoch: 4 | iteration: 55/66 | Loss: 0.6425763368606567Validating Epoch: 4 | iteration: 56/66 | Loss: 0.6550714373588562Validating Epoch: 4 | iteration: 57/66 | Loss: 0.5828996896743774Validating Epoch: 4 | iteration: 58/66 | Loss: 0.6229188442230225Validating Epoch: 4 | iteration: 59/66 | Loss: 0.7035082578659058Validating Epoch: 4 | iteration: 60/66 | Loss: 0.6795316934585571Validating Epoch: 4 | iteration: 61/66 | Loss: 0.6594293117523193Validating Epoch: 4 | iteration: 62/66 | Loss: 0.6666576862335205Validating Epoch: 4 | iteration: 63/66 | Loss: 0.6575134992599487Validating Epoch: 4 | iteration: 64/66 | Loss: 0.6069962382316589Validating Epoch: 4 | iteration: 65/66 | Loss: 0.8602263927459717Temperature:  0.5
Sampling: iteration: 0/51{'Validity': 0.9951171875, 'Novelty': 1.0, 'Uniqueness': 0.9911678115799804}
Training Epoch: 5 | iteration: 0/262 | Loss: 0.580330491065979Training Epoch: 5 | iteration: 1/262 | Loss: 0.6582708954811096Training Epoch: 5 | iteration: 2/262 | Loss: 0.610342264175415Training Epoch: 5 | iteration: 3/262 | Loss: 0.6153156757354736Training Epoch: 5 | iteration: 4/262 | Loss: 0.6635816097259521Training Epoch: 5 | iteration: 5/262 | Loss: 0.6059740781784058Training Epoch: 5 | iteration: 6/262 | Loss: 0.6789876818656921Training Epoch: 5 | iteration: 7/262 | Loss: 0.724441647529602Training Epoch: 5 | iteration: 8/262 | Loss: 0.6727406978607178Training Epoch: 5 | iteration: 9/262 | Loss: 0.5848581194877625Training Epoch: 5 | iteration: 10/262 | Loss: 0.664826512336731Training Epoch: 5 | iteration: 11/262 | Loss: 0.6584017276763916Training Epoch: 5 | iteration: 12/262 | Loss: 0.7007958889007568Training Epoch: 5 | iteration: 13/262 | Loss: 0.6461157202720642Training Epoch: 5 | iteration: 14/262 | Loss: 0.5609814524650574Training Epoch: 5 | iteration: 15/262 | Loss: 0.6686124801635742Training Epoch: 5 | iteration: 16/262 | Loss: 0.6250216960906982Training Epoch: 5 | iteration: 17/262 | Loss: 0.5917044878005981Training Epoch: 5 | iteration: 18/262 | Loss: 0.5800234079360962Training Epoch: 5 | iteration: 19/262 | Loss: 0.6689289808273315Training Epoch: 5 | iteration: 20/262 | Loss: 0.690692663192749Training Epoch: 5 | iteration: 21/262 | Loss: 0.6748723983764648Training Epoch: 5 | iteration: 22/262 | Loss: 0.6606607437133789Training Epoch: 5 | iteration: 23/262 | Loss: 0.5932109951972961Training Epoch: 5 | iteration: 24/262 | Loss: 0.6592916250228882Training Epoch: 5 | iteration: 25/262 | Loss: 0.5824124813079834Training Epoch: 5 | iteration: 26/262 | Loss: 0.60908043384552Training Epoch: 5 | iteration: 27/262 | Loss: 0.632272481918335Training Epoch: 5 | iteration: 28/262 | Loss: 0.7526593208312988Training Epoch: 5 | iteration: 29/262 | Loss: 0.5859792232513428Training Epoch: 5 | iteration: 30/262 | Loss: 0.6080102324485779Training Epoch: 5 | iteration: 31/262 | Loss: 0.7051241397857666Training Epoch: 5 | iteration: 32/262 | Loss: 0.6456059217453003Training Epoch: 5 | iteration: 33/262 | Loss: 0.5807257890701294Training Epoch: 5 | iteration: 34/262 | Loss: 0.6090503931045532Training Epoch: 5 | iteration: 35/262 | Loss: 0.5870398283004761Training Epoch: 5 | iteration: 36/262 | Loss: 0.5704947710037231Training Epoch: 5 | iteration: 37/262 | Loss: 0.5954505801200867Training Epoch: 5 | iteration: 38/262 | Loss: 0.5941716432571411Training Epoch: 5 | iteration: 39/262 | Loss: 0.6563959121704102Training Epoch: 5 | iteration: 40/262 | Loss: 0.5949975252151489Training Epoch: 5 | iteration: 41/262 | Loss: 0.6051720380783081Training Epoch: 5 | iteration: 42/262 | Loss: 0.573104739189148Training Epoch: 5 | iteration: 43/262 | Loss: 0.6471110582351685Training Epoch: 5 | iteration: 44/262 | Loss: 0.645269513130188Training Epoch: 5 | iteration: 45/262 | Loss: 0.6335852742195129Training Epoch: 5 | iteration: 46/262 | Loss: 0.5699884295463562Training Epoch: 5 | iteration: 47/262 | Loss: 0.6380636692047119Training Epoch: 5 | iteration: 48/262 | Loss: 0.6412957906723022Training Epoch: 5 | iteration: 49/262 | Loss: 0.6974581480026245Training Epoch: 5 | iteration: 50/262 | Loss: 0.6981825828552246Training Epoch: 5 | iteration: 51/262 | Loss: 0.6064169406890869Training Epoch: 5 | iteration: 52/262 | Loss: 0.6224803328514099Training Epoch: 5 | iteration: 53/262 | Loss: 0.6675610542297363Training Epoch: 5 | iteration: 54/262 | Loss: 0.6493728756904602Training Epoch: 5 | iteration: 55/262 | Loss: 0.640657901763916Training Epoch: 5 | iteration: 56/262 | Loss: 0.6020954847335815Training Epoch: 5 | iteration: 57/262 | Loss: 0.5634268522262573Training Epoch: 5 | iteration: 58/262 | Loss: 0.6638485789299011Training Epoch: 5 | iteration: 59/262 | Loss: 0.634623646736145Training Epoch: 5 | iteration: 60/262 | Loss: 0.587281346321106Training Epoch: 5 | iteration: 61/262 | Loss: 0.6351754665374756Training Epoch: 5 | iteration: 62/262 | Loss: 0.6198079586029053Training Epoch: 5 | iteration: 63/262 | Loss: 0.6750174760818481Training Epoch: 5 | iteration: 64/262 | Loss: 0.6618483066558838Training Epoch: 5 | iteration: 65/262 | Loss: 0.674653947353363Training Epoch: 5 | iteration: 66/262 | Loss: 0.6206710338592529Training Epoch: 5 | iteration: 67/262 | Loss: 0.6875133514404297Training Epoch: 5 | iteration: 68/262 | Loss: 0.6401200294494629Training Epoch: 5 | iteration: 69/262 | Loss: 0.6054362654685974Training Epoch: 5 | iteration: 70/262 | Loss: 0.6464418172836304Training Epoch: 5 | iteration: 71/262 | Loss: 0.5748962759971619Training Epoch: 5 | iteration: 72/262 | Loss: 0.6331723928451538Training Epoch: 5 | iteration: 73/262 | Loss: 0.6594171524047852Training Epoch: 5 | iteration: 74/262 | Loss: 0.724563717842102Training Epoch: 5 | iteration: 75/262 | Loss: 0.649201512336731Training Epoch: 5 | iteration: 76/262 | Loss: 0.696204662322998Training Epoch: 5 | iteration: 77/262 | Loss: 0.6447353363037109Training Epoch: 5 | iteration: 78/262 | Loss: 0.6143726110458374Training Epoch: 5 | iteration: 79/262 | Loss: 0.6549594402313232Training Epoch: 5 | iteration: 80/262 | Loss: 0.7139977216720581Training Epoch: 5 | iteration: 81/262 | Loss: 0.6834890842437744Training Epoch: 5 | iteration: 82/262 | Loss: 0.665238618850708Training Epoch: 5 | iteration: 83/262 | Loss: 0.6310080289840698Training Epoch: 5 | iteration: 84/262 | Loss: 0.6708992123603821Training Epoch: 5 | iteration: 85/262 | Loss: 0.6326919794082642Training Epoch: 5 | iteration: 86/262 | Loss: 0.5988514423370361Training Epoch: 5 | iteration: 87/262 | Loss: 0.6133748292922974Training Epoch: 5 | iteration: 88/262 | Loss: 0.5746520757675171Training Epoch: 5 | iteration: 89/262 | Loss: 0.6194366812705994Training Epoch: 5 | iteration: 90/262 | Loss: 0.6318576335906982Training Epoch: 5 | iteration: 91/262 | Loss: 0.7322700619697571Training Epoch: 5 | iteration: 92/262 | Loss: 0.712954044342041Training Epoch: 5 | iteration: 93/262 | Loss: 0.6206276416778564Training Epoch: 5 | iteration: 94/262 | Loss: 0.6774115562438965Training Epoch: 5 | iteration: 95/262 | Loss: 0.6427062749862671Training Epoch: 5 | iteration: 96/262 | Loss: 0.7068137526512146Training Epoch: 5 | iteration: 97/262 | Loss: 0.6361040472984314Training Epoch: 5 | iteration: 98/262 | Loss: 0.6467323303222656Training Epoch: 5 | iteration: 99/262 | Loss: 0.6673436760902405Training Epoch: 5 | iteration: 100/262 | Loss: 0.6806540489196777Training Epoch: 5 | iteration: 101/262 | Loss: 0.6305257678031921Training Epoch: 5 | iteration: 102/262 | Loss: 0.6124524474143982Training Epoch: 5 | iteration: 103/262 | Loss: 0.6697038412094116Training Epoch: 5 | iteration: 104/262 | Loss: 0.6413635015487671Training Epoch: 5 | iteration: 105/262 | Loss: 0.6433262228965759Training Epoch: 5 | iteration: 106/262 | Loss: 0.6021150350570679Training Epoch: 5 | iteration: 107/262 | Loss: 0.6670029759407043Training Epoch: 5 | iteration: 108/262 | Loss: 0.5879234075546265Training Epoch: 5 | iteration: 109/262 | Loss: 0.6430317163467407Training Epoch: 5 | iteration: 110/262 | Loss: 0.6152805685997009Training Epoch: 5 | iteration: 111/262 | Loss: 0.6305878758430481Training Epoch: 5 | iteration: 112/262 | Loss: 0.5958220958709717Training Epoch: 5 | iteration: 113/262 | Loss: 0.6598626375198364Training Epoch: 5 | iteration: 114/262 | Loss: 0.6452125310897827Training Epoch: 5 | iteration: 115/262 | Loss: 0.6536316275596619Training Epoch: 5 | iteration: 116/262 | Loss: 0.6685096025466919Training Epoch: 5 | iteration: 117/262 | Loss: 0.7128700613975525Training Epoch: 5 | iteration: 118/262 | Loss: 0.6263477802276611Training Epoch: 5 | iteration: 119/262 | Loss: 0.6211715936660767Training Epoch: 5 | iteration: 120/262 | Loss: 0.6022971868515015Training Epoch: 5 | iteration: 121/262 | Loss: 0.7190543413162231Training Epoch: 5 | iteration: 122/262 | Loss: 0.653786301612854Training Epoch: 5 | iteration: 123/262 | Loss: 0.6437891721725464Training Epoch: 5 | iteration: 124/262 | Loss: 0.6895706057548523Training Epoch: 5 | iteration: 125/262 | Loss: 0.6183086037635803Training Epoch: 5 | iteration: 126/262 | Loss: 0.6382080316543579Training Epoch: 5 | iteration: 127/262 | Loss: 0.6598480343818665Training Epoch: 5 | iteration: 128/262 | Loss: 0.6242906451225281Training Epoch: 5 | iteration: 129/262 | Loss: 0.6608565449714661Training Epoch: 5 | iteration: 130/262 | Loss: 0.6832262277603149Training Epoch: 5 | iteration: 131/262 | Loss: 0.5718764066696167Training Epoch: 5 | iteration: 132/262 | Loss: 0.5955569744110107Training Epoch: 5 | iteration: 133/262 | Loss: 0.6580610871315002Training Epoch: 5 | iteration: 134/262 | Loss: 0.7190849184989929Training Epoch: 5 | iteration: 135/262 | Loss: 0.6840490102767944Training Epoch: 5 | iteration: 136/262 | Loss: 0.6108717322349548Training Epoch: 5 | iteration: 137/262 | Loss: 0.5618574619293213Training Epoch: 5 | iteration: 138/262 | Loss: 0.6794359087944031Training Epoch: 5 | iteration: 139/262 | Loss: 0.6528782844543457Training Epoch: 5 | iteration: 140/262 | Loss: 0.7021484375Training Epoch: 5 | iteration: 141/262 | Loss: 0.6218252182006836Training Epoch: 5 | iteration: 142/262 | Loss: 0.7526237964630127Training Epoch: 5 | iteration: 143/262 | Loss: 0.7372184991836548Training Epoch: 5 | iteration: 144/262 | Loss: 0.6445432305335999Training Epoch: 5 | iteration: 145/262 | Loss: 0.637475848197937Training Epoch: 5 | iteration: 146/262 | Loss: 0.5845613479614258Training Epoch: 5 | iteration: 147/262 | Loss: 0.5577647089958191Training Epoch: 5 | iteration: 148/262 | Loss: 0.6183379888534546Training Epoch: 5 | iteration: 149/262 | Loss: 0.6747663021087646Training Epoch: 5 | iteration: 150/262 | Loss: 0.6253727674484253Training Epoch: 5 | iteration: 151/262 | Loss: 0.5927977561950684Training Epoch: 5 | iteration: 152/262 | Loss: 0.6691426038742065Training Epoch: 5 | iteration: 153/262 | Loss: 0.6656687259674072Training Epoch: 5 | iteration: 154/262 | Loss: 0.6292720437049866Training Epoch: 5 | iteration: 155/262 | Loss: 0.642508327960968Training Epoch: 5 | iteration: 156/262 | Loss: 0.6104205846786499Training Epoch: 5 | iteration: 157/262 | Loss: 0.5972856879234314Training Epoch: 5 | iteration: 158/262 | Loss: 0.6074938178062439Training Epoch: 5 | iteration: 159/262 | Loss: 0.6142032146453857Training Epoch: 5 | iteration: 160/262 | Loss: 0.6406194567680359Training Epoch: 5 | iteration: 161/262 | Loss: 0.5918151140213013Training Epoch: 5 | iteration: 162/262 | Loss: 0.6153278350830078Training Epoch: 5 | iteration: 163/262 | Loss: 0.6395304203033447Training Epoch: 5 | iteration: 164/262 | Loss: 0.7002465128898621Training Epoch: 5 | iteration: 165/262 | Loss: 0.6858569383621216Training Epoch: 5 | iteration: 166/262 | Loss: 0.6128286123275757Training Epoch: 5 | iteration: 167/262 | Loss: 0.6865882277488708Training Epoch: 5 | iteration: 168/262 | Loss: 0.6387476325035095Training Epoch: 5 | iteration: 169/262 | Loss: 0.6778928637504578Training Epoch: 5 | iteration: 170/262 | Loss: 0.6201440095901489Training Epoch: 5 | iteration: 171/262 | Loss: 0.6277929544448853Training Epoch: 5 | iteration: 172/262 | Loss: 0.6599093675613403Training Epoch: 5 | iteration: 173/262 | Loss: 0.599776029586792Training Epoch: 5 | iteration: 174/262 | Loss: 0.6162285804748535Training Epoch: 5 | iteration: 175/262 | Loss: 0.635017991065979Training Epoch: 5 | iteration: 176/262 | Loss: 0.587639331817627Training Epoch: 5 | iteration: 177/262 | Loss: 0.684898316860199Training Epoch: 5 | iteration: 178/262 | Loss: 0.5878504514694214Training Epoch: 5 | iteration: 179/262 | Loss: 0.6487740278244019Training Epoch: 5 | iteration: 180/262 | Loss: 0.6448392868041992Training Epoch: 5 | iteration: 181/262 | Loss: 0.6212145090103149Training Epoch: 5 | iteration: 182/262 | Loss: 0.7353063225746155Training Epoch: 5 | iteration: 183/262 | Loss: 0.6688557267189026Training Epoch: 5 | iteration: 184/262 | Loss: 0.6002233028411865Training Epoch: 5 | iteration: 185/262 | Loss: 0.5472335815429688Training Epoch: 5 | iteration: 186/262 | Loss: 0.5702931880950928Training Epoch: 5 | iteration: 187/262 | Loss: 0.650407075881958Training Epoch: 5 | iteration: 188/262 | Loss: 0.7220431566238403Training Epoch: 5 | iteration: 189/262 | Loss: 0.6374813318252563Training Epoch: 5 | iteration: 190/262 | Loss: 0.646782398223877Training Epoch: 5 | iteration: 191/262 | Loss: 0.6210184097290039Training Epoch: 5 | iteration: 192/262 | Loss: 0.7365045547485352Training Epoch: 5 | iteration: 193/262 | Loss: 0.5796936750411987Training Epoch: 5 | iteration: 194/262 | Loss: 0.63263338804245Training Epoch: 5 | iteration: 195/262 | Loss: 0.6560919284820557Training Epoch: 5 | iteration: 196/262 | Loss: 0.6997067928314209Training Epoch: 5 | iteration: 197/262 | Loss: 0.6700562834739685Training Epoch: 5 | iteration: 198/262 | Loss: 0.6271855235099792Training Epoch: 5 | iteration: 199/262 | Loss: 0.6238003373146057Training Epoch: 5 | iteration: 200/262 | Loss: 0.6952911615371704Training Epoch: 5 | iteration: 201/262 | Loss: 0.7140834331512451Training Epoch: 5 | iteration: 202/262 | Loss: 0.642513632774353Training Epoch: 5 | iteration: 203/262 | Loss: 0.597953200340271Training Epoch: 5 | iteration: 204/262 | Loss: 0.5889358520507812Training Epoch: 5 | iteration: 205/262 | Loss: 0.6476320028305054Training Epoch: 5 | iteration: 206/262 | Loss: 0.675767183303833Training Epoch: 5 | iteration: 207/262 | Loss: 0.5875736474990845Training Epoch: 5 | iteration: 208/262 | Loss: 0.6615017056465149Training Epoch: 5 | iteration: 209/262 | Loss: 0.5964165925979614Training Epoch: 5 | iteration: 210/262 | Loss: 0.7130438089370728Training Epoch: 5 | iteration: 211/262 | Loss: 0.6667965054512024Training Epoch: 5 | iteration: 212/262 | Loss: 0.7084915637969971Training Epoch: 5 | iteration: 213/262 | Loss: 0.7205028533935547Training Epoch: 5 | iteration: 214/262 | Loss: 0.6082882881164551Training Epoch: 5 | iteration: 215/262 | Loss: 0.6679207682609558Training Epoch: 5 | iteration: 216/262 | Loss: 0.6383987069129944Training Epoch: 5 | iteration: 217/262 | Loss: 0.6653497219085693Training Epoch: 5 | iteration: 218/262 | Loss: 0.6444076895713806Training Epoch: 5 | iteration: 219/262 | Loss: 0.5581691265106201Training Epoch: 5 | iteration: 220/262 | Loss: 0.5998350381851196Training Epoch: 5 | iteration: 221/262 | Loss: 0.6647077798843384Training Epoch: 5 | iteration: 222/262 | Loss: 0.6967105865478516Training Epoch: 5 | iteration: 223/262 | Loss: 0.6507067680358887Training Epoch: 5 | iteration: 224/262 | Loss: 0.6327986121177673Training Epoch: 5 | iteration: 225/262 | Loss: 0.7011328935623169Training Epoch: 5 | iteration: 226/262 | Loss: 0.5834978222846985Training Epoch: 5 | iteration: 227/262 | Loss: 0.6683217287063599Training Epoch: 5 | iteration: 228/262 | Loss: 0.6263533234596252Training Epoch: 5 | iteration: 229/262 | Loss: 0.5408051609992981Training Epoch: 5 | iteration: 230/262 | Loss: 0.6900593042373657Training Epoch: 5 | iteration: 231/262 | Loss: 0.6060695648193359Training Epoch: 5 | iteration: 232/262 | Loss: 0.5426716804504395Training Epoch: 5 | iteration: 233/262 | Loss: 0.6133109331130981Training Epoch: 5 | iteration: 234/262 | Loss: 0.7045031189918518Training Epoch: 5 | iteration: 235/262 | Loss: 0.6479088068008423Training Epoch: 5 | iteration: 236/262 | Loss: 0.6741738319396973Training Epoch: 5 | iteration: 237/262 | Loss: 0.6342052221298218Training Epoch: 5 | iteration: 238/262 | Loss: 0.620415210723877Training Epoch: 5 | iteration: 239/262 | Loss: 0.5939313173294067Training Epoch: 5 | iteration: 240/262 | Loss: 0.6734094023704529Training Epoch: 5 | iteration: 241/262 | Loss: 0.6441600918769836Training Epoch: 5 | iteration: 242/262 | Loss: 0.6770729422569275Training Epoch: 5 | iteration: 243/262 | Loss: 0.6213728189468384Training Epoch: 5 | iteration: 244/262 | Loss: 0.7035670280456543Training Epoch: 5 | iteration: 245/262 | Loss: 0.6859469413757324Training Epoch: 5 | iteration: 246/262 | Loss: 0.6388454437255859Training Epoch: 5 | iteration: 247/262 | Loss: 0.6171852946281433Training Epoch: 5 | iteration: 248/262 | Loss: 0.5879632234573364Training Epoch: 5 | iteration: 249/262 | Loss: 0.6414544582366943Training Epoch: 5 | iteration: 250/262 | Loss: 0.6264928579330444Training Epoch: 5 | iteration: 251/262 | Loss: 0.675274670124054Training Epoch: 5 | iteration: 252/262 | Loss: 0.6231372356414795Training Epoch: 5 | iteration: 253/262 | Loss: 0.5852378606796265Training Epoch: 5 | iteration: 254/262 | Loss: 0.6614680886268616Training Epoch: 5 | iteration: 255/262 | Loss: 0.661665678024292Training Epoch: 5 | iteration: 256/262 | Loss: 0.661347508430481Training Epoch: 5 | iteration: 257/262 | Loss: 0.6132113933563232Training Epoch: 5 | iteration: 258/262 | Loss: 0.675984263420105Training Epoch: 5 | iteration: 259/262 | Loss: 0.6082453727722168Training Epoch: 5 | iteration: 260/262 | Loss: 0.5764672756195068Training Epoch: 5 | iteration: 261/262 | Loss: 0.5017451643943787Validating Epoch: 5 | iteration: 0/66 | Loss: 0.6077300310134888Validating Epoch: 5 | iteration: 1/66 | Loss: 0.5705294013023376Validating Epoch: 5 | iteration: 2/66 | Loss: 0.5576110482215881Validating Epoch: 5 | iteration: 3/66 | Loss: 0.6357927322387695Validating Epoch: 5 | iteration: 4/66 | Loss: 0.6534112095832825Validating Epoch: 5 | iteration: 5/66 | Loss: 0.6714694499969482Validating Epoch: 5 | iteration: 6/66 | Loss: 0.623986005783081Validating Epoch: 5 | iteration: 7/66 | Loss: 0.6992416381835938Validating Epoch: 5 | iteration: 8/66 | Loss: 0.7278780341148376Validating Epoch: 5 | iteration: 9/66 | Loss: 0.6723087430000305Validating Epoch: 5 | iteration: 10/66 | Loss: 0.6124885082244873Validating Epoch: 5 | iteration: 11/66 | Loss: 0.6156491041183472Validating Epoch: 5 | iteration: 12/66 | Loss: 0.6803299188613892Validating Epoch: 5 | iteration: 13/66 | Loss: 0.6483787298202515Validating Epoch: 5 | iteration: 14/66 | Loss: 0.6216100454330444Validating Epoch: 5 | iteration: 15/66 | Loss: 0.6228182315826416Validating Epoch: 5 | iteration: 16/66 | Loss: 0.780366063117981Validating Epoch: 5 | iteration: 17/66 | Loss: 0.7145934104919434Validating Epoch: 5 | iteration: 18/66 | Loss: 0.5987275242805481Validating Epoch: 5 | iteration: 19/66 | Loss: 0.7367188334465027Validating Epoch: 5 | iteration: 20/66 | Loss: 0.6941608190536499Validating Epoch: 5 | iteration: 21/66 | Loss: 0.6972547173500061Validating Epoch: 5 | iteration: 22/66 | Loss: 0.5997203588485718Validating Epoch: 5 | iteration: 23/66 | Loss: 0.6808605790138245Validating Epoch: 5 | iteration: 24/66 | Loss: 0.6878389120101929Validating Epoch: 5 | iteration: 25/66 | Loss: 0.7205801010131836Validating Epoch: 5 | iteration: 26/66 | Loss: 0.6526793241500854Validating Epoch: 5 | iteration: 27/66 | Loss: 0.6239348649978638Validating Epoch: 5 | iteration: 28/66 | Loss: 0.5917476415634155Validating Epoch: 5 | iteration: 29/66 | Loss: 0.6693613529205322Validating Epoch: 5 | iteration: 30/66 | Loss: 0.5544869899749756Validating Epoch: 5 | iteration: 31/66 | Loss: 0.7181107997894287Validating Epoch: 5 | iteration: 32/66 | Loss: 0.6476129293441772Validating Epoch: 5 | iteration: 33/66 | Loss: 0.6055281758308411Validating Epoch: 5 | iteration: 34/66 | Loss: 0.6958535313606262Validating Epoch: 5 | iteration: 35/66 | Loss: 0.6929864883422852Validating Epoch: 5 | iteration: 36/66 | Loss: 0.5858994722366333Validating Epoch: 5 | iteration: 37/66 | Loss: 0.6824679970741272Validating Epoch: 5 | iteration: 38/66 | Loss: 0.6719312071800232Validating Epoch: 5 | iteration: 39/66 | Loss: 0.6194891929626465Validating Epoch: 5 | iteration: 40/66 | Loss: 0.691385805606842Validating Epoch: 5 | iteration: 41/66 | Loss: 0.5938049554824829Validating Epoch: 5 | iteration: 42/66 | Loss: 0.5424068570137024Validating Epoch: 5 | iteration: 43/66 | Loss: 0.639951229095459Validating Epoch: 5 | iteration: 44/66 | Loss: 0.695414662361145Validating Epoch: 5 | iteration: 45/66 | Loss: 0.6047394275665283Validating Epoch: 5 | iteration: 46/66 | Loss: 0.649959921836853Validating Epoch: 5 | iteration: 47/66 | Loss: 0.5988484621047974Validating Epoch: 5 | iteration: 48/66 | Loss: 0.5087873935699463Validating Epoch: 5 | iteration: 49/66 | Loss: 0.6153424382209778Validating Epoch: 5 | iteration: 50/66 | Loss: 0.6550660133361816Validating Epoch: 5 | iteration: 51/66 | Loss: 0.5890095233917236Validating Epoch: 5 | iteration: 52/66 | Loss: 0.6274415254592896Validating Epoch: 5 | iteration: 53/66 | Loss: 0.7065112590789795Validating Epoch: 5 | iteration: 54/66 | Loss: 0.6118574738502502Validating Epoch: 5 | iteration: 55/66 | Loss: 0.5965331792831421Validating Epoch: 5 | iteration: 56/66 | Loss: 0.6102047562599182Validating Epoch: 5 | iteration: 57/66 | Loss: 0.6329329013824463Validating Epoch: 5 | iteration: 58/66 | Loss: 0.7282290458679199Validating Epoch: 5 | iteration: 59/66 | Loss: 0.6979391574859619Validating Epoch: 5 | iteration: 60/66 | Loss: 0.5693327188491821Validating Epoch: 5 | iteration: 61/66 | Loss: 0.7327105402946472Validating Epoch: 5 | iteration: 62/66 | Loss: 0.602437436580658Validating Epoch: 5 | iteration: 63/66 | Loss: 0.5958849787712097Validating Epoch: 5 | iteration: 64/66 | Loss: 0.6135807037353516Validating Epoch: 5 | iteration: 65/66 | Loss: 0.5939144492149353Temperature:  0.5
Sampling: iteration: 0/51{'Validity': 0.9833984375, 'Novelty': 1.0, 'Uniqueness': 0.9900695134061569}
Training Epoch: 6 | iteration: 0/262 | Loss: 0.5669084191322327Training Epoch: 6 | iteration: 1/262 | Loss: 0.6050349473953247Training Epoch: 6 | iteration: 2/262 | Loss: 0.6182472109794617Training Epoch: 6 | iteration: 3/262 | Loss: 0.6376747488975525Training Epoch: 6 | iteration: 4/262 | Loss: 0.6924424171447754Training Epoch: 6 | iteration: 5/262 | Loss: 0.5697112083435059Training Epoch: 6 | iteration: 6/262 | Loss: 0.6217727661132812Training Epoch: 6 | iteration: 7/262 | Loss: 0.6119894981384277Training Epoch: 6 | iteration: 8/262 | Loss: 0.6619957685470581Training Epoch: 6 | iteration: 9/262 | Loss: 0.6296889781951904Training Epoch: 6 | iteration: 10/262 | Loss: 0.5527199506759644Training Epoch: 6 | iteration: 11/262 | Loss: 0.6743987202644348Training Epoch: 6 | iteration: 12/262 | Loss: 0.6353204846382141Training Epoch: 6 | iteration: 13/262 | Loss: 0.6538420915603638Training Epoch: 6 | iteration: 14/262 | Loss: 0.6496283411979675Training Epoch: 6 | iteration: 15/262 | Loss: 0.6313821077346802Training Epoch: 6 | iteration: 16/262 | Loss: 0.6775106191635132Training Epoch: 6 | iteration: 17/262 | Loss: 0.5705388784408569Training Epoch: 6 | iteration: 18/262 | Loss: 0.573763906955719Training Epoch: 6 | iteration: 19/262 | Loss: 0.607170820236206Training Epoch: 6 | iteration: 20/262 | Loss: 0.5987958312034607Training Epoch: 6 | iteration: 21/262 | Loss: 0.612152099609375Training Epoch: 6 | iteration: 22/262 | Loss: 0.6018465161323547Training Epoch: 6 | iteration: 23/262 | Loss: 0.6485805511474609Training Epoch: 6 | iteration: 24/262 | Loss: 0.6066064834594727Training Epoch: 6 | iteration: 25/262 | Loss: 0.6234922409057617Training Epoch: 6 | iteration: 26/262 | Loss: 0.6232680678367615Training Epoch: 6 | iteration: 27/262 | Loss: 0.6221212148666382Training Epoch: 6 | iteration: 28/262 | Loss: 0.6211683750152588Training Epoch: 6 | iteration: 29/262 | Loss: 0.7143517136573792Training Epoch: 6 | iteration: 30/262 | Loss: 0.5665490627288818Training Epoch: 6 | iteration: 31/262 | Loss: 0.6272498965263367Training Epoch: 6 | iteration: 32/262 | Loss: 0.6283414363861084Training Epoch: 6 | iteration: 33/262 | Loss: 0.678902268409729Training Epoch: 6 | iteration: 34/262 | Loss: 0.6095847487449646Training Epoch: 6 | iteration: 35/262 | Loss: 0.5973692536354065Training Epoch: 6 | iteration: 36/262 | Loss: 0.6574758291244507Training Epoch: 6 | iteration: 37/262 | Loss: 0.6155556440353394Training Epoch: 6 | iteration: 38/262 | Loss: 0.6710413694381714Training Epoch: 6 | iteration: 39/262 | Loss: 0.5890593528747559Training Epoch: 6 | iteration: 40/262 | Loss: 0.6162402629852295Training Epoch: 6 | iteration: 41/262 | Loss: 0.5620342493057251Training Epoch: 6 | iteration: 42/262 | Loss: 0.5887207984924316Training Epoch: 6 | iteration: 43/262 | Loss: 0.6085655093193054Training Epoch: 6 | iteration: 44/262 | Loss: 0.6299371719360352Training Epoch: 6 | iteration: 45/262 | Loss: 0.6835553646087646Training Epoch: 6 | iteration: 46/262 | Loss: 0.6165812015533447Training Epoch: 6 | iteration: 47/262 | Loss: 0.6301785111427307Training Epoch: 6 | iteration: 48/262 | Loss: 0.6166563034057617Training Epoch: 6 | iteration: 49/262 | Loss: 0.6595180630683899Training Epoch: 6 | iteration: 50/262 | Loss: 0.6765445470809937Training Epoch: 6 | iteration: 51/262 | Loss: 0.6127680540084839Training Epoch: 6 | iteration: 52/262 | Loss: 0.5668151378631592Training Epoch: 6 | iteration: 53/262 | Loss: 0.7146305441856384Training Epoch: 6 | iteration: 54/262 | Loss: 0.6279726624488831Training Epoch: 6 | iteration: 55/262 | Loss: 0.5916563868522644Training Epoch: 6 | iteration: 56/262 | Loss: 0.6547814607620239Training Epoch: 6 | iteration: 57/262 | Loss: 0.6585525274276733Training Epoch: 6 | iteration: 58/262 | Loss: 0.6160989999771118Training Epoch: 6 | iteration: 59/262 | Loss: 0.6696379780769348Training Epoch: 6 | iteration: 60/262 | Loss: 0.6632095575332642Training Epoch: 6 | iteration: 61/262 | Loss: 0.5682134628295898Training Epoch: 6 | iteration: 62/262 | Loss: 0.5749390721321106Training Epoch: 6 | iteration: 63/262 | Loss: 0.6520658135414124Training Epoch: 6 | iteration: 64/262 | Loss: 0.6054009199142456Training Epoch: 6 | iteration: 65/262 | Loss: 0.6202090382575989Training Epoch: 6 | iteration: 66/262 | Loss: 0.6902545690536499Training Epoch: 6 | iteration: 67/262 | Loss: 0.5610253810882568Training Epoch: 6 | iteration: 68/262 | Loss: 0.6225669384002686Training Epoch: 6 | iteration: 69/262 | Loss: 0.5909755229949951Training Epoch: 6 | iteration: 70/262 | Loss: 0.7488043308258057Training Epoch: 6 | iteration: 71/262 | Loss: 0.6056151390075684Training Epoch: 6 | iteration: 72/262 | Loss: 0.593891441822052Training Epoch: 6 | iteration: 73/262 | Loss: 0.6038001775741577Training Epoch: 6 | iteration: 74/262 | Loss: 0.7003097534179688Training Epoch: 6 | iteration: 75/262 | Loss: 0.5364916324615479Training Epoch: 6 | iteration: 76/262 | Loss: 0.6703751087188721Training Epoch: 6 | iteration: 77/262 | Loss: 0.5639463663101196Training Epoch: 6 | iteration: 78/262 | Loss: 0.614173173904419Training Epoch: 6 | iteration: 79/262 | Loss: 0.5820587873458862Training Epoch: 6 | iteration: 80/262 | Loss: 0.6405413150787354Training Epoch: 6 | iteration: 81/262 | Loss: 0.6711740493774414Training Epoch: 6 | iteration: 82/262 | Loss: 0.658985435962677Training Epoch: 6 | iteration: 83/262 | Loss: 0.5981870889663696Training Epoch: 6 | iteration: 84/262 | Loss: 0.6536731719970703Training Epoch: 6 | iteration: 85/262 | Loss: 0.5853893756866455Training Epoch: 6 | iteration: 86/262 | Loss: 0.6108613014221191Training Epoch: 6 | iteration: 87/262 | Loss: 0.6573008298873901Training Epoch: 6 | iteration: 88/262 | Loss: 0.6264114379882812Training Epoch: 6 | iteration: 89/262 | Loss: 0.5779439210891724Training Epoch: 6 | iteration: 90/262 | Loss: 0.6483703255653381Training Epoch: 6 | iteration: 91/262 | Loss: 0.6300747990608215Training Epoch: 6 | iteration: 92/262 | Loss: 0.64618980884552Training Epoch: 6 | iteration: 93/262 | Loss: 0.6255327463150024Training Epoch: 6 | iteration: 94/262 | Loss: 0.6042883396148682Training Epoch: 6 | iteration: 95/262 | Loss: 0.6848822832107544Training Epoch: 6 | iteration: 96/262 | Loss: 0.6275596618652344Training Epoch: 6 | iteration: 97/262 | Loss: 0.6431269645690918Training Epoch: 6 | iteration: 98/262 | Loss: 0.6317392587661743Training Epoch: 6 | iteration: 99/262 | Loss: 0.5644140243530273Training Epoch: 6 | iteration: 100/262 | Loss: 0.5649444460868835Training Epoch: 6 | iteration: 101/262 | Loss: 0.5903534889221191Training Epoch: 6 | iteration: 102/262 | Loss: 0.5698610544204712Training Epoch: 6 | iteration: 103/262 | Loss: 0.7008476257324219Training Epoch: 6 | iteration: 104/262 | Loss: 0.7073414325714111Training Epoch: 6 | iteration: 105/262 | Loss: 0.5940656661987305Training Epoch: 6 | iteration: 106/262 | Loss: 0.5820660591125488Training Epoch: 6 | iteration: 107/262 | Loss: 0.5909110307693481Training Epoch: 6 | iteration: 108/262 | Loss: 0.6006734371185303Training Epoch: 6 | iteration: 109/262 | Loss: 0.5789728760719299Training Epoch: 6 | iteration: 110/262 | Loss: 0.6090416312217712Training Epoch: 6 | iteration: 111/262 | Loss: 0.657172679901123Training Epoch: 6 | iteration: 112/262 | Loss: 0.6450111269950867Training Epoch: 6 | iteration: 113/262 | Loss: 0.5790278911590576Training Epoch: 6 | iteration: 114/262 | Loss: 0.6837915778160095Training Epoch: 6 | iteration: 115/262 | Loss: 0.6321344971656799Training Epoch: 6 | iteration: 116/262 | Loss: 0.6405808329582214Training Epoch: 6 | iteration: 117/262 | Loss: 0.7034651041030884Training Epoch: 6 | iteration: 118/262 | Loss: 0.6608802676200867Training Epoch: 6 | iteration: 119/262 | Loss: 0.6739656329154968Training Epoch: 6 | iteration: 120/262 | Loss: 0.6479625701904297Training Epoch: 6 | iteration: 121/262 | Loss: 0.6519632935523987Training Epoch: 6 | iteration: 122/262 | Loss: 0.5810785293579102Training Epoch: 6 | iteration: 123/262 | Loss: 0.6703047156333923Training Epoch: 6 | iteration: 124/262 | Loss: 0.5873861908912659Training Epoch: 6 | iteration: 125/262 | Loss: 0.6368074417114258Training Epoch: 6 | iteration: 126/262 | Loss: 0.5935695171356201Training Epoch: 6 | iteration: 127/262 | Loss: 0.5929170846939087Training Epoch: 6 | iteration: 128/262 | Loss: 0.6512397527694702Training Epoch: 6 | iteration: 129/262 | Loss: 0.7307571172714233Training Epoch: 6 | iteration: 130/262 | Loss: 0.6234316825866699Training Epoch: 6 | iteration: 131/262 | Loss: 0.5959649682044983Training Epoch: 6 | iteration: 132/262 | Loss: 0.6495888829231262Training Epoch: 6 | iteration: 133/262 | Loss: 0.5880008935928345Training Epoch: 6 | iteration: 134/262 | Loss: 0.5690211653709412Training Epoch: 6 | iteration: 135/262 | Loss: 0.7178040146827698Training Epoch: 6 | iteration: 136/262 | Loss: 0.6611315011978149Training Epoch: 6 | iteration: 137/262 | Loss: 0.5324418544769287Training Epoch: 6 | iteration: 138/262 | Loss: 0.6180850267410278Training Epoch: 6 | iteration: 139/262 | Loss: 0.5959267616271973Training Epoch: 6 | iteration: 140/262 | Loss: 0.5828121304512024Training Epoch: 6 | iteration: 141/262 | Loss: 0.6108537912368774Training Epoch: 6 | iteration: 142/262 | Loss: 0.6868419647216797Training Epoch: 6 | iteration: 143/262 | Loss: 0.5470674633979797Training Epoch: 6 | iteration: 144/262 | Loss: 0.5697628259658813Training Epoch: 6 | iteration: 145/262 | Loss: 0.6430953145027161Training Epoch: 6 | iteration: 146/262 | Loss: 0.7047325968742371Training Epoch: 6 | iteration: 147/262 | Loss: 0.577838659286499Training Epoch: 6 | iteration: 148/262 | Loss: 0.5984028577804565Training Epoch: 6 | iteration: 149/262 | Loss: 0.5968837738037109Training Epoch: 6 | iteration: 150/262 | Loss: 0.5654251575469971Training Epoch: 6 | iteration: 151/262 | Loss: 0.670490562915802Training Epoch: 6 | iteration: 152/262 | Loss: 0.6291484236717224Training Epoch: 6 | iteration: 153/262 | Loss: 0.6564093828201294Training Epoch: 6 | iteration: 154/262 | Loss: 0.6055072546005249Training Epoch: 6 | iteration: 155/262 | Loss: 0.6215556859970093Training Epoch: 6 | iteration: 156/262 | Loss: 0.5648854970932007Training Epoch: 6 | iteration: 157/262 | Loss: 0.6377494931221008Training Epoch: 6 | iteration: 158/262 | Loss: 0.5707045793533325Training Epoch: 6 | iteration: 159/262 | Loss: 0.6323874592781067Training Epoch: 6 | iteration: 160/262 | Loss: 0.6701348423957825Training Epoch: 6 | iteration: 161/262 | Loss: 0.6277586221694946Training Epoch: 6 | iteration: 162/262 | Loss: 0.7075586318969727Training Epoch: 6 | iteration: 163/262 | Loss: 0.6363394260406494Training Epoch: 6 | iteration: 164/262 | Loss: 0.6456782221794128Training Epoch: 6 | iteration: 165/262 | Loss: 0.6069315671920776Training Epoch: 6 | iteration: 166/262 | Loss: 0.6380998492240906Training Epoch: 6 | iteration: 167/262 | Loss: 0.6720922589302063Training Epoch: 6 | iteration: 168/262 | Loss: 0.5543580651283264Training Epoch: 6 | iteration: 169/262 | Loss: 0.6616271734237671Training Epoch: 6 | iteration: 170/262 | Loss: 0.6650714874267578Training Epoch: 6 | iteration: 171/262 | Loss: 0.6224907636642456Training Epoch: 6 | iteration: 172/262 | Loss: 0.6519327163696289Training Epoch: 6 | iteration: 173/262 | Loss: 0.684558093547821Training Epoch: 6 | iteration: 174/262 | Loss: 0.6365718841552734Training Epoch: 6 | iteration: 175/262 | Loss: 0.6043572425842285Training Epoch: 6 | iteration: 176/262 | Loss: 0.637980580329895Training Epoch: 6 | iteration: 177/262 | Loss: 0.677830696105957Training Epoch: 6 | iteration: 178/262 | Loss: 0.6357188820838928Training Epoch: 6 | iteration: 179/262 | Loss: 0.6354179382324219Training Epoch: 6 | iteration: 180/262 | Loss: 0.7100382447242737Training Epoch: 6 | iteration: 181/262 | Loss: 0.5886987447738647Training Epoch: 6 | iteration: 182/262 | Loss: 0.5547569990158081Training Epoch: 6 | iteration: 183/262 | Loss: 0.6013386845588684Training Epoch: 6 | iteration: 184/262 | Loss: 0.598706066608429Training Epoch: 6 | iteration: 185/262 | Loss: 0.688901960849762Training Epoch: 6 | iteration: 186/262 | Loss: 0.5688410997390747Training Epoch: 6 | iteration: 187/262 | Loss: 0.6511781811714172Training Epoch: 6 | iteration: 188/262 | Loss: 0.651526927947998Training Epoch: 6 | iteration: 189/262 | Loss: 0.633301854133606Training Epoch: 6 | iteration: 190/262 | Loss: 0.6947697997093201Training Epoch: 6 | iteration: 191/262 | Loss: 0.5724732875823975Training Epoch: 6 | iteration: 192/262 | Loss: 0.6163508892059326Training Epoch: 6 | iteration: 193/262 | Loss: 0.5680562257766724Training Epoch: 6 | iteration: 194/262 | Loss: 0.6650238633155823Training Epoch: 6 | iteration: 195/262 | Loss: 0.5789133310317993Training Epoch: 6 | iteration: 196/262 | Loss: 0.6294282674789429Training Epoch: 6 | iteration: 197/262 | Loss: 0.5551401376724243Training Epoch: 6 | iteration: 198/262 | Loss: 0.6906845569610596Training Epoch: 6 | iteration: 199/262 | Loss: 0.6558096408843994Training Epoch: 6 | iteration: 200/262 | Loss: 0.6651712656021118Training Epoch: 6 | iteration: 201/262 | Loss: 0.5762529969215393Training Epoch: 6 | iteration: 202/262 | Loss: 0.6203550100326538Training Epoch: 6 | iteration: 203/262 | Loss: 0.6808617115020752Training Epoch: 6 | iteration: 204/262 | Loss: 0.6714847087860107Training Epoch: 6 | iteration: 205/262 | Loss: 0.6291593313217163Training Epoch: 6 | iteration: 206/262 | Loss: 0.7119044065475464Training Epoch: 6 | iteration: 207/262 | Loss: 0.641984224319458Training Epoch: 6 | iteration: 208/262 | Loss: 0.5672547817230225Training Epoch: 6 | iteration: 209/262 | Loss: 0.5791940093040466Training Epoch: 6 | iteration: 210/262 | Loss: 0.6443502902984619Training Epoch: 6 | iteration: 211/262 | Loss: 0.6092616319656372Training Epoch: 6 | iteration: 212/262 | Loss: 0.6378604173660278Training Epoch: 6 | iteration: 213/262 | Loss: 0.6304755210876465Training Epoch: 6 | iteration: 214/262 | Loss: 0.5592019557952881Training Epoch: 6 | iteration: 215/262 | Loss: 0.6796354055404663Training Epoch: 6 | iteration: 216/262 | Loss: 0.6516215205192566Training Epoch: 6 | iteration: 217/262 | Loss: 0.5791983008384705Training Epoch: 6 | iteration: 218/262 | Loss: 0.5941375494003296Training Epoch: 6 | iteration: 219/262 | Loss: 0.6750476360321045Training Epoch: 6 | iteration: 220/262 | Loss: 0.6116161346435547Training Epoch: 6 | iteration: 221/262 | Loss: 0.6151939630508423Training Epoch: 6 | iteration: 222/262 | Loss: 0.6782503724098206Training Epoch: 6 | iteration: 223/262 | Loss: 0.6150806546211243Training Epoch: 6 | iteration: 224/262 | Loss: 0.6465532779693604Training Epoch: 6 | iteration: 225/262 | Loss: 0.6100703477859497Training Epoch: 6 | iteration: 226/262 | Loss: 0.6726983785629272Training Epoch: 6 | iteration: 227/262 | Loss: 0.5760855674743652Training Epoch: 6 | iteration: 228/262 | Loss: 0.6361231207847595Training Epoch: 6 | iteration: 229/262 | Loss: 0.6444976329803467Training Epoch: 6 | iteration: 230/262 | Loss: 0.6085542440414429Training Epoch: 6 | iteration: 231/262 | Loss: 0.6753288507461548Training Epoch: 6 | iteration: 232/262 | Loss: 0.5726620554924011Training Epoch: 6 | iteration: 233/262 | Loss: 0.5887820720672607Training Epoch: 6 | iteration: 234/262 | Loss: 0.7358107566833496Training Epoch: 6 | iteration: 235/262 | Loss: 0.6733245253562927Training Epoch: 6 | iteration: 236/262 | Loss: 0.5916715860366821Training Epoch: 6 | iteration: 237/262 | Loss: 0.591171145439148Training Epoch: 6 | iteration: 238/262 | Loss: 0.5894653797149658Training Epoch: 6 | iteration: 239/262 | Loss: 0.597082257270813Training Epoch: 6 | iteration: 240/262 | Loss: 0.6892675161361694Training Epoch: 6 | iteration: 241/262 | Loss: 0.553842306137085Training Epoch: 6 | iteration: 242/262 | Loss: 0.6341723799705505Training Epoch: 6 | iteration: 243/262 | Loss: 0.5968459844589233Training Epoch: 6 | iteration: 244/262 | Loss: 0.6673394441604614Training Epoch: 6 | iteration: 245/262 | Loss: 0.5906766653060913Training Epoch: 6 | iteration: 246/262 | Loss: 0.6688731908798218Training Epoch: 6 | iteration: 247/262 | Loss: 0.6590979099273682Training Epoch: 6 | iteration: 248/262 | Loss: 0.7186964750289917Training Epoch: 6 | iteration: 249/262 | Loss: 0.6660021543502808Training Epoch: 6 | iteration: 250/262 | Loss: 0.6352323293685913Training Epoch: 6 | iteration: 251/262 | Loss: 0.6160932779312134Training Epoch: 6 | iteration: 252/262 | Loss: 0.6230531930923462Training Epoch: 6 | iteration: 253/262 | Loss: 0.6090660095214844Training Epoch: 6 | iteration: 254/262 | Loss: 0.6065760254859924Training Epoch: 6 | iteration: 255/262 | Loss: 0.6332152485847473Training Epoch: 6 | iteration: 256/262 | Loss: 0.6538266539573669Training Epoch: 6 | iteration: 257/262 | Loss: 0.6144102811813354Training Epoch: 6 | iteration: 258/262 | Loss: 0.7511945962905884Training Epoch: 6 | iteration: 259/262 | Loss: 0.5697345733642578Training Epoch: 6 | iteration: 260/262 | Loss: 0.7025081515312195Training Epoch: 6 | iteration: 261/262 | Loss: 0.5962260365486145Validating Epoch: 6 | iteration: 0/66 | Loss: 0.5791763067245483Validating Epoch: 6 | iteration: 1/66 | Loss: 0.6917146444320679Validating Epoch: 6 | iteration: 2/66 | Loss: 0.611226499080658Validating Epoch: 6 | iteration: 3/66 | Loss: 0.5941904783248901Validating Epoch: 6 | iteration: 4/66 | Loss: 0.5896528363227844Validating Epoch: 6 | iteration: 5/66 | Loss: 0.6204489469528198Validating Epoch: 6 | iteration: 6/66 | Loss: 0.6384638547897339Validating Epoch: 6 | iteration: 7/66 | Loss: 0.6488017439842224Validating Epoch: 6 | iteration: 8/66 | Loss: 0.6643618941307068Validating Epoch: 6 | iteration: 9/66 | Loss: 0.7104654908180237Validating Epoch: 6 | iteration: 10/66 | Loss: 0.6139786243438721Validating Epoch: 6 | iteration: 11/66 | Loss: 0.6322792172431946Validating Epoch: 6 | iteration: 12/66 | Loss: 0.6086522936820984Validating Epoch: 6 | iteration: 13/66 | Loss: 0.6945837736129761Validating Epoch: 6 | iteration: 14/66 | Loss: 0.6593818664550781Validating Epoch: 6 | iteration: 15/66 | Loss: 0.643478512763977Validating Epoch: 6 | iteration: 16/66 | Loss: 0.6393355131149292Validating Epoch: 6 | iteration: 17/66 | Loss: 0.6200771331787109Validating Epoch: 6 | iteration: 18/66 | Loss: 0.6104775667190552Validating Epoch: 6 | iteration: 19/66 | Loss: 0.6398177742958069Validating Epoch: 6 | iteration: 20/66 | Loss: 0.7510426044464111Validating Epoch: 6 | iteration: 21/66 | Loss: 0.635672926902771Validating Epoch: 6 | iteration: 22/66 | Loss: 0.6759678721427917Validating Epoch: 6 | iteration: 23/66 | Loss: 0.7103618383407593Validating Epoch: 6 | iteration: 24/66 | Loss: 0.6917556524276733Validating Epoch: 6 | iteration: 25/66 | Loss: 0.6527075171470642Validating Epoch: 6 | iteration: 26/66 | Loss: 0.7112517356872559Validating Epoch: 6 | iteration: 27/66 | Loss: 0.6478257179260254Validating Epoch: 6 | iteration: 28/66 | Loss: 0.5661683082580566Validating Epoch: 6 | iteration: 29/66 | Loss: 0.6627494692802429Validating Epoch: 6 | iteration: 30/66 | Loss: 0.6398195624351501Validating Epoch: 6 | iteration: 31/66 | Loss: 0.5864571928977966Validating Epoch: 6 | iteration: 32/66 | Loss: 0.5785521268844604Validating Epoch: 6 | iteration: 33/66 | Loss: 0.7551721334457397Validating Epoch: 6 | iteration: 34/66 | Loss: 0.6253552436828613Validating Epoch: 6 | iteration: 35/66 | Loss: 0.6400827169418335Validating Epoch: 6 | iteration: 36/66 | Loss: 0.5999729633331299Validating Epoch: 6 | iteration: 37/66 | Loss: 0.6508730053901672Validating Epoch: 6 | iteration: 38/66 | Loss: 0.6081191897392273Validating Epoch: 6 | iteration: 39/66 | Loss: 0.642402708530426Validating Epoch: 6 | iteration: 40/66 | Loss: 0.6902951002120972Validating Epoch: 6 | iteration: 41/66 | Loss: 0.6379303336143494Validating Epoch: 6 | iteration: 42/66 | Loss: 0.6929131746292114Validating Epoch: 6 | iteration: 43/66 | Loss: 0.6684583425521851Validating Epoch: 6 | iteration: 44/66 | Loss: 0.6081265807151794Validating Epoch: 6 | iteration: 45/66 | Loss: 0.6530753374099731Validating Epoch: 6 | iteration: 46/66 | Loss: 0.6002980470657349Validating Epoch: 6 | iteration: 47/66 | Loss: 0.61896812915802Validating Epoch: 6 | iteration: 48/66 | Loss: 0.6360121965408325Validating Epoch: 6 | iteration: 49/66 | Loss: 0.6505129933357239Validating Epoch: 6 | iteration: 50/66 | Loss: 0.5942751169204712Validating Epoch: 6 | iteration: 51/66 | Loss: 0.6249246597290039Validating Epoch: 6 | iteration: 52/66 | Loss: 0.5512864589691162Validating Epoch: 6 | iteration: 53/66 | Loss: 0.6277999877929688Validating Epoch: 6 | iteration: 54/66 | Loss: 0.7431936860084534Validating Epoch: 6 | iteration: 55/66 | Loss: 0.6037741899490356Validating Epoch: 6 | iteration: 56/66 | Loss: 0.585380494594574Validating Epoch: 6 | iteration: 57/66 | Loss: 0.6078741550445557Validating Epoch: 6 | iteration: 58/66 | Loss: 0.665060818195343Validating Epoch: 6 | iteration: 59/66 | Loss: 0.5989052057266235Validating Epoch: 6 | iteration: 60/66 | Loss: 0.6661982536315918Validating Epoch: 6 | iteration: 61/66 | Loss: 0.7146947383880615Validating Epoch: 6 | iteration: 62/66 | Loss: 0.6171320080757141Validating Epoch: 6 | iteration: 63/66 | Loss: 0.6427704095840454Validating Epoch: 6 | iteration: 64/66 | Loss: 0.6237605810165405Validating Epoch: 6 | iteration: 65/66 | Loss: 0.6174963712692261Temperature:  0.5
Sampling: iteration: 0/51{'Validity': 0.97265625, 'Novelty': 1.0, 'Uniqueness': 0.9899598393574297}
Training Epoch: 7 | iteration: 0/262 | Loss: 0.609237551689148Training Epoch: 7 | iteration: 1/262 | Loss: 0.6482914686203003Training Epoch: 7 | iteration: 2/262 | Loss: 0.5353280305862427Training Epoch: 7 | iteration: 3/262 | Loss: 0.6409910917282104Training Epoch: 7 | iteration: 4/262 | Loss: 0.5877559185028076Training Epoch: 7 | iteration: 5/262 | Loss: 0.5747382640838623Training Epoch: 7 | iteration: 6/262 | Loss: 0.5377264022827148Training Epoch: 7 | iteration: 7/262 | Loss: 0.5592278242111206Training Epoch: 7 | iteration: 8/262 | Loss: 0.6434826850891113Training Epoch: 7 | iteration: 9/262 | Loss: 0.5311952829360962Training Epoch: 7 | iteration: 10/262 | Loss: 0.6363449096679688Training Epoch: 7 | iteration: 11/262 | Loss: 0.6050792932510376Training Epoch: 7 | iteration: 12/262 | Loss: 0.599415123462677Training Epoch: 7 | iteration: 13/262 | Loss: 0.5805628299713135Training Epoch: 7 | iteration: 14/262 | Loss: 0.5977097749710083Training Epoch: 7 | iteration: 15/262 | Loss: 0.5898705720901489Training Epoch: 7 | iteration: 16/262 | Loss: 0.5496252775192261Training Epoch: 7 | iteration: 17/262 | Loss: 0.6550248861312866Training Epoch: 7 | iteration: 18/262 | Loss: 0.6095596551895142Training Epoch: 7 | iteration: 19/262 | Loss: 0.6048518419265747Training Epoch: 7 | iteration: 20/262 | Loss: 0.5797264575958252Training Epoch: 7 | iteration: 21/262 | Loss: 0.6220575571060181Training Epoch: 7 | iteration: 22/262 | Loss: 0.6158689260482788Training Epoch: 7 | iteration: 23/262 | Loss: 0.5955029726028442Training Epoch: 7 | iteration: 24/262 | Loss: 0.6832212209701538Training Epoch: 7 | iteration: 25/262 | Loss: 0.6360335350036621Training Epoch: 7 | iteration: 26/262 | Loss: 0.5920823812484741Training Epoch: 7 | iteration: 27/262 | Loss: 0.5606511831283569Training Epoch: 7 | iteration: 28/262 | Loss: 0.6339664459228516Training Epoch: 7 | iteration: 29/262 | Loss: 0.6019147634506226Training Epoch: 7 | iteration: 30/262 | Loss: 0.6156031489372253Training Epoch: 7 | iteration: 31/262 | Loss: 0.6188269853591919Training Epoch: 7 | iteration: 32/262 | Loss: 0.5773512125015259Training Epoch: 7 | iteration: 33/262 | Loss: 0.6047776937484741Training Epoch: 7 | iteration: 34/262 | Loss: 0.6413373947143555Training Epoch: 7 | iteration: 35/262 | Loss: 0.6246373057365417Training Epoch: 7 | iteration: 36/262 | Loss: 0.663862407207489Training Epoch: 7 | iteration: 37/262 | Loss: 0.6472101211547852Training Epoch: 7 | iteration: 38/262 | Loss: 0.607921838760376Training Epoch: 7 | iteration: 39/262 | Loss: 0.6169986724853516Training Epoch: 7 | iteration: 40/262 | Loss: 0.6171382665634155Training Epoch: 7 | iteration: 41/262 | Loss: 0.6108775734901428Training Epoch: 7 | iteration: 42/262 | Loss: 0.6511807441711426Training Epoch: 7 | iteration: 43/262 | Loss: 0.6016068458557129Training Epoch: 7 | iteration: 44/262 | Loss: 0.675148606300354Training Epoch: 7 | iteration: 45/262 | Loss: 0.6275537014007568Training Epoch: 7 | iteration: 46/262 | Loss: 0.5736442804336548Training Epoch: 7 | iteration: 47/262 | Loss: 0.5802711248397827Training Epoch: 7 | iteration: 48/262 | Loss: 0.5753912925720215Training Epoch: 7 | iteration: 49/262 | Loss: 0.6286624670028687Training Epoch: 7 | iteration: 50/262 | Loss: 0.5898019075393677Training Epoch: 7 | iteration: 51/262 | Loss: 0.6088815927505493Training Epoch: 7 | iteration: 52/262 | Loss: 0.589576244354248Training Epoch: 7 | iteration: 53/262 | Loss: 0.6106001138687134Training Epoch: 7 | iteration: 54/262 | Loss: 0.6403318643569946Training Epoch: 7 | iteration: 55/262 | Loss: 0.5760366916656494Training Epoch: 7 | iteration: 56/262 | Loss: 0.5730950832366943Training Epoch: 7 | iteration: 57/262 | Loss: 0.5593563318252563Training Epoch: 7 | iteration: 58/262 | Loss: 0.6252808570861816Training Epoch: 7 | iteration: 59/262 | Loss: 0.6096580028533936Training Epoch: 7 | iteration: 60/262 | Loss: 0.5354557037353516Training Epoch: 7 | iteration: 61/262 | Loss: 0.6478190422058105Training Epoch: 7 | iteration: 62/262 | Loss: 0.612877607345581Training Epoch: 7 | iteration: 63/262 | Loss: 0.6210455894470215Training Epoch: 7 | iteration: 64/262 | Loss: 0.6130648851394653Training Epoch: 7 | iteration: 65/262 | Loss: 0.6351848840713501Training Epoch: 7 | iteration: 66/262 | Loss: 0.5738168358802795Training Epoch: 7 | iteration: 67/262 | Loss: 0.5954852104187012Training Epoch: 7 | iteration: 68/262 | Loss: 0.635276198387146Training Epoch: 7 | iteration: 69/262 | Loss: 0.6270513534545898Training Epoch: 7 | iteration: 70/262 | Loss: 0.566312313079834Training Epoch: 7 | iteration: 71/262 | Loss: 0.5088940858840942Training Epoch: 7 | iteration: 72/262 | Loss: 0.580494225025177Training Epoch: 7 | iteration: 73/262 | Loss: 0.590872585773468Training Epoch: 7 | iteration: 74/262 | Loss: 0.5896575450897217Training Epoch: 7 | iteration: 75/262 | Loss: 0.6196553707122803Training Epoch: 7 | iteration: 76/262 | Loss: 0.6305811405181885Training Epoch: 7 | iteration: 77/262 | Loss: 0.7041405439376831Training Epoch: 7 | iteration: 78/262 | Loss: 0.5882188081741333Training Epoch: 7 | iteration: 79/262 | Loss: 0.6379526853561401Training Epoch: 7 | iteration: 80/262 | Loss: 0.6232597827911377Training Epoch: 7 | iteration: 81/262 | Loss: 0.5888383388519287Training Epoch: 7 | iteration: 82/262 | Loss: 0.5930464267730713Training Epoch: 7 | iteration: 83/262 | Loss: 0.56467205286026Training Epoch: 7 | iteration: 84/262 | Loss: 0.6480004787445068Training Epoch: 7 | iteration: 85/262 | Loss: 0.5904960036277771Training Epoch: 7 | iteration: 86/262 | Loss: 0.6652428507804871Training Epoch: 7 | iteration: 87/262 | Loss: 0.5719426870346069Training Epoch: 7 | iteration: 88/262 | Loss: 0.5853559970855713Training Epoch: 7 | iteration: 89/262 | Loss: 0.5874930024147034Training Epoch: 7 | iteration: 90/262 | Loss: 0.6719247102737427Training Epoch: 7 | iteration: 91/262 | Loss: 0.5863980650901794Training Epoch: 7 | iteration: 92/262 | Loss: 0.5864390134811401Training Epoch: 7 | iteration: 93/262 | Loss: 0.5963088274002075Training Epoch: 7 | iteration: 94/262 | Loss: 0.6025092601776123Training Epoch: 7 | iteration: 95/262 | Loss: 0.5482444763183594Training Epoch: 7 | iteration: 96/262 | Loss: 0.5425169467926025Training Epoch: 7 | iteration: 97/262 | Loss: 0.6157046556472778Training Epoch: 7 | iteration: 98/262 | Loss: 0.543997049331665Training Epoch: 7 | iteration: 99/262 | Loss: 0.6405571699142456Training Epoch: 7 | iteration: 100/262 | Loss: 0.557678759098053Training Epoch: 7 | iteration: 101/262 | Loss: 0.673513650894165Training Epoch: 7 | iteration: 102/262 | Loss: 0.6765850782394409Training Epoch: 7 | iteration: 103/262 | Loss: 0.6445087194442749Training Epoch: 7 | iteration: 104/262 | Loss: 0.635269284248352Training Epoch: 7 | iteration: 105/262 | Loss: 0.5780328512191772Training Epoch: 7 | iteration: 106/262 | Loss: 0.6522238254547119Training Epoch: 7 | iteration: 107/262 | Loss: 0.669218897819519Training Epoch: 7 | iteration: 108/262 | Loss: 0.6106481552124023Training Epoch: 7 | iteration: 109/262 | Loss: 0.5939092636108398Training Epoch: 7 | iteration: 110/262 | Loss: 0.698164165019989Training Epoch: 7 | iteration: 111/262 | Loss: 0.6584160327911377Training Epoch: 7 | iteration: 112/262 | Loss: 0.5612781047821045Training Epoch: 7 | iteration: 113/262 | Loss: 0.5254589915275574Training Epoch: 7 | iteration: 114/262 | Loss: 0.6254101991653442Training Epoch: 7 | iteration: 115/262 | Loss: 0.5870240330696106Training Epoch: 7 | iteration: 116/262 | Loss: 0.604587197303772Training Epoch: 7 | iteration: 117/262 | Loss: 0.5749605298042297Training Epoch: 7 | iteration: 118/262 | Loss: 0.5813146829605103Training Epoch: 7 | iteration: 119/262 | Loss: 0.6346285343170166Training Epoch: 7 | iteration: 120/262 | Loss: 0.5691798329353333Training Epoch: 7 | iteration: 121/262 | Loss: 0.5838359594345093Training Epoch: 7 | iteration: 122/262 | Loss: 0.6041967868804932Training Epoch: 7 | iteration: 123/262 | Loss: 0.6044065356254578Training Epoch: 7 | iteration: 124/262 | Loss: 0.5586192607879639Training Epoch: 7 | iteration: 125/262 | Loss: 0.6048670411109924Training Epoch: 7 | iteration: 126/262 | Loss: 0.5945970416069031Training Epoch: 7 | iteration: 127/262 | Loss: 0.5905689001083374Training Epoch: 7 | iteration: 128/262 | Loss: 0.5244496464729309Training Epoch: 7 | iteration: 129/262 | Loss: 0.6559337377548218Training Epoch: 7 | iteration: 130/262 | Loss: 0.6089728474617004Training Epoch: 7 | iteration: 131/262 | Loss: 0.6326544284820557Training Epoch: 7 | iteration: 132/262 | Loss: 0.6010900139808655Training Epoch: 7 | iteration: 133/262 | Loss: 0.6170575618743896Training Epoch: 7 | iteration: 134/262 | Loss: 0.6180887818336487Training Epoch: 7 | iteration: 135/262 | Loss: 0.6313947439193726Training Epoch: 7 | iteration: 136/262 | Loss: 0.5647867918014526Training Epoch: 7 | iteration: 137/262 | Loss: 0.6175601482391357Training Epoch: 7 | iteration: 138/262 | Loss: 0.5930728316307068Training Epoch: 7 | iteration: 139/262 | Loss: 0.576468288898468Training Epoch: 7 | iteration: 140/262 | Loss: 0.6454677581787109Training Epoch: 7 | iteration: 141/262 | Loss: 0.6475104689598083Training Epoch: 7 | iteration: 142/262 | Loss: 0.5836131572723389Training Epoch: 7 | iteration: 143/262 | Loss: 0.6521854400634766Training Epoch: 7 | iteration: 144/262 | Loss: 0.624024510383606Training Epoch: 7 | iteration: 145/262 | Loss: 0.5923717021942139Training Epoch: 7 | iteration: 146/262 | Loss: 0.5527195930480957Training Epoch: 7 | iteration: 147/262 | Loss: 0.5472060441970825Training Epoch: 7 | iteration: 148/262 | Loss: 0.6215845942497253Training Epoch: 7 | iteration: 149/262 | Loss: 0.6040963530540466Training Epoch: 7 | iteration: 150/262 | Loss: 0.6367316246032715Training Epoch: 7 | iteration: 151/262 | Loss: 0.6109268665313721Training Epoch: 7 | iteration: 152/262 | Loss: 0.6243021488189697Training Epoch: 7 | iteration: 153/262 | Loss: 0.5690668821334839Training Epoch: 7 | iteration: 154/262 | Loss: 0.6825503706932068Training Epoch: 7 | iteration: 155/262 | Loss: 0.6208183169364929Training Epoch: 7 | iteration: 156/262 | Loss: 0.6420693397521973Training Epoch: 7 | iteration: 157/262 | Loss: 0.6219019889831543Training Epoch: 7 | iteration: 158/262 | Loss: 0.583532452583313Training Epoch: 7 | iteration: 159/262 | Loss: 0.6095883250236511Training Epoch: 7 | iteration: 160/262 | Loss: 0.5797202587127686Training Epoch: 7 | iteration: 161/262 | Loss: 0.6385481953620911Training Epoch: 7 | iteration: 162/262 | Loss: 0.6557936668395996Training Epoch: 7 | iteration: 163/262 | Loss: 0.6171523332595825Training Epoch: 7 | iteration: 164/262 | Loss: 0.6138796806335449Training Epoch: 7 | iteration: 165/262 | Loss: 0.6195590496063232Training Epoch: 7 | iteration: 166/262 | Loss: 0.571739673614502Training Epoch: 7 | iteration: 167/262 | Loss: 0.6025073528289795Training Epoch: 7 | iteration: 168/262 | Loss: 0.5382348299026489Training Epoch: 7 | iteration: 169/262 | Loss: 0.6550142765045166Training Epoch: 7 | iteration: 170/262 | Loss: 0.5543094873428345Training Epoch: 7 | iteration: 171/262 | Loss: 0.5966403484344482Training Epoch: 7 | iteration: 172/262 | Loss: 0.6583018898963928Training Epoch: 7 | iteration: 173/262 | Loss: 0.7100495100021362Training Epoch: 7 | iteration: 174/262 | Loss: 0.5919743776321411Training Epoch: 7 | iteration: 175/262 | Loss: 0.6176618933677673Training Epoch: 7 | iteration: 176/262 | Loss: 0.6181128025054932Training Epoch: 7 | iteration: 177/262 | Loss: 0.6752101182937622Training Epoch: 7 | iteration: 178/262 | Loss: 0.5844930410385132Training Epoch: 7 | iteration: 179/262 | Loss: 0.650314450263977Training Epoch: 7 | iteration: 180/262 | Loss: 0.6003193855285645Training Epoch: 7 | iteration: 181/262 | Loss: 0.7064903378486633Training Epoch: 7 | iteration: 182/262 | Loss: 0.5846195220947266Training Epoch: 7 | iteration: 183/262 | Loss: 0.5924252271652222Training Epoch: 7 | iteration: 184/262 | Loss: 0.6422802805900574Training Epoch: 7 | iteration: 185/262 | Loss: 0.6299822330474854Training Epoch: 7 | iteration: 186/262 | Loss: 0.5471689105033875Training Epoch: 7 | iteration: 187/262 | Loss: 0.6243201494216919Training Epoch: 7 | iteration: 188/262 | Loss: 0.6315430998802185Training Epoch: 7 | iteration: 189/262 | Loss: 0.5722103714942932Training Epoch: 7 | iteration: 190/262 | Loss: 0.6383229494094849Training Epoch: 7 | iteration: 191/262 | Loss: 0.6113294959068298Training Epoch: 7 | iteration: 192/262 | Loss: 0.5905011296272278Training Epoch: 7 | iteration: 193/262 | Loss: 0.6729481220245361Training Epoch: 7 | iteration: 194/262 | Loss: 0.5494732856750488Training Epoch: 7 | iteration: 195/262 | Loss: 0.6397678852081299Training Epoch: 7 | iteration: 196/262 | Loss: 0.6326318383216858Training Epoch: 7 | iteration: 197/262 | Loss: 0.6253875494003296Training Epoch: 7 | iteration: 198/262 | Loss: 0.6661303639411926Training Epoch: 7 | iteration: 199/262 | Loss: 0.5987746715545654Training Epoch: 7 | iteration: 200/262 | Loss: 0.6245487928390503Training Epoch: 7 | iteration: 201/262 | Loss: 0.5258557796478271Training Epoch: 7 | iteration: 202/262 | Loss: 0.5933810472488403Training Epoch: 7 | iteration: 203/262 | Loss: 0.6846471428871155Training Epoch: 7 | iteration: 204/262 | Loss: 0.6193038821220398Training Epoch: 7 | iteration: 205/262 | Loss: 0.6485479474067688Training Epoch: 7 | iteration: 206/262 | Loss: 0.5710069537162781Training Epoch: 7 | iteration: 207/262 | Loss: 0.5982798337936401Training Epoch: 7 | iteration: 208/262 | Loss: 0.6226379871368408Training Epoch: 7 | iteration: 209/262 | Loss: 0.5619957447052002Training Epoch: 7 | iteration: 210/262 | Loss: 0.5498073101043701Training Epoch: 7 | iteration: 211/262 | Loss: 0.5910422205924988Training Epoch: 7 | iteration: 212/262 | Loss: 0.5906313061714172Training Epoch: 7 | iteration: 213/262 | Loss: 0.6180455684661865Training Epoch: 7 | iteration: 214/262 | Loss: 0.5813214778900146Training Epoch: 7 | iteration: 215/262 | Loss: 0.6695883274078369Training Epoch: 7 | iteration: 216/262 | Loss: 0.5830001831054688Training Epoch: 7 | iteration: 217/262 | Loss: 0.5923711657524109Training Epoch: 7 | iteration: 218/262 | Loss: 0.6111308336257935Training Epoch: 7 | iteration: 219/262 | Loss: 0.5329079031944275Training Epoch: 7 | iteration: 220/262 | Loss: 0.5535159111022949Training Epoch: 7 | iteration: 221/262 | Loss: 0.5963085889816284Training Epoch: 7 | iteration: 222/262 | Loss: 0.6041082739830017Training Epoch: 7 | iteration: 223/262 | Loss: 0.5724138021469116Training Epoch: 7 | iteration: 224/262 | Loss: 0.5950183868408203Training Epoch: 7 | iteration: 225/262 | Loss: 0.6451754570007324Training Epoch: 7 | iteration: 226/262 | Loss: 0.6507999897003174Training Epoch: 7 | iteration: 227/262 | Loss: 0.6314923763275146Training Epoch: 7 | iteration: 228/262 | Loss: 0.6062978506088257Training Epoch: 7 | iteration: 229/262 | Loss: 0.6351537704467773Training Epoch: 7 | iteration: 230/262 | Loss: 0.6395342946052551Training Epoch: 7 | iteration: 231/262 | Loss: 0.5638828873634338Training Epoch: 7 | iteration: 232/262 | Loss: 0.5929902791976929Training Epoch: 7 | iteration: 233/262 | Loss: 0.640721321105957Training Epoch: 7 | iteration: 234/262 | Loss: 0.6687461137771606Training Epoch: 7 | iteration: 235/262 | Loss: 0.6498928666114807Training Epoch: 7 | iteration: 236/262 | Loss: 0.700796902179718Training Epoch: 7 | iteration: 237/262 | Loss: 0.600928544998169Training Epoch: 7 | iteration: 238/262 | Loss: 0.5625914335250854Training Epoch: 7 | iteration: 239/262 | Loss: 0.6249445676803589Training Epoch: 7 | iteration: 240/262 | Loss: 0.6094756126403809Training Epoch: 7 | iteration: 241/262 | Loss: 0.6374270915985107Training Epoch: 7 | iteration: 242/262 | Loss: 0.5676785111427307Training Epoch: 7 | iteration: 243/262 | Loss: 0.6017625331878662Training Epoch: 7 | iteration: 244/262 | Loss: 0.5559895038604736Training Epoch: 7 | iteration: 245/262 | Loss: 0.6574569940567017Training Epoch: 7 | iteration: 246/262 | Loss: 0.5613101720809937Training Epoch: 7 | iteration: 247/262 | Loss: 0.5845904350280762Training Epoch: 7 | iteration: 248/262 | Loss: 0.5805193185806274Training Epoch: 7 | iteration: 249/262 | Loss: 0.6916353106498718Training Epoch: 7 | iteration: 250/262 | Loss: 0.6381601095199585Training Epoch: 7 | iteration: 251/262 | Loss: 0.5878449082374573Training Epoch: 7 | iteration: 252/262 | Loss: 0.5627090334892273Training Epoch: 7 | iteration: 253/262 | Loss: 0.6314244866371155Training Epoch: 7 | iteration: 254/262 | Loss: 0.6486706733703613Training Epoch: 7 | iteration: 255/262 | Loss: 0.6057796478271484Training Epoch: 7 | iteration: 256/262 | Loss: 0.6692540645599365Training Epoch: 7 | iteration: 257/262 | Loss: 0.6140478253364563Training Epoch: 7 | iteration: 258/262 | Loss: 0.6017388701438904Training Epoch: 7 | iteration: 259/262 | Loss: 0.5839564800262451Training Epoch: 7 | iteration: 260/262 | Loss: 0.6076212525367737Training Epoch: 7 | iteration: 261/262 | Loss: 0.5482112765312195Validating Epoch: 7 | iteration: 0/66 | Loss: 0.6675102114677429Validating Epoch: 7 | iteration: 1/66 | Loss: 0.7003076076507568Validating Epoch: 7 | iteration: 2/66 | Loss: 0.6228659749031067Validating Epoch: 7 | iteration: 3/66 | Loss: 0.6024776101112366Validating Epoch: 7 | iteration: 4/66 | Loss: 0.6486565470695496Validating Epoch: 7 | iteration: 5/66 | Loss: 0.7038955092430115Validating Epoch: 7 | iteration: 6/66 | Loss: 0.5895168781280518Validating Epoch: 7 | iteration: 7/66 | Loss: 0.6356064081192017Validating Epoch: 7 | iteration: 8/66 | Loss: 0.5784887671470642Validating Epoch: 7 | iteration: 9/66 | Loss: 0.6790415048599243Validating Epoch: 7 | iteration: 10/66 | Loss: 0.6385825872421265Validating Epoch: 7 | iteration: 11/66 | Loss: 0.6154487133026123Validating Epoch: 7 | iteration: 12/66 | Loss: 0.6067116856575012Validating Epoch: 7 | iteration: 13/66 | Loss: 0.6324484348297119Validating Epoch: 7 | iteration: 14/66 | Loss: 0.6784695386886597Validating Epoch: 7 | iteration: 15/66 | Loss: 0.6019231677055359Validating Epoch: 7 | iteration: 16/66 | Loss: 0.6156694889068604Validating Epoch: 7 | iteration: 17/66 | Loss: 0.682995617389679Validating Epoch: 7 | iteration: 18/66 | Loss: 0.6133872866630554Validating Epoch: 7 | iteration: 19/66 | Loss: 0.6634237766265869Validating Epoch: 7 | iteration: 20/66 | Loss: 0.5847625732421875Validating Epoch: 7 | iteration: 21/66 | Loss: 0.6131818294525146Validating Epoch: 7 | iteration: 22/66 | Loss: 0.5898176431655884Validating Epoch: 7 | iteration: 23/66 | Loss: 0.6364688873291016Validating Epoch: 7 | iteration: 24/66 | Loss: 0.7200724482536316Validating Epoch: 7 | iteration: 25/66 | Loss: 0.5814595222473145Validating Epoch: 7 | iteration: 26/66 | Loss: 0.6327320337295532Validating Epoch: 7 | iteration: 27/66 | Loss: 0.571343183517456Validating Epoch: 7 | iteration: 28/66 | Loss: 0.7124840021133423Validating Epoch: 7 | iteration: 29/66 | Loss: 0.7437330484390259Validating Epoch: 7 | iteration: 30/66 | Loss: 0.6415179967880249Validating Epoch: 7 | iteration: 31/66 | Loss: 0.6551817655563354Validating Epoch: 7 | iteration: 32/66 | Loss: 0.5642898082733154Validating Epoch: 7 | iteration: 33/66 | Loss: 0.6257073879241943Validating Epoch: 7 | iteration: 34/66 | Loss: 0.6220923066139221Validating Epoch: 7 | iteration: 35/66 | Loss: 0.6181920766830444Validating Epoch: 7 | iteration: 36/66 | Loss: 0.6318851709365845Validating Epoch: 7 | iteration: 37/66 | Loss: 0.6171902418136597Validating Epoch: 7 | iteration: 38/66 | Loss: 0.6097557544708252Validating Epoch: 7 | iteration: 39/66 | Loss: 0.6940062046051025Validating Epoch: 7 | iteration: 40/66 | Loss: 0.6143500208854675Validating Epoch: 7 | iteration: 41/66 | Loss: 0.6224586367607117Validating Epoch: 7 | iteration: 42/66 | Loss: 0.6736947298049927Validating Epoch: 7 | iteration: 43/66 | Loss: 0.5565636157989502Validating Epoch: 7 | iteration: 44/66 | Loss: 0.7022255063056946Validating Epoch: 7 | iteration: 45/66 | Loss: 0.587560772895813Validating Epoch: 7 | iteration: 46/66 | Loss: 0.631468653678894Validating Epoch: 7 | iteration: 47/66 | Loss: 0.6477265357971191Validating Epoch: 7 | iteration: 48/66 | Loss: 0.6623275279998779Validating Epoch: 7 | iteration: 49/66 | Loss: 0.6522635221481323Validating Epoch: 7 | iteration: 50/66 | Loss: 0.6155186891555786Validating Epoch: 7 | iteration: 51/66 | Loss: 0.6260654330253601Validating Epoch: 7 | iteration: 52/66 | Loss: 0.658076286315918Validating Epoch: 7 | iteration: 53/66 | Loss: 0.7005066871643066Validating Epoch: 7 | iteration: 54/66 | Loss: 0.7166160345077515Validating Epoch: 7 | iteration: 55/66 | Loss: 0.7612495422363281Validating Epoch: 7 | iteration: 56/66 | Loss: 0.5769398212432861Validating Epoch: 7 | iteration: 57/66 | Loss: 0.6009761691093445Validating Epoch: 7 | iteration: 58/66 | Loss: 0.5999008417129517Validating Epoch: 7 | iteration: 59/66 | Loss: 0.6708337068557739Validating Epoch: 7 | iteration: 60/66 | Loss: 0.5500231981277466Validating Epoch: 7 | iteration: 61/66 | Loss: 0.6969174742698669Validating Epoch: 7 | iteration: 62/66 | Loss: 0.7117441892623901Validating Epoch: 7 | iteration: 63/66 | Loss: 0.6608230471611023Validating Epoch: 7 | iteration: 64/66 | Loss: 0.6282402276992798Validating Epoch: 7 | iteration: 65/66 | Loss: 0.5629113912582397Temperature:  0.5
Sampling: iteration: 0/51{'Validity': 0.966796875, 'Novelty': 1.0, 'Uniqueness': 0.990909090909091}
Training Epoch: 8 | iteration: 0/262 | Loss: 0.5284256339073181Training Epoch: 8 | iteration: 1/262 | Loss: 0.6124662160873413Training Epoch: 8 | iteration: 2/262 | Loss: 0.6309815645217896Training Epoch: 8 | iteration: 3/262 | Loss: 0.5484534502029419Training Epoch: 8 | iteration: 4/262 | Loss: 0.6036698818206787Training Epoch: 8 | iteration: 5/262 | Loss: 0.592522382736206Training Epoch: 8 | iteration: 6/262 | Loss: 0.5565366744995117Training Epoch: 8 | iteration: 7/262 | Loss: 0.5727143287658691Training Epoch: 8 | iteration: 8/262 | Loss: 0.6311954259872437Training Epoch: 8 | iteration: 9/262 | Loss: 0.5846360921859741Training Epoch: 8 | iteration: 10/262 | Loss: 0.5843465924263Training Epoch: 8 | iteration: 11/262 | Loss: 0.6183161735534668Training Epoch: 8 | iteration: 12/262 | Loss: 0.7041693925857544Training Epoch: 8 | iteration: 13/262 | Loss: 0.565697968006134Training Epoch: 8 | iteration: 14/262 | Loss: 0.6074613332748413Training Epoch: 8 | iteration: 15/262 | Loss: 0.5713703036308289Training Epoch: 8 | iteration: 16/262 | Loss: 0.6049854755401611Training Epoch: 8 | iteration: 17/262 | Loss: 0.5734480023384094Training Epoch: 8 | iteration: 18/262 | Loss: 0.5590389370918274Training Epoch: 8 | iteration: 19/262 | Loss: 0.6228663921356201Training Epoch: 8 | iteration: 20/262 | Loss: 0.5964881181716919Training Epoch: 8 | iteration: 21/262 | Loss: 0.5941574573516846Training Epoch: 8 | iteration: 22/262 | Loss: 0.5654957294464111Training Epoch: 8 | iteration: 23/262 | Loss: 0.5920751094818115Training Epoch: 8 | iteration: 24/262 | Loss: 0.6077458262443542Training Epoch: 8 | iteration: 25/262 | Loss: 0.607463002204895Training Epoch: 8 | iteration: 26/262 | Loss: 0.6010608673095703Training Epoch: 8 | iteration: 27/262 | Loss: 0.6336023807525635Training Epoch: 8 | iteration: 28/262 | Loss: 0.590419352054596Training Epoch: 8 | iteration: 29/262 | Loss: 0.5935885310173035Training Epoch: 8 | iteration: 30/262 | Loss: 0.5612004399299622Training Epoch: 8 | iteration: 31/262 | Loss: 0.5725334882736206Training Epoch: 8 | iteration: 32/262 | Loss: 0.6479077339172363Training Epoch: 8 | iteration: 33/262 | Loss: 0.5997702479362488Training Epoch: 8 | iteration: 34/262 | Loss: 0.5669229030609131Training Epoch: 8 | iteration: 35/262 | Loss: 0.6455600261688232Training Epoch: 8 | iteration: 36/262 | Loss: 0.5962319374084473Training Epoch: 8 | iteration: 37/262 | Loss: 0.6203348636627197Training Epoch: 8 | iteration: 38/262 | Loss: 0.5674739480018616Training Epoch: 8 | iteration: 39/262 | Loss: 0.60160231590271Training Epoch: 8 | iteration: 40/262 | Loss: 0.5872803926467896Training Epoch: 8 | iteration: 41/262 | Loss: 0.6014280915260315Training Epoch: 8 | iteration: 42/262 | Loss: 0.5671671032905579Training Epoch: 8 | iteration: 43/262 | Loss: 0.6191524267196655Training Epoch: 8 | iteration: 44/262 | Loss: 0.6283785104751587Training Epoch: 8 | iteration: 45/262 | Loss: 0.6351770162582397Training Epoch: 8 | iteration: 46/262 | Loss: 0.5541903376579285Training Epoch: 8 | iteration: 47/262 | Loss: 0.6076821088790894Training Epoch: 8 | iteration: 48/262 | Loss: 0.5998685359954834Training Epoch: 8 | iteration: 49/262 | Loss: 0.6004194021224976Training Epoch: 8 | iteration: 50/262 | Loss: 0.5283259153366089Training Epoch: 8 | iteration: 51/262 | Loss: 0.6129665374755859Training Epoch: 8 | iteration: 52/262 | Loss: 0.55279541015625Training Epoch: 8 | iteration: 53/262 | Loss: 0.5927302837371826Training Epoch: 8 | iteration: 54/262 | Loss: 0.6073113083839417Training Epoch: 8 | iteration: 55/262 | Loss: 0.5997830033302307Training Epoch: 8 | iteration: 56/262 | Loss: 0.6230261325836182Training Epoch: 8 | iteration: 57/262 | Loss: 0.6082812547683716Training Epoch: 8 | iteration: 58/262 | Loss: 0.5473829507827759Training Epoch: 8 | iteration: 59/262 | Loss: 0.5693397521972656Training Epoch: 8 | iteration: 60/262 | Loss: 0.6057968139648438Training Epoch: 8 | iteration: 61/262 | Loss: 0.5658330917358398Training Epoch: 8 | iteration: 62/262 | Loss: 0.5457099080085754Training Epoch: 8 | iteration: 63/262 | Loss: 0.5885424613952637Training Epoch: 8 | iteration: 64/262 | Loss: 0.5619438290596008Training Epoch: 8 | iteration: 65/262 | Loss: 0.617089569568634Training Epoch: 8 | iteration: 66/262 | Loss: 0.5159159898757935Training Epoch: 8 | iteration: 67/262 | Loss: 0.6226094961166382Training Epoch: 8 | iteration: 68/262 | Loss: 0.5820385813713074Training Epoch: 8 | iteration: 69/262 | Loss: 0.5657999515533447Training Epoch: 8 | iteration: 70/262 | Loss: 0.5404849648475647Training Epoch: 8 | iteration: 71/262 | Loss: 0.5876510143280029Training Epoch: 8 | iteration: 72/262 | Loss: 0.5670846104621887Training Epoch: 8 | iteration: 73/262 | Loss: 0.6056824922561646Training Epoch: 8 | iteration: 74/262 | Loss: 0.6043781638145447Training Epoch: 8 | iteration: 75/262 | Loss: 0.6055094003677368Training Epoch: 8 | iteration: 76/262 | Loss: 0.6259232759475708Training Epoch: 8 | iteration: 77/262 | Loss: 0.6421760320663452Training Epoch: 8 | iteration: 78/262 | Loss: 0.5880815982818604Training Epoch: 8 | iteration: 79/262 | Loss: 0.6611204147338867Training Epoch: 8 | iteration: 80/262 | Loss: 0.6046043634414673Training Epoch: 8 | iteration: 81/262 | Loss: 0.662715494632721Training Epoch: 8 | iteration: 82/262 | Loss: 0.5923914909362793Training Epoch: 8 | iteration: 83/262 | Loss: 0.5396251678466797Training Epoch: 8 | iteration: 84/262 | Loss: 0.6133766174316406Training Epoch: 8 | iteration: 85/262 | Loss: 0.6262816190719604Training Epoch: 8 | iteration: 86/262 | Loss: 0.5333247780799866Training Epoch: 8 | iteration: 87/262 | Loss: 0.6017985343933105Training Epoch: 8 | iteration: 88/262 | Loss: 0.5982919335365295Training Epoch: 8 | iteration: 89/262 | Loss: 0.6325535178184509Training Epoch: 8 | iteration: 90/262 | Loss: 0.5933808088302612Training Epoch: 8 | iteration: 91/262 | Loss: 0.6412005424499512Training Epoch: 8 | iteration: 92/262 | Loss: 0.6210954785346985Training Epoch: 8 | iteration: 93/262 | Loss: 0.5787081718444824Training Epoch: 8 | iteration: 94/262 | Loss: 0.565750241279602Training Epoch: 8 | iteration: 95/262 | Loss: 0.5719188451766968Training Epoch: 8 | iteration: 96/262 | Loss: 0.5849435329437256Training Epoch: 8 | iteration: 97/262 | Loss: 0.6257724761962891Training Epoch: 8 | iteration: 98/262 | Loss: 0.5894975662231445Training Epoch: 8 | iteration: 99/262 | Loss: 0.6113933324813843Training Epoch: 8 | iteration: 100/262 | Loss: 0.5255846381187439Training Epoch: 8 | iteration: 101/262 | Loss: 0.6212853789329529Training Epoch: 8 | iteration: 102/262 | Loss: 0.6184842586517334Training Epoch: 8 | iteration: 103/262 | Loss: 0.6199637651443481Training Epoch: 8 | iteration: 104/262 | Loss: 0.6105777025222778Training Epoch: 8 | iteration: 105/262 | Loss: 0.6330305337905884Training Epoch: 8 | iteration: 106/262 | Loss: 0.6156221628189087Training Epoch: 8 | iteration: 107/262 | Loss: 0.5566055774688721Training Epoch: 8 | iteration: 108/262 | Loss: 0.596930980682373Training Epoch: 8 | iteration: 109/262 | Loss: 0.6128534078598022Training Epoch: 8 | iteration: 110/262 | Loss: 0.5997061729431152Training Epoch: 8 | iteration: 111/262 | Loss: 0.6380493640899658Training Epoch: 8 | iteration: 112/262 | Loss: 0.6437696218490601Training Epoch: 8 | iteration: 113/262 | Loss: 0.6401523351669312Training Epoch: 8 | iteration: 114/262 | Loss: 0.6843271255493164Training Epoch: 8 | iteration: 115/262 | Loss: 0.5765342712402344Training Epoch: 8 | iteration: 116/262 | Loss: 0.6270944476127625Training Epoch: 8 | iteration: 117/262 | Loss: 0.784712553024292Training Epoch: 8 | iteration: 118/262 | Loss: 0.6111752986907959Training Epoch: 8 | iteration: 119/262 | Loss: 0.5872162580490112Training Epoch: 8 | iteration: 120/262 | Loss: 0.5501023530960083Training Epoch: 8 | iteration: 121/262 | Loss: 0.5234997272491455Training Epoch: 8 | iteration: 122/262 | Loss: 0.5794147253036499Training Epoch: 8 | iteration: 123/262 | Loss: 0.5749759078025818Training Epoch: 8 | iteration: 124/262 | Loss: 0.557106614112854Training Epoch: 8 | iteration: 125/262 | Loss: 0.5985043048858643Training Epoch: 8 | iteration: 126/262 | Loss: 0.6278080344200134Training Epoch: 8 | iteration: 127/262 | Loss: 0.6379192471504211Training Epoch: 8 | iteration: 128/262 | Loss: 0.5670267343521118Training Epoch: 8 | iteration: 129/262 | Loss: 0.560509443283081Training Epoch: 8 | iteration: 130/262 | Loss: 0.6151920557022095Training Epoch: 8 | iteration: 131/262 | Loss: 0.5717490911483765Training Epoch: 8 | iteration: 132/262 | Loss: 0.5706485509872437Training Epoch: 8 | iteration: 133/262 | Loss: 0.6056032776832581Training Epoch: 8 | iteration: 134/262 | Loss: 0.6391942501068115Training Epoch: 8 | iteration: 135/262 | Loss: 0.5586626529693604Training Epoch: 8 | iteration: 136/262 | Loss: 0.5438770055770874Training Epoch: 8 | iteration: 137/262 | Loss: 0.6485229730606079Training Epoch: 8 | iteration: 138/262 | Loss: 0.5881549715995789Training Epoch: 8 | iteration: 139/262 | Loss: 0.6233394145965576Training Epoch: 8 | iteration: 140/262 | Loss: 0.6206589937210083Training Epoch: 8 | iteration: 141/262 | Loss: 0.600517988204956Training Epoch: 8 | iteration: 142/262 | Loss: 0.6532403230667114Training Epoch: 8 | iteration: 143/262 | Loss: 0.6007099151611328Training Epoch: 8 | iteration: 144/262 | Loss: 0.5756930112838745Training Epoch: 8 | iteration: 145/262 | Loss: 0.5686583518981934Training Epoch: 8 | iteration: 146/262 | Loss: 0.607818603515625Training Epoch: 8 | iteration: 147/262 | Loss: 0.561031699180603Training Epoch: 8 | iteration: 148/262 | Loss: 0.6420199871063232Training Epoch: 8 | iteration: 149/262 | Loss: 0.581003725528717Training Epoch: 8 | iteration: 150/262 | Loss: 0.5824020504951477Training Epoch: 8 | iteration: 151/262 | Loss: 0.6351397037506104Training Epoch: 8 | iteration: 152/262 | Loss: 0.5713708996772766Training Epoch: 8 | iteration: 153/262 | Loss: 0.5646682977676392Training Epoch: 8 | iteration: 154/262 | Loss: 0.5904383659362793Training Epoch: 8 | iteration: 155/262 | Loss: 0.5857809782028198Training Epoch: 8 | iteration: 156/262 | Loss: 0.5715752840042114Training Epoch: 8 | iteration: 157/262 | Loss: 0.5392701625823975Training Epoch: 8 | iteration: 158/262 | Loss: 0.5973102450370789Training Epoch: 8 | iteration: 159/262 | Loss: 0.6123744249343872Training Epoch: 8 | iteration: 160/262 | Loss: 0.5390231609344482Training Epoch: 8 | iteration: 161/262 | Loss: 0.5787440538406372Training Epoch: 8 | iteration: 162/262 | Loss: 0.5740066170692444Training Epoch: 8 | iteration: 163/262 | Loss: 0.5359227061271667Training Epoch: 8 | iteration: 164/262 | Loss: 0.6272457838058472Training Epoch: 8 | iteration: 165/262 | Loss: 0.5998457670211792Training Epoch: 8 | iteration: 166/262 | Loss: 0.5315185785293579Training Epoch: 8 | iteration: 167/262 | Loss: 0.692780613899231Training Epoch: 8 | iteration: 168/262 | Loss: 0.6148411631584167Training Epoch: 8 | iteration: 169/262 | Loss: 0.5880607962608337Training Epoch: 8 | iteration: 170/262 | Loss: 0.5197297930717468Training Epoch: 8 | iteration: 171/262 | Loss: 0.579892635345459Training Epoch: 8 | iteration: 172/262 | Loss: 0.6342465877532959Training Epoch: 8 | iteration: 173/262 | Loss: 0.5936497449874878Training Epoch: 8 | iteration: 174/262 | Loss: 0.5668067932128906Training Epoch: 8 | iteration: 175/262 | Loss: 0.5700534582138062Training Epoch: 8 | iteration: 176/262 | Loss: 0.6004821062088013Training Epoch: 8 | iteration: 177/262 | Loss: 0.584885835647583Training Epoch: 8 | iteration: 178/262 | Loss: 0.5248122215270996Training Epoch: 8 | iteration: 179/262 | Loss: 0.683582067489624Training Epoch: 8 | iteration: 180/262 | Loss: 0.6208062171936035Training Epoch: 8 | iteration: 181/262 | Loss: 0.6006013751029968Training Epoch: 8 | iteration: 182/262 | Loss: 0.519048273563385Training Epoch: 8 | iteration: 183/262 | Loss: 0.6376515030860901Training Epoch: 8 | iteration: 184/262 | Loss: 0.5858218669891357Training Epoch: 8 | iteration: 185/262 | Loss: 0.5932861566543579Training Epoch: 8 | iteration: 186/262 | Loss: 0.5299365520477295Training Epoch: 8 | iteration: 187/262 | Loss: 0.6289665699005127Training Epoch: 8 | iteration: 188/262 | Loss: 0.6128560900688171Training Epoch: 8 | iteration: 189/262 | Loss: 0.6232504844665527Training Epoch: 8 | iteration: 190/262 | Loss: 0.5893576741218567Training Epoch: 8 | iteration: 191/262 | Loss: 0.6189799308776855Training Epoch: 8 | iteration: 192/262 | Loss: 0.6089794635772705Training Epoch: 8 | iteration: 193/262 | Loss: 0.577719509601593Training Epoch: 8 | iteration: 194/262 | Loss: 0.5704799890518188Training Epoch: 8 | iteration: 195/262 | Loss: 0.5980960130691528Training Epoch: 8 | iteration: 196/262 | Loss: 0.5923347473144531Training Epoch: 8 | iteration: 197/262 | Loss: 0.6218001842498779Training Epoch: 8 | iteration: 198/262 | Loss: 0.6118766069412231Training Epoch: 8 | iteration: 199/262 | Loss: 0.5977246165275574Training Epoch: 8 | iteration: 200/262 | Loss: 0.5805439949035645Training Epoch: 8 | iteration: 201/262 | Loss: 0.6408207416534424Training Epoch: 8 | iteration: 202/262 | Loss: 0.581647515296936Training Epoch: 8 | iteration: 203/262 | Loss: 0.5681316256523132Training Epoch: 8 | iteration: 204/262 | Loss: 0.588515043258667Training Epoch: 8 | iteration: 205/262 | Loss: 0.633091926574707Training Epoch: 8 | iteration: 206/262 | Loss: 0.576467752456665Training Epoch: 8 | iteration: 207/262 | Loss: 0.5859107971191406Training Epoch: 8 | iteration: 208/262 | Loss: 0.605904221534729Training Epoch: 8 | iteration: 209/262 | Loss: 0.6221258640289307Training Epoch: 8 | iteration: 210/262 | Loss: 0.5980199575424194Training Epoch: 8 | iteration: 211/262 | Loss: 0.576675295829773Training Epoch: 8 | iteration: 212/262 | Loss: 0.6289538145065308Training Epoch: 8 | iteration: 213/262 | Loss: 0.6026606559753418Training Epoch: 8 | iteration: 214/262 | Loss: 0.5500760078430176Training Epoch: 8 | iteration: 215/262 | Loss: 0.5831083059310913Training Epoch: 8 | iteration: 216/262 | Loss: 0.6033713817596436Training Epoch: 8 | iteration: 217/262 | Loss: 0.6744020581245422Training Epoch: 8 | iteration: 218/262 | Loss: 0.6223968267440796Training Epoch: 8 | iteration: 219/262 | Loss: 0.59565669298172Training Epoch: 8 | iteration: 220/262 | Loss: 0.6822227239608765Training Epoch: 8 | iteration: 221/262 | Loss: 0.6074540019035339Training Epoch: 8 | iteration: 222/262 | Loss: 0.5617519617080688Training Epoch: 8 | iteration: 223/262 | Loss: 0.5900406837463379Training Epoch: 8 | iteration: 224/262 | Loss: 0.6294942498207092Training Epoch: 8 | iteration: 225/262 | Loss: 0.6106000542640686Training Epoch: 8 | iteration: 226/262 | Loss: 0.6081336736679077Training Epoch: 8 | iteration: 227/262 | Loss: 0.6390470266342163Training Epoch: 8 | iteration: 228/262 | Loss: 0.5681251883506775Training Epoch: 8 | iteration: 229/262 | Loss: 0.5915610790252686Training Epoch: 8 | iteration: 230/262 | Loss: 0.594086766242981Training Epoch: 8 | iteration: 231/262 | Loss: 0.568767786026001Training Epoch: 8 | iteration: 232/262 | Loss: 0.649233877658844Training Epoch: 8 | iteration: 233/262 | Loss: 0.6342772245407104Training Epoch: 8 | iteration: 234/262 | Loss: 0.5887763500213623Training Epoch: 8 | iteration: 235/262 | Loss: 0.6117082834243774Training Epoch: 8 | iteration: 236/262 | Loss: 0.5886386036872864Training Epoch: 8 | iteration: 237/262 | Loss: 0.5700626373291016Training Epoch: 8 | iteration: 238/262 | Loss: 0.5790029764175415Training Epoch: 8 | iteration: 239/262 | Loss: 0.5820823311805725Training Epoch: 8 | iteration: 240/262 | Loss: 0.5737528800964355Training Epoch: 8 | iteration: 241/262 | Loss: 0.6038528084754944Training Epoch: 8 | iteration: 242/262 | Loss: 0.5458851456642151Training Epoch: 8 | iteration: 243/262 | Loss: 0.644972562789917Training Epoch: 8 | iteration: 244/262 | Loss: 0.5787707567214966Training Epoch: 8 | iteration: 245/262 | Loss: 0.6096324324607849Training Epoch: 8 | iteration: 246/262 | Loss: 0.6213301420211792Training Epoch: 8 | iteration: 247/262 | Loss: 0.6850030422210693Training Epoch: 8 | iteration: 248/262 | Loss: 0.6098577976226807Training Epoch: 8 | iteration: 249/262 | Loss: 0.6144483089447021Training Epoch: 8 | iteration: 250/262 | Loss: 0.5470761060714722Training Epoch: 8 | iteration: 251/262 | Loss: 0.560983419418335Training Epoch: 8 | iteration: 252/262 | Loss: 0.5981232523918152Training Epoch: 8 | iteration: 253/262 | Loss: 0.6218075752258301Training Epoch: 8 | iteration: 254/262 | Loss: 0.6150372624397278Training Epoch: 8 | iteration: 255/262 | Loss: 0.6254957914352417Training Epoch: 8 | iteration: 256/262 | Loss: 0.6292994022369385Training Epoch: 8 | iteration: 257/262 | Loss: 0.6696390509605408Training Epoch: 8 | iteration: 258/262 | Loss: 0.6136334538459778Training Epoch: 8 | iteration: 259/262 | Loss: 0.5512305498123169Training Epoch: 8 | iteration: 260/262 | Loss: 0.5941270589828491Training Epoch: 8 | iteration: 261/262 | Loss: 0.6361072063446045Validating Epoch: 8 | iteration: 0/66 | Loss: 0.7060439586639404Validating Epoch: 8 | iteration: 1/66 | Loss: 0.6144185066223145Validating Epoch: 8 | iteration: 2/66 | Loss: 0.6094710230827332Validating Epoch: 8 | iteration: 3/66 | Loss: 0.7038172483444214Validating Epoch: 8 | iteration: 4/66 | Loss: 0.6261325478553772Validating Epoch: 8 | iteration: 5/66 | Loss: 0.6341648101806641Validating Epoch: 8 | iteration: 6/66 | Loss: 0.6843686699867249Validating Epoch: 8 | iteration: 7/66 | Loss: 0.6678975820541382Validating Epoch: 8 | iteration: 8/66 | Loss: 0.6602615118026733Validating Epoch: 8 | iteration: 9/66 | Loss: 0.7109059691429138Validating Epoch: 8 | iteration: 10/66 | Loss: 0.6655195355415344Validating Epoch: 8 | iteration: 11/66 | Loss: 0.6095348596572876Validating Epoch: 8 | iteration: 12/66 | Loss: 0.6634833812713623Validating Epoch: 8 | iteration: 13/66 | Loss: 0.6820688247680664Validating Epoch: 8 | iteration: 14/66 | Loss: 0.5447304248809814Validating Epoch: 8 | iteration: 15/66 | Loss: 0.6125224828720093Validating Epoch: 8 | iteration: 16/66 | Loss: 0.6649707555770874Validating Epoch: 8 | iteration: 17/66 | Loss: 0.5945677757263184Validating Epoch: 8 | iteration: 18/66 | Loss: 0.6662550568580627Validating Epoch: 8 | iteration: 19/66 | Loss: 0.6081740260124207Validating Epoch: 8 | iteration: 20/66 | Loss: 0.7753487825393677Validating Epoch: 8 | iteration: 21/66 | Loss: 0.6233661770820618Validating Epoch: 8 | iteration: 22/66 | Loss: 0.6557216644287109Validating Epoch: 8 | iteration: 23/66 | Loss: 0.5929326415061951Validating Epoch: 8 | iteration: 24/66 | Loss: 0.6017006635665894Validating Epoch: 8 | iteration: 25/66 | Loss: 0.5681278705596924Validating Epoch: 8 | iteration: 26/66 | Loss: 0.7450317144393921Validating Epoch: 8 | iteration: 27/66 | Loss: 0.6337944269180298Validating Epoch: 8 | iteration: 28/66 | Loss: 0.6534980535507202Validating Epoch: 8 | iteration: 29/66 | Loss: 0.6315270662307739Validating Epoch: 8 | iteration: 30/66 | Loss: 0.6224642992019653Validating Epoch: 8 | iteration: 31/66 | Loss: 0.6802655458450317Validating Epoch: 8 | iteration: 32/66 | Loss: 0.6170449256896973Validating Epoch: 8 | iteration: 33/66 | Loss: 0.653091311454773Validating Epoch: 8 | iteration: 34/66 | Loss: 0.5424115657806396Validating Epoch: 8 | iteration: 35/66 | Loss: 0.6853927373886108Validating Epoch: 8 | iteration: 36/66 | Loss: 0.6429994106292725Validating Epoch: 8 | iteration: 37/66 | Loss: 0.6118423938751221Validating Epoch: 8 | iteration: 38/66 | Loss: 0.5753241777420044Validating Epoch: 8 | iteration: 39/66 | Loss: 0.6063492298126221Validating Epoch: 8 | iteration: 40/66 | Loss: 0.6366667747497559Validating Epoch: 8 | iteration: 41/66 | Loss: 0.599022388458252Validating Epoch: 8 | iteration: 42/66 | Loss: 0.6994853615760803Validating Epoch: 8 | iteration: 43/66 | Loss: 0.5795167684555054Validating Epoch: 8 | iteration: 44/66 | Loss: 0.6102811098098755Validating Epoch: 8 | iteration: 45/66 | Loss: 0.6090071201324463Validating Epoch: 8 | iteration: 46/66 | Loss: 0.6063395738601685Validating Epoch: 8 | iteration: 47/66 | Loss: 0.5378612279891968Validating Epoch: 8 | iteration: 48/66 | Loss: 0.6679242253303528Validating Epoch: 8 | iteration: 49/66 | Loss: 0.6786816120147705Validating Epoch: 8 | iteration: 50/66 | Loss: 0.6695912480354309Validating Epoch: 8 | iteration: 51/66 | Loss: 0.5707102417945862Validating Epoch: 8 | iteration: 52/66 | Loss: 0.6867323517799377Validating Epoch: 8 | iteration: 53/66 | Loss: 0.689974308013916Validating Epoch: 8 | iteration: 54/66 | Loss: 0.6799799799919128Validating Epoch: 8 | iteration: 55/66 | Loss: 0.6534785628318787Validating Epoch: 8 | iteration: 56/66 | Loss: 0.6099735498428345Validating Epoch: 8 | iteration: 57/66 | Loss: 0.6628949046134949Validating Epoch: 8 | iteration: 58/66 | Loss: 0.7365611791610718Validating Epoch: 8 | iteration: 59/66 | Loss: 0.5893535614013672Validating Epoch: 8 | iteration: 60/66 | Loss: 0.5931443572044373Validating Epoch: 8 | iteration: 61/66 | Loss: 0.6534129977226257Validating Epoch: 8 | iteration: 62/66 | Loss: 0.7369189858436584Validating Epoch: 8 | iteration: 63/66 | Loss: 0.6192368268966675Validating Epoch: 8 | iteration: 64/66 | Loss: 0.5883097648620605Validating Epoch: 8 | iteration: 65/66 | Loss: 0.593862771987915Temperature:  0.5
Sampling: iteration: 0/51{'Validity': 0.984375, 'Novelty': 1.0, 'Uniqueness': 0.9930555555555556}
Training Epoch: 9 | iteration: 0/262 | Loss: 0.5756468772888184Training Epoch: 9 | iteration: 1/262 | Loss: 0.5645318031311035Training Epoch: 9 | iteration: 2/262 | Loss: 0.5705610513687134Training Epoch: 9 | iteration: 3/262 | Loss: 0.5702012777328491Training Epoch: 9 | iteration: 4/262 | Loss: 0.6202713251113892Training Epoch: 9 | iteration: 5/262 | Loss: 0.5754746198654175Training Epoch: 9 | iteration: 6/262 | Loss: 0.5512499213218689Training Epoch: 9 | iteration: 7/262 | Loss: 0.5538294315338135Training Epoch: 9 | iteration: 8/262 | Loss: 0.556925356388092Training Epoch: 9 | iteration: 9/262 | Loss: 0.6130338907241821Training Epoch: 9 | iteration: 10/262 | Loss: 0.5562776327133179Training Epoch: 9 | iteration: 11/262 | Loss: 0.5724136233329773Training Epoch: 9 | iteration: 12/262 | Loss: 0.6080954074859619Training Epoch: 9 | iteration: 13/262 | Loss: 0.561525821685791Training Epoch: 9 | iteration: 14/262 | Loss: 0.548365592956543Training Epoch: 9 | iteration: 15/262 | Loss: 0.5912851095199585Training Epoch: 9 | iteration: 16/262 | Loss: 0.5323842167854309Training Epoch: 9 | iteration: 17/262 | Loss: 0.6143949031829834Training Epoch: 9 | iteration: 18/262 | Loss: 0.5614854097366333Training Epoch: 9 | iteration: 19/262 | Loss: 0.5800502300262451Training Epoch: 9 | iteration: 20/262 | Loss: 0.5957947969436646Training Epoch: 9 | iteration: 21/262 | Loss: 0.5787628293037415Training Epoch: 9 | iteration: 22/262 | Loss: 0.628067135810852Training Epoch: 9 | iteration: 23/262 | Loss: 0.5881195068359375Training Epoch: 9 | iteration: 24/262 | Loss: 0.6382135152816772Training Epoch: 9 | iteration: 25/262 | Loss: 0.5817149877548218Training Epoch: 9 | iteration: 26/262 | Loss: 0.5773506164550781Training Epoch: 9 | iteration: 27/262 | Loss: 0.5804442167282104Training Epoch: 9 | iteration: 28/262 | Loss: 0.6165088415145874Training Epoch: 9 | iteration: 29/262 | Loss: 0.5827487707138062Training Epoch: 9 | iteration: 30/262 | Loss: 0.6002956032752991Training Epoch: 9 | iteration: 31/262 | Loss: 0.5690237283706665Training Epoch: 9 | iteration: 32/262 | Loss: 0.5704468488693237Training Epoch: 9 | iteration: 33/262 | Loss: 0.5590606331825256Training Epoch: 9 | iteration: 34/262 | Loss: 0.5629180669784546Training Epoch: 9 | iteration: 35/262 | Loss: 0.6012160778045654Training Epoch: 9 | iteration: 36/262 | Loss: 0.5890500545501709Training Epoch: 9 | iteration: 37/262 | Loss: 0.5815358757972717Training Epoch: 9 | iteration: 38/262 | Loss: 0.5177478790283203Training Epoch: 9 | iteration: 39/262 | Loss: 0.5300108194351196Training Epoch: 9 | iteration: 40/262 | Loss: 0.5502293109893799Training Epoch: 9 | iteration: 41/262 | Loss: 0.5452015995979309Training Epoch: 9 | iteration: 42/262 | Loss: 0.6110330820083618Training Epoch: 9 | iteration: 43/262 | Loss: 0.6060576438903809Training Epoch: 9 | iteration: 44/262 | Loss: 0.6241693496704102Training Epoch: 9 | iteration: 45/262 | Loss: 0.6061558723449707Training Epoch: 9 | iteration: 46/262 | Loss: 0.5705634951591492Training Epoch: 9 | iteration: 47/262 | Loss: 0.5322932600975037Training Epoch: 9 | iteration: 48/262 | Loss: 0.5424817800521851Training Epoch: 9 | iteration: 49/262 | Loss: 0.5513156652450562Training Epoch: 9 | iteration: 50/262 | Loss: 0.6424922347068787Training Epoch: 9 | iteration: 51/262 | Loss: 0.6153705716133118Training Epoch: 9 | iteration: 52/262 | Loss: 0.5524334907531738Training Epoch: 9 | iteration: 53/262 | Loss: 0.585620641708374Training Epoch: 9 | iteration: 54/262 | Loss: 0.6492842435836792Training Epoch: 9 | iteration: 55/262 | Loss: 0.5718103647232056Training Epoch: 9 | iteration: 56/262 | Loss: 0.6083717346191406Training Epoch: 9 | iteration: 57/262 | Loss: 0.579232931137085Training Epoch: 9 | iteration: 58/262 | Loss: 0.5809006690979004Training Epoch: 9 | iteration: 59/262 | Loss: 0.59358811378479Training Epoch: 9 | iteration: 60/262 | Loss: 0.5949664115905762Training Epoch: 9 | iteration: 61/262 | Loss: 0.5560302734375Training Epoch: 9 | iteration: 62/262 | Loss: 0.6329487562179565Training Epoch: 9 | iteration: 63/262 | Loss: 0.5263493657112122Training Epoch: 9 | iteration: 64/262 | Loss: 0.5653318166732788Training Epoch: 9 | iteration: 65/262 | Loss: 0.6416643857955933Training Epoch: 9 | iteration: 66/262 | Loss: 0.6137830018997192Training Epoch: 9 | iteration: 67/262 | Loss: 0.5552293062210083Training Epoch: 9 | iteration: 68/262 | Loss: 0.622085452079773Training Epoch: 9 | iteration: 69/262 | Loss: 0.5777599215507507Training Epoch: 9 | iteration: 70/262 | Loss: 0.5769466161727905Training Epoch: 9 | iteration: 71/262 | Loss: 0.6302058100700378Training Epoch: 9 | iteration: 72/262 | Loss: 0.56786048412323Training Epoch: 9 | iteration: 73/262 | Loss: 0.6071789860725403Training Epoch: 9 | iteration: 74/262 | Loss: 0.6069610118865967Training Epoch: 9 | iteration: 75/262 | Loss: 0.5157068967819214Training Epoch: 9 | iteration: 76/262 | Loss: 0.6292276382446289Training Epoch: 9 | iteration: 77/262 | Loss: 0.511465847492218Training Epoch: 9 | iteration: 78/262 | Loss: 0.551387369632721Training Epoch: 9 | iteration: 79/262 | Loss: 0.5785938501358032Training Epoch: 9 | iteration: 80/262 | Loss: 0.618187427520752Training Epoch: 9 | iteration: 81/262 | Loss: 0.55875563621521Training Epoch: 9 | iteration: 82/262 | Loss: 0.5641863942146301Training Epoch: 9 | iteration: 83/262 | Loss: 0.5960404872894287Training Epoch: 9 | iteration: 84/262 | Loss: 0.6316286325454712Training Epoch: 9 | iteration: 85/262 | Loss: 0.6530447006225586Training Epoch: 9 | iteration: 86/262 | Loss: 0.5390036106109619Training Epoch: 9 | iteration: 87/262 | Loss: 0.5045865774154663Training Epoch: 9 | iteration: 88/262 | Loss: 0.6208609938621521Training Epoch: 9 | iteration: 89/262 | Loss: 0.5135228633880615Training Epoch: 9 | iteration: 90/262 | Loss: 0.500057578086853Training Epoch: 9 | iteration: 91/262 | Loss: 0.5826280117034912Training Epoch: 9 | iteration: 92/262 | Loss: 0.5710073709487915Training Epoch: 9 | iteration: 93/262 | Loss: 0.5626360177993774Training Epoch: 9 | iteration: 94/262 | Loss: 0.6368216276168823Training Epoch: 9 | iteration: 95/262 | Loss: 0.5459396839141846Training Epoch: 9 | iteration: 96/262 | Loss: 0.5911737680435181Training Epoch: 9 | iteration: 97/262 | Loss: 0.5321112871170044Training Epoch: 9 | iteration: 98/262 | Loss: 0.5905948877334595Training Epoch: 9 | iteration: 99/262 | Loss: 0.5880169868469238Training Epoch: 9 | iteration: 100/262 | Loss: 0.561073899269104Training Epoch: 9 | iteration: 101/262 | Loss: 0.5446866750717163Training Epoch: 9 | iteration: 102/262 | Loss: 0.5827786922454834Training Epoch: 9 | iteration: 103/262 | Loss: 0.5807673931121826Training Epoch: 9 | iteration: 104/262 | Loss: 0.6010382175445557Training Epoch: 9 | iteration: 105/262 | Loss: 0.5813380479812622Training Epoch: 9 | iteration: 106/262 | Loss: 0.5423794984817505Training Epoch: 9 | iteration: 107/262 | Loss: 0.5850122570991516Training Epoch: 9 | iteration: 108/262 | Loss: 0.5677682161331177Training Epoch: 9 | iteration: 109/262 | Loss: 0.6477369070053101Training Epoch: 9 | iteration: 110/262 | Loss: 0.5796470642089844Training Epoch: 9 | iteration: 111/262 | Loss: 0.561691403388977Training Epoch: 9 | iteration: 112/262 | Loss: 0.5924742221832275Training Epoch: 9 | iteration: 113/262 | Loss: 0.568173348903656Training Epoch: 9 | iteration: 114/262 | Loss: 0.552712619304657Training Epoch: 9 | iteration: 115/262 | Loss: 0.5702432990074158Training Epoch: 9 | iteration: 116/262 | Loss: 0.5940611362457275Training Epoch: 9 | iteration: 117/262 | Loss: 0.5517544150352478Training Epoch: 9 | iteration: 118/262 | Loss: 0.6011072397232056Training Epoch: 9 | iteration: 119/262 | Loss: 0.5704213976860046Training Epoch: 9 | iteration: 120/262 | Loss: 0.486513614654541Training Epoch: 9 | iteration: 121/262 | Loss: 0.6168042421340942Training Epoch: 9 | iteration: 122/262 | Loss: 0.6985464096069336Training Epoch: 9 | iteration: 123/262 | Loss: 0.5080939531326294Training Epoch: 9 | iteration: 124/262 | Loss: 0.5690271258354187Training Epoch: 9 | iteration: 125/262 | Loss: 0.6164283752441406Training Epoch: 9 | iteration: 126/262 | Loss: 0.608038067817688Training Epoch: 9 | iteration: 127/262 | Loss: 0.5758184790611267Training Epoch: 9 | iteration: 128/262 | Loss: 0.5473368763923645Training Epoch: 9 | iteration: 129/262 | Loss: 0.6082153916358948Training Epoch: 9 | iteration: 130/262 | Loss: 0.5495973825454712Training Epoch: 9 | iteration: 131/262 | Loss: 0.5823464393615723Training Epoch: 9 | iteration: 132/262 | Loss: 0.6206573247909546Training Epoch: 9 | iteration: 133/262 | Loss: 0.6821507215499878Training Epoch: 9 | iteration: 134/262 | Loss: 0.5995898842811584Training Epoch: 9 | iteration: 135/262 | Loss: 0.6565195322036743Training Epoch: 9 | iteration: 136/262 | Loss: 0.5633241534233093Training Epoch: 9 | iteration: 137/262 | Loss: 0.648686945438385Training Epoch: 9 | iteration: 138/262 | Loss: 0.5465375185012817Training Epoch: 9 | iteration: 139/262 | Loss: 0.5358275175094604Training Epoch: 9 | iteration: 140/262 | Loss: 0.6251990795135498Training Epoch: 9 | iteration: 141/262 | Loss: 0.503149151802063Training Epoch: 9 | iteration: 142/262 | Loss: 0.5841901302337646Training Epoch: 9 | iteration: 143/262 | Loss: 0.6142550110816956Training Epoch: 9 | iteration: 144/262 | Loss: 0.5493388175964355Training Epoch: 9 | iteration: 145/262 | Loss: 0.605883002281189Training Epoch: 9 | iteration: 146/262 | Loss: 0.571650505065918Training Epoch: 9 | iteration: 147/262 | Loss: 0.6229192018508911Training Epoch: 9 | iteration: 148/262 | Loss: 0.6312832236289978Training Epoch: 9 | iteration: 149/262 | Loss: 0.5472586750984192Training Epoch: 9 | iteration: 150/262 | Loss: 0.6355425119400024Training Epoch: 9 | iteration: 151/262 | Loss: 0.5778356790542603Training Epoch: 9 | iteration: 152/262 | Loss: 0.5548316836357117Training Epoch: 9 | iteration: 153/262 | Loss: 0.552414059638977Training Epoch: 9 | iteration: 154/262 | Loss: 0.559898853302002Training Epoch: 9 | iteration: 155/262 | Loss: 0.5493013858795166Training Epoch: 9 | iteration: 156/262 | Loss: 0.6025863289833069Training Epoch: 9 | iteration: 157/262 | Loss: 0.5777246952056885Training Epoch: 9 | iteration: 158/262 | Loss: 0.6129136085510254Training Epoch: 9 | iteration: 159/262 | Loss: 0.5805476903915405Training Epoch: 9 | iteration: 160/262 | Loss: 0.649543285369873Training Epoch: 9 | iteration: 161/262 | Loss: 0.5400965213775635Training Epoch: 9 | iteration: 162/262 | Loss: 0.5857279300689697Training Epoch: 9 | iteration: 163/262 | Loss: 0.5398245453834534Training Epoch: 9 | iteration: 164/262 | Loss: 0.6221154928207397Training Epoch: 9 | iteration: 165/262 | Loss: 0.583154559135437Training Epoch: 9 | iteration: 166/262 | Loss: 0.628545880317688Training Epoch: 9 | iteration: 167/262 | Loss: 0.6265641450881958Training Epoch: 9 | iteration: 168/262 | Loss: 0.618847668170929Training Epoch: 9 | iteration: 169/262 | Loss: 0.5524275898933411Training Epoch: 9 | iteration: 170/262 | Loss: 0.5521102547645569Training Epoch: 9 | iteration: 171/262 | Loss: 0.5636904239654541Training Epoch: 9 | iteration: 172/262 | Loss: 0.5472337007522583Training Epoch: 9 | iteration: 173/262 | Loss: 0.6067463159561157Training Epoch: 9 | iteration: 174/262 | Loss: 0.5661137700080872Training Epoch: 9 | iteration: 175/262 | Loss: 0.5884469747543335Training Epoch: 9 | iteration: 176/262 | Loss: 0.586806058883667Training Epoch: 9 | iteration: 177/262 | Loss: 0.6079530715942383Training Epoch: 9 | iteration: 178/262 | Loss: 0.5164352655410767Training Epoch: 9 | iteration: 179/262 | Loss: 0.5492677092552185Training Epoch: 9 | iteration: 180/262 | Loss: 0.546550452709198Training Epoch: 9 | iteration: 181/262 | Loss: 0.5802663564682007Training Epoch: 9 | iteration: 182/262 | Loss: 0.6544644236564636Training Epoch: 9 | iteration: 183/262 | Loss: 0.5481302738189697Training Epoch: 9 | iteration: 184/262 | Loss: 0.5411982536315918Training Epoch: 9 | iteration: 185/262 | Loss: 0.6291114687919617Training Epoch: 9 | iteration: 186/262 | Loss: 0.649549126625061Training Epoch: 9 | iteration: 187/262 | Loss: 0.6393277645111084Training Epoch: 9 | iteration: 188/262 | Loss: 0.613375186920166Training Epoch: 9 | iteration: 189/262 | Loss: 0.6385091543197632Training Epoch: 9 | iteration: 190/262 | Loss: 0.5611400604248047Training Epoch: 9 | iteration: 191/262 | Loss: 0.5633571743965149Training Epoch: 9 | iteration: 192/262 | Loss: 0.5330523252487183Training Epoch: 9 | iteration: 193/262 | Loss: 0.6131168007850647Training Epoch: 9 | iteration: 194/262 | Loss: 0.6191846132278442Training Epoch: 9 | iteration: 195/262 | Loss: 0.5335775017738342Training Epoch: 9 | iteration: 196/262 | Loss: 0.5930100679397583Training Epoch: 9 | iteration: 197/262 | Loss: 0.6100481748580933Training Epoch: 9 | iteration: 198/262 | Loss: 0.5910484790802002Training Epoch: 9 | iteration: 199/262 | Loss: 0.6257655620574951Training Epoch: 9 | iteration: 200/262 | Loss: 0.5908075571060181Training Epoch: 9 | iteration: 201/262 | Loss: 0.5957549810409546Training Epoch: 9 | iteration: 202/262 | Loss: 0.5992604494094849Training Epoch: 9 | iteration: 203/262 | Loss: 0.581192672252655Training Epoch: 9 | iteration: 204/262 | Loss: 0.551490306854248Training Epoch: 9 | iteration: 205/262 | Loss: 0.5641428232192993Training Epoch: 9 | iteration: 206/262 | Loss: 0.5572225451469421Training Epoch: 9 | iteration: 207/262 | Loss: 0.5562102794647217Training Epoch: 9 | iteration: 208/262 | Loss: 0.5597503185272217Training Epoch: 9 | iteration: 209/262 | Loss: 0.5352967977523804Training Epoch: 9 | iteration: 210/262 | Loss: 0.5993087887763977Training Epoch: 9 | iteration: 211/262 | Loss: 0.5088193416595459Training Epoch: 9 | iteration: 212/262 | Loss: 0.5975781679153442Training Epoch: 9 | iteration: 213/262 | Loss: 0.6719779968261719Training Epoch: 9 | iteration: 214/262 | Loss: 0.6282635927200317Training Epoch: 9 | iteration: 215/262 | Loss: 0.5288788676261902Training Epoch: 9 | iteration: 216/262 | Loss: 0.6293835639953613Training Epoch: 9 | iteration: 217/262 | Loss: 0.6520172953605652Training Epoch: 9 | iteration: 218/262 | Loss: 0.5465683937072754Training Epoch: 9 | iteration: 219/262 | Loss: 0.6015828251838684Training Epoch: 9 | iteration: 220/262 | Loss: 0.5894255042076111Training Epoch: 9 | iteration: 221/262 | Loss: 0.5328198671340942Training Epoch: 9 | iteration: 222/262 | Loss: 0.6700555682182312Training Epoch: 9 | iteration: 223/262 | Loss: 0.5620306134223938Training Epoch: 9 | iteration: 224/262 | Loss: 0.6029602885246277Training Epoch: 9 | iteration: 225/262 | Loss: 0.5515138506889343Training Epoch: 9 | iteration: 226/262 | Loss: 0.5367125272750854Training Epoch: 9 | iteration: 227/262 | Loss: 0.5930798053741455Training Epoch: 9 | iteration: 228/262 | Loss: 0.595630407333374Training Epoch: 9 | iteration: 229/262 | Loss: 0.518811821937561Training Epoch: 9 | iteration: 230/262 | Loss: 0.5411597490310669Training Epoch: 9 | iteration: 231/262 | Loss: 0.5769107937812805Training Epoch: 9 | iteration: 232/262 | Loss: 0.5939719676971436Training Epoch: 9 | iteration: 233/262 | Loss: 0.6679487228393555Training Epoch: 9 | iteration: 234/262 | Loss: 0.6452324390411377Training Epoch: 9 | iteration: 235/262 | Loss: 0.5892977714538574Training Epoch: 9 | iteration: 236/262 | Loss: 0.5318572521209717Training Epoch: 9 | iteration: 237/262 | Loss: 0.6452069878578186Training Epoch: 9 | iteration: 238/262 | Loss: 0.6078590750694275Training Epoch: 9 | iteration: 239/262 | Loss: 0.6545072793960571Training Epoch: 9 | iteration: 240/262 | Loss: 0.6028026938438416Training Epoch: 9 | iteration: 241/262 | Loss: 0.6739869117736816Training Epoch: 9 | iteration: 242/262 | Loss: 0.5847684144973755Training Epoch: 9 | iteration: 243/262 | Loss: 0.5324424505233765Training Epoch: 9 | iteration: 244/262 | Loss: 0.6268604397773743Training Epoch: 9 | iteration: 245/262 | Loss: 0.5943350195884705Training Epoch: 9 | iteration: 246/262 | Loss: 0.5655950307846069Training Epoch: 9 | iteration: 247/262 | Loss: 0.5819487571716309Training Epoch: 9 | iteration: 248/262 | Loss: 0.5669143199920654Training Epoch: 9 | iteration: 249/262 | Loss: 0.5575904846191406Training Epoch: 9 | iteration: 250/262 | Loss: 0.6108587980270386Training Epoch: 9 | iteration: 251/262 | Loss: 0.5631753206253052Training Epoch: 9 | iteration: 252/262 | Loss: 0.5622313618659973Training Epoch: 9 | iteration: 253/262 | Loss: 0.6123685240745544Training Epoch: 9 | iteration: 254/262 | Loss: 0.6310193538665771Training Epoch: 9 | iteration: 255/262 | Loss: 0.5780605673789978Training Epoch: 9 | iteration: 256/262 | Loss: 0.5960296392440796Training Epoch: 9 | iteration: 257/262 | Loss: 0.5782128572463989Training Epoch: 9 | iteration: 258/262 | Loss: 0.6096817255020142Training Epoch: 9 | iteration: 259/262 | Loss: 0.6060876846313477Training Epoch: 9 | iteration: 260/262 | Loss: 0.5287024974822998Training Epoch: 9 | iteration: 261/262 | Loss: 0.5193321108818054Validating Epoch: 9 | iteration: 0/66 | Loss: 0.6692759990692139Validating Epoch: 9 | iteration: 1/66 | Loss: 0.6134939193725586Validating Epoch: 9 | iteration: 2/66 | Loss: 0.6506206393241882Validating Epoch: 9 | iteration: 3/66 | Loss: 0.6557965874671936Validating Epoch: 9 | iteration: 4/66 | Loss: 0.6253832578659058Validating Epoch: 9 | iteration: 5/66 | Loss: 0.6511860489845276Validating Epoch: 9 | iteration: 6/66 | Loss: 0.5932329297065735Validating Epoch: 9 | iteration: 7/66 | Loss: 0.6983617544174194Validating Epoch: 9 | iteration: 8/66 | Loss: 0.6594086289405823Validating Epoch: 9 | iteration: 9/66 | Loss: 0.6222529411315918Validating Epoch: 9 | iteration: 10/66 | Loss: 0.701413631439209Validating Epoch: 9 | iteration: 11/66 | Loss: 0.6116691827774048Validating Epoch: 9 | iteration: 12/66 | Loss: 0.6788545250892639Validating Epoch: 9 | iteration: 13/66 | Loss: 0.6550105214118958Validating Epoch: 9 | iteration: 14/66 | Loss: 0.586915135383606Validating Epoch: 9 | iteration: 15/66 | Loss: 0.6721181869506836Validating Epoch: 9 | iteration: 16/66 | Loss: 0.5845417380332947Validating Epoch: 9 | iteration: 17/66 | Loss: 0.6658201217651367Validating Epoch: 9 | iteration: 18/66 | Loss: 0.6423302888870239Validating Epoch: 9 | iteration: 19/66 | Loss: 0.5906860828399658Validating Epoch: 9 | iteration: 20/66 | Loss: 0.6005144119262695Validating Epoch: 9 | iteration: 21/66 | Loss: 0.6495795845985413Validating Epoch: 9 | iteration: 22/66 | Loss: 0.6617888808250427Validating Epoch: 9 | iteration: 23/66 | Loss: 0.5971461534500122Validating Epoch: 9 | iteration: 24/66 | Loss: 0.6050900220870972Validating Epoch: 9 | iteration: 25/66 | Loss: 0.6308441162109375Validating Epoch: 9 | iteration: 26/66 | Loss: 0.5673125982284546Validating Epoch: 9 | iteration: 27/66 | Loss: 0.5785360336303711Validating Epoch: 9 | iteration: 28/66 | Loss: 0.6874935626983643Validating Epoch: 9 | iteration: 29/66 | Loss: 0.6550524234771729Validating Epoch: 9 | iteration: 30/66 | Loss: 0.6314955949783325Validating Epoch: 9 | iteration: 31/66 | Loss: 0.6267821192741394Validating Epoch: 9 | iteration: 32/66 | Loss: 0.6291890144348145Validating Epoch: 9 | iteration: 33/66 | Loss: 0.644870400428772Validating Epoch: 9 | iteration: 34/66 | Loss: 0.7289276123046875Validating Epoch: 9 | iteration: 35/66 | Loss: 0.6143254041671753Validating Epoch: 9 | iteration: 36/66 | Loss: 0.5929535031318665Validating Epoch: 9 | iteration: 37/66 | Loss: 0.698417067527771Validating Epoch: 9 | iteration: 38/66 | Loss: 0.6864079236984253Validating Epoch: 9 | iteration: 39/66 | Loss: 0.6478073596954346Validating Epoch: 9 | iteration: 40/66 | Loss: 0.6931109428405762Validating Epoch: 9 | iteration: 41/66 | Loss: 0.6661167144775391Validating Epoch: 9 | iteration: 42/66 | Loss: 0.6295965909957886Validating Epoch: 9 | iteration: 43/66 | Loss: 0.6981300115585327Validating Epoch: 9 | iteration: 44/66 | Loss: 0.6184669733047485Validating Epoch: 9 | iteration: 45/66 | Loss: 0.6732720732688904Validating Epoch: 9 | iteration: 46/66 | Loss: 0.6725980043411255Validating Epoch: 9 | iteration: 47/66 | Loss: 0.7124481201171875Validating Epoch: 9 | iteration: 48/66 | Loss: 0.6776465177536011Validating Epoch: 9 | iteration: 49/66 | Loss: 0.6317653656005859Validating Epoch: 9 | iteration: 50/66 | Loss: 0.6334693431854248Validating Epoch: 9 | iteration: 51/66 | Loss: 0.6433306336402893Validating Epoch: 9 | iteration: 52/66 | Loss: 0.5708483457565308Validating Epoch: 9 | iteration: 53/66 | Loss: 0.6460691690444946Validating Epoch: 9 | iteration: 54/66 | Loss: 0.7110941410064697Validating Epoch: 9 | iteration: 55/66 | Loss: 0.6739298105239868Validating Epoch: 9 | iteration: 56/66 | Loss: 0.653340220451355Validating Epoch: 9 | iteration: 57/66 | Loss: 0.5946177244186401Validating Epoch: 9 | iteration: 58/66 | Loss: 0.6018295288085938Validating Epoch: 9 | iteration: 59/66 | Loss: 0.6553628444671631Validating Epoch: 9 | iteration: 60/66 | Loss: 0.5877219438552856Validating Epoch: 9 | iteration: 61/66 | Loss: 0.6003377437591553Validating Epoch: 9 | iteration: 62/66 | Loss: 0.6005696058273315Validating Epoch: 9 | iteration: 63/66 | Loss: 0.5823789238929749Validating Epoch: 9 | iteration: 64/66 | Loss: 0.6238648295402527Validating Epoch: 9 | iteration: 65/66 | Loss: 0.6715777516365051No of GPUs available 4

==================================================
Generating molecules with target properties...
==================================================

Generating for -10_70...
Generating for -9_70...
Generating for -8_70...
Generating for -7_70...
Generating for -6_70...

==================================================
Generated molecules saved to: ../checkpoints/DPO_Finetuning_BETA_0.11_epochs_10_LCK_DOCKSTRING_FAST_ACTUAL_affinity_tpsas/generated_molecules.pkl
To analyze and plot results, run:
python analyze_generated_molecules.py --checkpoint_dir ../checkpoints/DPO_Finetuning_BETA_0.11_epochs_10_LCK_DOCKSTRING_FAST_ACTUAL_affinity_tpsas --properties affinity tpsas
==================================================

No of GPUs available 4
Temperature:  0.5
Sampling: iteration: 0/51{'Validity': 0.9873046875, 'Novelty': 1.0, 'Uniqueness': 0.9950544015825915}
dict_keys(['smiles_rep', 'properties', 'smiles'])
validity rate 0.9873046875
[1;34mwandb[0m: 
[1;34mwandb[0m:  View run [33mDPO_Finetuning_BETA_0.11_epochs_10_LCK_DOCKSTRING_FAST_ACTUAL_affinity_tpsas[0m at: [34mhttps://wandb.ai/bhuvan-kapur1-iiith/molgpt2.0%20FINAL/runs/nzt563fg[0m
[1;34mwandb[0m: Find logs at: [1;35mwandb/run-20260117_185948-nzt563fg/logs[0m
